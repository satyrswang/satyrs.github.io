<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Apriori_fptree</title>
    <url>/2021/03/11/Apriori-fptree/</url>
    <content><![CDATA[<h2 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h2><ul>
<li>input<br>{ID1:[item1,item2…],ID2:[item3,item2…],…}</li>
</ul>
<span id="more"></span>
<ul>
<li><p>output<br>item1-&gt;item2,item3…</p>
</li>
<li><p>math<br>频率及条件概率的简单应用<br>支持度: 频数/样本总量<br>置信度: 条件概率<br>如果子集不为频繁集则超集一定不为；反之如果超集是频繁集则子集均为频繁集。</p>
</li>
<li><p>parameters<br><code>minSupport</code><br><code>minConf</code></p>
</li>
<li><p>过程<br>1-4完成了频繁集的搜寻，5及之后产生规则。</p>
</li>
</ul>
<p>1、<code>list</code>存放每个用户的  <code>itemset</code>，生成[[],[]]存放所有item并排序，<code>map(frozenset, C1)</code><br>2、第一轮遍历，生成一个<code>list</code>存放所有支持度大于<code>minSupport</code>，大于则<code>insert(0,key)</code>。遍历的函数<code>scanD</code>所生成<code>list</code>的元素是<code>frozenset</code>。如果<code>can.issubset(tid)</code>且<code>！ssCnt.has_key(can)</code>则加入到<code>supportData</code>。<br>3、list存放所有频繁物品集，<code>supportData</code>存放所有计算过的物品集的支持度，以<code>frozenset</code>作为<code>key</code>。两者更新的过程是在<code>apriori</code>中循环调用<code>aprioriGen</code>和<code>scanD </code>，再更新。直到<code>scanD</code>得到的频繁物品集为空。<br>4、<code>aprioriGen</code>的过程，两层循环，遍历上一个(即物品集中个数为目前需要产生的个数-1)频繁物品集，<code>L1 = list(Lk[i])[:k-2];L2 = list(Lk[j])[:k-2]</code>取前k-2项比较，若相同则<code>retList.append(Lk[i] | Lk[j])</code>将物品集union。是一个merging过程。</p>
<p>5、如果频繁集<code>frozenset</code>中item多于2，先<code>rulesFromConseq</code>再<code>calcConf</code>。<code>rulesFromConseq</code>是个递归过程，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rulesFromConseq(freqSet, H, supportData, brl, minConf&#x3D;0.7):</span><br><span class="line">    m &#x3D; len(H[0])</span><br><span class="line">    if (len(freqSet) &gt; (m + 1)): #停止条件</span><br><span class="line">        Hmp1 &#x3D; aprioriGen(H, m+1)</span><br><span class="line">        Hmp1 &#x3D; calcConf(freqSet, Hmp1, supportData, brl, minConf)</span><br><span class="line">        if (len(Hmp1) &gt; 1):   #至少需要两个元素才能够merge</span><br><span class="line">            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)</span><br></pre></td></tr></table></figure>
<p>这里一个问题是，如果<code>len(Hmp1) =1</code>，但是<code>len(freqSet) &gt; (len(Hmp1[0]) + 1)</code>仍然成立，因此修改为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rulesFromConseq(freqSet, H, supportData, brl, minConf&#x3D;0.7):</span><br><span class="line">    m &#x3D; len(H[0])</span><br><span class="line">    if (len(freqSet) &gt; (m + 1)): #停止条件</span><br><span class="line">        if(len(H) &gt; 1):</span><br><span class="line">            Hmp1 &#x3D; aprioriGen(H, m+1)</span><br><span class="line">        Hmp1 &#x3D; calcConf(freqSet, Hmp1, supportData, brl, minConf)</span><br><span class="line">        rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)</span><br></pre></td></tr></table></figure>

<br>


<h2 id="fptree"><a href="#fptree" class="headerlink" title="fptree"></a>fptree</h2><ul>
<li><p>概念<br>闭项：超集的支持度为s，子集的支持度都不超过s。<br>条件模式基： 即一个item追溯到root的所有路径<br>条件fp树 ： 即根据条件模式基创建的tree<br><code>myCondTree, myHead = createTree(condPattBases, minSup)</code></p>
</li>
<li><p>准备<br><code>headertable</code> : <code>dict</code>，<code>value</code>为<code>[count,treeNode]</code><br><code>treeNode</code> : <code>class</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def __init__(self, nameValue, numOccur, parentNode):</span><br><span class="line">     self.name &#x3D; nameValue</span><br><span class="line">     self.count &#x3D; numOccur</span><br><span class="line">     self.nodeLink &#x3D; None</span><br><span class="line">     self.parent &#x3D; parentNode      </span><br><span class="line">     self.children &#x3D; &#123;&#125; </span><br></pre></td></tr></table></figure>
<p>主要方法：<code>createTree</code> <code>updateTree</code>  <code>mineTree</code><br>辅助方法：<code>updateHeader</code> <code>findPrefixPath</code> <code>ascendTree</code></p>
</li>
<li><p>过程</p>
</li>
</ul>
<ol>
<li><code>createTree</code>首先第一次遍历计算<code>count</code>，<code>headerTable[item] = headerTable.get(item, 0) + dataSet[trans]</code>将<code>count</code>简单赋值为1。注意到这里的dataSet类型。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def createInitSet(dataSet):</span><br><span class="line">    retDict &#x3D; &#123;&#125;</span><br><span class="line">    for trans in dataSet:</span><br><span class="line">        retDict[frozenset(trans)] &#x3D; 1</span><br><span class="line">    return retDict</span><br></pre></td></tr></table></figure>
获得<code>freqItemSet = set(headerTable.keys())</code>后第二次遍历。先对item排序，<code>orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)] </code>，再<code>updateTree(orderedItems, retTree, headerTable, count)</code>。</li>
<li><code>updateTree</code>，这里就是普通的对树的操作，也是递归。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if len(items) &gt; 1:#call updateTree() with remaining ordered items</span><br><span class="line">        updateTree(items[1::], inTree.children[items[0]], headerTable, count)</span><br></pre></td></tr></table></figure>
唯一注意之处，需要<code>updateHeader</code>。</li>
<li><code>mineTree</code>和<code>Apriori</code>中的<code>rulesFromConseq </code>一样较复杂，包含递归。先生成条件模式基，再生成conditional-fptree，再递归。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if myHead !&#x3D; None:                  </span><br><span class="line">   mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)</span><br></pre></td></tr></table></figure>
循环递归mineTree的过程即生成规则过程，直到频繁2项集都得到。</li>
</ol>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF后续</title>
    <url>/2021/04/14/CRF%E5%90%8E%E7%BB%AD/</url>
    <content><![CDATA[<br>

<p>介绍下BiLSTM+CRF吧</p>
<br>

<span id="more"></span>



<h4 id="Q1、为啥搞一个LSTM-CRF的hybrid-model"><a href="#Q1、为啥搞一个LSTM-CRF的hybrid-model" class="headerlink" title="Q1、为啥搞一个LSTM+CRF的hybrid model?"></a>Q1、为啥搞一个LSTM+CRF的hybrid model?</h4><p>用LSTM，整体的预测accuracy是不错indeed, 但是会出现上述的错误：在B之后再来一个B。这个错误在CRF中是不存在的，因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。</p>
<p>那就把CRF接到LSTM上面，把LSTM在time<em>step上把每一个hidden</em>state的tensor输入给CRF，让LSTM负责在CRF的特征限定下，依照新的loss function，学习出一套新的非线性变换空间。</p>
<h4 id="Q2、CRF-源码"><a href="#Q2、CRF-源码" class="headerlink" title="Q2、CRF++源码"></a>Q2、CRF++源码</h4><ul>
<li>输入</li>
</ul>
<p>He reckons the current account deficit will narrow to only #1.8 billion in September .”代表一个训练句子xx，而CRF++要求将这样的句子拆成 <strong>每一个词一行并且是固定列数的数据</strong>，其中列除了原始输入，还可以包含一些其他信息，比如第二列包含了POS信息，最后一列是Label信息。而不同的训练序列与序列之间的相隔，就靠一个空白行来区分。</p>
<ul>
<li>特征模版</li>
</ul>
<p>1、“%x[row, column]” 代表获得<strong>当前指向位置向上或向下偏移|row|行</strong>，并指向第column列的值。</p>
<p>2、CRF++中主要有两种特征模版，<strong>Unigram和Bigram 模版</strong>，注意Unigram和Bigram是<strong>相对于输出序列</strong>而言，而不是相对于输入序列。对于”U01:%x[0,1]”这样一个模版，上面例子的输入数据会产生如下的特征函数：</p>
<p>3、</p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF</title>
    <url>/2021/03/09/CRF/</url>
    <content><![CDATA[<br>

<p>本文主要介绍 线性链的CRF。应用于标注问题。<br>介绍：概率图、MEMM问题、势函数、CRF的定义和表示方法，以及基本问题。</p>
<span id="more"></span>

<br>



<h3 id="00概率无向图模型"><a href="#00概率无向图模型" class="headerlink" title="00概率无向图模型"></a>00概率无向图模型</h3><p>1、无向图表示的随机变量之间：成对马尔科夫性、局部马尔科夫性、全局马尔科夫性。</p>
<p>2、概率无向图模型定义：<br>联合概率满足上面的三个马尔科夫性，则称此联合概率分布为概率无向图模型或者马尔科夫随机出。</p>
<p>3、团与最大团</p>
<p>4、因子分解<br>联合分布表示为 最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解。</p>
<p>这个函数，就是 势函数。要求是正的。一般为指数函数。</p>
<p>5、条件随机场</p>
<p>给定随机变量x的条件下，随机变量y的马尔科夫随机场。</p>
<p>学习时，学习条件概率模型。预测时利用条件概率，求得最大输出序列y。</p>
<p>定义：</p>
<p><img src="/2021/03/09/CRF/crf.png" alt="crf"></p>
<!-- <img src=crf.png width=80% height=300> -->

<p><img src="/2021/03/09/CRF/lccrf.png" alt="lccrf"></p>
<!-- <img src=lccrf.png width=80% height=300> -->

<br>

<br>



<h3 id="01引入"><a href="#01引入" class="headerlink" title="01引入"></a>01引入</h3><p>MEMM有的缺陷是标签偏向。</p>
<p>标签偏向问题的原因是 – MEMM在每一步都用softmax进行了归一化</p>
<p>CRF通过全局归一化解决了标签偏向问题。</p>
<p>这样讲是比较费解的。论文里的例子容易理解。两个词，rib rob。</p>
<p>概率图如下(来自论文)</p>
<p><img src="/2021/03/09/CRF/bias.png" alt="img"></p>
<p>这里隐状态1到2只有一条路，4到5也只有一条路，所以整个模型没有考虑观测值i 和 o不同，所以导致词性标注过程中，不考虑观测变量。这个就不符合逻辑了。</p>
<p>因此局部的归一化，容易受隐变量影响，而不去考虑观测变量。这个带来的问题是：</p>
<p>如果training数据，3个都是rib，一个是rob。那么在预测过程中，求max P(y1y2y3|rob)时，viterbi算法走出来的，将会是0-&gt;1-&gt;2-&gt;3。</p>
<p>即，<strong>如果上一个状态是低熵的，那么很容易忽略观测变量，而在预测时，受样本影响较大。</strong></p>
<p>即，无论观测值，当State1比State2转移少，State1会一直转移到State1，即使全局看转移到State2概率更高。即有更少转移的状态、拥有的转移概率普遍偏高，概率最大路径更容易出现转移少的状态。</p>
<h3 id="02CRF解决label-bias"><a href="#02CRF解决label-bias" class="headerlink" title="02CRF解决label bias"></a>02CRF解决label bias</h3><p>一，crf将输入序列和输出标注映射为一个d维实数向量，而MEMM的特征函数拥有的信息只是输入序列和当前状态以及上一个状态，也就是说CRF的特征函数掌握信息量更多，从而表达能力更强。</p>
<p>第二个的改进是它不再每一次状态转移进行归一化，而是在全局进行归一化，这样完美解决Label Bias问题。即CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。</p>
<p>有得必有失，注意到模型的分母需要罗列所有的状态序列，对于序列长度为n的输入序列，状态序列的个数为S^n ，对于这种指数增长问题，在实际应用中一般都是intractable的，只能付诸于近似求解，比如我们之前提过的Variational Bayes或者Gibbs Sampling等等。</p>
<p>不过有一种特殊结构的CRF – linear chain crf，精确快速求解的方式是存在的。</p>
<p>(注：马尔科夫随机场就是概率无向图，而crf是马尔科夫随机场特殊一种，而线性链是crf的特殊一种。隐状态是随机场，而观测变量是条件。条件随机场还有其他多种形式。)</p>
<br>

<br>



<h3 id="03最大图、势函数"><a href="#03最大图、势函数" class="headerlink" title="03最大图、势函数"></a>03最大图、势函数</h3><h4 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h4><p>例子更清楚：<br><img src="/2021/03/09/CRF/%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E4%BE%8B%E5%AD%901.png" alt="矩阵形式例子1"><br><img src="/2021/03/09/CRF/%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E4%BE%8B%E5%AD%902.png" alt="矩阵形式例子2"></p>
<h4 id="linear-crf-是如何定义特征函数的？如何将原来intractable的问题变为tractable？"><a href="#linear-crf-是如何定义特征函数的？如何将原来intractable的问题变为tractable？" class="headerlink" title="linear crf 是如何定义特征函数的？如何将原来intractable的问题变为tractable？"></a>linear crf 是如何定义特征函数的？如何将原来intractable的问题变为tractable？</h4><p>linear让最大团的个数为序列长度-1，并且，最大团内部的特征函数为 t1 个状态特征函数 和 t2个转移特征函数 之和的形式。</p>
<h4 id="那，无向图的条件概率怎么表达呢？"><a href="#那，无向图的条件概率怎么表达呢？" class="headerlink" title="那，无向图的条件概率怎么表达呢？"></a>那，无向图的条件概率怎么表达呢？</h4><p>从MEMM可见，本质上有两点，一是x到y的函数，即自定义的特征函数，二是loss。和MEMM不同的是，CRF是无向图。无向图的条件概率怎么表达呢？</p>
<p>来自google图片</p>
<p><img src="/2021/03/09/CRF/crftu.png" alt="img"></p>
<p>这里就引入了最大团概念。由上文介绍。得到</p>
<p><img src="/2021/03/09/CRF/crfp.jpg" alt="img"></p>
<p>这个公式，本质是求 观测变量下隐状态序列的概率。c为最大团个数，k为自定义的k个特征函数。</p>
<p>对于CRF，可以为他定义两款特征函数：转移特征&amp;状态特征。 即将建模总公式展开，见下面👇的参数化表达。</p>
<p>省得写了，下面大量贴图。</p>
<br>

<br>





<h3 id="04三种表达形式"><a href="#04三种表达形式" class="headerlink" title="04三种表达形式"></a>04三种表达形式</h3><p>1、参数化形式</p>
<p>公式：<br><img src="/2021/03/09/CRF/%E5%8F%82%E6%95%B0%E5%8C%96%E5%85%AC%E5%BC%8F.png" alt="参数化公式"></p>
<p>解释如下：<br><img src="/2021/03/09/CRF/%E5%8F%82%E6%95%B0%E5%8C%96%E5%85%AC%E5%BC%8F%E8%A7%A3%E9%87%8A2_1.png" alt="参数化公式解释2_1"></p>
<p><img src="/2021/03/09/CRF/%E5%8F%82%E6%95%B0%E5%8C%96%E5%85%AC%E5%BC%8F%E8%A7%A3%E9%87%8A2_2.png" alt="参数化公式解释2_2"></p>
<p>即：<br><img src="/2021/03/09/CRF/%E8%A7%92%E5%BA%A6.png" alt="角度"></p>
<p>把两种特征合在一起：</p>
<p><img src="/2021/03/09/CRF/hebing1.jpg" alt="1"></p>
<p>合并的公式中，我们并没有显示将边和节点区分开来，而只是写出了边的特征函数，因为从某种程度上边包含的信息已经涵盖了节点所拥有的信息，将两者统一起来可以有利于我们数学公式表达的方便性，另一方面，将边和节点进行单独讨论，从理论上可能有一点冗余，但是从实际效果中，节点信息可以充当一种backoff，起到一定的平滑效果(Smoothing)。</p>
<p>特征函数部分和MEMM一样记做score：</p>
<p><img src="/2021/03/09/CRF/hebingscore.jpg" alt="2"></p>
<p>可见：</p>
<p>我们为 token(隐状态) 打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值。对数线性的思路。</p>
<p>这个的推导在视频里去看。<a href="https://www.bilibili.com/video/BV19t411R7QU?p=4">https://www.bilibili.com/video/BV19t411R7QU?p=4</a></p>
<br>

<p>2、参数化简化的形式推导<br>略</p>
<br>

<p>3、矩阵形式</p>
<p>矩阵形式推导：<br><img src="/2021/03/09/CRF/%E7%9F%A9%E9%98%B5%E6%8E%A8%E5%AF%BC.png" alt="矩阵推导"></p>
<!-- <img src=矩阵推导.png width=70% height=500> -->

<br>
<br>


<h3 id="05三个问题"><a href="#05三个问题" class="headerlink" title="05三个问题"></a>05三个问题</h3><h4 id="模型基本问题"><a href="#模型基本问题" class="headerlink" title="模型基本问题"></a>模型基本问题</h4><p>模型问题包含 learning 和 inference。<br>learning即 parameter estimation。求 θ。<br>inference：<br>(以下 y 为隐变量)<br>1 marginal prob ： 当建模对象为joint distribution，求p(y_t|x)即求边缘概率问题。<br>2 conditional prob ： 求 p(x|y)  –此为生成模型才有的问题，在crf中不考虑<br>3 MAP inference ： decoding ， 求  y_pred = argmax p(y|x)          其中y_pred = y_1y_2y_3 …</p>
<p><img src="/2021/03/09/CRF/wenti.jpg" alt="问题定义"></p>
<br>

<h4 id="CRF-learning"><a href="#CRF-learning" class="headerlink" title="CRF learning"></a>CRF learning</h4><p>1、问题定义<br><img src="/2021/03/09/CRF/learning%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89.png" alt="learning问题定义"></p>
<p>CRF也是极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。能用在log-linear models上的求参方法都可以用过来。</p>
<p>2、todo – 数学推导</p>
<br>




<h4 id="CRF-marginal-prob"><a href="#CRF-marginal-prob" class="headerlink" title="CRF marginal prob"></a>CRF marginal prob</h4><p>1、即求 P(yt = i | x)<br>给定x条件下的 隐变量yt 的边缘概率。<br>即 词性标注中，给定一句话，判断出 y1(第一个词)是动词(i)的概率是多少。</p>
<p>2、硬算，复杂度高。指数级计算不可行。<br>简化：</p>
<p>数学转化。<br>用到了变量消除法， sum+product，又叫信念传播，belief propagate。<br>HMM的前向 后向传播也就是belief propagate。</p>
<p><img src="/2021/03/09/CRF/sumproduct.png" alt="sumproduct"><br><img src="/2021/03/09/CRF/sumproduct2.png" alt="sumproduct2"></p>
<h4 id><a href="#" class="headerlink" title></a><br></h4><h4 id="CRF-decoding"><a href="#CRF-decoding" class="headerlink" title="CRF decoding"></a>CRF decoding</h4><p>求 y的序列，使得 argmax p(y|x)<br>– 类似于 HMM 问题。vertbi，既然是dp，核心要梳理清楚dp的转移函数。</p>
<p>我们就定义i处的局部状态为 f(I) ,表示在位置i处的隐状态的各种取值可能为 I，然后递推位置i+1处的隐状态，写出来DP转移公式。</p>
<br>

<p>reference</p>
<p><img src="https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico" alt="img"><a href="https://arxiv.org/abs/1011.4088">https://arxiv.org/abs/1011.4088</a> </p>
<p><a href="http://www.ai.mit.edu/courses/6.891-nlp/ASSIGNMENT1/t11.1.pdf">http://www.ai.mit.edu/courses/6.891-nlp/ASSIGNMENT1/t11.1.pdf</a></p>
<p><a href="https://www.cnblogs.com/en-heng/p/6201893.html">https://www.cnblogs.com/en-heng/p/6201893.html</a></p>
<p><a href="https://blog.csdn.net/aws3217150/article/details/68935789">https://blog.csdn.net/aws3217150/article/details/68935789</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF前续知识</title>
    <url>/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<br>

<p>从HMM MEMM介绍概率图、生成和判别模型</p>
<br>

<span id="more"></span>

<h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>我理解，机器学习，本质是一种反向求解过程。数据的具象世界里找到抽象的规律。比由规律推演出具象要复杂。</p>
<p>并且反向求解还有各种现实的局限，数据上、算力上、可行性上、性能约束上。就会有各种trick。就像工程中缓存击穿穿透问题，通过小trick来避免。而其根基还是在于统计。</p>
<p>由机器学习到深度学习，根基和理论框架都没有变。</p>
<p>CRF即是一种对序列数据的建模。放松了HMM的1-gram依赖假设和观测独立假设。转移和发射矩阵也进一步抽象为函数。因此就比HMM更实际些。也由于放松， 需要各种理论(和SVM一样涉及到了许多定理，更多是概率统计里分布相关的定理)支持，就比HMM 更加复杂。</p>
<p>很容易从概率图，看到机器学习为何向深度转化。本质都是对现实世界里的学习任务进行建模。</p>
<h2 id="00-生成和判别模型"><a href="#00-生成和判别模型" class="headerlink" title="00 生成和判别模型"></a>00 生成和判别模型</h2><p>概率图为什么那么难。核心在 是否理解 生成和判别模型的区别。</p>
<p>神经网络模型、SVM、perceptron、LR、DT……</p>
<p>NB、LDA ……</p>
<p>核心区别在于： 对 联合概率  还是对 条件概率 建模。</p>
<p><strong>角度一</strong>用inference过程举例：假设我知道P(X) P(X,Y) ，那我就根据条件概率公式预测出来y_pred 了。</p>
<p>这个是 生成模型。</p>
<p>而判别 ： 假设我知道P(Y|X) ，那我直接代入X就可得到y_pred</p>
<p><strong>角度二</strong>用learning过程举例：生成我要对 P(X,Y) 建模，判别我要对P(Y|X) 建模。</p>
<p>其实是很简单的。但是我们 对于不同模型去整体把握的时候 ，忽略了top down思维方式。bottom up的确是不容易想清楚。</p>
<p>进一步： 你能从这里思考出，生成和判别的各自局限吗？</p>
<p>判别边界和(结果的)生成过程、先验假设、数据量和性能</p>
<h2 id="01-概率图"><a href="#01-概率图" class="headerlink" title="01 概率图"></a>01 概率图</h2><p>序列数据 – 标注 分类 走势分析 –&gt;其中概率图解决标注和分类</p>
<p>概率图的话，需要去学习基本的有向图表示含义，图画出来(如下)则能判断，给出隐状态c，a\b之间是否独立。</p>
<p>来自知乎某文：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/tu.png" alt="img"></p>
<p>focus在HMM MEMM CRF –&gt;如何解决标注问题</p>
<h2 id="02-马尔科夫随机场定义-概率无向图定义"><a href="#02-马尔科夫随机场定义-概率无向图定义" class="headerlink" title="02 马尔科夫随机场定义(概率无向图定义)"></a>02 马尔科夫随机场定义(概率无向图定义)</h2><p>马尔科夫假设 – 马尔科夫模型的前提与局限</p>
<p>马尔科夫性质 – a. 成对，b. 局部，c. 全局。</p>
<p>这个性质主要有助于无向图里势函数的推导。不在乎数学，可忽略。势函数见下04。</p>
<h2 id="03-有向图和无向图"><a href="#03-有向图和无向图" class="headerlink" title="03 有向图和无向图"></a>03 有向图和无向图</h2><p>图模型 – 有向(贝叶斯网络)和无向(马尔科夫网络)  </p>
<p>HMM、 Karman filter是有向，boltzman 、CRF是无向</p>
<p>核心区别：在于建模过程如何求 node 代表的token ，该随机变量的联合概率</p>
<h2 id="04-无向图的联合概率-–-势函数"><a href="#04-无向图的联合概率-–-势函数" class="headerlink" title="04 无向图的联合概率 – 势函数"></a>04 无向图的联合概率 – 势函数</h2><p>势函数哪里来的or为啥这个样子？ – 或者说为啥无向图的联合概率是这个样子的因子分解？</p>
<p>： 由 Hammersley-Clifford定理 保证。</p>
<h2 id="05-HMM"><a href="#05-HMM" class="headerlink" title="05 HMM"></a>05 HMM</h2><h3 id="HMM之前的NB"><a href="#HMM之前的NB" class="headerlink" title="HMM之前的NB"></a>HMM之前的NB</h3><p>来自google图片：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/nb.png" alt="img"></p>
<h3 id="HMM五要素"><a href="#HMM五要素" class="headerlink" title="HMM五要素"></a>HMM五要素</h3><p>第一个隐状态的概率分布、转移矩阵、发射矩阵、状态集、观测集</p>
<p>来自google图片：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/hmm.png" alt="img"></p>
<p>进一步： 你能从这里思考出，HMM是生成还是判别？</p>
<p>这也是HMM痛点。假设不合实际、生成自身局限、没有更多特征信息。</p>
<h3 id="HMM问题"><a href="#HMM问题" class="headerlink" title="HMM问题"></a>HMM问题</h3><p>1、学习过程 ，即计算出五要素(建模过程)</p>
<p>监督学习：</p>
<p>MLE – 本质就是频率来估计概率</p>
<p>非监督学习：</p>
<p> Baum-Welch– 本质即EM</p>
<p>2、概率计算过程 ，即建好的模型下(五要素已知下)，当前观测序列的联合概率</p>
<p>直接计算 – 本质就是个暴力穷举</p>
<p>前向算法、后向算法  – 本质和viterbi一样也是DP优化</p>
<p>3、预测过程 ，即预测出当前时刻，哪个隐状态概率最大</p>
<p>近似算法 – 复杂度高且是局部最优：即使当转移概率为0时，仍可能出现在预测出来的状态序列里</p>
<p>viterbi算法 – 本质就是个动态规划</p>
<p>4、filter问题</p>
<p>略</p>
<p>5、smoothing问题</p>
<p>略</p>
<p>这些算法并不需要太复杂的证明，都比较直观。这也是HMM好理解的原因。</p>
<p>感兴趣来bili  一键三连。</p>
<h2 id="06-MEMM最大熵马尔科夫模型"><a href="#06-MEMM最大熵马尔科夫模型" class="headerlink" title="06 MEMM最大熵马尔科夫模型"></a>06 MEMM最大熵马尔科夫模型</h2><p>整个模型的框架和HMM一样，不同之处在于：</p>
<p>1、观测独立的假设被打破</p>
<p>2、局部采用LR(最大熵模型)</p>
<p>3、因为2，建模过程就通过梯度下降</p>
<h3 id="MEMM的概率图"><a href="#MEMM的概率图" class="headerlink" title="MEMM的概率图"></a>MEMM的概率图</h3><p>来自google图片：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memm.png" alt="img"></p>
<p>从图中可见：</p>
<p>1、根据01概率图的基础知识，MEMM中，当y_t已知，这种是个v字形的有向图，则x_t和y_t-1 、x_t-1 是相关的–即路径是连通的。因此打破了HMM的观测独立假设。</p>
<p>2、这里是判别模型，因此对P(Y|X,θ)建模</p>
<p>3、“x_t-&gt;y_t” 每个这个箭头的过程，都是一个逻辑回归LR。因此，每一个这样的局部都需要进行一次softmax，而softmax中e的指数是：w*f( h, y_t) ，也叫做质量分数(mass score)(这个mass来自pmf的m)，其中h是 (y_t-1, x_1:t)</p>
<p>4、概率计算：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memmp.jpg" alt="img"></p>
<p>这里，就是softmax得到的隐状态y的概率，加上log就是最大似然</p>
<p>其中，t为第几个隐状态，i为隐状态值，o为观测值，a为不同特征函数个数–自定义，Z为softmax的归一化。</p>
<p>特征函数是自己定义的，可求导即可。(本质即为，从某个词到某个词性的函数)。特征函数权重是可训练参数。</p>
<p>相当于dfm的子网、graphsage或者GCN的聚合子网。</p>
<p>又找到了一个图，这个更清楚：👈🏻左边为HMM 右边👉🏻为MEMM</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memm3.png" alt="img"></p>
<h3 id="MEMM问题"><a href="#MEMM问题" class="headerlink" title="MEMM问题"></a>MEMM问题</h3><p>和HMM一样，涉及到建模、预测。</p>
<p>区别的地方是：</p>
<p>建模：MEMM的loss是-log极大似然，然后梯度下降去求。</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memmloss.png" alt="img"></p>
<p>这里，对n个样本，每个样本长度为mi，的softmax结果求log，然后加了正则。</p>
<p>最大熵推导出softmax。整个loss的本质框架是MLE。</p>
<h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2><p>补充：</p>
<p>来自google图片：</p>
<p>NB在sequence建模下拓展到了HMM；LR在sequence建模下拓展到了CRF。</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/more.png" alt="img"></p>
<p>reference：</p>
<p><a href="http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf">http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf</a></p>
<p><a href="http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf">http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf</a></p>
<p><a href="https://blog.csdn.net/taoqick/article/details/102672110">https://blog.csdn.net/taoqick/article/details/102672110</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/37163081">https://zhuanlan.zhihu.com/p/37163081</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/33397147">https://zhuanlan.zhihu.com/p/33397147</a></p>
<p><a href="https://www.quora.com/What-is-the-best-resource-to-understand-Conditional-Random-Fields">https://www.quora.com/What-is-the-best-resource-to-understand-Conditional-Random-Fields</a></p>
<p>谷歌图片</p>
<p><a href="https://zhuanlan.zhihu.com/p/113187662">https://zhuanlan.zhihu.com/p/113187662</a></p>
<p><a href="https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://en.wikipedia.org/&amp;httpsredir=1&amp;article=1162&amp;context=cis_papers">https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://en.wikipedia.org/&amp;httpsredir=1&amp;article=1162&amp;context=cis_papers</a></p>
<p><a href="https://www.bilibili.com/video/BV19t411R7QU?p=3">https://www.bilibili.com/video/BV19t411R7QU?p=3</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>Decorator</title>
    <url>/2021/02/04/Decorator/</url>
    <content><![CDATA[<h2 id="functional-programming-concepts"><a href="#functional-programming-concepts" class="headerlink" title="functional programming concepts"></a><strong>functional programming concepts</strong></h2><p>分两类：</p>
<ul>
<li>Function decorators</li>
<li>Class decorators<span id="more"></span></li>
</ul>
<blockquote>
<p>A decorator in Python is any callable Python object that is used to <strong>modify a function or a class</strong>. A reference to a function “func” or a class “C” is passed to a decorator and the decorator returns a modified function or class. The modified functions or classes usually contain calls to the original function “func” or class “C”. </p>
</blockquote>
<h2 id="show-me-the-code"><a href="#show-me-the-code" class="headerlink" title="show me the code"></a><strong>show me the code</strong></h2><ul>
<li><p><em>参数检查</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def argument_test_natural_number(f):</span><br><span class="line">    def helper(x):</span><br><span class="line">        if type(x) &#x3D;&#x3D; int and x &gt; 0:</span><br><span class="line">            return f(x)</span><br><span class="line">        else:</span><br><span class="line">            raise Exception(&quot;Argument is not an integer&quot;)</span><br><span class="line">    return helper</span><br><span class="line">    </span><br><span class="line">@argument_test_natural_number</span><br><span class="line">def factorial(n):</span><br><span class="line">    if n &#x3D;&#x3D; 1:</span><br><span class="line">        return 1</span><br><span class="line">    else:</span><br><span class="line">        return n * factorial(n-1)</span><br><span class="line"></span><br><span class="line">for i in range(1,10):</span><br><span class="line">	print(i, factorial(i))</span><br><span class="line"></span><br><span class="line">print(factorial(-1))</span><br></pre></td></tr></table></figure></li>
<li><p><em>统计次数</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def call_counter(func):</span><br><span class="line">    def helper(*args, **kwargs):</span><br><span class="line">        helper.calls +&#x3D; 1</span><br><span class="line">        return func(*args, **kwargs)</span><br><span class="line">    helper.calls &#x3D; 0</span><br><span class="line"></span><br><span class="line">    return helper</span><br><span class="line"></span><br><span class="line">@call_counter</span><br><span class="line">def succ(x):</span><br><span class="line">    return x + 1</span><br><span class="line"></span><br><span class="line">@call_counter</span><br><span class="line">def mul1(x, y&#x3D;1):</span><br><span class="line">    return x*y + 1</span><br><span class="line"></span><br><span class="line">print(succ.calls)</span><br><span class="line">for i in range(10):</span><br><span class="line">    succ(i)</span><br><span class="line">mul1(3, 4)</span><br><span class="line">mul1(4)</span><br><span class="line">mul1(y&#x3D;3, x&#x3D;2)</span><br><span class="line">    </span><br><span class="line">print(succ.calls)</span><br><span class="line">print(mul1.calls)</span><br></pre></td></tr></table></figure></li>
<li><p><em>含参数的decorator</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def greeting(expr):</span><br><span class="line">    def greeting_decorator(func):</span><br><span class="line">        def function_wrapper(x):</span><br><span class="line">            print(expr + &quot;, &quot; + func.__name__ + &quot; returns:&quot;)</span><br><span class="line">            func(x)</span><br><span class="line">        return function_wrapper</span><br><span class="line">    return greeting_decorator</span><br><span class="line"></span><br><span class="line">@greeting(&quot;-wyq-&quot;)</span><br><span class="line">def foo(x):</span><br><span class="line">    print(42)</span><br><span class="line"></span><br><span class="line">foo(&quot;Hi&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p><em>using import 要注意变量的域</em></p>
</li>
</ul>
<p>greeting_decorator.py  没用functools的版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def greeting(func):</span><br><span class="line">    def function_wrapper(x):</span><br><span class="line">        &quot;&quot;&quot; function_wrapper of greeting &quot;&quot;&quot;</span><br><span class="line">        print(&quot;Hi, &quot; + func.__name__ + &quot; returns:&quot;)</span><br><span class="line">        return func(x)</span><br><span class="line">    function_wrapper.__name__ &#x3D; func.__name__</span><br><span class="line">    function_wrapper.__doc__ &#x3D; func.__doc__</span><br><span class="line">    function_wrapper.__module__ &#x3D; func.__module__</span><br><span class="line">    return function_wrapper</span><br></pre></td></tr></table></figure>
<p>anotherfile.py:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from greeting_decorator import greeting</span><br><span class="line"></span><br><span class="line">@greeting</span><br><span class="line">def f(x):</span><br><span class="line">    &quot;&quot;&quot; just some silly function &quot;&quot;&quot;</span><br><span class="line">    return x + 4</span><br><span class="line"></span><br><span class="line">f(10)</span><br><span class="line">print(&quot;function name: &quot; + f.__name__)</span><br><span class="line">print(&quot;docstring: &quot; + f.__doc__)</span><br><span class="line">print(&quot;module name: &quot; + f.__module__)</span><br></pre></td></tr></table></figure>
<p>greeting_decorator.py</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from functools import wraps</span><br><span class="line"></span><br><span class="line">def greeting(func):</span><br><span class="line">    @wraps(func)</span><br><span class="line">    def function_wrapper(x):</span><br><span class="line">        &quot;&quot;&quot; function_wrapper of greeting &quot;&quot;&quot;</span><br><span class="line">        print(&quot;Hi, &quot; + func.__name__ + &quot; returns:&quot;)</span><br><span class="line">        return func(x)</span><br><span class="line">    return function_wrapper</span><br></pre></td></tr></table></figure>
<p>否则将返回</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function name: function_wrapper</span><br><span class="line">docstring:  function_wrapper of greeting </span><br><span class="line">module name: greeting_decorator</span><br></pre></td></tr></table></figure>
<ul>
<li><em><code>__call__</code></em><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Fibonacci:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.cache &#x3D; &#123;&#125;</span><br><span class="line">    def __call__(self, n):</span><br><span class="line">        if n not in self.cache:</span><br><span class="line">            if n &#x3D;&#x3D; 0:</span><br><span class="line">                self.cache[0] &#x3D; 0</span><br><span class="line">            elif n &#x3D;&#x3D; 1:</span><br><span class="line">                self.cache[1] &#x3D; 1</span><br><span class="line">            else:</span><br><span class="line">                self.cache[n] &#x3D; self.__call__(n-1) + self.__call__(n-2)</span><br><span class="line">        return self.cache[n]</span><br><span class="line"></span><br><span class="line">fib &#x3D; Fibonacci()</span><br><span class="line"></span><br><span class="line">for i in range(15):</span><br><span class="line">    print(fib(i), end&#x3D;&quot;, &quot;)</span><br></pre></td></tr></table></figure></li>
<li><em>using class</em><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class decorator2:</span><br><span class="line">    </span><br><span class="line">    def __init__(self, f):</span><br><span class="line">        self.f &#x3D; f</span><br><span class="line">        </span><br><span class="line">    def __call__(self):</span><br><span class="line">        print(&quot;Decorating&quot;, self.f.__name__)</span><br><span class="line">        self.f()</span><br><span class="line"></span><br><span class="line">@decorator2</span><br><span class="line">def foo():</span><br><span class="line">    print(&quot;inside foo()&quot;)</span><br><span class="line"></span><br><span class="line">foo()</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepFm</title>
    <url>/2021/03/08/DeepFm/</url>
    <content><![CDATA[<br>

<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在推荐领域较流行的深度模型方案：</p>
<p>实战：阿里MLR、阿里DIN、阿里ESSM、京东强化学习推荐、facebook个性化推荐dlrm、Deep Neural Networks for YouTube Recommendations、华为DeepFM、google2016 wide&amp;deep learning、googleDeep&amp;Cross Network等。</p>
<span id="more"></span>
<p>DeepFm模型及进化：XDeepFM(deep进化)、AFM(加入attention)、FFM(field-aware)、PNN、FNN、NFM等。</p>
<p>其中，DeepFM在论文中通过大量实验证明，DeepFM的AUC和Logloss都优于目前的最好效果。效率上，DeepFM和目前最优的效果的深度模型相当。在Benchmark数据集和商业数据集上，DeepFM效果超过目前所有模型。</p>
<h3 id="Q1：FM解决什么问题？"><a href="#Q1：FM解决什么问题？" class="headerlink" title="Q1：FM解决什么问题？"></a>Q1：FM解决什么问题？</h3><p>1、普通的线性模型，我们都是将各个特征独立考虑的，并没有考虑到特征与特征之间的相互关系。<br>为了表述特征间的相关性，我们采用多项式模型。</p>
<p>在多项式模型中，特征xi与xj的组合用xixj表示。为了简单起见，我们讨论二阶多项式模型。<br>2、与线性模型相比，FM的模型就多了后面特征组合的部分。<br><img src="/2021/03/08/DeepFm/%E4%BA%8C%E9%A1%B9%E5%BC%8F.png" alt="二项式"></p>
<h3 id="Q2：FM参数求解？"><a href="#Q2：FM参数求解？" class="headerlink" title="Q2：FM参数求解？"></a>Q2：FM参数求解？</h3><p>1、公式中，组合部分的特征相关参数共有n(n−1)/2个。在数据很稀疏的情况下xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。</p>
<p>为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解。</p>
<p><img src="/2021/03/08/DeepFm/%E5%BC%95%E5%85%A5V.png" alt="引入V"></p>
<p>2、如何求解V？</p>
<p>推导公式网上到处都有。</p>
<p>3、得到公式推导结果后，对w求导，梯度下降进行训练。</p>
<h4 id="Q3：FFM？"><a href="#Q3：FFM？" class="headerlink" title="Q3：FFM？"></a>Q3：FFM？</h4><p>公式：</p>
<p><img src="/2021/03/08/DeepFm/ffm_gs.png" alt="ffm_gs"></p>
<p>举例：</p>
<p><img src="/2021/03/08/DeepFm/ffm.png" alt="ffm"></p>
<h4 id="Q4：why-DeepFm？"><a href="#Q4：why-DeepFm？" class="headerlink" title="Q4：why DeepFm？"></a>Q4：why DeepFm？</h4><p>1、因子分解机(Factorization Machines, FM)通过对于每一维特征的隐变量内积来提取特征组合。最终的结果也非常好。<br>但是，虽然理论上来讲FM可以对高阶特征组合进行建模，但实际上因为计算复杂度的原因一般都只用到了二阶特征组合。<br>那么对于高阶的特征组合来说，通过多层的神经网络即DNN去解决。</p>
<p>2、One-hot类型的特征输入到DNN中，会导致网络参数太多。<br>如何解决这个问题呢，类似于FFM中的思想，将特征分为不同的field：让Dense Vector进行组合，来表示高阶特征。</p>
<p><img src="/2021/03/08/DeepFm/field.png" alt="field"></p>
<p>3、但是低阶和高阶特征组合隐含地体现在隐藏层中，如果我们希望把低阶特征组合单独建模，然后融合高阶特征组合。<br>=&gt;就得到了DeepFm。</p>
<h4 id="Q5：what-DeepFm？"><a href="#Q5：what-DeepFm？" class="headerlink" title="Q5：what DeepFm？"></a>Q5：what DeepFm？</h4><p>1、有两种融合方式，分别为串行和并行的结构。<br>这里介绍并行结构。</p>
<p><img src="/2021/03/08/DeepFm/structure.png" alt="structure"><br><img src="/2021/03/08/DeepFm/deepfm.png" alt="deepfm"></p>
<p>2、emb部分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#embeddings</span></span><br><span class="line"><span class="comment">#weights[&#x27;feature_embeddings&#x27;] 存放的每一个值其实就是FM中的vik</span></span><br><span class="line">weights[<span class="string">&#x27;feature_embeddings&#x27;</span>] = tf.Variable(</span><br><span class="line">    tf.random_normal([self.feature_size,self.embedding_size],<span class="number">0.0</span>,<span class="number">0.01</span>),</span><br><span class="line">    name=<span class="string">&#x27;feature_embeddings&#x27;</span>)</span><br><span class="line"></span><br><span class="line">weights[<span class="string">&#x27;feature_bias&#x27;</span>] = tf.Variable(tf.random_normal([self.feature_size,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>),name=<span class="string">&#x27;feature_bias&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">self.embeddings = tf.nn.embedding_lookup(self.weights[<span class="string">&#x27;feature_embeddings&#x27;</span>],self.feat_index) <span class="comment"># N * F * K</span></span><br><span class="line">feat_value = tf.reshape(self.feat_value,shape=[-<span class="number">1</span>,self.field_size,<span class="number">1</span>])</span><br><span class="line">self.embeddings = tf.multiply(self.embeddings,feat_value)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果是离散值，则embedding lookup之后，每个维度✖️1，连续值则✖️连续值。<br>✖️后的结果就是公式里的 Vi,f · Xi</p>
<p>3、dnn部分<br>为了更好的发挥DNN模型学习high-order特征的能力，文中设计了一套子网络结构，将原始的稀疏表示特征映射为稠密的特征向量。<br>子网络设计时的两个要点：</p>
<p>不同field特征长度不同，但是子网络输出的向量需具有相同维度；<br>利用FM模型的隐特征向量V作为网络权重初始化来获得子网络输出向量。文中将FM的预训练V向量作为网络权重初始化替换为直接将FM和DNN进行整体联合训练，从而实现了一个端到端的模型。 （即lookup）</p>
<p>4、fm</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># second order term</span></span><br><span class="line"><span class="comment"># sum-square-part</span></span><br><span class="line">self.summed_features_emb = tf.reduce_sum(self.embeddings,<span class="number">1</span>) <span class="comment"># None * k</span></span><br><span class="line">self.summed_features_emb_square = tf.square(self.summed_features_emb) <span class="comment"># None * K</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># squre-sum-part</span></span><br><span class="line">self.squared_features_emb = tf.square(self.embeddings)</span><br><span class="line">self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, <span class="number">1</span>)  <span class="comment"># None * K</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#second order</span></span><br><span class="line">self.y_second_order = <span class="number">0.5</span> * tf.subtract(self.summed_features_emb_square,self.squared_sum_features_emb)</span><br><span class="line">self.y_second_order = tf.nn.dropout(self.y_second_order,self.dropout_keep_fm[<span class="number">1</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="Q6：扩展，自定义算子？"><a href="#Q6：扩展，自定义算子？" class="headerlink" title="Q6：扩展，自定义算子？"></a>Q6：扩展，自定义算子？</h4><p>比如我们输入是每个item的idor一些离散特征时候。需要对离散特征的各个值–&gt;index构建一个table。然后每次输入转换成对应的index，再根据index去tf.nn.embedding_lookup。离散特征对应的weight dict大小，需要给个预估值，大一些，囊括各种离散、连续的取值总数。</p>
<p>这里就可以对tensorflow里的hashtable算子进行扩展。构建一个table，动态增长index，来了一个新的取值，就对应index++。</p>
<p>实现：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">Status <span class="title">Find</span><span class="params">(OpKernelContext* ctx, <span class="keyword">const</span> Tensor&amp; key, Tensor* value,</span></span></span><br><span class="line"><span class="function"><span class="params">              <span class="keyword">const</span> Tensor&amp; default_value)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Grab the input tensor</span></span><br><span class="line">	<span class="keyword">const</span> V default_val = default_value.flat&lt;V&gt;()(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">auto</span> A = key.flat&lt;K&gt;();</span><br><span class="line">    <span class="comment">// Create an output tensor</span></span><br><span class="line">	<span class="keyword">auto</span> output_flat = value-&gt;flat&lt;V&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set all but the first element of the output tensor to 0.</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> N = A.size();</span><br><span class="line">    <span class="keyword">int</span> pos = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">    K find_key;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function">tf_shared_lock <span class="title">sl</span><span class="params">(mu_)</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (; pos &lt; N; pos++) &#123;</span><br><span class="line">            find_key = SubtleMustCopyIfIntegral(A(pos));</span><br><span class="line">            <span class="keyword">auto</span> it = table_.find(find_key);</span><br><span class="line">            <span class="keyword">if</span>(it == table_.end()) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                output_flat(pos) = it-&gt;second;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(pos &lt; N) &#123;</span><br><span class="line">        <span class="keyword">int64_t</span> hsize;</span><br><span class="line">        <span class="function">mutex_lock <span class="title">l</span><span class="params">(mu_)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(; pos &lt; N; pos++) &#123;</span><br><span class="line">            find_key = SubtleMustCopyIfIntegral(A(pos));</span><br><span class="line">            <span class="keyword">auto</span> it = table_.find(find_key);</span><br><span class="line">            <span class="keyword">if</span>(it == table_.end()) &#123;</span><br><span class="line">               hsize = table_.size();</span><br><span class="line">               <span class="keyword">if</span>(hsize &gt;= max_size_) &#123;</span><br><span class="line">                <span class="keyword">return</span> errors::ResourceExhausted(<span class="string">&quot;max size limit&quot;</span>);</span><br><span class="line">               &#125;</span><br><span class="line">               table_.insert(&#123;find_key, hsize&#125;);</span><br><span class="line">               output_flat(pos) = hsize;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                output_flat(pos) = it-&gt;second;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Status::OK();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> dynamic_unique_table <span class="keyword">import</span> DynamicUniqueTable</span><br><span class="line"></span><br><span class="line">table = DynamicUniqueTable(key_dtype=tf.string,</span><br><span class="line">                     value_dtype=tf.int64,</span><br><span class="line">                     default_value=-<span class="number">1</span>,</span><br><span class="line">                     max_ids=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(table.lookup(<span class="string">&quot;asdfetryuijkn&quot;</span>)))</span><br><span class="line">    print(sess.run(table.lookup(<span class="string">&quot;hahahahah&quot;</span>)))</span><br><span class="line">    print(sess.run(table.export()))</span><br><span class="line">    tf.train.Saver().save(sess, <span class="string">&#x27;index_test_model2/&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p>相关命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">本地：</span><br><span class="line">python pai&#x2F;main.py --train_tables&#x3D;.&#x2F;dub_train.csv --test_tables&#x3D;.&#x2F;dub_test.csv --data_type&#x3D;file --discrete_size&#x3D;15 --continue_size&#x3D;14 --epoch&#x3D;30 --buckets&#x3D;.&#x2F; --save_predict&#x3D;True</span><br><span class="line">pai:</span><br><span class="line">组件无法支持多张表输入， 直接在odps的sql页面执行：</span><br><span class="line"></span><br><span class="line">PAI-name tensorflow1120_ext</span><br><span class="line"></span><br><span class="line">-project algo_public</span><br><span class="line">-Dscript&#x3D;&#39;oss:&#x2F;&#x2F;datadrive.oss-cn-shanghai-internal.aliyuncs.com&#x2F;tmp&#x2F;tl_deepfm.tar.gz&#39;</span><br><span class="line">-DentryFile&#x3D;&#39;pai&#x2F;main.py&#39;</span><br><span class="line">-DgpuRequired&#x3D;100</span><br><span class="line">-Dtables&#x3D;&#39;odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_fit_normal,odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_eval_normal&#39;</span><br><span class="line">-Dbuckets&#x3D;&#39;oss:&#x2F;&#x2F;datadrive.oss-cn-shanghai-internal.aliyuncs.com&#x2F;tmp&#x2F;&#39;</span><br><span class="line">-Darn&#x3D;&#39;acs:ram::1872928906167841:role&#x2F;aliyunodpspaidefaultrole&#39;</span><br><span class="line">-DossHost&#x3D;&quot;oss-cn-shanghai-internal.aliyuncs.com&quot;</span><br><span class="line">-DuserDefinedParameters&#x3D;&quot;--train_tables&#x3D;odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_fit_normal--test_tables&#x3D;odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_eval_normal--discrete_size&#x3D;20 --continue_size&#x3D;49 --data_type&#x3D;odps --input_need_hash&#x3D;True --use_input_bn&#x3D;False --epoch&#x3D;100&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>




]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2021/05/21/LPR/</url>
    <content><![CDATA[<p>货币、利率、风险收益</p>
<h3 id="LPR"><a href="#LPR" class="headerlink" title="LPR"></a>LPR</h3><p>目的，替换贷款利率在贷款定价中发挥基准作用。</p>
<p>1】利率市场化改革</p>
<p>​    开始LPR的13年，也取消了贷款利率的下限。</p>
<p>​    没有起到作用：</p>
<p>​        1、政策属性强，盯住的是贷款基准利率而不是货币市场利率 – 盯住MLF 投放规模大、期限合适、使用频率高、央行选定的中期政策利率</p>
<p>​        2、中小行参与度低 ，报价和lpr的使用方面都只有少量银行参与 – 加入银行</p>
<p>​        3、只有1年期限的报价，对其他期限的利率无法提供参照 – 增加5y期</p>
<p>​    </p>
<p>意义-新增贷款盯住LPR，LPR盯住MLF。MLF由央行市场化招标形成，兼具灵活(相对于贷款基准利率)和稳定性(相对于货币市场利率)。</p>
<p>提高了LPR的市场化程度，淡化了贷款基准利率影响，促进解决货币市场化利率和贷款基准利率并存的 利率双轨制问题。推动贷款利率市场化改革。</p>
<p>2】提高利率传导效率</p>
<p>18年通过MLF等扩张性操作投放了流动性后，货币市场利率下降后，贷款利率并没有随之下降。价格型传导机制受阻。</p>
<p>欧美经验中，市场化利率传导路径：</p>
<p>央行制定政策利率（欧元区利率走廊、降准、MLF、美国公开市场操作、隔夜逆回购等操作）-&gt;货币市场利率（同业拆借、回购）-&gt;存款利率（银行的负债分为存款性和借款性，两者具有替代性。如货币市场利率低时瑞银对存款征收年费）-&gt;货币市场利率和存款利率导致负债成本降低，决定了银行的贷款利率（贷款的成本加成定价模型FTP 贷款利率=资金成本+风险成本+业务成本+合理利润）</p>
<p>传导成立条件：1存款和 同业负债的替代性（监管机构对银行负债结构的考核使得替代性弱，有专门针对存款的考核如存贷比&lt;=3/4、同业负债占比&lt;1/3。因为借款占比更高时则有流动性和交叉风险。处于对稳定性考虑，银行人才稀缺，放开这样的约束不太现实。） 2银行内部完善的FTP（我国存在双轨制，银行内部存在资产负债部和金融市场部，资产负债部进行存贷款定价，参考存贷款基准利率，然后将多余的资金给金融市场部交易，金融市场参考货币市场利率。货币市场作为后发一方对于存贷款利率影响较小）。我国不满足。</p>
<p>在我国不太可行的欧美路线，央行考虑跳过欧美传导的路径。通过MLF影响LPR，然后LPR传导给贷款利率。”子午谷奇谋“。1、增加了利率的可控性，货币调控向价格型转变创造空间 2增加了MLF中期货币政策利率的影响力，完善了短期利率走廊+中期政策利率的调控框架</p>
<p>3】降低实体经济融资成本</p>
<p>货币政策传导受阻，中小企业融资问题仍然突出；贷款基准利率环境下，协同保留贷款利率0.9倍隐性下限。</p>
<p>改革后的LPR报价方式，相较于4.35%，新增贷款的基准利率的确有所下降。</p>
<p>基准利率是贷款客户和银行讨价还价的锚，基准利率下降使得企业议价能力上升。并使得银行难以再设置隐形下限。</p>
<p>问题：</p>
<p>1、期限 2、参与度与报价广泛性 3、mlf的政策属性强 4、央行借款占比低，对整体负债成本影响小，传导有限。5、融资贵更多贵在风险成本和业务成本。而非资金成本。需要其他配套措施解决融资问题。6、存款利率不下，仍然参考基准存款利率。存贷差收缩，压低银行利润，扩大基差风险。银行需要下沉资质，增强风控能力。</p>
]]></content>
  </entry>
  <entry>
    <title>LSTM</title>
    <url>/2021/01/31/LSTM/</url>
    <content><![CDATA[<ul>
<li>篇幅稍长，分为四个部分</li>
</ul>
<ol>
<li>background</li>
<li>step-by-step</li>
<li>show me the code</li>
<li>deep thinking</li>
</ol>
<p>codes<br><a href="https://github.com/satyrswang/blog-jianshu/blob/master/LSTM.lua">https://github.com/satyrswang/blog-jianshu/blob/master/LSTM.lua</a></p>
<h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><ul>
<li><p>what?<br>rnn和feedforward network有嘛不同？</p>
<blockquote>
<p>It’s the easiest to implement an RNN just as a feedforward network with some parts of the input feeding into the middle of the stack, and a bunch of outputs coming out from there as well. There is no magic internal state kept in the network. It’s provided as a part of the input!</p>
</blockquote>
<span id="more"></span>
<p>只是把隐层有拎出来作为下一个隐层的input。=_=<br>然而，理论支持吗？</p>
</li>
<li><p>Problem</p>
<ul>
<li>视频那么多帧，前一帧连着后一帧，间隔又短，那么是否可用前一帧来预测后一帧？<br>看情况。</li>
<li>完形填空 the clouds are in the ___<br>I grew up in France… I speak fluent <em>French</em>.<br>当gap变大，France和<em>French</em>距离那么远，RNN没用了。</li>
<li>为什么gap大了，就没用了？理论证明如下：<br><a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf">Bengio, et al. (1994)</a><br><a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">Hochreiter (1991) German</a></li>
</ul>
</li>
<li><p>然而，LSTMs 不会因为gap惹事儿。<br>Long Short Term Memory networks<br><img src="/2021/01/31/LSTM/chain.webp" alt="chain"></p>
</li>
</ul>
<h2 id="step-by-step"><a href="#step-by-step" class="headerlink" title="step-by-step"></a>step-by-step</h2><ul>
<li>图第二个干嘛了？你先别看图，听我讲：<br>注意这里横着看，看的是chain中第t个</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#过程1 --名字是 input transform</span><br><span class="line">输入 ：input中的x(t)，chain中前一个x输出的结果h(t-1)</span><br><span class="line">参数 ：x的权重w1，h的权重w2，加一个bias</span><br><span class="line">激活函数 ：tanh</span><br></pre></td></tr></table></figure>
<p>以上得到一个结果记为c_in</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#过程2 --名字是 三个gates，每个gate如下</span><br><span class="line">输入 ：input中的x(t)，chain中前一个x输出的结果h(t-1)</span><br><span class="line">参数 ：x的权重w1，h的权重w2，加一个bias</span><br><span class="line">激活函数 ：g</span><br></pre></td></tr></table></figure>
<p>得到三个结果记为i , f , o<br>先保留一个问题： 过程1、2的输入虽然都是x h变量，但是是一样的吗？还是x h这两个向量的部分值呢？<br>有了c_in,i , f , o 之后干嘛，我怎么得到这一层的h？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#过程3 -- 名字是 state update </span><br><span class="line">输入 ：c_in,i , f , o ,c_out(t-1)</span><br><span class="line">输出 ：新的h(t)， c_out(t)</span><br></pre></td></tr></table></figure>
<p>c_out(t-1) 是chain中前一个的输出呗。h、c_out怎么计算的？<br>c_out(t)  =  f * c_out(t-1) + i * c_in<br>h(t)  =  o * tanh(c_out(t))</p>
<ul>
<li>就这么简单？<br>是的。为什么能这样呢？<blockquote>
<p>Because of the <strong>gating mechanism</strong> the cell can keep a piece of information for long periods of time during work and <strong>protect the gradient inside the cell from harmful changes during the training</strong>. Vanilla LSTMs don’t have <strong>a forget gate</strong> and add unchanged cell state during the update (it can be seen as a recurrent connection with a constant weight of 1), what is often referred to as a Constant Error Carousel (CEC). It’s called like that, because <strong>it solves a serious RNN training problem of vanishing and exploding gradients</strong>, which in turn makes it possible to learn long-term relationships.</p>
</blockquote>
</li>
</ul>
<p>原来，因为有个<strong>gating mechanism</strong> 就是 过程2 嘛，解决了RNN的gradient的问题。为什么能解决<strong>vanishing and exploding gradients</strong>的问题呢？理论支持去看论文。</p>
<h2 id="show-me-the-code"><a href="#show-me-the-code" class="headerlink" title="show me the code"></a>show me the code</h2><p>基于 Torch7</p>
<ul>
<li>snippet1: inputs<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local inputs &#x3D; &#123;&#125;</span><br><span class="line">table.insert(inputs, nn.Identity()())   -- x(t)</span><br><span class="line">table.insert(inputs, nn.Identity()())   -- c_out(t-1)</span><br><span class="line">table.insert(inputs, nn.Identity()())   -- h(t-1)</span><br><span class="line">local input &#x3D; inputs[1]</span><br><span class="line">local prev_c &#x3D; inputs[2]</span><br><span class="line">local prev_h &#x3D; inputs[3]</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li><p>  想想看我们要什么？你回答完了之后，听我讲：<br>三个变量 ：过程1、2要的x(t) h(t-1)和过程3还要的c_out(t-1)</p>
</li>
<li><p>  怎么得到？<br>这里用到了<code>nn.Identity()()</code> 和 <code>table.insert</code></p>
<blockquote>
<p>The array-like objects in lua are called tables.<br>nn.Identity() - passes on the input (used as a placeholder for input)</p>
</blockquote>
</li>
</ol>
<p>如果你用tf，那么nn.Identify就是placeholder</p>
<ul>
<li>snippet2: Computing gate values<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local i2h &#x3D; nn.Linear(input_size, 4 * rnn_size)(input) </span><br><span class="line">local h2h &#x3D; nn.Linear(rnn_size, 4 * rnn_size)(prev_h)   </span><br><span class="line">local preactivations &#x3D; nn.CAddTable()(&#123;i2h, h2h&#125;)    </span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li> <code>4 * rnn_size</code>什么鬼？<br>过程1、2在激活前是不是都是x(t) h(t-1)的线性变换？即<code>nn.Linear</code>。<br><code>preactivations</code>将i2h, h2h作加法运算返回一个vector。<br>我们将线性变换的结果分成4份，每份<code>rnn_size</code>多个值。为什么分为4份？记得我们有三个gates吗 ，得到i,f,o？<blockquote>
<p>The first will be used for <strong>i</strong>n gates, second for <strong>f</strong>orget gates, third for <strong>o</strong>ut gates and the last one <strong>as a cell input</strong> .</p>
</blockquote>
</li>
</ol>
<p>就跟玩儿似的。这里<strong>as a cell input</strong>就是直赋值给了h(t)，作为chain下一个的输入。也解释了之前的保留问题，即输入并不是一样的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local pre_sigmoid_chunk &#x3D; nn.Narrow(2, 1, 3 * rnn_size)(preactivations)</span><br><span class="line">local all_gates &#x3D; nn.Sigmoid()(pre_sigmoid_chunk)</span><br><span class="line">local in_chunk &#x3D; nn.Narrow(2, 3 * rnn_size + 1, rnn_size)(preactivations)</span><br><span class="line">local in_transform &#x3D; nn.Tanh()(in_chunk)</span><br><span class="line">local in_gate &#x3D; nn.Narrow(2, 1, rnn_size)(all_gates)</span><br><span class="line">local forget_gate &#x3D; nn.Narrow(2, rnn_size + 1, rnn_size)(all_gates)</span><br><span class="line">local out_gate &#x3D; nn.Narrow(2, 2 * rnn_size + 1, rnn_size)(all_gates)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p> <code>nn.Narrow</code>什么鬼？</p>
<blockquote>
<p>select appropriate parts of the preactivation vector.</p>
</blockquote>
</li>
<li><p>  其他很简单啊，前3份传入gates要<code>nn.Sigmoid()</code>激活。3另一份只需要<code>nn.Tanh()</code>激活。</p>
</li>
</ol>
<ul>
<li><p>snippet3: Cell and hidden state<br>gates结果i f o也有了。进入过程3了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local c_forget &#x3D; nn.CMulTable()(&#123;forget_gate, prev_c&#125;)</span><br><span class="line">local c_input &#x3D; nn.CMulTable()(&#123;in_gate, in_transform&#125;)</span><br><span class="line">local next_c &#x3D; nn.CAddTable()(&#123; c_forget, c_input&#125;)</span><br><span class="line">local c_transform &#x3D; nn.Tanh()(next_c)</span><br><span class="line">local next_h &#x3D; nn.CMulTable()(&#123;out_gate, c_transform&#125;)</span><br></pre></td></tr></table></figure>
<p>按公式计算。没说的。得到<code>next_c</code>和<code>next_h</code></p>
</li>
<li><p>snippet4: define module</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">outputs &#x3D; &#123;&#125;</span><br><span class="line">table.insert(outputs, next_c)</span><br><span class="line">table.insert(outputs, next_h)</span><br><span class="line">return nn.gModule(inputs, outputs)</span><br></pre></td></tr></table></figure></li>
<li><p>手残党的snippet5: 栗子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">require &#39;nn&#39;</span><br><span class="line">require &#39;nngraph&#39;</span><br><span class="line">LSTM &#x3D; require &#39;LSTM.lua&#39;  --以上snippet</span><br><span class="line">--创建3层LSTM，输入3输出3</span><br><span class="line">network &#x3D; &#123;LSTM.create(3, 4), LSTM.create(4, 4), LSTM.create(4, 3)&#125;</span><br><span class="line">--准备</span><br><span class="line">local x &#x3D; torch.randn(1, 3)</span><br><span class="line">local previous_state &#x3D; &#123;</span><br><span class="line">  &#123;torch.zeros(1, 4), torch.zeros(1,4)&#125;,</span><br><span class="line">  &#123;torch.zeros(1, 4), torch.zeros(1,4)&#125;,</span><br><span class="line">  &#123;torch.zeros(1, 3), torch.zeros(1,3)&#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#x3D; nil</span><br><span class="line">next_state &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line">--feed数据</span><br><span class="line">local layer_input &#x3D; &#123;x, table.unpack(previous_state[1])&#125;</span><br><span class="line">for l &#x3D; 1, #network do</span><br><span class="line">  local layer_output &#x3D; network[l]:forward(layer_input)</span><br><span class="line">  table.insert(next_state, layer_output)</span><br><span class="line">  local layer_h &#x3D; layer_output[2]</span><br><span class="line">  if l &lt; #network then</span><br><span class="line">    layer_input &#x3D; &#123;layer_h, table.unpack(previous_state[l + 1])&#125;</span><br><span class="line">  else</span><br><span class="line">    output &#x3D; layer_h</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">print(next_state)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="deep-thinking"><a href="#deep-thinking" class="headerlink" title="deep thinking"></a>deep thinking</h2><p>尽管已经很长了。还是要写理解。这时你可以看图了。</p>
<blockquote>
<p>what information we’re going to throw away from the cell state<br> what new information we’re going to store in the cell state</p>
</blockquote>
<p>1、 什么是forget gate？</p>
<ul>
<li>其实就是将x h线性变换后做一个sigmoid， 如果结果是0，代表forget  c_out(t-1)。</li>
<li>这个例子非常好：<blockquote>
<p>the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p>
</blockquote>
</li>
</ul>
<p>2、 i 和 c_in?</p>
<ul>
<li>两步，第一步i，i = 1相当于是确定哪些值我们需要update或者说需要更新输入的多大成分，想象为将c_in scale了i倍；而tanh相当于为需要更新的值确定了更新成什么c_in。</li>
<li>相乘，则确定了新的候选值，再与f相加，我们便确定了新的状态。</li>
</ul>
<blockquote>
<p>we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.</p>
</blockquote>
<p>3、 那么输出什么？</p>
<ul>
<li>首先我们需要确定哪些更新后的状态需要输出，用sigmoid，得到的o就是我们想要输出的部分。 </li>
<li> 然后 基于更新好的状态c_out(t)，将其tanh控制在[-1,1]之间。乘以o，输出我们要输出的。<blockquote>
<p>since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output <strong>whether the subject is singular or plural</strong>, so that we know what form a verb should be conjugated into if that’s what follows next.</p>
</blockquote>
</li>
</ul>
<p>4、 各类变种<br> <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf">Gers &amp; Schmidhuber (2000)</a><br><a href="http://arxiv.org/pdf/1406.1078v3.pdf">Cho, et al. (2014)</a><br><a href="http://arxiv.org/pdf/1508.03790v2.pdf">Yao, et al. (2015)</a><br><a href="http://arxiv.org/pdf/1402.3511v1.pdf">Koutnik, et al. (2014)</a></p>
<p>5、 比对各类变种的结论<br><a href="http://arxiv.org/pdf/1503.04069.pdf">Greff, et al. (2015)</a><br><a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz, et al. (2015)</a></p>
<p>欢迎补充材料。<br>reference:<br><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>Report读后感</title>
    <url>/2021/05/09/Report%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<br>

<br>

<h3 id="Financial-Stability-Report读后感"><a href="#Financial-Stability-Report读后感" class="headerlink" title="Financial Stability Report读后感"></a>Financial Stability Report读后感</h3><p>1、Asset Valuations</p>
<p>Size of Selected Asset Markets各类资产价格涨幅</p>
<span id="more"></span>

<p>Yields on Nominal Treasury Securities 2、10年名义国债利率</p>
<p>Term Premium on 10-Year Nominal Treasury Securities 10年国债期限溢价</p>
<p>Implied volatility of 10-Year Swap rate 110互换利率隐含波动率</p>
<p>Treasury market Depth –</p>
<p>1、时限，给定价格和数量，多快成交 </p>
<p>2、深度，给定时间和价格，多大数量</p>
<p> 3、宽度，给定时限和数量，多低价格</p>
<p><strong>多样性</strong>是指两个队伍中的人不是因为同一原因而排在这个队的，否则整个队伍会一起出现或者一起离开，对流动性不利<br><strong>恢复性</strong>是指对如果有一个队伍的人突然被清空，那么重新排成类似的队伍需要的时间，时间越短流动性越好。</p>
<p>Corporate Bond Yields</p>
<p>Corporate Bond Spreads to Similar-maturity Treasury Securities   Source: ICe Data Indices, LL</p>
<p>低评级公司债和可比到期国债利率差价变低。</p>
<p>excess Bond Premium</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow源码解读01</title>
    <url>/2021/04/16/TensorFlow%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB01/</url>
    <content><![CDATA[<br>

<p>架构</p>
<span id="more"></span>



<p>1、在网络通信、硬件上开发变量、基础op和一些函数。在之上有执行器、分布式。然后是api接口和应用。</p>
<p>2、核心为，通信、设备管理、op对数据操作、图计算。</p>
<p>3、core目录包含了核心模块。public和client，一个是api接口头文件、一个是api接口实现。platform为和os相关接口文件。protobuf为数据传输的结构化序列。framework包含log、tensor、memory、registey等。graph，distributed，都好理解。kernels是核心的op，ops为梯度、io等op。lib为公共库，如hash等。stream_executor是并行计算框架。common_runtime有session、threadpool、executor管理等。contrib是contributor贡献。gpus封装了cuda cudnn编程库。</p>
<p>4、tensor相关操作，slice add reshape reduce shuffle 等。<code>tensorbuffer</code>指针，指向Eigen::Tensor</p>
<p>5、</p>
<p>Python所构建好的graph模型，会在底下悄悄地生成一个由GraphDef表示的图结构来。然后我们使用Python等语言里的Session具体去分配内存，初使化参数，运行计算图时，TF的后端会将我们前一步所构建的GraphDef转化为一个可执行的Graph。</p>
<p>构建scope，即new graph，包含着op registry。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//static OpRegistry* global_op_registry = new OpRegistry; graph里包含 registry</span></span><br><span class="line">Graph* graph = <span class="keyword">new</span> Graph(OpRegistry::Global());</span><br><span class="line"></span><br><span class="line"><span class="comment">// ShapeRefiner performs shape inference for TensorFlow Graphs.  It is</span></span><br><span class="line"><span class="comment">// responsible for instantiating InferenceContext objects for each</span></span><br><span class="line"><span class="comment">// Node in the Graph, and providing/storing the &#x27;input_tensor&#x27; Tensors</span></span><br><span class="line"><span class="comment">// used by Shape Inference functions, when available at graph</span></span><br><span class="line"><span class="comment">// construction time.</span></span><br><span class="line">ShapeRefiner* refiner =</span><br><span class="line">      <span class="keyword">new</span> ShapeRefiner(graph-&gt;versions(), graph-&gt;op_registry());</span><br><span class="line">  <span class="keyword">return</span> Scope(<span class="keyword">new</span> Impl(graph, <span class="keyword">new</span> Status, <span class="keyword">new</span> Impl::NameMap, refiner,</span><br><span class="line">                        <span class="comment">/* disable_shape_inference */</span> <span class="literal">false</span>));</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>tensorflow源码</category>
      </categories>
      <tags>
        <tag>tensorflow源码</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow源码解读02</title>
    <url>/2021/04/16/TensorFlow%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB02/</url>
    <content><![CDATA[<br>

<p>Tensor、op相关</p>
<span id="more"></span>



<p>从api开始挖。然后再上升到框架。然后再topdown地整合。</p>
<p>友好小白。</p>
<br>

<p>1、Tensor\TensorShape</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建tensor - constructor包含TensorShape</span></span><br><span class="line"><span class="comment">//TensorShape继承自Base ，constructor传入一个initializer_list</span></span><br><span class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">TensorShapeBase</span><span class="params">(gtl::ArraySlice&lt;int64&gt; dim_sizes)</span></span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<br>

<p>2、DeepCopy</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//主要两个步骤 ，在tensor_utils中</span></span><br><span class="line"><span class="comment">//1 数据的memcpy -- 这里用到了string_view，use StringPiece as a convenient map over the tensor buffer</span></span><br><span class="line"><span class="keyword">using</span> StringPiece = absl::string_view;</span><br><span class="line">StringPiece(<span class="keyword">static_cast</span>&lt;<span class="keyword">char</span>*&gt;(buf_-&gt;data()), TotalBytes());</span><br><span class="line"><span class="built_in">memcpy</span>(<span class="keyword">const_cast</span>&lt;<span class="keyword">char</span>*&gt;(output_data.data()), input_data.data(), input_data.size());</span><br><span class="line"></span><br><span class="line"><span class="comment">//2 类型 和 维度</span></span><br><span class="line">output-&gt;unaligned_flat&lt;Variant&gt;() =input.unaligned_flat&lt;Variant&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//这里unaligned_flat，为unaligned_shaped&lt;T, 1&gt;(&#123;NumElements()&#125;);</span></span><br><span class="line"><span class="comment">//主要检查type ，用span来填充维度：FillDimsAndValidateCompatibleShape，添入Eigen的dims并返回Eigen的tensor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> ArraySlice = absl::Span&lt;<span class="keyword">const</span> T&gt;;</span><br><span class="line"><span class="keyword">typedef</span> Eigen::TensorMap&lt;Eigen::Tensor&lt;T, NDIMS, Eigen::RowMajor, IndexType&gt; &gt;</span><br><span class="line">      UnalignedTensor;</span><br></pre></td></tr></table></figure>

<p>类似的操作如 slice concat split等不再赘述。</p>
<br>

<p>3、gtest</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">   <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values_to_test = LoadValuesFromConfig();</span><br><span class="line">   RegisterMyTests(values_to_test);</span><br><span class="line">   ...</span><br><span class="line">   <span class="keyword">return</span> RUN_ALL_TESTS();</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//写法如</span></span><br><span class="line">TEST(TensorProtoUtil, CompressTensorProtoInPlaceTooSmall) &#123;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> kLength = <span class="number">63</span>;</span><br><span class="line">  TensorProto tensor_proto =</span><br><span class="line">      tensor::CreateTensorProto(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;(kLength), &#123;kLength&#125;);</span><br><span class="line">  EXPECT_FALSE(tensor::CompressTensorProtoInPlace(&amp;tensor_proto));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//CreateTensorProto的测试</span></span><br><span class="line"><span class="keyword">auto</span> proto = tensor::CreateTensorProto(values, shape);</span><br><span class="line">EXPECT_EQ(proto.DebugString(),</span><br><span class="line">          <span class="string">&quot;dtype: DT_STRING\n&quot;</span></span><br><span class="line">          <span class="string">&quot;tensor_shape &#123;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  dim &#123;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;    size: 1\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  &#125;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  dim &#123;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;    size: 3\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  &#125;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;&#125;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;string_val: \&quot;a\&quot;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;string_val: \&quot;b\&quot;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;string_val: \&quot;c\&quot;\n&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>tensorflow 的test非常多，都是基于gtest框架。</p>
<br>

<p>4、CreateTensorProto</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//同样两个部分，shape和value进行create。tensor_util.h中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//shape部分是protobuf相关操作</span></span><br><span class="line">internal::SetTensorProtoShape(shape, &amp;tensor_shape_proto);</span><br><span class="line"><span class="comment">//set的方式主要是：循环获得shapeproto中的RepeatedPtrField&lt; ::tensorflow::TensorShapeProto_Dim &gt;</span></span><br><span class="line"><span class="comment">// RepeatedPtrField is like RepeatedField, but used for repeated strings or Messages.</span></span><br><span class="line"><span class="comment">//获得后，再调用其Add方法，将shape的每一维 生成一个element并且set值</span></span><br><span class="line"><span class="function">Element* <span class="title">Add</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//value用internal::TensorProtoHelper&lt;Type&gt; 中的AddValues这个static方法，把value中的值一个个加进proto</span></span><br><span class="line">tensor.set_dtype(TypeHelper::GetDataType());</span><br><span class="line">tensor.mutable_tensor_shape()-&gt;Swap(&amp;tensor_shape_proto);</span><br><span class="line">TypeHelper::AddValues(values.begin(), values.end(), &amp;tensor);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>还有诸如CompressTensorProtoInPlace（主要是CompressRepeatedField，核心两个阈值<code> static const int64 kDefaultMinNumElements = 64;static const float kDefaultMinCompressionRatio = 2.0f;</code>）。</p>
<br>

<p>5、protobuf</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//tensor、op、graph等的proto 涉及到很多protobuf操作，不赘述</span></span><br><span class="line"><span class="comment">//比如Arena中</span></span><br><span class="line"><span class="keyword">auto</span>* p = CreateMaybeMessage&lt;::tensorflow::TensorShapeProto&gt;(GetArenaNoVirtual());</span><br></pre></td></tr></table></figure>

<br>

<p>6、OpRegistry</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// The standard implementation of OpRegistryInterface, along with a</span></span><br><span class="line"><span class="comment">// global singleton used for registering ops via the REGISTER</span></span><br><span class="line"><span class="comment">// macros below.  Thread-safe.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Example registration:</span></span><br><span class="line"><span class="comment">//   OpRegistry::Global()-&gt;Register(</span></span><br><span class="line"><span class="comment">//     [](OpRegistrationData* op_reg_data)-&gt;Status &#123;</span></span><br><span class="line"><span class="comment">//       // Populate *op_reg_data here.</span></span><br><span class="line"><span class="comment">//       return Status::OK();</span></span><br><span class="line"><span class="comment">//   &#125;);</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>









<br>
总结：
1、A `Span<T>` is somewhat analogous to an `absl::string_view`, but for an array of elements of type `T`. A user of `Span` must ensure that the data being pointed to outlives the `Span` itself.

<p>2、string_view </p>
<p>3、# ##  </p>
<p>4、gtest</p>
<p>5、initializer_list</p>
</T>]]></content>
      <categories>
        <category>tensorflow源码</category>
      </categories>
      <tags>
        <tag>tensorflow源码</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow源码解读03</title>
    <url>/2021/05/02/TensorFlow%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB03/</url>
    <content><![CDATA[<p>1、</p>
]]></content>
  </entry>
  <entry>
    <title>batchnorm</title>
    <url>/2021/04/05/batchnorm/</url>
    <content><![CDATA[<br>

<p>bn相关问题</p>
<span id="more"></span>
<ul>
<li><p>白化</p>
<ul>
<li><p>目的</p>
<ul>
<li><p>1）去除特征之间的相关性 —&gt; 独立；</p>
</li>
<li><p>2）使得所有特征具有相同的均值和方差 —&gt; 同分布。</p>
</li>
</ul>
</li>
<li><p>PCA白化保证了所有特征分布均值为0，方差为1</p>
</li>
<li><p>ZCA白化则保证了所有特征分布均值为0，方差相同；</p>
</li>
<li><p>白化操作，固定了每一层网络输入分布，加速网络训练过程的收敛</p>
</li>
</ul>
</li>
<li><p>Internal Covariate Shift</p>
<ul>
<li><p>内容</p>
<ul>
<li><p>covariate shift 就是分布不一致假设之下的一个分支问题<br>它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同</p>
</li>
<li><p>对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大</p>
</li>
<li><p>可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由</p>
</li>
</ul>
</li>
<li><p>问题</p>
<ul>
<li><p>简而言之，每个神经元的输入数据不再是“独立同分布”。</p>
</li>
<li><p>学习速度、饱和区(早停)、影响其他层</p>
</li>
</ul>
</li>
<li><p>解决</p>
<ul>
<li>bn框架<ul>
<li>h = f ( g * {(x-μ)/θ} + b)</li>
</ul>
</li>
<li>经过这么的变回来再变过去，会不会跟没变一样<ul>
<li>再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数</li>
<li>但是新参数有不同的学习动态。</li>
<li>在旧参数中，x的均值取决于下层神经网络的复杂关联；<br>但在新参数中， 仅由 b 来确定，去除了与下层计算的密切耦合。</li>
</ul>
</li>
<li>这样的 Normalization 与标准的白化区别<ul>
<li>标准白化操作的目的是“独立同分布”</li>
<li>变换为均值为 b  、方差为 g^2 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>BN</p>
<ul>
<li><p>分类</p>
<ul>
<li>Batch Normalization<ul>
<li>每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的</li>
<li>分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性</li>
<li>但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。</li>
</ul>
</li>
<li>Layer Normalization </li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>防止模型梯度爆炸</li>
<li>为什么加速收敛<ul>
<li>梯度的方向为垂直等高线的方向而走之字形路线，这样会使迭代很 </li>
</ul>
</li>
</ul>
</li>
<li><p>常见的方法有</p>
<ul>
<li>min-max标准化（min-max normalization）</li>
<li>归一化</li>
<li>log函数转换</li>
<li>atan函数转换</li>
<li>z-score标准化（zero-mena normalization，此方法比较常用）- 模糊量化法</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>c++tips上</title>
    <url>/2021/04/11/c-tips/</url>
    <content><![CDATA[<br>
<br>


<p>基础汇总</p>
<span id="more"></span>

<br>



<ul>
<li><p>内存对齐</p>
</li>
<li><p>构造与析构函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、是否在析构函数抛出异常：</span><br><span class="line">	1异常点之后的程序如果有释放资源，会造成诸如资源泄漏</span><br><span class="line">	2通常异常发生时，c++的机制会调用已经构造对象的析构函数来释放资源，此时若析构函数本身也抛出异常，则前一个异常尚未处理，又有新的异常，会造成程序崩溃的问题。</span><br><span class="line">	3把异常完全封装在析构函数内部，决不让异常抛出函数之外</span><br><span class="line">	</span><br><span class="line">2、构造函数是否可以抛出异常：</span><br><span class="line">	1构造函数可以抛出异常。构造函数中尽量不要抛出异常。</span><br><span class="line">	2既需要分配内存，又需要抛出异常时要特别注意防止内存泄露的情况发生。因为在构造函数中抛出异常，在概念上将被视为该对象没有被成功构造，因此当前对象的析构函数就不会被调用，就会造成内存泄漏。</span><br><span class="line">	3同时，由于构造函数本身也是一个函数，在函数体内抛出异常将导致当前函数运行结束，并释放已经构造的成员对象，包括其基类的成员，即执行直接基类和成员对象的析构函数</span><br><span class="line">	</span><br><span class="line">3、构造函数和析构函数可以调用虚函数吗：</span><br><span class="line">	1当创建某个派生类的对象时，如果在它的基类的构造函数中调用虚函数，那么此时派生类的构造函数并未执行，所调用的函数可能操作还没有被初始化的成员，将导致灾难的发生。</span><br><span class="line">	2即使想在构造函数中实现动态联编，在实现上也会遇到困难。这涉及到对象虚指针（vptr）的建立问题。一个类的构造函数在执行时，并不能保证该函数所能访问到的虚指针就是当前被构造对象最后所拥有的虚指针，因为后面派生类的构造函数会对当前被构造对象的虚指针进行重写，因此无法完成动态联编</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>堆栈</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、堆栈和缓存的区别</span><br><span class="line">	1栈使用的是一级缓存， 他们通常都是被调用时处于存储空间中，调用完毕立即释放；</span><br><span class="line">	2堆是存放在二级缓存中，堆的首地址放在一级缓存缓存中，分配和释放会产生系统调用，由用户态进入内核态，所以速度会慢一些</span><br></pre></td></tr></table></figure></li>
<li><p>安全漏洞、内存越界</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  </span><br></pre></td></tr></table></figure></li>
<li><p>野指针</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">“野指针”不是NULL指针，是指指向“垃圾”内存的指针。即指针指向的内容是不确定的。</span><br><span class="line">产生的原因：</span><br><span class="line">1）指针变量没有初始化。因此，创建指针变量时，该变量要被置为NULL或者指向合法的内存单元。</span><br><span class="line">2）指针p被free之后，没有置为NULL，让人误以为p是个合法的指针。</span><br><span class="line">3）指针跨越合法范围操作。不要返回指向栈内存(非静态局部变量）的指针或引用。</span><br><span class="line"></span><br><span class="line">可能后果：</span><br><span class="line">若操作系统将这部分已经释放的内存重新分配给另外一个进程，而原来的程序重新引用现在的迷途指针，向其中写入数据，则这部分程序内容将被破坏，而导致程序错误。这种类型的程序错误，通常会导致segment fault和一般的保护错误。</span><br><span class="line">其他常见错误：</span><br><span class="line">返回一个基于栈分配的局部变量的地址时，一旦调用的函数返回，分配给这些变量的空间将回收，此时它们拥有的是垃圾值，如return &amp;num，如果要使它的生命周期加长，应该将其声明为static</span><br></pre></td></tr></table></figure></li>
<li><p>STL容器及常见算法</p>
</li>
</ul>
<blockquote>
<p><a href="http://vernlium.github.io/2019/12/29/C-STL%E5%B8%B8%E7%94%A8%E5%AE%B9%E5%99%A8API%E6%80%BB%E7%BB%93/">http://vernlium.github.io/2019/12/29/C-STL%E5%B8%B8%E7%94%A8%E5%AE%B9%E5%99%A8API%E6%80%BB%E7%BB%93/</a></p>
<p><a href="https://blog.csdn.net/weixin_43150428/article/details/82469933">https://blog.csdn.net/weixin_43150428/article/details/82469933</a></p>
</blockquote>
<ul>
<li><p>tuple</p>
</li>
<li><p>array container</p>
</li>
<li><p>range-base for</p>
</li>
<li><p>initializer lists</p>
</li>
<li><p>delegate/inheriting constructors</p>
</li>
<li><p>nullptr</p>
</li>
<li><p>inline</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、内联展开避免函数中断调用开销</span><br><span class="line">2、宏：内联函数在运行时可调试，而宏定义不可以；编译器会对内联函数的参数类型做安全检查或自动类型转换（同普通函数），而宏定义则不会；内联函数可以访问类的成员变量，宏定义则不能；在类中声明同时定义的成员函数，自动转化为内联函数</span><br><span class="line"></span><br><span class="line">inline一般只用于如下情况：</span><br><span class="line">一个函数不断被重复调用函数只有简单的几行，且函数不包含for、while、switch语句，递归。</span><br></pre></td></tr></table></figure></li>
<li><p>unordered_table</p>
</li>
<li><p>shared_ptr/weak_ptr</p>
</li>
<li><p> regexp</p>
</li>
<li><p>cost of exception handling</p>
</li>
<li><p>type trait</p>
</li>
<li><p>strong exception gaurantee</p>
</li>
<li><p>CRTP</p>
</li>
<li><p>smart pointer</p>
</li>
<li><p>std::function and function pointer</p>
</li>
<li><p>runtime cost of lambda function</p>
</li>
<li><p>虚继承</p>
</li>
<li><p>Rvalue reference</p>
</li>
<li><p>function/bind</p>
</li>
<li><p>Lambda expression and closure</p>
</li>
<li><p>auto/decltype</p>
</li>
<li><p>static_assert</p>
</li>
<li><p>可变模板参数</p>
</li>
<li><p>指针和引用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、指针可以是null，是对象</span><br><span class="line">2、引用必须初始化，不是对象，且不能重新赋值</span><br><span class="line">3、传入函数时带上&amp; 如s(&amp;a)，则对形参的修改其实就是对实参的修改。传入的是变量a的地址。而s(int* x)里，*x即为a的值，x为传入的a的地址。</span><br><span class="line">4、double &amp;b &#x3D; a中，b引用和变量a本质是是同一块内存</span><br><span class="line">5、引用最主要的作用是减少了内存拷贝的过程</span><br><span class="line"></span><br><span class="line">而当我们通过引用减少内存拷贝，传入的则为实参本身时，加上const可以保证，函数体内对实参不进行修改</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>全局作用域</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">::</span><br></pre></td></tr></table></figure></li>
<li><p>static</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">面向过程：</span><br><span class="line">静态全局变量</span><br><span class="line">静态局部变量</span><br><span class="line">静态函数</span><br><span class="line"></span><br><span class="line">面向对象：</span><br><span class="line">静态成员变量</span><br><span class="line">静态成员函数</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>malloc free 和new delete</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、malloc只分配指定大小的堆内存空间，而new可以根据对象类型分配合适的堆内存空间，当然还可以通过重载operator new 自定义内存分配策略，其次还能够构造对象</span><br><span class="line">2、free释放对应的堆内存空间，delete,先执行对象的析构函数，在释放对象所占空间。</span><br><span class="line">3、malloc与free是C++&#x2F;C 语言的标准库函数，new&#x2F;delete 是C++的运算符。</span><br><span class="line">4、malloc返回类型是void*,使用时需要类型转换，而new在分配时，编译器能够根据对象类型自动计算出大小，返回类型是指向对象类型的指针，其封装了sizeof和类型转换功能，实际上new分为两步，第一步是通过调用operator new函数分配一块合适，原始的，未命名的内存空间，返回类型也是void *,而且operator new可以重载，可以自定义内存分配策略，甚至不做内存分配，甚至分配到非内存设备上，而malloc无能为力，第二步，调用构造函数构造对象，new将调用constructor，而malloc不能；delete将调用destructor，而free不能</span><br><span class="line"></span><br><span class="line">总结：1、大小or分配策略是否可自定义 2、析构和构造函数 3、库函数和运算符(重载) </span><br></pre></td></tr></table></figure></li>
<li><p>const</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、变量 初始化后，不被改变</span><br><span class="line"></span><br><span class="line">2、函数 </span><br><span class="line">1. 修饰返回值 const int func() -- 不能改变返回值</span><br><span class="line">2. 修饰参数 int func(const ) -- 函数体内不能改变参数</span><br><span class="line">3. 修饰成员函数 int func() const -- 函数体内不能改变成员变量的值</span><br><span class="line"></span><br><span class="line">3、指针</span><br><span class="line">1. const int* -- 常量指针，pointer指向的变量的值不能变，pointer可指向其他对象</span><br><span class="line">2. int const* -- 指针常量，可以改变指针指向变量的值，pointer不能指向其他对象</span><br><span class="line"></span><br><span class="line">4、对象 const对象只能调用const成员函数，不能调用普通函数</span><br></pre></td></tr></table></figure></li>
<li><p>多态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、虚函数的类 至少有一个(多继承会有多个)一维的虚函数表叫做虚表(virtual table)，属于类成员，虚表的元素值是虚函数的入口地址，在编译时就已经为其在数据端分配了空间</span><br><span class="line"></span><br><span class="line">2、确定的虚函数对应virtual table中一个固定位置n，n是一个在编译时期就确定的常量，所以，使用vptr加上对应的n，就可以得到对应的函数入口地址。C++采用的这种绝对地址+偏移量的方法调用虚函数，查找速度快执行效率高，时间复杂度为O(1)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/11/c-tips/vptr.jpg" alt="vptr"></p>
</li>
<li><p>c++ 与java .NET</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一，C++的难不仅仅在于其静态结构体系，还有很多源于语言设计上的包袱，比如对C的兼容，比如没有垃圾收集机制，比如对效率的强调，等等。一旦把这些包袱丢掉，设计的难度确实可以大大下降。</span><br><span class="line"></span><br><span class="line">第二，Java和.NET的核心类库是在C++十几年成功和失败的经验教训基础之上，结合COM体系优点设计实现的，自然要好上一大块。事实上，在Java和.NET核心类库的设计中很多地方，体现的是基于接口的设计，和真正的基于对象的设计。有了这两个主角站台，“面向类的设计”不能喧宾夺主，也能发挥一些好的作用。</span><br><span class="line"></span><br><span class="line">第三，Java和.NET中分别对C++最大的问题——缺少对象级别的delegate机制做出了自己的回应，这就大大弥补了原来的问题。</span><br><span class="line"></span><br><span class="line">尽管如此，Java还是沾染上了“面向类设计”的癌症，基础类库里就有很多架床叠屋的设计，而J2EE&#x2F;Java EE当中，这种形而上学的设计也很普遍，所以也引发了好几次轻量化的运动。</span><br><span class="line"></span><br><span class="line">至于.NET，我听陈榕介绍过，在设计.NET的时候，微软内部对于是否允许继承爆发了非常激烈的争论。很多资深高人都强烈反对继承。至于最后引入继承，很大程度上是营销需要压倒了技术理性。尽管如此，由于有COM的基础，又实现了非常彻底的delegate，所以 .NET 的设计水平还是很高的。它的主要问题不在这，在于太急于求胜，更新速度太快，基础不牢。当然，根本问题还是微软没有能够在Web和Mobile领域里占到多大的优势，也就使得.NET没有用武之地。</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>c++ 与 c</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、C面向过程，C++面向对象</span><br></pre></td></tr></table></figure></li>
</ul>
<p>reference：<br><a href="https://blog.csdn.net/myan/article/details/5928531">https://blog.csdn.net/myan/article/details/5928531</a></p>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>bean解析及注册源码</title>
    <url>/2021/03/11/bean%E8%A7%A3%E6%9E%90%E5%8F%8A%E6%B3%A8%E5%86%8C%E6%BA%90%E7%A0%81/</url>
    <content><![CDATA[<br>

<p>S1<code>xml文件等资源类 </code> – 对各种资源类的封装+encode</p>
<span id="more"></span>
<p><img src="https://upload-images.jianshu.io/upload_images/8716089-f03da4d523f26bbb.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="resource.jpg"></p>
<p>S2<code>读取xml文件</code> – 对xml文件的校验、load、read(这里的read调用BeanDefinitionDocumentReader)<br>主要在<code>XmlBeanDefinitionReader</code>中</p>
<p>S3<code>解析属性(xml中标签)</code> –从xml到Bean<br>实现在<code>DefaultBeanDefinitionDocumentReader</code><br><code>doRegisterBeanDefinitions()</code> 中解析了<code>profile</code>属性，并且其中的<code>parseBeanDefinitions()</code>是解析xml的开始。</p>
<p>根据不同的<code>namespace</code>和<code>nodename</code>，分别不同处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123;</span><br><span class="line">	importBeanDefinitionResource(ele);</span><br><span class="line">&#125;</span><br><span class="line">else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123;</span><br><span class="line">	processAliasRegistration(ele);</span><br><span class="line">&#125;</span><br><span class="line">else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123;</span><br><span class="line">	processBeanDefinition(ele, delegate);</span><br><span class="line">&#125;</span><br><span class="line">else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123;</span><br><span class="line">	&#x2F;&#x2F; recurse</span><br><span class="line">	doRegisterBeanDefinitions(ele);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>处理过程涉及到<code>BeanDefinitionParserDelegate BeanDefinitionHolder BeanDefinitionReaderUtils XmlReaderContext</code></p>
<blockquote>
<p>delegate中对元素(属性)进行解析，结果放入holder中，此时holder已经包含了各种属性。再由Utils中将holder进行注册。最后由context将注册结果通知监听器。</p>
</blockquote>
<p>S4<code>注册</code></p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>cmake</title>
    <url>/2021/04/13/cmake/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>cuda编程</title>
    <url>/2021/04/12/cuda%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<br>

<p>cuda c的hello world</p>
<span id="more"></span>

<br> 

<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">*hello_world.cu</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">hello_world</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;GPU: Hello world!\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;CPU: Hello world!\n&quot;</span>);</span><br><span class="line">  hello_world&lt;&lt;&lt;<span class="number">1</span>,<span class="number">10</span>&gt;&gt;&gt;();</span><br><span class="line">  cudaDeviceReset();<span class="comment">//if no this line ,it can not output hello world from gpu</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<h4 id="异构计算"><a href="#异构计算" class="headerlink" title="异构计算"></a>异构计算</h4><p>1、一台 intel i7-4790 CPU加上两台Titan x GPU构成的工作站，GPU插在主板的PCIe卡口上，运行程序的时候，CPU像是一个控制者，指挥两台Titan完成工作后进行汇总，和下一步工作安排，所以CPU我们可以把它看做一个指挥者，主机端，host，而完成大量计算的GPU是我们的计算设备，device。</p>
<p>2、一个四核CPU一般有四个ALU，ALU是完成逻辑计算的核心，也是我们平时说四核八核的核，控制单元，缓存也在片上，DRAM是内存，一般不在片上，CPU通过总线访问内存。</p>
<p>3、GPU，排列成行的一组ALU 公用一个Control单元和Cache，这个部分相当于一个完整的多核CPU。但是不同的是ALU多了，control部分变小，可见计算能力提升了，控制能力减弱了。</p>
<p>4、CPU和GPU之间通过PCIe总线连接，用于传递指令和数据，这部分也是后面要讨论的性能瓶颈之一。</p>
<p>5、低并行逻辑复杂的程序适合用CPU；高并行逻辑简单的大数据计算适合GPU</p>
<h4 id="衡量GPU计算能力"><a href="#衡量GPU计算能力" class="headerlink" title="衡量GPU计算能力"></a><br>衡量GPU计算能力</h4><p>容量：CUDA核心数量（越多越好）；内存大小（越大越好）</p>
<p>性能：峰值计算能力；内存带宽</p>
<br>

<h4 id="CPU和GPU线程的区别："><a href="#CPU和GPU线程的区别：" class="headerlink" title="CPU和GPU线程的区别："></a>CPU和GPU线程的区别：</h4><p>1、CPU线程是重量级实体，操作系统交替执行线程，线程上下文切换花销很大</p>
<p>2、GPU线程是轻量级的，GPU应用一般包含成千上万的线程，多数在排队状态，线程之间切换基本没有开销。</p>
<p>3、CPU的核被设计用来尽可能减少一个或两个线程运行时间的延迟，而GPU核则是大量线程，最大幅度提高吞吐量</p>
<br>

<h4 id="cuda-c"><a href="#cuda-c" class="headerlink" title="cuda c"></a>cuda c</h4><p>1、CUDA C 是标准ANSI C语言的扩展，扩展出一些语法和关键字来编写设备端代码，而且CUDA库本身提供了大量API来操作设备完成计算。</p>
<p>2、驱动API是低级的API，使用相对困难，运行时API是高级API使用简单，其实现基于驱动API。<br>这两种API是互斥的，也就是你只能用一个，两者之间的函数不可以混合调用，只能用其中的一个库。</p>
<p>3、一个CUDA应用通常可以分解为两部分</p>
<p>CPU 主机端代码和GPU 设备端代码</p>
<p>4、nvcc 是从LLVM开源编译系统为基础开发的。CUDA nvcc编译器会自动分离你代码里面的不同部分，如图中主机代码用C写成，使用本地的C语言编译器编译，设备端代码，也就是核函数，用CUDA C编写，通过nvcc编译，链接阶段，在内核程序调用或者明显的GPU设备操作时，添加运行时库。</p>
<h4 id="cuda程序步骤"><a href="#cuda程序步骤" class="headerlink" title="cuda程序步骤"></a><br>cuda程序步骤</h4><ol>
<li>分配GPU内存</li>
<li>拷贝内存到设备</li>
<li>调用CUDA内核函数来执行计算</li>
<li>把计算完成数据拷贝回主机端</li>
<li>内存销毁</li>
</ol>
<br>

<h4 id="CUDA抽象了硬件实现"><a href="#CUDA抽象了硬件实现" class="headerlink" title="CUDA抽象了硬件实现"></a>CUDA抽象了硬件实现</h4><ol>
<li>线程组的层次结构</li>
<li>内存的层次结构</li>
<li>障碍同步</li>
</ol>
<br>

<h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul>
<li>Nvidia Nsight集成开发环境</li>
<li>CUDA-GDB 命令行调试器</li>
<li>性能分析可视化工具</li>
<li>CUDA-MEMCHECK工具</li>
<li>GPU设备管理工具</li>
</ul>
]]></content>
      <categories>
        <category>gpu</category>
      </categories>
      <tags>
        <tag>gpu</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>cuda编程模型</title>
    <url>/2021/04/12/cuda%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>gpu</category>
      </categories>
      <tags>
        <tag>gpu</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>dssm</title>
    <url>/2021/03/08/dssm/</url>
    <content><![CDATA[<h4 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h4><ul>
<li><p>任务<br>用来预测两个句子的语义相似度，又可以获得某句子的低维语义Embedding向量。</p>
<span id="more"></span></li>
<li><p>场景<br>DSSM 模型的最大特点就是 Query 和 Document 是两个独立的子网络，后来这一特色被移植到推荐算法的召回环节，即对用户端（User）和物品端（Item）分别构建独立的子网络塔式结构。<br>两个子网络产生的 Embedding 向量可以独自获取及缓存。</p>
</li>
</ul>
<p>当模型训练完成时，物品的 Embedding 是可以保存成词表的，线上应用的时候只需要查找对应的 Embedding 即可。因此线上只需要计算 （用户，上下文） 一侧的 Embedding，基于 Annoy 或 Faiss 技术索引得到用户偏好的候选集。</p>
<ul>
<li>word hashing<br>word hashing方法是用来减少输入向量的维度，该方法基于字母的n-gram。给定一个单词（good），我们首先增加词的开始和结束部分（#good#），然后将该词转换为字母 [公式] -gram的形式（假设为trigrams：#go，goo，ood，od#）。最后该词使用字母 n-gram的向量来表示。</li>
</ul>
<p>这种方法的问题在于有可能造成冲突，因为两个不同的词可能有相同的n-gram向量来表示。与原始的ont-hot向量表示的词典大小相比，word hashing明显降低了向量表示的维度。</p>
<ul>
<li>优点</li>
</ul>
<p>1、解决了LSA、LDA、Autoencoder等方法存在的一个最大的问题：字典爆炸（导致计算复杂度非常高），因为在英文单词中，词的数量可能是没有限制的，但是字母n-gram的数量通常是有限的.<br>2、基于词的特征表示比较难处理新词，字母的 n-gram可以有效表示，鲁棒性较强<br>3、使用有监督方法，优化语义embedding的映射问题<br>4、省去了人工的特征工程</p>
<ul>
<li>缺点</li>
</ul>
<p>1、word hashing可能造成冲突<br>2、DSSM采用了词袋模型，损失了上下文信息<br>3、在排序中，搜索引擎的排序由多种因素决定，由于用户点击时doc的排名越靠前，点击的概率就越大，如果仅仅用点击来判断是否为正负样本，噪声比较大，难以收敛<br>4、对于中文而言，处理方式与英文有很多不一样的地方。中文往往需要进行分词，但是我们可以仿照英文的处理方式，将中文的最小粒度看作是单字（在某些文献里看到过用偏旁部首，笔画，拼音等方法）</p>
<ul>
<li><p>扩展<br>对DSSM的优化出现了很多的变种，有CNN-DSSM，LSTM-DSSM，MV-DSSM等。</p>
</li>
<li><p>trick<br><img src="/2021/03/08/dssm/trick.png" alt="trick"></p>
</li>
</ul>
<hr>
<ul>
<li>架构<br><img src="/2021/03/08/dssm/dssm.png" alt="dssm"></li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>dubbo</title>
    <url>/2021/04/05/dubbo/</url>
    <content><![CDATA[<p>dubbo–rpc框架相关</p>
<span id="more"></span>

<ul>
<li><p>分层</p>
<ul>
<li>service、config、cluster、 monitor、protocol、exchange、registry、proxy、transport、serialize等</li>
</ul>
</li>
<li><p>网络通信协议、序列化协议</p>
<ul>
<li>dubbo 长连接、nio、高并发、数据量小</li>
<li>hessian </li>
<li>还有其他 json 、java二进制、rmi等</li>
</ul>
</li>
<li><p>负载均衡</p>
<ul>
<li>权重、轮询、自动感知、一致性hash</li>
<li>方法级别配置</li>
</ul>
</li>
<li><p>spi</p>
<ul>
<li>插件扩展</li>
</ul>
</li>
</ul>
<ul>
<li><p>动态代理</p>
<ul>
<li><p>静态代理</p>
<ul>
<li>编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件</li>
<li>代理类中包含具体代理的接口类</li>
</ul>
</li>
<li><p>动态</p>
<ul>
<li><p>可以不需要针对每个目标类都创建一个代理类，静态代理中接口一旦新增加方法，目标对象和代理对象都要进行修改，非常麻烦</p>
</li>
<li><p>jdk动态</p>
<ul>
<li> InvocationHandler 接口和 Proxy 类 (InvocationHandler里面会用到Proxy类)</li>
<li><code>SmsService smsService = (SmsService) JdkProxyFactory.getProxy(new SmsServiceImpl());</code></li>
<li>JDK 动态代理只能只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类</li>
</ul>
</li>
<li><p>cglib动态代理</p>
<ul>
<li>自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法</li>
<li><code>AliSmsService aliSmsService = (AliSmsService) CglibProxyFactory.getProxy(AliSmsService.class);</code></li>
<li> CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/basis/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3.md">代理模式详解.</a></p>
</li>
</ul>
</li>
<li><p>服务治理</p>
<ul>
<li>链路、压力、时长、可用性(成功率)、服务分层(循环依赖)</li>
<li>服务降级<ul>
<li>dubbo中的mock<ul>
<li>类加上Mock后缀，作为降级策略</li>
</ul>
</li>
</ul>
</li>
<li>失败重试、超时重试<ul>
<li>200ms超时，重试3次</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title>dubbo-spi</title>
    <url>/2021/01/31/dubbo-spi/</url>
    <content><![CDATA[<h2 id="spi"><a href="#spi" class="headerlink" title="spi"></a>spi</h2><p><img src="/2021/01/31/dubbo-spi/jdbc.png" alt="jdbc实现"></p>
<p><strong>SPI 的缺点</strong><br>JDK 标准的 SPI 会一次性加载实例化扩展点的所有实现，JDK 启动的时候会一次性全部加载。<br>1如果有的扩展点实现初始化很耗时或者如果有些实现类并没有用到， 会很浪费资源。<br>2如果扩展点加载失败，会导致调用方报错，而且这个错误很难定位到。</p>
<span id="more"></span>

<p><strong>dubbo spi</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Protocol  p &#x3D; ExtensionLoader.getExtensionLoader(xxx.class).getAdaptiveExtension();</span><br><span class="line">ExtensionLoader.getExtensionLoader(xxx.class).getExtension(name);</span><br><span class="line">ExtensionLoader.getExtensionLoader(xxx.class).getActivateExtension(url, key);</span><br></pre></td></tr></table></figure>
<p><code>protocol</code>会在运行的时候判断一下应该选用这个Protocol接口的哪个实现类来实例化对象。<br>动态的根据配置去找到对应的实现类。如果你没有配置，那就走默认的实现类。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@SPI(&quot;dubbo&quot;)  </span><br><span class="line">public interface Protocol &#123;  </span><br><span class="line">      </span><br><span class="line">    int getDefaultPort();  </span><br><span class="line">  </span><br><span class="line">    @Adaptive  </span><br><span class="line">    &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException;  </span><br><span class="line">  </span><br><span class="line">    @Adaptive  </span><br><span class="line">    &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException;  </span><br><span class="line"></span><br><span class="line">    void destroy();  </span><br><span class="line">  </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">dubbo&#x3D;org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol</span><br></pre></td></tr></table></figure>
<p><code>@SPI(“dubbo”)</code>：通过 SPI 机制来提供实现类，实现类是通过 dubbo 作为默认 key 去配置文件里找到的，配置文件名称与接口全限定名一样的，通过 dubbo 作为 key 可以找到默认的实现类就是 org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol</p>
<p><code>@Adaptive</code>：如果想要动态替换掉默认的实现类，需要使用 @Adaptive 。表示动态代理实现。在运行的时候会针对 Protocol 生成<code>代理类</code>，这个代理类的那俩方法里面会有<code>代理代码</code>，代理代码会在运行的时候动态根据 url 中的 protocol 来获取那个 key，默认是 dubbo，自己指定则获取相应的实现。</p>
<h2 id="扩展dubbo组件"><a href="#扩展dubbo组件" class="headerlink" title="扩展dubbo组件"></a>扩展dubbo组件</h2><p><strong>step1</strong><br><img src="/2021/01/31/dubbo-spi/implement.png" alt="自定义方法"></p>
<p><strong>step2</strong><br><img src="/2021/01/31/dubbo-spi/properties.png" alt="添加配置"></p>
<p><strong>step3</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class App </span><br><span class="line">&#123;</span><br><span class="line">    public static void main( String[] args )</span><br><span class="line">    &#123;</span><br><span class="line">&#x2F;&#x2F;        调用方代码</span><br><span class="line">    	Protocol protocol &#x3D; ExtensionLoader.getExtensionLoader(Protocol.class).getExtension(&quot;myProtocol&quot;);</span><br><span class="line">    	System.out.println(protocol.getDefaultPort());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="实现原理探析"><a href="#实现原理探析" class="headerlink" title="实现原理探析"></a>实现原理探析</h2><p>getExtension方法获取一个SPI接口的扩展类实例的流程: 分为解析配置文件、加载并缓存扩展类、创建并加工(属性注入与层层包装)扩展类实例 几个步骤。</p>
<p>通过getAdaptiveExtension方法的流程可以发现，要想获得一个SPI接口的自适应扩展类实例，有2种方式：<br>1在SPI接口的配置文件中配置具有@Adaptive注解的扩展类，在执行解析SPI接口配置文件方法getExtensionClasses时，它会调用loadClass方法，该方法判断扩展类是否具有@Adaptive注解，如果有，则将该类Class缓存到ExtensionLoader的字段“cachedAdaptiveClass”中，然后直接实例化该Class的实例并进行自动装配；<br>2如果未配置@Adaptive修饰的扩展类，则Dubbo会使用字节码技术创建一个自适应扩展类，前提是SPI接口上至少有一个被@Adaptive注解的方法；</p>
<p>todo</p>
]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title>elasticsearch</title>
    <url>/2021/04/27/elasticsearch/</url>
    <content><![CDATA[<br>

<h2 id="相关笔记"><a href="#相关笔记" class="headerlink" title="相关笔记"></a>相关笔记</h2><p>1、搜索引擎或者elk(es、logstash、kibana)系统，都是根据内容中的关键字建立倒排索引即反向索引，便于搜索。</p>
<span id="more"></span>



<p>2、 在 Lucene 的基础上进行封装，实现了分布式搜索引擎。</p>
<p>3、es的索引、类型和文档的概念比较重要，类似于 MySQL 中的数据库、表和行。</p>
<p>4、Elasticsearch 也是 Master-slave 架构，也实现了数据的分片和备份。写入index和type时是和master打交道，然后同步到slave，但是写入doc数据，不需要如此。为了提高性能，写数据是采取routing，将写入压力分散。</p>
<p>5、</p>
<p>Reference:</p>
<p><a href="https://zhuanlan.zhihu.com/p/62892586">https://zhuanlan.zhihu.com/p/62892586</a></p>
]]></content>
      <categories>
        <category>es</category>
      </categories>
      <tags>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title>flink</title>
    <url>/2021/03/08/flink/</url>
    <content><![CDATA[<br>


<h1 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h1><h2 id="state"><a href="#state" class="headerlink" title="state"></a>state</h2><ul>
<li><p>场景</p>
<ul>
<li>有状态的逻辑是因为数据之间存在关联，单条数据是没有办法把所有的信息给表现出来。<span id="more"></span>
<ul>
<li>去重</li>
<li>窗口计算</li>
<li>机器学习参数</li>
<li>访问历史数据</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么要管理状态</p>
<ul>
<li>内存<ul>
<li>流式作业：24 小时的数据都放到内存，可能会出现内存不足。</li>
</ul>
</li>
<li>高可用<ul>
<li>机器若出现故障或者宕机，需要考虑如何备份及从备份中去恢复，</li>
</ul>
</li>
<li>扩展性<ul>
<li>单节点无法处理全部访问数据，增加几个节点进行横向扩展，这时数据的状态如何平均分配到新增加的节点。</li>
</ul>
</li>
</ul>
</li>
<li><p>方案</p>
<ul>
<li><p>Managed State &amp; Raw State</p>
<ul>
<li>自定义operator用 raw</li>
<li>raw 必须能够转成字节数组</li>
<li>managed，flink自动存储和恢复，并进行内存优化<br>支持已知的数据结构，如 Value、List、Map</li>
</ul>
</li>
<li><p>Managed State</p>
<ul>
<li><p>Keyed State </p>
<ul>
<li><p>每个 Key 对应一个 State</p>
</li>
<li><p>整个程序中没有 keyBy 的过程就没有办法使用KeyedStream。</p>
</li>
<li><p>并发改变时状态重新分配：内置了 2 种分配方式</p>
</li>
<li><p>Keyed State 通过 RuntimeContext 访问，这需要 Operator 是一个 Rich Function。</p>
</li>
<li><p>几种 Keyed State 的差异</p>
<ul>
<li><p>ReducingState 和 AggregatingState 与 ListState 都是同一个父类，但状态数据类型上是单个值</p>
</li>
<li><p> AggregatingState 输入的 IN，输出的是 OUT。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Operator State </p>
<ul>
<li>可以用于所有算子，常用于 Source</li>
<li>一个 Operator 实例对应一个 State</li>
<li>Operator  State 需要自己实现 CheckpointedFunction 或 ListCheckpointed 接口。</li>
<li>支持的数据结构相对较少 </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>使用示例</p>
<ul>
<li><p><a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/StateMachineExample.java">写状态机是如何实现</a></p>
</li>
<li><p>实现的是：首先下订单，订单生成后状态为待付款，当再来一个事件状态付款成功，则事件的状态将会从待付款变为已付款，待发货…</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>状态的保存和恢复</p>
<ul>
<li><p>保存</p>
<ul>
<li>Checkpoint 会定时制作分布式快照</li>
</ul>
</li>
<li><p>恢复</p>
<ul>
<li><p>checkpoint</p>
<ul>
<li>数据源需要支持数据重新发送</li>
<li>两种一致性语义，一种是恰好一次，一种是至少一次</li>
<li>1、把进程或者线程移到 active 的 其他台机器上<br>2、整个作业的所有 Task 都回滚到最后一次成功 Checkpoint 中的状态</li>
</ul>
</li>
<li><p>savepoint</p>
<ul>
<li>手动调整并发，必须要重启作业并会提示 Checkpoint 已经不存在–&gt; 此时savepoint</li>
<li>比较持久，以标准格式存储</li>
<li>允许代码或配置发生改变，恢复需要启动作业手动指定一个路径恢复</li>
</ul>
</li>
</ul>
</li>
<li><p>checkpoint实现</p>
<ul>
<li>运行环境 env.enableCheckpointing 传入间隔时间。越频繁，恢复时追数据就会相对减少，IO 消耗增加。</li>
<li>设置了 Exactly_Once 语义，并且需要 Barries 对齐，这样可以保证消息不会丢失也不会重复。</li>
<li>setMinPauseBetweenCheckpoints 防止 Checkpoint 太过于频繁</li>
<li>setCheckpointTimeout 表示做 Checkpoint 多久超时</li>
<li>setMaxConcurrentCheckpoints</li>
<li>enableExternalizedCheckpoints。默认 Checkpoint 会在整个作业 Cancel 时被删除。Checkpoint 是作业级别的保存点。</li>
</ul>
</li>
<li><p>checkpoint可选的状态存储方式</p>
<ul>
<li><p>MemoryStateBackend</p>
<ul>
<li>构造方法是设置最大的 StateSize，选择是否做异步快照</li>
<li>且需要注意 maxStateSize &lt;= akka.framesize 默认 10 M</li>
<li>Checkpoint 存储在 JobManager 内存中，因此总大小不超过 JobManager 的内存。- 本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。不推荐在生产场景使用。</li>
</ul>
</li>
<li><p>FsStateBackend</p>
<ul>
<li>需要传一个文件路径和是否异步快照</li>
<li>State 依然在 TaskManager 内存中- Checkpoint 存储在外部文件系统（本地或 HDFS）</li>
<li>常规使用状态的作业、例如分钟级窗口聚合或 join、需要开启 HA 的作业。</li>
</ul>
</li>
<li><p>RocksDBStateBackend</p>
<ul>
<li>key/value 的内存存储系统</li>
<li>不支持同步的 Checkpoint</li>
<li>支持增量的 Checkpoint</li>
<li>存储在外部文件系统（本地或 HDFS）</li>
<li>单个 TaskManager 上 State 总量不超过它的内存+磁盘，单 Key 最大 2G，总大小不超过配置的文件系统容量即可</li>
<li>超大状态的作业，例如天级窗口聚合、需要开启 HA 的作业、最好是对状态读写性能要求不高的作业。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h2><ul>
<li>todo</li>
</ul>
<h2 id="watermark"><a href="#watermark" class="headerlink" title="watermark"></a>watermark</h2><ul>
<li>todo</li>
</ul>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><ul>
<li>todo</li>
</ul>
<h2 id="CET"><a href="#CET" class="headerlink" title="CET"></a>CET</h2><ul>
<li>todo</li>
</ul>
<h2 id="概念-amp-角色"><a href="#概念-amp-角色" class="headerlink" title="概念&amp;角色"></a>概念&amp;角色</h2><ul>
<li>TaskManager &amp; slot<ul>
<li>每一个 TaskManager 都是一个JVM进程</li>
<li>每个task slot表示TaskManager拥有资源的一个固定大小的子集</li>
<li>将其管理的内存均分给各个slot</li>
<li>一个TaskManager一个slot时，那么每个task group运行在独立的JVM中</li>
<li>多个slot时，多个subtask可以共同享有一个JVM</li>
<li>在同一个JVM进程中的task将共享TCP连接和心跳消息，也可能共享数据集和数据结构，从而减少每个task的负载。</li>
</ul>
</li>
</ul>
<ul>
<li>todo</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li><a href="https://www.infoq.cn/article/vgkza-s9fmbgabp71pgh">状态管理及容错机制</a></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>流处理</tag>
      </tags>
  </entry>
  <entry>
    <title>hive</title>
    <url>/2021/03/03/hive/</url>
    <content><![CDATA[<br>


<ul>
<li><p>组件架构：<br>hiveserver2（beeline）,hive,metadb</p>
<span id="more"></span>
<blockquote>
<p>Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.</p>
</blockquote>
</li>
<li><p>连接hiveserver2  <br>GUI CLI JDBC (beeline)</p>
</li>
<li><p>数据源<br>用kafka，sqoop等获得data，放入hdfs，这些数据各种结构都有。<br>关系数据库的表，MongoDB 或json数据，或日志</p>
</li>
<li><p>执行hql<br>背后运行的是mapreduce or Tez jobs(类似于pig latin脚本执行pig)<br><code>insert into test values(&quot;wangyuq&quot;,&quot;123&quot;);</code><br>查看tracking url</p>
</li>
<li><p>stage<br>将你的数据移到目的位置之前，将会staing 那儿一段时间。staging文件最终丢弃。</p>
</li>
<li><p>比对<br>pig是对非结构化数据处理的好的etl。<br>hive不是关系数据库，只是维护存储在HDFS的数据的metadata，使得对大数据操作就像sql操作表一样，只不过hql和sql稍有出入。使我们能用sql来执行mr。可以对hdfs数据进行query。<br>hive使用metastore存表。hive默认derby但是可自定义更换。</p>
</li>
<li><p>劣<br>hive不能承诺优化，只是简单，因此hive不能支持实时，性能差<br>index view有限制（partition bucket 弥补）<br>和sql 的datatype不完全一样</p>
</li>
<li><p>与hdfs关系<br>hdfs里有hive，data在hdfs上，schema在metastore里。<br>load语句： 将hdfs搬运到hive，hdfs不再有该数据。只是将真正的data转到了hive目录下。</p>
</li>
</ul>
<ul>
<li> Making Multiple Passes over the Same Data<blockquote>
<p>Hive has a special syntax for producing multiple aggregations from a single pass through a source of data, rather than rescanning it for each aggregation. This change can save considerable processing time for large input data sets. </p>
</blockquote>
</li>
</ul>
<p>因此如下方式更加高效,并且可开启并行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM pv_users</span><br><span class="line">    INSERT OVERWRITE TABLE pv_gender_sum</span><br><span class="line">        SELECT pv_users.gender, count_distinct(pv_users.userid)</span><br><span class="line">        GROUP BY pv_users.gender</span><br><span class="line"></span><br><span class="line">    INSERT OVERWRITE DIRECTORY &#39;&#x2F;user&#x2F;data&#x2F;tmp&#x2F;pv_age_sum&#39;</span><br><span class="line">        SELECT pv_users.age, count_distinct(pv_users.userid)</span><br><span class="line">        GROUP BY pv_users.age;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.exec.parallel&#x3D;true;   &#x2F;&#x2F;打开任务并行执行</span><br><span class="line">set hive.exec.parallel.thread.number&#x3D;16; &#x2F;&#x2F;同一个sql允许最大并行度，默认为8。</span><br></pre></td></tr></table></figure>

<ul>
<li><p>日期处理<br>查看N天前的日期：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select from_unixtime(unix_timestamp(&#39;20111102&#39;,&#39;yyyyMMdd&#39;) - N*86400,&#39;yyyyMMdd&#39;) from t_lxw_test1 limit 1;  </span><br></pre></td></tr></table></figure>
<p>获取两个日期之间的天数/秒数/分钟数等等：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select ( unix_timestamp(&#39;2011-11-02&#39;,&#39;yyyy-MM-dd&#39;)-unix_timestamp(&#39;2011-11-01&#39;,&#39;yyyy-MM-dd&#39;) ) &#x2F; 86400  from t_lxw_test limit 1; </span><br></pre></td></tr></table></figure></li>
<li><p>left outer join</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--query 1</span><br><span class="line">select count(id) from  </span><br><span class="line">(select id  from   a  left outer join   b  </span><br><span class="line">on a.id&#x3D;b.id and  b.date&#x3D;&#39;2017-10-27&#39;    </span><br><span class="line">where to_date(a.adate) &gt;&#x3D; &#39;2017-10-27&#39;   and a.date&#x3D;&#39;2017-07-24&#39;  </span><br><span class="line">) a </span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--query 2</span><br><span class="line">select count(id) from  </span><br><span class="line">(select id  from   a  left outer join   b  </span><br><span class="line">on a.id&#x3D;b.id and  b.date&#x3D;&#39;2017-10-27&#39;  and a.date&#x3D;&#39;2017-07-24&#39;  </span><br><span class="line">where to_date(a.adate) &gt;&#x3D; &#39;2017-10-27&#39;  </span><br><span class="line">) a </span><br></pre></td></tr></table></figure>
<p>区别？where 后面跟的是过滤条件，query 1 中的a.date=’2017-07-24’, 在table scan之前就会Partition Pruner 过滤分区，所以只有’2017-07-24’下的数据会和b进行join。<br>而query 2中会读入所有partition下的数据，再和b join，并且根据join的关联条件只有a.date=’2017-07-24’  的时候才会真正执行join，其余情况下又由于是left outer join, 右面会留NULL</p>
</li>
<li><p><a href="http://beadooper.com/?page_id=313">配置文件</a></p>
</li>
<li><p> <a href="http://superlxw1234.iteye.com/blog/1751216">正则</a><br>java中的正则匹配即可:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name rlike &#39;^[\\u4e00-\\u9fa5]+$&#39;</span><br><span class="line">select mobile from phone where mobile rlike &#39;^\\d+$&#39; ;  </span><br></pre></td></tr></table></figure></li>
<li><p><a href="http://superlxw1234.iteye.com/blog/1582880">控制hive任务中的map数和reduce数</a></p>
</li>
<li><p><a href="https://github.com/hbutani/SQLWindowing"> SQLWindowing</a></p>
</li>
<li><p><a href="http://blog.csdn.net/yeweiouyang/article/details/42082663">hdfs目录创建hive表,指定分区</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE if not exists push_log(</span><br><span class="line">     hostid STRING, dayid STRING</span><br><span class="line">     plmn STRING)</span><br><span class="line"> COMMENT &#39; log table&#39;</span><br><span class="line"> PARTITIONED BY (hostid STRING, dayid STRING) </span><br><span class="line"> ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\001&#39;</span><br><span class="line"> STORED AS TEXTFILE</span><br><span class="line"> LOCATION &#39;&#x2F;user&#x2F;data&#x2F;push&#39;;</span><br><span class="line">alter table push_log add partition(hostid&#x3D;&#39;$hostid&#39;, dayid&#x3D;&#39;$dayid&#39;) location &#39;&#x2F;user&#x2F;data&#x2F;push&#x2F;$hostid&#x2F;$dayid&#39;;</span><br></pre></td></tr></table></figure>
<p>testtext 数据<code>wer 46 weree   78 wer 89 rr  89</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table d_part(name string)  partitioned by(value string) row format delimited fields terminated by &#39;\t&#39;  lines terminated by &#39;\n&#39; stored as textfile;</span><br><span class="line"></span><br><span class="line">set hive.exec.dynamic.partition&#x3D;true;</span><br><span class="line">set hive.exec.dynamic.partition.mode&#x3D;nonstrick;</span><br><span class="line"></span><br><span class="line">insert overwrite table d_part partition(value) select name,addr as value from testtext;</span><br><span class="line"></span><br><span class="line">select * from d_part;</span><br><span class="line">show partitions d_part;</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; create table d_part2(</span><br><span class="line">    &gt; name string</span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; partitioned by(value string,dt string)</span><br><span class="line">    &gt; row format delimited fields terminated by &#39;\t&#39; </span><br><span class="line">    &gt; lines terminated by &#39;\n&#39;</span><br><span class="line">    &gt; stored as textfile;</span><br><span class="line">hive&gt; insert overwrite table d_part2 partition(value,dt)</span><br><span class="line">    &gt; select &#39;test&#39; as name,  </span><br><span class="line">    &gt; addr as value,</span><br><span class="line">    &gt; name as dt</span><br><span class="line">    &gt; from testtext;</span><br><span class="line">show partitions d_part2;</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="http://superlxw1234.iteye.com/blog/1568739">hive中转义特殊字符</a></li>
<li>schema tool<br><a href="https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive_schema_tool.html">https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive_schema_tool.html</a></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>git命令</title>
    <url>/2021/04/13/git%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>graphsage</title>
    <url>/2021/03/08/graphsage/</url>
    <content><![CDATA[<h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h4><ul>
<li>卷积<br>数学上卷积的两个例子：</li>
</ul>
<p>一个对象（吃冰淇凌）对一个系统（体重）的作用效果满足线性原理、累加原理。该对象对这个系统连续作用了一段时间后，求该系统的状态。这个时候，一个卷积就可以求出来了！</p>
<span id="more"></span>

<br>
第二个例子：
![卷积](卷积.jpg)


<hr>
<p>DL中的卷积：</p>
<p>CNN卷积本质是，共享参数的filter过滤器，像素点加权构成feature map 实现特征提取。<br>a）平滑滤波 b）边缘提取，很容易通过设计特定的“卷积核”，然后将其与像素矩阵的对应元素（不进行旋转）相乘得到。<br>a）就是将中心像素值与周围临近的像素进行平均，自然就能“削峰填谷”，实现平滑处理<br>b) 中心像素复制n份，减去周围n个临近的像素值。相近的则减为0，边缘才被留下。<br>卷积神经网络中“卷积”，是为了提取图像的特征，其实只借鉴了数学卷积中“加权求和”的特点。</p>
<ul>
<li><p>为什么需要GCN<br>CNN LSTM等 对非欧几里得空间数据(eg：社交网络、信息网络等)进行处理上却存在一定的局限性。<br>用GCN：拓扑图中每个node相邻的个数不同，不能用同样大小的filter进行平移提取feature。任何数据在赋范空间内都可以建立拓扑关联，如谱聚类。GCN是区别于CV NLP的任务的模型。</p>
</li>
<li><p>图学习任务<br>1、图节点分类任务：图中每个节点都有对应的特征，当我们已知一些节点的类别的时候，可以设计分类任务针对未知节点进行分类。我们接下来要介绍的 GCN、GraphSAGE、GAT模型都是对图上的节点分类。<br>2、图边结构预测任务：图中的节点和节点之间的边关系可能在输入数据中能够采集到，而有些隐藏的边需要我们挖掘出来，这类任务就是对边的预测任务，也就是对节点和节点之间关系的预测。<br>3、图的分类：对于整个图来说，我们也可以对图分类，图分类又称为图的同构问题，基本思路是将图中节点的特征聚合起来作为图的特征，再进行分类。</p>
</li>
</ul>
<p>如：<br>1、节点分类—反欺诈：因为图中每个节点都拥有自己的特征信息。通过该特征信息，我们可以构建一个风控系统，如果交易节点所关联的用户 IP 和收货地址与用户注册 IP 和注册地址不匹配，那么系统将有可能认为该用户存在欺诈风险。<br>2、边结构预测—商品推荐：图中每个节点都具有结构信息。如果用户频繁购买某种类别商品或对某种类别商品评分较高，那么系统就可以认定该用户对该类商品比较感兴趣，所以就可以向该用户推荐更多该类别的商品。</p>
<ul>
<li>拉普拉斯矩阵<br><img src="/2021/03/08/graphsage/laplacian.png" alt="laplacian"></li>
</ul>
<hr>
<ul>
<li><p>GCN主要贡献<br>这篇文章的主要贡献是为图半监督分类任务设计了一个简单并且效果好的神经网络模型，这个模型由谱图卷积(spectral graph convolution)的一阶近似推导而来，具有理论基础。</p>
</li>
<li><p>GCN学习策略</p>
</li>
</ul>
<p><img src="/2021/03/08/graphsage/GCN%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5.png" alt="GCN学习策略"></p>
<ul>
<li><p>多层图卷积网络(Graph Convolutional Network, GCN)的逐层传播公式<br><img src="/2021/03/08/graphsage/%E9%80%90%E5%B1%82%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F.png" alt="逐层传播公式"></p>
</li>
<li><p>谱图卷积(Spectral Graph Convolutions)</p>
</li>
<li><p>逐层线性模型</p>
</li>
<li><p>半监督学习节点分类</p>
</li>
<li><p>传播公式解释</p>
</li>
</ul>
<p>todo</p>
<h4 id="graphsage"><a href="#graphsage" class="headerlink" title="graphsage"></a>graphsage</h4><p>1、比GCN进步之处<br>GCN的训练方式需要将邻接矩阵和特征矩阵一起放到内存或者显存里，在大规模图数据上是不可取的。其次，GCN在训练时需要知道整个图的结构信息(包括待预测的节点), 这在现实某些任务中也不能实现(比如用今天训练的图模型预测明天的数据，那么明天的节点是拿不到的)。GraphSAGE的出现就是为了解决这样的问题。</p>
<p>GraphSAGE采用了采样的机制，使得图模型可以应用到大规模的图结构数据中，是目前几乎所有工业上图模型的雏形。<br>进一步：每个节点这么多邻居，采样能否考虑到邻居的相对重要性呢，或者我们在聚合计算中能否考虑到邻居的相对重要性? </p>
<p>2、inductive 还是 transductive<br>如果训练时用到了测试集或验证集样本的信息(或者说，测试集和验证集在训练的时候是可见的), 我们把这种学习方式叫做transductive learning, 反之，称为inductive learning. 显然，我们所处理的大多数机器学习问题都是inductive learning, 因为我们刻意的将样本集分为训练/验证/测试，并且训练的时候只用训练样本。然而，在GCN中，训练节点收集邻居信息的时候，用到了测试或者验证样本，所以它是transductive的。</p>
<p>3、简单过程<br>思路一个网络里，我们知道部分点的分类，我们希望通过各种方法，知道其他未知点的属性。解决对未知节点的泛化问题。</p>
<p>GraphSAGE是一个inductive框架，在具体实现中，训练时它仅仅保留训练样本到训练样本的边。inductive learning 的优点是可以利用已知节点的信息为未知节点生成Embedding. GraphSAGE 取自 Graph SAmple and aggreGatE, SAmple指如何对邻居个数进行采样。aggreGatE指拿到邻居的embedding之后如何汇聚这些embedding以更新自己的embedding信息。<br><br></p>
<p><img src="/2021/03/08/graphsage/visual_graphsage.webp" alt="visual_graphsage"></p>
<hr>
<p>4、具体步骤及伪代码</p>
<p>步骤：<br>1.对邻居采样<br>2.采样后的邻居embedding传到节点上来，并使用一个聚合函数聚合这些邻居信息以更新节点的embedding<br>3.根据更新后的embedding预测节点的标签</p>
<p><img src="/2021/03/08/graphsage/graphsage.png" alt="graphsage"></p>
<br>


<p>描述：初始化各个节点emb，对每个节点emb采样邻居的emb，对邻居进行聚合，将自己的emb和聚合后的emb做一个非线性变换，更新为自己的emb。</p>
<p>5、K的解释</p>
<p>K：聚合器数量，权重矩阵数量，层数。</p>
<p><img src="/2021/03/08/graphsage/K%E8%A7%A3%E9%87%8A.jpg" alt="K解释"></p>
<br>

<p>6、采样<br>定长抽样。定义邻居个数，进行有放回的重采样/负采样达到个数。每个node采样个数一致，为了把多个邻居拼成tensor放入gpu批量训练。</p>
<p>7、聚合器<br>平均效果最好。<br>也有用lstm聚合器和pooling聚合器，</p>
<p>8、学习过程<br>有监督：交叉熵<br>无监督：学习出来的相邻node的emb应该尽可能接近。此时的loss如下。<br><img src="/2021/03/08/graphsage/loss.jpg" alt="loss"><br><img src="/2021/03/08/graphsage/loss%E8%A7%A3%E9%87%8A.jpg" alt="loss解释"></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>juc</title>
    <url>/2021/04/05/juc/</url>
    <content><![CDATA[<p>线程池、各种锁、高并发解决方案相关</p>
<span id="more"></span>


<ul>
<li><p>线程池</p>
<ul>
<li>阻塞队列<ul>
<li>任务队列中没有任务时阻塞获取任务的线程，使得线程进入wait状态，释放cpu资源。</li>
<li>有任务时才唤醒对应线程从队列中取出消息进行执行。</li>
</ul>
</li>
<li>任务类型<ul>
<li>CPU密集型任务<ul>
<li>尽量使用较小的线程池，一般为CPU核心数+1</li>
</ul>
</li>
<li>IO密集型任务<ul>
<li>可以使用稍大的线程池，一般为2*CPU核心数。</li>
</ul>
</li>
<li>混合<ul>
<li>分别用不同的线程池去处理</li>
</ul>
</li>
</ul>
</li>
<li>4类<ul>
<li>newCachedThreadPool, newFixedThreadPool, newScheduledThreadPool, newSingleThreadExecutor</li>
</ul>
</li>
</ul>
</li>
<li><p>乐观锁 悲观锁</p>
<ul>
<li>悲观<ul>
<li>db里的行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。</li>
</ul>
</li>
<li>乐观<ul>
<li>在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制</li>
<li>乐观锁适用于多读的应用类型，这样可以提高吞吐量</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
        <category>java</category>
        <category>并发</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>linux&amp;mac命令</title>
    <url>/2021/04/13/linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<br> 

<p>记录常见linux mac命令</p>
<span id="more"></span>

<br> 

<h4 id="端口、进程"><a href="#端口、进程" class="headerlink" title="端口、进程"></a>端口、进程</h4><p>1、查看进程号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef | grep 进程名</span><br></pre></td></tr></table></figure>



<p>2、查看端口被哪个进程监听</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo lsof -i :端口</span><br></pre></td></tr></table></figure>



<p>3、查看进程监听的端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo lsof -nP -p 进程号 | grep LISTEN</span><br><span class="line">sudo lsof -nP | grep LISTEN | grep 进程号</span><br></pre></td></tr></table></figure>



<p>4、查看监听端口的进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo lsof -nP | grep LISTEN | grep 端口号</span><br></pre></td></tr></table></figure>



<p>5、看到一个新的方法（MacOS统计TCP/UDP端口号与对应服务）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;### TCP LISTEN ###&quot;</span><br><span class="line">lsof -nP -iTCP -sTCP:LISTEN</span><br></pre></td></tr></table></figure>



<p>6、关于环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Mac系统的环境变量，加载顺序为：</span><br><span class="line">&#x2F;etc&#x2F;profile &#x2F;etc&#x2F;paths ~&#x2F;.bash_profile ~&#x2F;.bash_login ~&#x2F;.profile ~&#x2F;.bashrc</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;profile和&#x2F;etc&#x2F;paths是系统级别的，系统启动就会加载，后面几个是当前用户级的环境变量。后面3个按照从前往后的顺序读取，如果&#x2F;.bash_profile文件存在，则后面的几个文件就会被忽略不读了，如果&#x2F;.bash_profile文件不存在，才会以此类推读取后面的文件。~&#x2F;.bashrc没有上述规则，它是bash shell打开的时候载入的。</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;bashrc （一般在这个文件中添加系统级环境变量）</span><br><span class="line">全局（公有）配置，bash shell执行时，不管是何种方式，都会读取此文件</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;acb1f062a925</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>mac</tag>
        <tag>cheetsheet</tag>
      </tags>
  </entry>
  <entry>
    <title>ml基础</title>
    <url>/2021/04/05/ml%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<br>

<p>ml知识点</p>
<span id="more"></span>

<ul>
<li><p>softmax和logistic</p>
<ul>
<li>sigmoid: 导数在(0,1/4)，当x&gt;1/4或者&lt;-1/4时，导数非常小 1/1+e^(-x)</li>
<li>当分类数为2时，可以推导出两个一致</li>
<li>softmax建模使用的分布是多项式分布，而logistic则基于二项式分布</li>
<li>softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类</li>
<li>多个logistic回归进行多分类，”苹果”这个词语既属于”水果”类也属于”3C”类别。</li>
</ul>
</li>
<li><p>二项式和多项式分布</p>
<ul>
<li>二项<ul>
<li>二项分布是n重伯努利试验中正例发生次数的离散概率分布</li>
<li><code>import numpy; a = numpy.random.binomial(n=10, p=0.7, size = 1)</code></li>
</ul>
</li>
<li>多项<ul>
<li>二项分布是单变量分布，而多项分布是多变量分布</li>
<li>多项分布的例子是扔骰子，每次试验有多种可能，进行多次试验，多项分布描述的是每种可能发生次数的联合概率分布</li>
<li><code>a = numpy.random.multinomial(n=10, pvals=[0.2,0.4,0.4], size = 1)</code></li>
</ul>
</li>
</ul>
</li>
<li><p>逻辑回归梯度下降伪代码</p>
<ul>
<li><a href="https://blog.csdn.net/qq_33391629/article/details/108711228">https://blog.csdn.net/qq_33391629/article/details/108711228</a></li>
</ul>
</li>
<li><p>最小均方误差和最小二乘</p>
<ul>
<li>算术平均数可以让均方误差误差最小</li>
<li>如果误差的分布是正态分布，那么最小二乘法得到的就是最有可能的值。即：最小二乘法是概率密度(误差的概率密度)是高斯分布的最大似然估计的状况</li>
<li><a href="https://www.zhihu.com/question/396712527/answer/1243682221">线性回归模型用最小二乘法得到的估计的均方误差怎么得到</a></li>
</ul>
</li>
<li><p>并行直方图</p>
<ul>
<li>分裂节点时，数据在block中按列存放，而且已经经过了预排序，因此可以并行计算，即同时对各个属性遍历最优分裂点</li>
</ul>
</li>
<li><p>特征点分割</p>
<ul>
<li>树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点</li>
<li>当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，XGBoost采用了一种近似的算法</li>
<li>近似的算法对每维特征加权分位进行分桶，具体的算法利用到了损失函数关于待求树的二阶导数。</li>
</ul>
</li>
<li><p>偏导数与残差</p>
<ul>
<li>偏导数的计算可以确定每个相关数据对最终结果的局部/全局最小值的一个梯度。<br>顺着梯度进行调整，会让结果逐步趋向局部/全局最小值。</li>
<li>gbdt是用梯度来近似残差：损失函数L对当前所学模型F预测值的负梯度，所以模型的更新是沿着梯度下降方向的。</li>
</ul>
</li>
</ul>
<ul>
<li><p>auc计算</p>
<ul>
<li>(auc计算)[<a href="https://blog.csdn.net/qq_22238533/article/details/78666436]">https://blog.csdn.net/qq_22238533/article/details/78666436]</a></li>
<li>为啥auc<ul>
<li>随机抽出一对样本（一个正样本，一个负样本），然后用训练得到的分类器来对这两个样本进行预测，预测得到正样本的概率大于负样本概率的概率。</li>
</ul>
</li>
</ul>
</li>
<li><p>模型评估</p>
<ul>
<li>精确率<ul>
<li>分类正确的正样本 占 判定为正样本的总数 。</li>
</ul>
</li>
<li>召回率<ul>
<li>分类正确的正样本 占 真正正样本总数 。</li>
</ul>
</li>
<li>问题<ul>
<li>提高精确率，会更倾向于 “更有把握才把样本判定为正” ，此时保守而降低了召回率</li>
<li>即排序的TOPN，精确率很高，但是会漏掉 很多其他正样本。导致用户找不到自己想要的。</li>
<li>综合考虑<ul>
<li>PR 曲线<ul>
<li>x召回，y精确。</li>
</ul>
</li>
<li>f1 score<ul>
<li>调和平均值</li>
</ul>
</li>
<li>ROC 曲线</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql性能优化</title>
    <url>/2021/04/05/mysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<br>

<p>background知识、性能优化</p>
<span id="more"></span>
<ul>
<li><p>MyISAM 与 InnoDB </p>
<ul>
<li>InnoDB 支持事务、能回滚，MyISAM 不支持</li>
<li>MyISAM 适合查询以及插入为主的应用，修改多或者要安全则innodb</li>
<li>InnoDB 中必须包含只有自增长字段的索引，MyISAM可以和其他字段一起建立联合索引</li>
<li>清空整个表时，InnoDB 是一行一行的删除，效率非常慢。MyISAM 则会重建表</li>
<li>InnoDB 支持外键，MyISAM 不支持   <ul>
<li>外键<ul>
<li>定义外键约束，关系数据库可以保证无法插入无效的数据</li>
<li>可以把数据与另一张表关联起来，这种列称为外键</li>
<li>外键约束会降低数据库的性能，追求速度并不设置外键约束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>多对多关系：通过一个中间表，关联两个一对多关系</p>
</li>
</ul>
<ul>
<li><p>分库分表</p>
<ul>
<li><p>原因</p>
<ul>
<li>单表数据大，sql效率低</li>
<li>并发高，单库撑不住；磁盘使用高</li>
</ul>
</li>
<li><p>分表</p>
<ul>
<li>经常读取和不经常读取的字段分开</li>
<li>把一个大的用户表分拆为用户基本信息表user_info和用户详细信息表user_profiles。大部分时候，只需要查询user_info表，提高了查询速度。</li>
</ul>
</li>
<li><p>中间件：proxy方案 or client层方案 sharding-jdbc mycat</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>crud</p>
</li>
<li><p>分布式id生成</p>
</li>
</ul>
<ul>
<li><p>垂直、水平拆分<br>中间件解决对某个字段值的自动路由，路由到相应的库、表<br>range、hash</p>
</li>
<li><p>迁移方案<br>数据库中间件的调研学习、设计分库分表方案、测试环境分库代码的正常读写<br>开始迁移：<br>1部署分库策略 2双写 3导数的时候，判断最后修改时间，确保只能新数据覆盖老数据 4自动校验，反复读写，直到新旧库数据一致 5停掉老库，部署分库分表的代码</p>
</li>
</ul>
<ul>
<li>动态扩容缩容分库分表<br>一般都够用：32库，每个库1500个写并发，qps可以承受4.8w。加上一个MQ削峰，qps接受8w，每秒消费5w。一个库32张表，1024表，每个表500w，一个mysql放50亿条数据。</li>
</ul>
<p>1确定机器数量、每台机器多少个库、多少表 2路由规则 3机器上装好mysql 4导数工具导数 5修改配置调整数据库服务器地址 6重发系统上线</p>
<ul>
<li><p>分库分表后的全局id<br>1并发不高，qps几百–直接往一个库写入，获得自增的id；起一个服务，专门地获取最大的id，返回一批新的id<br>2sequence+固定步长<br>3uuid –不具有有序性，b+树过多随机读写<br>4系统时间 + 业务数据<br>5雪花算法<br>第一个bit是0。41bit-时间，10bit是工作机器id，12bits序列号<br>同一ms内有12bits可表达的id</p>
</li>
<li><p>mysql读写分离<br>Case1在binlog没有被从拉到本地的中继日志中、串行地执行relaylog中的命令时，master宕机<br>Solution半同步，至少一个从返回ack才认为写操作完成。<br>Case2由于串行重发， 会有延迟几十到百毫秒不等<br>Solution2并行同步，从开启多线程，并行重放日志，库级别并行<br>mysql中查看延迟，严重解决：<br>分库、并行复制、新插入的数据查询不到时处理、查询直连主库</p>
</li>
<li><p>单库单表和分库分表的join</p>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql索引</title>
    <url>/2021/04/05/mysql%E7%B4%A2%E5%BC%95/</url>
    <content><![CDATA[<br>

<p>性能优化之 索引</p>
<span id="more"></span>


<ul>
<li><p>优点</p>
<ul>
<li>分组、排序操作</li>
<li>加速表与表之间的连接</li>
</ul>
</li>
<li><p>分类</p>
<ul>
<li><p>普通索引</p>
<ul>
<li>允许重复值和空值</li>
</ul>
</li>
<li><p>唯一索引</p>
<ul>
<li>必须唯一，允许空值</li>
<li>组合索引，则列值的组合必须唯一</li>
</ul>
</li>
<li><p>主键索引</p>
<ul>
<li>不允许值重复或者值为空</li>
</ul>
</li>
<li><p>空间索引</p>
<ul>
<li>空间数据类型的字段建立的索引</li>
</ul>
</li>
<li><p>全文索引</p>
<ul>
<li> CHAR、VARCHAR 或 TEXT 类型的列上创建</li>
<li>只有 MyISAM 存储引擎支持全文索引</li>
</ul>
</li>
</ul>
</li>
<li><p>位图索引</p>
<ul>
<li>　“select * from table where Gender=‘男’ and Marital=“未婚”;”<br>  首先取出男向量10100…，然后取出未婚向量00100…，<br>  将两个向量做and操作，这时生成新向量0010</li>
</ul>
</li>
<li><p>B与B+ </p>
<ul>
<li>B：树内的每个节点都存储数据；叶子节点之间无指针连接</li>
<li>B+：数据只出现在叶子节点；所有叶子节点增加了一个链指针</li>
</ul>
</li>
<li><p>聚集索引、非聚集索引 </p>
<ul>
<li>区别<ul>
<li>聚集：行中数据的物理顺序与键值的逻辑（索引）顺序相同</li>
<li>非聚集索引叶子节点上存储了索引字段自身值和主键索引</li>
</ul>
</li>
<li>聚集相关<ul>
<li>聚簇索引默认就是主键索引；第一个唯一非空索引被作为聚集索引；内部会生成一个隐藏的主键</li>
<li>uuid作为主键，写入性能低了</li>
</ul>
</li>
<li>非聚集<ul>
<li>索引不能随意增加。在做写库操作的时候，需要同时维护这几颗树的变化，导致效率降低</li>
<li>回表或者二次查询<ul>
<li>使用聚集索引查询可以直接定位到记录，而普通索引通常需要扫描两遍索引树</li>
<li>解决<ul>
<li>联合索引</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>联合索引</p>
<ul>
<li>最左匹配</li>
</ul>
</li>
<li><p>查询失效</p>
<ul>
<li>条件<ul>
<li>最左</li>
<li>OR条件中的每个列都加上索引</li>
<li>not in</li>
<li>前置通配符</li>
<li>is null</li>
</ul>
</li>
<li>函数<ul>
<li>where age-1=17</li>
<li>内置函数，索引失效 </li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title>node2vec</title>
    <url>/2021/04/05/node2vec/</url>
    <content><![CDATA[<ul>
<li><p>node2vec的随机游走</p>
<span id="more"></span>
<ul>
<li><p><a href="https://blog.csdn.net/rover2002/article/details/106760664">Alias Method: 非均匀随机抽样算法</a></p>
<ul>
<li>空间换时间的方法，在常数时间内，完成非均匀到均匀采样的映射。</li>
<li>主要思想： 概率转换为面积，然后把面积填到其他地方，总体长方形面积最小。由于有填到其他地方的，就有alias。这样之后再掷筛子，随机。</li>
</ul>
</li>
<li><p>跳转概率</p>
<ul>
<li><p>Node2vec 的跳转概率为 自定义的权重*有向边权重。</p>
</li>
<li><p>自定义权重，考虑了当前点vi的所有相关点 和 vi前一个node的距离。距离为0则概率1/p(即重复访问刚刚访问过的node)，为1则1，为2则1/q。</p>
</li>
<li><p>根据论文做法，对p q在{0.25,0.5,1,2,4}中进行grid search。</p>
</li>
<li><p>参数p控制重复访问刚刚访问过的顶点的概率。若p较大，则访问刚刚访问过的顶点的概率会变低。</p>
</li>
<li><p>参数q控制着游走是向外还是向内：<br>  若q&gt;1，随机游走倾向于访问和上一次的t接近的顶点(偏向BFS)；<br>  若q&lt;1，倾向于访问远离t的顶点(偏向DFS)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>python</title>
    <url>/2021/04/05/python/</url>
    <content><![CDATA[<br>

<p>语言、语法糖相关</p>
<span id="more"></span>


<ul>
<li><p>python2 3 区别</p>
<ul>
<li><p>map filter，由function变为class，前者返回列表，后者为object</p>
</li>
<li><p>print由命令变为函数</p>
</li>
<li><p>编码由ASCII变为utf8</p>
</li>
<li><p>xrange</p>
<ul>
<li>要生成很大的数字序列的时候，用xrange会比range性能优很多，因为不需要一上来就开辟一块很大的内存空间。</li>
<li>range创建列表，xrange是生成器</li>
<li>3只保留了生成器的方式并命名为range</li>
</ul>
</li>
<li><p>除法 1/2 = 0 0.5</p>
</li>
</ul>
</li>
<li><p>滑动窗口算法 </p>
<ul>
<li>单调栈</li>
<li><a href="https://leetcode-cn.com/problems/sliding-window-maximum/">https://leetcode-cn.com/problems/sliding-window-maximum/</a></li>
</ul>
</li>
<li><p>abc</p>
<ul>
<li>注解abstractmethod，抽象基类</li>
</ul>
</li>
<li><p>标记清除</p>
<ul>
<li>对执行删除（-1）后的每个引用-1，为0的放到死亡容器，否则放到存活容器</li>
<li>循环存活容器，复活死亡中的变量放到存活容器内</li>
<li>删除死亡容器内的所有对象</li>
</ul>
</li>
<li><p>分代</p>
<ul>
<li>新创建的对象做为0代。每执行一个【标记-删除】，存活的对象代数就+1</li>
<li>代数越高的对象（存活越持久的对象），进行【标记-删除】的时间间隔就越长</li>
<li>一个对象10次检测都没给它干掉, 就认定这个对象一定很长寿, 就减少这货的”检测频率”</li>
</ul>
</li>
<li><p>触发垃圾回收</p>
<ul>
<li>调用gc.collect()</li>
<li>GC达到阀值时</li>
<li>程序退出时</li>
</ul>
</li>
<li><p>intern机制</p>
</li>
<li><p>copy deepcopy</p>
<ul>
<li>复制的值是不可变对象（数值，字符串，元组）,对象的id值与等式左边的id值相同。</li>
<li>复制的值是可变对象（列表和字典）<ul>
<li>浅拷贝两种情况：<ul>
<li>复制的 对象中无 复杂 子对象，浅复制的值改变也并不会影响原来的值反之也不影响</li>
<li>复制的对象中有 复杂 子对象， 改变原来的值 中的复杂子对象的值 ，会影响浅复制的值</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>cookie和session</p>
<ul>
<li>session 在服务器端，cookie 在客户端（浏览器）</li>
<li>session id 是存在 cookie 中</li>
<li>浏览器禁用了 cookie ，同时 session 也会失效</li>
<li>cookie安全性比session差</li>
</ul>
</li>
<li><p>正则</p>
<ul>
<li>正则表达式匹配中，（.<em>）和（.</em>?）匹配区别？<ul>
<li>贪婪、非贪婪</li>
</ul>
</li>
<li>正则re.complie<ul>
<li>re.compile是将正则表达式编译成一个对象，加快速度，并重复使用</li>
</ul>
</li>
</ul>
</li>
<li><p>动态类型dynamic typing</p>
<ul>
<li>将string传入mod，compilier并不会报错，仍然是原来compile出来的步骤。</li>
<li>mod(“%s%s”, (“py”,”thon”) )结果将是python，在c中，遇到BINARY_MOULO指令操作，如果是string类型则直接调用PyString_Format()。</li>
</ul>
</li>
</ul>
<ul>
<li><p>一切发生在运行时、命名空间。   </p>
<ul>
<li><p>命名空间分模块、类、方法。</p>
</li>
<li><p>编译只是从将代码在运行时转成成code object，当执行时转成function object。</p>
</li>
<li><p>def是一个assignment function(将return 的结果assign给变量)，遇到def 的时候实则就是call这个function并对参数进行assign。</p>
</li>
<li><p>类在创建时就开始执行，在一个字典所表示的命名空间中执行。这个命名空间用来创建类对象。</p>
</li>
<li><p>都是 对象/引用。变量只是名称，不是容器。字典中将名称映射到对象上。</p>
</li>
<li><p>三个scopes，local，global:module，builtin</p>
</li>
<li><p>not pthon object hooks：1，赋值，赋值将改变命名空间，而非对象自身 2，类型检查 3，is运算 4，and or not 都是布尔运算 5，调用，需要实现getattr来返回属性对象，该属性对象在call时执行某方法</p>
</li>
<li><p>如果要对object 移动，需要delete并清楚所有的references。</p>
</li>
<li><p>弱引用是当object删除之后被通知，即callback。</p>
</li>
<li><p>互相引用的reference cycle–&gt;需要cycle gc</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Cython PyPy</p>
<ul>
<li>Cpython中，PyObject都是以struct实现，包含type pointer、reference counting或其他C type。PyType struct有对type的一些描述信息。</li>
</ul>
</li>
<li><p>常用到的文本提取，任意字符串</p>
<ul>
<li>.* .*?</li>
</ul>
</li>
<li><p>编译 解释</p>
<ul>
<li>compiler部分很少，interpreter工作量远远多。</li>
<li>Python interpreter是一个virtual machine(模拟物理机)，是个stack machine(操作stacks)，和register machine(从内存不同位置读写)不同。</li>
<li>且是byte interpreter，input是instruction sets，元素为byte codes，lexing&amp;parsing将python codes 转为byte codes。相当于C codes和assembly codes。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>redis</title>
    <url>/2021/01/31/redis/</url>
    <content><![CDATA[<h2 id="相关笔记"><a href="#相关笔记" class="headerlink" title="相关笔记"></a>相关笔记</h2><p>1、从自己拉还是定期拉 还是master发<br>第一次全量复制的时候，从发命令后主发rdb和写缓存<br>续传的时候：master维护backlog里有offset、master run id,从发送offset给主，如果没有则全量<br>其他情况，master会异步发送</p>
<span id="more"></span>
<p>2、单机写 读的并发量、集群并发量<br>几万、十万qps、几十万<br>单个value 1g</p>
<p>3、哨兵没有检测到主failure， 主自动重启，导致数据清空</p>
<p>4、通信及复制：<br>启动slave时：PSYNC给master<br>第一次连接—全量复制中，两个点：1rdb快照 2写命令缓存<br>全量参数：1时间60s 2内存缓冲区持续消耗和一次性超过<br>断点续传：网络故障的部分复制，offset</p>
<p>5、哨兵+主从复制<br>不保证0丢失，保证高可用</p>
<p>6、heartbeat<br>互相发送，主每10s、从每1s</p>
<p>7、主备切换前提、选举算法、哨兵master信息同步<br>选举前提 quorum和majority,至少满足max(quorum,majority)<br>选举算法：四个参数。<br>切换后其他的哨兵更新master配置：通过监听的channel中的version号，version号是负责切换的哨兵从新的master中获得的configuration epoch</p>
<p>8、持久化方式、优缺点、实现<br>Rdb：定期地冷备数据 — fork子进程进行磁盘io，不影响高性能，但如果rdb文件特别大则会影响服务，每隔5min宕机丢数据<br>aof：每1s后台fsync，最多丢1s数据。append写入文件，无磁盘寻址时间，文件尾部破损容易修复(?)、命令基于内存的数据重新构建而不是基于旧的指令日志 – rewrite log（指令压缩、旧的仍然提供服务，新的好了后替换），灾难性误删除.</p>
<p>9、并发竞争<br>redis的cas？zookeeper分布式锁？<br>采用CAS协议，则是如下的情景。<br> •第一步，A取出数据对象X，并获取到CAS-ID1；<br>•第二步，B取出数据对象X，并获取到CAS-ID2； <br>•第三步，B修改数据对象X，在写入缓存前，检查CAS-ID与缓存空间中该数据的CAS-ID是否一致。结果是“一致”，就将修改后的带有CAS-ID2的X写入到缓存。<br> •第四步，A修改数据对象Y，在写入缓存前，检查CAS-ID与缓存空间中该数据的CAS-ID是否一致。结果是“不一致”，则拒绝写入，返回存储失败。<br>这样CAS协议就用了“版本号”的思想，解决了冲突问题。（乐观锁概念）</p>
<p>在使用redis的setnx方法和memcace的add方法时，如果指定的key已经存在，则返回false。利用这个特性，实现全局锁<br>每次生成全局id前，先检测指定的key是否存在，如果不存在则使用redis的incr方法或者memcache的increment进行加1操作。这两个方法的返回值是加1后的值，如果存在，则程序进入循环等待状态。循环过程中不断检测key是否还存在，如果key不存在就执行上面的操作。</p>
<p>10、数据恢复<br>放到指定目录，然后重启redis，redis会恢复内存中的数据然后继续提供服务</p>
<p>11、哨兵–分布式<br>监控、修改地址(确保slave连接正确的master)、主从切换（确保潜在master的slave复制了所有的数据）、故障通知<br>两个配置：quorum、majority<br>quorum是至少多少哨兵认为宕机，才是master真的宕机<br>majority是必须满足大多数的哨兵是运行，才能进行故障转移。<br>=&gt;主备切换至少满足max(quorum,majority)<br>sdown odown</p>
<p>12、丢失：<br>case1slave没有同步完master的数据，master宕机<br>case2master机器脱离了集群，但是仍然运行，哨兵又选举了新的master。但是client还是在旧的写，旧的成为slave去新的master更新数据。这部分写丢失。<br>解决：两个参数。master宕机控制在丢失数据10s内。一点超过10s的数据复制，则master停止写请求。</p>
<p>13、哨兵自动发现<br>1往自己监控的channel里发消息 ，包括： runid、master监控配置、hostip<br>2监听channel，感知其他的哨兵<br>3监控配置的同步</p>
<p>14、cluster<br>Cluster bus通信、gossip协议</p>
<p>15、集群元数据维护方式<br>集中式：zookeeper作为实现，时效性高，存储更新有压力<br>gossip：分散更新，滞后</p>
<p>16、一致性hash、hash slot<br>hash的值空间组成一个环，将master的ip进行hash，确定在环的位置。数据找到位置后，存入顺时针走的第一个遇到的master。<br>当master宕机，则master和前一个master之间的数据受到影响。<br>热点问题：master计算多个hash值，在环中增加虚拟节点<br>slot：每个key计算crc16值，然后对16384取模，对应到相应的hash slot。每个master持有部分的slot</p>
<p>17、cluster中的选举、复制和哨兵<br>大于一半的master投票给该slave则该slave可以替换为master<br>cluster直接集成了复制和哨兵功能</p>
<p>18、缓存雪崩<br>所有的请求在redis都没有命中<br>解决：<br>redis高可用(主从+哨兵)、hystrix限流+本地ehcache缓存、redis持久化<br>先查本地、再redis、再限流，未通过的请求则降级</p>
<p>19、穿透<br>没查到则写一个空值到redis</p>
<p>20、击穿<br>热点key失效时缓存被瞬时击穿<br>1不用更新则永不过期 2更新频率高或者时间长则定时线程提前主动重新构建缓存或者延后过期时间 3更新频率低或者时间短，则分布式互斥锁或者本地锁保证少量请求能重新构建缓存，其余则锁释放后再访问新的缓存</p>
<p>21、双写一致性<br>Cache aside：读-先读缓存、再读数据库、再写入缓存，更新则先更新数据库、再删除缓存<br>为什么是删除不是更新？lazy<br>删除缓存失败？–先删除缓存再更新数据库<br>先删除缓存再更新数据库–还是会有不一致，每秒并发几万：当更新没完成，一个请求来了，写入缓存的是旧数据，然后又更新了数据库。</p>
<p>解决：串行化。<br>更新的数据都附带上数据标识，如果不在缓存中，则将读和更新发到一个队列中，线程从队列中串行地拿操作执行。<br>注意：1请求操作的超时（过多的写操作积压，一般单机20个队列，qps500的写可以支持，200ms里100个写，每个队列5个，每个20ms完成，那么读请求也可以在200ms内返回。否则扩容机器） 2读并发高 3热点商品的请求倾斜 4路由到同一台机子</p>
<p>22、线上部署情况<br>cluster模式，10台机器，5台master，一主一从<br>每个master高峰的qps 5w<br>master配置：32g8core+1T，分给redis内存最多10g<br>内存中放商品数据，每条数据大概10kb，10w条则1g，一般200w条，占20g。目前的qps高峰是3500左右请求量。</p>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>redis_mindroad</title>
    <url>/2021/04/05/redis-mindroad/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>汇总</strong></p>
<span id="more"></span>


<ul>
<li><p>基础数据结构5种及场景</p>
</li>
<li><p>高可用</p>
<ul>
<li>持久化</li>
<li>集群<ul>
<li>主从</li>
<li>cluster</li>
<li>sentinel</li>
</ul>
</li>
</ul>
</li>
<li><p>高级数据结构</p>
<ul>
<li>bitmap</li>
<li>geo</li>
<li>hyperloglog</li>
</ul>
</li>
<li><p>并发竞争</p>
</li>
<li><p>分布式锁</p>
</li>
<li><p>内存淘汰机制</p>
</li>
<li><p>读写，cache aside pattern</p>
</li>
<li><p>单线程</p>
</li>
<li><p>选型</p>
</li>
<li><p>事务</p>
</li>
<li><p>分区</p>
</li>
<li><p>性能</p>
<ul>
<li>expire时间</li>
<li>pipeline</li>
<li>keys scan</li>
</ul>
</li>
<li><p>缓存</p>
<ul>
<li>缓存击穿</li>
<li>缓存穿透</li>
<li>缓存雪崩</li>
</ul>
</li>
<li><p>新特性</p>
</li>
<li><p>底层</p>
</li>
<li><p>ziplist、skiplist…</p>
</li>
</ul>
<p><img src="/2021/04/05/redis-mindroad/redis.png" alt="脑图"></p>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>spark</title>
    <url>/2021/04/05/spark/</url>
    <content><![CDATA[<br>

<p>spark job相关</p>
<span id="more"></span>

<ul>
<li><p>groupby reduceby区别</p>
<ul>
<li>reduceByKey 会进行分区内聚合，函数内部调用了combineByKey，然后再进行网络传输</li>
<li>groupByKey 不会进行局部聚合</li>
</ul>
</li>
<li><p>依赖</p>
<ul>
<li>NarrowDependency分为OneToOneDependency和RangeDependency两种</li>
<li>ShuffleDependency：子依赖多个父RDD</li>
</ul>
</li>
<li><p>stage &amp; task</p>
<ul>
<li><p>DAGScheduler将Stage划分-&gt;把Stage转换为TaskSet-&gt;TaskScheduler将计算任务最终提交到集群</p>
</li>
<li><p>最后的Stage包含了一组ResultTask</p>
</li>
<li><p>task</p>
<ul>
<li>task和partition是一对一。一组Task就组成了一个Stage</li>
<li>Task<ul>
<li>ShuffleMapTask<ul>
<li>根据Task的partitioner将计算结果放到不同的bucket</li>
</ul>
</li>
<li>ResultTask<ul>
<li>计算结果发送回Driver</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>如何将stage划分为taskset</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Spark Standalone-cluster</p>
<ul>
<li>流程<ul>
<li>集群启动后worker向Master汇报资源，Master掌握集群资源</li>
<li>client提交sparkApplication，向Master申请启动Driver</li>
<li>Master收到请求后随机找一台节点启动Driver，Driver向Master申请executor进程资源</li>
<li>driver 里scheduler将DAG划分为stage，taskscheduler划分stage为taskset</li>
<li>Master找到满足资源的worker节点，启动Excutor，反向注册给Driver</li>
<li>Driver的context taskscheduler再把task发给executor，监控task，回收结果（collect方法）</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>缓存</p>
<ul>
<li>persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中</li>
<li>触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用</li>
<li>StorageLevel</li>
<li>shuffle会涉及到网络传输，可能会丢失数据，shuffle之前做persist，框架默认将数据持久化到磁盘</li>
</ul>
</li>
<li><p>checkpoint</p>
<ul>
<li>在checkpoint的时候强烈建议先进行cache</li>
<li>当你checkpoint执行成功了,那么前面所有的RDD依赖都会被销毁</li>
<li>区别persist<ul>
<li>persist可以将 RDD 的 partition 持久化到磁盘，但该 partition 由 blockManager 管理, blockManager stop，文件夹被删除</li>
<li>checkpoint 将 RDD 持久化到 HDFS 或本地文件夹，是一直存在的，也就是说可以被下一个 driver program 使用</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>rdd</p>
<ul>
<li>本质上是多个Partition组成的List。一个RDD可以包含多个分区，每个分区就是一个dataset片段。分区数决定了并行计算的数量<br>默认情况下，HDFS上面一个Block就是一个Partition。</li>
<li>RDD如何保障数据处理效率<ul>
<li>通过persist与patitionBy函数来控制RDD的分区与持久化</li>
<li>底层接口则是基于迭代器的</li>
</ul>
</li>
</ul>
</li>
<li><p>容错</p>
<ul>
<li><p>rdd记住构建它的操作图（Graph of Operation），Worker失败时重新计算，无需replication</p>
</li>
<li><p>需要恢复执行过程的中间状态：通过Spark提供的checkpoint</p>
</li>
<li><p>lineAge：每次更新都会记录下来，比较复杂且比较耗费性能。适用于DAG中重算太耗费时间的</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>数据倾斜<ul>
<li>先局部聚合，再全局聚合。</li>
<li>并行度太少了，导致个别Task的压力太大</li>
<li>自定义partition，分散key的分布，使其更加均匀</li>
</ul>
</li>
</ul>
<ul>
<li><p>累加器和广播变量</p>
<ul>
<li><p>累加器</p>
<ul>
<li>累计计数等场景</li>
<li>Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新。</li>
</ul>
</li>
<li><p>广播变量</p>
<ul>
<li>每个executor一个副本，普通变量每个task一个副本</li>
<li>driver定义和修改</li>
<li>节点间高效分发大对象</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>序列化</p>
<ul>
<li><p>java序列化</p>
<ul>
<li>static和transient修饰的变量不会被序列化</li>
<li>readObject()方法和writeObject()自定义实现序列化</li>
</ul>
</li>
<li><p>spark序列反序列化过程</p>
<ul>
<li>代码中对象在driver本地序列化；</li>
<li>对象序列化后传输到远程executor节点；</li>
<li>远程executor节点反序列化对象</li>
</ul>
</li>
<li><p>SerializationDebugger在日志中出问题的类和属性</p>
</li>
<li><p>kryo </p>
<ul>
<li>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好</li>
<li><code>.registerKryoClasses(Array(classOf[Student])) // 将自定义的类注册到Kryo</code></li>
<li>Spark 2.0.0以来,sparkcontext初始化时某些类型已被注册进去</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>repartition和coalesce的区别、partitionby</p>
<ul>
<li>coalesce<ul>
<li>repartition底层为coalesce</li>
<li>一般增大rdd分区用repartition，减小用coalesce</li>
<li>repartition一定涉及shuffle，coalesce根据传入参数判断是否发生shuffle</li>
</ul>
</li>
<li>partitionby<ul>
<li>repartition 和 partitionBy 都是对数据进行重新分区，默认都是使用 HashPartitioner，区别在于partitionBy 只能用于 PairRDD</li>
<li>repartition 随机生成的数来当做 Key，partitionby是自己的key</li>
</ul>
</li>
</ul>
</li>
<li><p>map()和mapPartition()的区别</p>
<ul>
<li>map()：每次处理一条数据；mapPartition()：每次处一个分区的数据，可能导致OOM。 </li>
<li>当内存空间较大的时候建议使用mapPartition()，以提高处理效率。</li>
<li>mapPartitionsWithIndex类似mapPartitions，但func带有一个整数参数表示分片的索引值</li>
</ul>
</li>
<li><p>glom</p>
<ul>
<li>glom会把每个批次每个分区的数据从Iterator类型转换为Array类型，所以如果每个分区的数据非常大的话会出现OOM的情况。</li>
</ul>
</li>
<li><p>pipe</p>
<ul>
<li>每个分区一个脚本</li>
</ul>
</li>
<li><p>比hive快</p>
<ul>
<li>HQL 引擎还比 Spark SQL 的引擎更快</li>
<li>Hadoop 每次 shuffle 操作后，必须写到磁盘，spark内存</li>
<li>消除了冗余的 MapReduce 阶段</li>
<li>Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM<br> Spark 基于线程，只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。</li>
</ul>
</li>
<li><p>aggregateByKey foldByKey combineByKey </p>
<ul>
<li>aggregateByKey <ul>
<li><code>rdd.aggregateByKey(0)(math.max(_,_),_+_)</code></li>
<li>每个分区的各个key的value和初始值0，进行max，获得到每个key对应value的max<br>  然后每个分区进行combine即相加</li>
</ul>
</li>
<li> 计算相同key对应值的相加结果：<code>rdd.foldByKey(0)(_+_)</code></li>
<li>combineByKey<ul>
<li><code>input.combineByKey((_,1), (acc:(Int,Int),v)=&gt;(acc._1+v,acc._2+1),//v为当前值 (acc1:(Int,Int),acc2:(Int,Int))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))</code></li>
<li>key对应的value先映射成一个二元组(value,1)，同一个分区内的相同key将二元组相加<br>再将不同分区的相同key的二元组相加。</li>
<li>相加时，元组的第一位和第二位都分别累加</li>
</ul>
</li>
</ul>
</li>
<li><p>cogroup</p>
<ul>
<li>第一个RDD元素是(1,”Allen”)，(2,”Bob”)，(3,”Carl”)，第二个RDD元素是(1,10000)，(1,5000)，(2,11000)，(2,6000)，(3,12000)，(3,6000)</li>
<li>join的结果是： (1,(Allen,10000)) (1,(Allen,5000)) (2,(Bob,11000)) (2,(Bob,6000)) (3,(Carl,12000)) (3,(Carl,6000)) </li>
<li>cogroup的结果是： (1,([Allen],[10000, 5000])) (2,([Bob],[11000, 6000])) (3,([Carl],[12000, 6000])) </li>
</ul>
</li>
<li><p>mapValues </p>
<ul>
<li>mapValues(_+”|||”)–每个value后加上”|||”</li>
</ul>
</li>
</ul>
<ul>
<li><p>T级别数据</p>
<ul>
<li><p>单个executor进程内RDD的分片数据是用Iterator流式访问的</p>
</li>
<li><p>RDD lineage上各个transformation携带的闭包函数复合而成的Iterator，<br>  每访问一个元素，就对该元素应用相应的复合函数，得到的结果再流式地落地<br>  对于shuffle stage是落地到本地文件系统留待后续stage访问，<br>  对于result stage是落地到HDFS或送回driver端等等，视选用的action而定</p>
</li>
<li><p>用户要求Spark cache该RDD，且storage level要求在内存中cache时，<br>Iterator计算出的结果才会被保留，通过cache manager放入内存池</p>
</li>
<li><p><a href="https://www.zhihu.com/question/23079001/answer/23569986">内存有限的情况下 Spark 如何处理 T 级别的数据</a></p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow</title>
    <url>/2021/04/05/tensorflow/</url>
    <content><![CDATA[<br>

<p>基础、遇到的问题、源码相关汇总</p>
<span id="more"></span>

<ul>
<li>tensorflow的调试<ul>
<li>nan灾难<ul>
<li>Debugger V2</li>
<li>tensorboard Debugger V2 GUI</li>
<li>tf.print()</li>
<li>tfdbg</li>
</ul>
</li>
<li>显存不断增长<ul>
<li>释放临时节点  auto reuse\是不是需要train的变量</li>
<li>避免在run里面进行运算产生临时节点，即出了结果再计算</li>
</ul>
</li>
<li>emb lookup耗时</li>
<li>多个id统一hash </li>
<li>gpu cpu copy</li>
</ul>
</li>
</ul>
<ul>
<li><p>神经网络八股<br>就不搞了。太多人做过了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">checkpoint_save_path &#x3D; &quot;.&#x2F;checkpoint&#x2F;fashion.ckpt&quot;</span><br><span class="line">if os.path.exists(checkpoint_save_path + &#39;.index&#39;):</span><br><span class="line">    print(&#39;-------------load the model-----------------&#39;)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback &#x3D; tf.keras.callbacks.ModelCheckpoint(filepath&#x3D;checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only&#x3D;True,</span><br><span class="line">                                                 save_best_only&#x3D;True)</span><br><span class="line"></span><br><span class="line">history &#x3D; model.fit(x_train, y_train, batch_size&#x3D;32, epochs&#x3D;5, validation_data&#x3D;(x_test, y_test), validation_freq&#x3D;1,</span><br><span class="line">                    callbacks&#x3D;[cp_callback])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">print(model.trainable_variables)</span><br><span class="line">file &#x3D; open(&#39;.&#x2F;weights.txt&#39;, &#39;w&#39;)</span><br><span class="line">for v in model.trainable_variables:</span><br><span class="line">    file.write(str(v.name) + &#39;\n&#39;)</span><br><span class="line">    file.write(str(v.shape) + &#39;\n&#39;)</span><br><span class="line">    file.write(str(v.numpy()) + &#39;\n&#39;)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"># 显示训练集和验证集的acc和loss曲线</span><br><span class="line">acc &#x3D; history.history[&#39;sparse_categorical_accuracy&#39;]</span><br><span class="line">val_acc &#x3D; history.history[&#39;val_sparse_categorical_accuracy&#39;]</span><br><span class="line">loss &#x3D; history.history[&#39;loss&#39;]</span><br><span class="line">val_loss &#x3D; history.history[&#39;val_loss&#39;]</span><br><span class="line"></span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line">plt.plot(acc, label&#x3D;&#39;Training Accuracy&#39;)</span><br><span class="line">plt.plot(val_acc, label&#x3D;&#39;Validation Accuracy&#39;)</span><br><span class="line">plt.title(&#39;Training and Validation Accuracy&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line">plt.plot(loss, label&#x3D;&#39;Training Loss&#39;)</span><br><span class="line">plt.plot(val_loss, label&#x3D;&#39;Validation Loss&#39;)</span><br><span class="line">plt.title(&#39;Training and Validation Loss&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>xgboost</title>
    <url>/2021/03/11/xgboost/</url>
    <content><![CDATA[<br>

<h1 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul>
<li>xgboost是Gradient Boosting的一种高效系统实现</li>
</ul>
<span id="more"></span>
<h2 id="基学习器"><a href="#基学习器" class="headerlink" title="基学习器"></a>基学习器</h2><ul>
<li>tree(gbtree)，也可用线性分类器(gblinear)。GBDT则特指梯度提升决策树算法</li>
</ul>
<h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><ul>
<li><p>xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数</p>
</li>
<li><p>xgboost工具支持自定义代价函数</p>
</li>
<li><p>代价函数里加入了正则项</p>
<ul>
<li>树的叶子节点个数</li>
<li>每个叶子节点上输出的score的L2模的平方和</li>
<li>正则项降低了模型的variance</li>
</ul>
</li>
<li><p>Shrinkage（xgboost中的eta）</p>
<ul>
<li>xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。</li>
<li>一般把eta设置得小一点，然后迭代次数设置得大一点</li>
</ul>
</li>
<li><p>gamma </p>
<ul>
<li>当增益大于阈值时才让节点分裂，gamma即阈值</li>
<li>它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。</li>
</ul>
</li>
<li><p>lambda</p>
<ul>
<li>正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。</li>
</ul>
</li>
</ul>
<h2 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h2><ul>
<li>缺失值<ul>
<li>对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向</li>
</ul>
</li>
<li>列抽样<ul>
<li>xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算</li>
</ul>
</li>
</ul>
<h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><ul>
<li><p>在特征粒度上</p>
</li>
<li><p>最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）</p>
</li>
<li><p>预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构</p>
</li>
<li><p>在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
</li>
<li><p>可并行的近似直方图算法</p>
<ul>
<li>用贪心法枚举所有可能的分割点，当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低</li>
</ul>
</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li><a href="https://www.zhihu.com/question/41354392">GBDT 和 XGBOOST 的区别有哪些</a></li>
<li>这个总结的也不错 <a href="https://zhuanlan.zhihu.com/p/38946959">https://zhuanlan.zhihu.com/p/38946959</a></li>
</ul>
<hr>
<p>mind:</p>
<p><img src="/2021/03/11/xgboost/xgboost.png" alt="xgboost"></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2021/02/21/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<br>
让我联想到博弈论中的决策树。根据多种可能地情况路径，利用概率和已知知识，进行决策判断。
<br>
<br>

<h4 id="基本问题"><a href="#基本问题" class="headerlink" title="基本问题"></a>基本问题</h4><p>Q1：如果建立一颗树，使得经过路径判断，走到叶子时候，能够得到最终样本归属的label(类别)或者y(连续值)？</p>
<ul>
<li>分析这棵树需要满足的基本性质<br>互斥、完备。即通过各条到叶子的不同路径，能够最终覆盖绝多数的样本。</li>
</ul>
<span id="more"></span>

<ul>
<li><p>生成这棵树的过程<br>1、要得到中间节点(特征)。分类能力越强的特征，越靠近根。<br>2、要找到当前特征的切分点，而进行分叉。<br>即涉及到特征选择(特征间重要性比较以及当前特征的切分点)。<br>而构建的过程，则是递归地选择特征的过程。<br>直到，没有特征可选、或者样本点已经完全覆盖(特征选择的标准达到阈值，不再分叉)。</p>
</li>
<li><p>分析回归树<br>1、回归树对特征选择和切分点选择的标准，肯定是区别于分类树的。<br>2、回归树的y如何生成？<br>分类树的label来自于多数投票。回归树则来自于，落入叶子的样本的均值。</p>
</li>
</ul>
<p>综上，<br>1、特征选择只在当前考虑选择最优，属于贪心策略，为局部最优。<br>2、本质上，是将样本空间进行直线(线性棱的空间体)切割。是概率中的条件概率。多个特征规则组合的条件下，样本所属类别的判定。<br>3、由于1，生成的树易产生过拟合。<br>4、回归树的回归结果，相比于lr等回归预测模型，结果数量是少的。</p>
<p>Q2：如何进行特征选择？</p>
<ul>
<li><p>对于分类树：<br>1、例如性别、年龄，如果已知性别能够比已知年龄，是更好的信息–即可以更大概率地判断出所属类别，那么就是更重要的特征。<br>2、其中，更大概率地判断出所属类别 – 即降低经验条件熵。<br>3、而经验条件熵的缺点是，对于取值比较多的特征，具有偏向性。其公式导致，取值多则经验条件熵值会小。可以考虑极端情况，当每个样本的该特征都取值不同，则经验条件熵为0。就有了C4.5，通过信息增益比来选择特征。<br>4、CART的gini系数本质和熵类似。越小，熵越小，确定性越高，特征越重要。</p>
</li>
<li><p>对于回归树：<br>1、cart回归树是二叉，要么是要么否。<br>2、回归树的切分选择不以熵入手考虑，而以均方误差作为判断标准。<br>3、对于当前特征，找到这样的切分点 – 使得此切分下的两个叶子样本集中，均方误差和最小。误差为：y-所有样本y的均值。<br>4、生成树过程和分类树一样。</p>
</li>
<li><p>为什么不用相关性？<br>余弦相似度是特征向量在label向量方向上的投影长度。一定程度也是特征重要的体现。<br>但是，在树模型中，由于本质是条件概率模型，熵就更能反映出概率模型的好坏。</p>
</li>
</ul>
<p>Q3：过拟合处理？</p>
<ul>
<li><p>剪枝<br>1、ID3,C4.5的剪枝是一个动态规划算法。对于某个非叶节点，计算剪去子树后的树的loss和不剪的loss。根据loss小的选择相应动作。而loss大小的比较，可以进行局部计算。因此可用动规实现。<br>2、loss为，每棵树的所有叶子上样本的熵之和，加上惩罚项(叶子数量或者节点数量)。<br>3、本质为，正则化的极大似然函数，来选择概率模型。</p>
</li>
<li><p>CART剪枝<br>1、区别于ID3&amp;C4.5，利用了递归思想，对所生成的树进行剪枝。<br>2、对于某个中间节点，找到进行剪枝动作的阈值–a的大小，a大于阈值则剪枝。即对于每一个中间节点，都有这样的阈值thre所决定的区间[thre,正无穷），剪枝后的树优于生成树。<br>3、对于所有的最优字数，交叉验证得到最终的最优树，并得到响应的a的thre。</p>
</li>
</ul>
<br>


<h4 id="扩展问题"><a href="#扩展问题" class="headerlink" title="扩展问题"></a>扩展问题</h4><p>Q4：信息增益比的缺点？<br>偏好取值少的特征。<br>C4.5不是直接选择增益比最大的特征，而是之前先把信息增益低于均值的属性剔除，然后在剩下的特征中选择而信息增益比最大的。得到兼顾。</p>
<p>Q5：预剪枝？<br>1、对当前节点在划分时，进行估计，如果不能够提升泛化能力则不进行划分。<br>2、本质是基于贪心，对当前节点进行判断是否划分。<br>3、坏处：当前虽然不能提升泛化能力，但可能划分后子树能够提升泛化能力。导致模型欠拟合。</p>
<p>Q6：对比LR和决策树？<br>1、y的结果上，决策树是固定的几个值。<br>2、LR比决策树慢，时间复杂度高。<br>3、LR无法处理缺失值，需要赋值。并且对极端值更敏感<br>4、LR对线性关系、全局拟合较好。决策树则是局部最优、局部数据探查更细致，不能更好地对多个特征同时考量。</p>
<p>Q7：CART做了哪些简化？<br>1、log函数的计算量大，而gini由图形可知，是对熵模型的很好的近似。<br>2、CART是二叉树，对每个特征进行二分而非多分，减少了特征选择时的计算。</p>
<p>Q8：测试集上缺失值的处理？<br>1、赋平均值或出现频次最高的值<br>2、走特征的常用分支<br>3、特征值专门处理的分支<br>C4.5的方式是，探查所有的分支，然后得到每个类别的概率，取最大的赋给该样本。</p>
<p>Q9：CART对于离散特征处理、连续特征处理的不同之处？<br>1、ID3&amp;C4.5都是多叉，特征只出现在一个中间节点；CART是对离散特征不断地二分。<br>2、连续特征：遍历每个特征，对某特征找到最小化均方误差(loss)的thre点，该点作为该特征的切分点。在所有特征中找到最小的loss，得到最优的(特征，特征切分点)作为中间节点。二分后，对每个孩子继续进行最优的(特征，特征切分点)的查找。同离散特征一样，连续特征可以重复出现，作为中间节点。直到达到停止条件。<br>3、CART连续特征：换种表述：对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。<br>4、ID3不支持连续特征划分。C4.5的做法是，将连续值进行排序，取两个值的中间值作为划分点，然后算信息增益比，(连续和离散的)最大的信息增益比为该中间节点。当连续值较多时，计算量的大。并且和CART一样，该连续特征同样可以继续作为中间节点，而非和离线一样一次性地生成多个孩子分叉。</p>
<p>Q10：缺点或者优化？<br>1、分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1。<br>2、如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。</p>
<p>其他–都可以在上述问题中找到答案：<br>如何找到切分点？</p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式mindroad</title>
    <url>/2021/04/05/%E5%88%86%E5%B8%83%E5%BC%8Fmindroad/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>汇总</strong></p>
<span id="more"></span>

<h2 id="分布式系统"><a href="#分布式系统" class="headerlink" title="分布式系统"></a>分布式系统</h2><p>1、层次结构</p>
<p>2、通信协议<br>默认dobbo<br>还有rmi hessian http webservice </p>
<p>3、序列化协议<br>默认hessian<br>java二进制序列化、json、soap</p>
<p>4、hessian数据结构<br>8种原始、3种递归、一种特殊类型</p>
<p>5、pb为什么最高性能<br>1编译器 2数据压缩</p>
<p>6、负载均衡策略<br>随机–根据权重，大则流量高<br>自动感知<br>一致性hash</p>
<p>7、集群容错<br>Failover<br>Failfast<br>Failsafe<br>Failback<br>Forking<br>Broadcast</p>
<p>8、动态代理<br>动态字节码生成</p>
<p>9、spi思想及扩展dubbo组件</p>
<p>10、服务治理<br> 1接口的调用次数和耗时 TP50/90/99、调用成功率、失败的监控<br> 2全链路的次数和耗时<br> 3timeout和retry<br> 4 Mock中实现降级逻辑<br> 5 分布式服务接口的幂等：redis+插入unique key<br> 6 接口调用的顺序性：会提升系统复杂度，降低效率-热点压力。dubbo进行hash负载均衡，然后打到一台机器，将order_id相同的请求放到该机器的一个线程内存队列中。更好的策略是合并请求。</p>
<hr>
<p>11、rpc框架设计</p>
<ul>
<li>zookeeper注册中心，本地动态代理发出请求，hessian序列化、长连接协议，负载均衡，服务端动态代理监听并代理实现</li>
<li>再加上 监控、配置化</li>
</ul>
<hr>
<p>12、zookeeper使用场景<br>分布式协调、分布式锁、注册中心(元数据管理)、HA高可用</p>
<ul>
<li>zk<ul>
<li>分布式协调<ul>
<li>A系统在zk建立一个监听，当系统B从MQ消费完订单消息，将zk状态改掉，zk则通知监听的系统A</li>
</ul>
</li>
<li>分布式锁<ul>
<li>a获取锁（尝试创建临时节点），b没得到就注册监听器（zk会将变化情况反向推送b），释放则获取到</li>
<li>临时节点保证，a宕机能够把临时节点自动删除，避免死锁</li>
</ul>
</li>
<li>配置中心/元数据<ul>
<li>dubbo的服务端地址都放到zk，消费从zk获取最新的地址，服务地址改动，zk及时地变化</li>
</ul>
</li>
<li>高可用<ul>
<li>a宕机了，删除zk的临时节点，监听的b及时发现则插入新的临时节点，a恢复发现有了b临时节点则注册监听</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>13、分布式锁及其效率<br>三点： 互斥、无死锁(业务意外情况下仍然可以获取锁)、高可用(锁的机子宕了，能够继续获取锁)</p>
<ul>
<li><p>zk</p>
<ul>
<li>while +  countdownlatch–await卡着 + 临时节点</li>
<li>临时顺序节点更好</li>
<li>创建临时的znode，别的客户端注册监听器，释放锁就是删除znode，释放了就会通知客户端</li>
</ul>
</li>
<li><p>redis<br>1如何保证原子性：lua<br>2业务时间&gt;定期删除时间：守护线程延长定期时间并设置超时放弃<br>3锁的误删除：uuid+threadid<br>4防止死锁：key加上过期时间 setnx expire</p>
</li>
<li><p>set orderic:lock 随机值 nx 3000 （nx代表无锁才能set返回ok），其他nil的节点每隔1s看是否还能set成功</p>
</li>
<li><p>释放：lua获取value对比是否是自己的，如果是则删除，否则不处理</p>
</li>
<li><p>redis主从架构时，由于异步更新到从，可能导致set的分布式锁失效</p>
</li>
</ul>
<p>5redis集群(三种:主从、哨兵、cluster)无法满足高可用：主从是为了读写分离，锁还是在master上面。哨兵机制的集群，当master上的锁没有同步到slave时，此时有加锁请求则仍能够获得锁，不满足互斥性。三主三从的cluster模式(hash槽分配)，超过了10w+的并发，则一个主的槽不可用，则该redis集群全不可用。</p>
<p>6时间漂移：硬件解决</p>
<p>7 redlock：多个实例，但是不是集群关系。锁的有效时间= ttl-时间漂移(所有实例加锁花费的时间)。</p>
<p>获取锁只需要超过一半的实例获取成功。避免死锁：重试机制-加锁不成功则撤回请求(避免死锁)，并且在重试时加上随机时间，避免同时加锁的请求都重试结果都无法获得。避免不互斥：延迟重启-如果master挂了，延迟ttl时间对slave重启替换为master。<br>TTL&gt; 业务执行时间+redis加锁时间+时钟漂移<br>删除锁则所有实例都执行删除。</p>
<ul>
<li>redlock 不健壮、无效请求<ul>
<li>至少一半以上节点set成功且在超时时间内，才算创建成功。否则说明失败，通过lua删除已经set的节点。</li>
<li>没成功就每隔1s去看是否被释放</li>
</ul>
</li>
</ul>
<hr>
<p>14、分布式接口幂等性</p>
<ul>
<li>数据库unique key</li>
<li>redis ：set orderid payed</li>
</ul>
<hr>
<p>15、分布式接口顺序性</p>
<ul>
<li>接入服务进行分发到(hash分发)不同机子，然后机子分发到一个线程的内存队列<br>  或者接入服务分发到MQ，然后其他机子从MQ拉消费</li>
<li>但如果接入服收到的顺序不是你要的，那还是无法完全保证顺序</li>
<li>完全保证顺序<ul>
<li>分布式锁<ul>
<li>不仅以orderid标识，还需要有标识请求序号的seq。<br>  请求先获取zookeeper锁，查库，如果seq不对则释放锁。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>16、分布式会话</p>
<ul>
<li>spring session + Redis，解耦web容器</li>
</ul>
<hr>
<p>17、分布式事务</p>
<ul>
<li><p>事务ACID </p>
<ul>
<li>全部结束或者回滚、和5000转账不影响和、并发事务相互不影响(脏读(看到另一个事务的中间状态)、commit读、重复读、串行化)、宕机之后恢复数据和事务成功结束后一致</li>
</ul>
</li>
<li><p>CAP </p>
<ul>
<li>获得准确数据、及时响应返回、单节点挂了不影响其他</li>
<li>一般选择AP – 柔性事务 base理论，达到最终一致。CA是一个必须有分布式锁，一个可以没有</li>
</ul>
</li>
<li><p>2pc</p>
<ul>
<li>依赖数据库、不适合高并发</li>
<li>第一阶段，每个节点记录log，并且返回给总协调是否成功，第二阶段，有一个不成功则协调让所有的人回滚。</li>
<li>SEATA <ul>
<li>第一阶段各个节点就提交然后释放锁，第二阶段如果成功则删除log，否则按照log回滚。（全局事务id和分支事务id，定位到log）</li>
</ul>
</li>
</ul>
</li>
<li><p>tcc</p>
<ul>
<li>转账的例子</li>
<li>严格、业务代码繁琐</li>
<li>try 系统a预留系统bc资源，锁</li>
<li>confirm  rpc调用系统b扣减，调用系统c转账，本地记下log</li>
<li>cancel 回滚</li>
</ul>
</li>
<li><p>可靠消息最终一致性</p>
<ul>
<li>本地消息表<ul>
<li>todo</li>
</ul>
</li>
<li>rocketmq事务消息</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式事务</title>
    <url>/2021/02/04/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-%E4%B8%8A/</url>
    <content><![CDATA[<p><strong>摘要</strong></p>
<p>1、什么是事务、分布式事务，分布式事务的场景<br>2、CAP理论，及BASE理论，柔性事务</p>
<span id="more"></span>
<p>3、分布式事务解决模型:2PC TCC （概念、区别）<br>4、2PC解决方案：XA、AT（角色、流程、实战、缺点、区别）<br>5、TCC解决方案：三种异常处理、Hmily实现</p>
<h2 id="1、事务、分布式事务及场景"><a href="#1、事务、分布式事务及场景" class="headerlink" title="1、事务、分布式事务及场景"></a>1、事务、分布式事务及场景</h2><ul>
<li><p>一般利用数据可事务特性，为数据库事务。数据库和应用在一个服务器，则为本地事务。</p>
</li>
<li><p>事务则需要有ACID性质。</p>
<ul>
<li>原子性、一致性、隔离性(一个事务不能看到其他事务的中间状态)、持久性</li>
</ul>
</li>
<li><p>而分布式事务则和分布式架构相关，当服务被拆分，并通过网络进行协作，则一个事务就涉及到多个服务以及远程调用。此时为分布式事务。</p>
</li>
<li><p>场景：</p>
<ul>
<li>微服务架构中，即需要跨JVM进程。无论是多个服务访问一个实例，还是多个服务多个实例，本地事务都无法解决。</li>
<li>单个系统访问多个数据库实例，就需要进行不同的数据库连接</li>
</ul>
</li>
</ul>
<br>

<h2 id="2、CAP"><a href="#2、CAP" class="headerlink" title="2、CAP"></a>2、CAP</h2><h3 id="分布式事务控制目标"><a href="#分布式事务控制目标" class="headerlink" title="分布式事务控制目标"></a>分布式事务控制目标</h3><p>场景：mysql一主一从，商品写主读从。</p>
<p>一致性、可用性、分区容错性</p>
<ul>
<li><p>consistency：写操作后的读(任意节点读)可以读到最近状态。</p>
<ul>
<li>在从同步数据的过程中，将从锁住，同步完再允许查询。 需要1写响应有延迟；2资源锁定与释放；3同步失败则返回错误信息而不能返回旧数据</li>
</ul>
</li>
<li><p>availability：任何事务操作都可以得到响应，且不会响应错误或超时。</p>
<ul>
<li>需要能够立即响应并非错误、非超时，可以允许旧数据。需要1同步时不能锁定；2返回旧数据或者默认值</li>
</ul>
</li>
<li><p>partitio tolerance：分布式各个节点在不同的子网，就形成了网络分区，彼此需要通过网络进行交互，当网络通信失败时，仍能够提供服务。</p>
<ul>
<li>单个节点挂了不影响其他节点，同步时不影响读写操作。需要1添加主备从备节点、避免主或从挂了；2异步进行数据从主到从的同步</li>
</ul>
</li>
</ul>
<blockquote>
<p>三个特性不能共存。一般选择AP，达到最终一致性即可。</p>
</blockquote>
<ul>
<li>AP，通常实现AP都会保证最终一致性</li>
<li>CP，zookeeper追求的就是强一致</li>
<li>CA，不进行分区，本地事务隔离级别即可。</li>
</ul>
<br>

<h2 id="BASE理论与柔性事务"><a href="#BASE理论与柔性事务" class="headerlink" title="BASE理论与柔性事务"></a>BASE理论与柔性事务</h2><p>在AP中，满足1 基本可用 2 软状态 3 最终一致。即满足base，为柔性事务。</p>
<ul>
<li>软状态: 中间状态– 当同步过程中来了查询，给出“支付中”状态，一致后再返回“成功”。</li>
</ul>
<br>

<h2 id="3、2PC"><a href="#3、2PC" class="headerlink" title="3、2PC"></a>3、2PC</h2><h3 id="3-1-2PC概念"><a href="#3-1-2PC概念" class="headerlink" title="3.1 2PC概念"></a>3.1 2PC概念</h3><ul>
<li><p>两阶段提交协议，prepare准备阶段和commit提交阶段。</p>
</li>
<li><p>包含 事务管理器和事务参与者(数据库实例)。管理器决定整个事务的提交和回滚，参与者负责本地事务的提交和回滚。</p>
</li>
<li><p>过程：</p>
<ul>
<li>prepare： 管理器向每个实例发送prepare消息，每个实例写本地的undo(修改前的数据)和redo(修改后的数据)日志。此时没有提交。</li>
<li>commit： 管理器收到参与者的失败或超时，则向每个参与者发送rollback消息。否则向每个实例发送commit。每个参与者进行执行指令并释放锁。</li>
</ul>
</li>
</ul>
<br>

<h3 id="3-2-2PC解决方案"><a href="#3-2-2PC解决方案" class="headerlink" title="3.2 2PC解决方案"></a>3.2 2PC解决方案</h3><h4 id="3-2-1-XA方案"><a href="#3-2-1-XA方案" class="headerlink" title="3.2.1 XA方案"></a>3.2.1 XA方案</h4><p>规范数据库实现2pc协议的分布式事务处理模型DTP。<br>定义了角色：AP RM TM,TM 和 RM 通讯接口为XA。即数据库提供的2pc接口协议，基于该协议的2pc实现为xa方案。</p>
<p>缺点：<br>1、资源锁需要事务的两个阶段结束才能释放<br>2、本地数据库需要支持XA协议<br><br></p>
<h4 id="3-2-2-Seata方案"><a href="#3-2-2-Seata方案" class="headerlink" title="3.2.2 Seata方案"></a>3.2.2 Seata方案</h4><p>Seata是提供AT和TCC模式的分布式事务解决方案。</p>
<ul>
<li><p>与XA区别：</p>
<ul>
<li>1、XA是两阶段后释放锁，而AT模式(2pc)第一阶段则提交释放锁</li>
<li>2、AT是应用层的中间件，对业务0侵入。</li>
</ul>
</li>
<li><p>实现：</p>
<ul>
<li>三个角色，TC TM RM。</li>
<li>TC负责，接收TM的全局事务提交或回滚指令，和RM通信协调分支事务。</li>
<li>TM，开启全局事务，向TC发出提交或回滚指令。</li>
<li>RM，分支注册、状态汇报、接收TC指令、驱动本地事务提交或回滚的执行。</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>具体的执行流程如下:</li>
</ul>
<ol>
<li>用户服务的 TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的XID。</li>
<li>用户服务的 RM 向 TC 注册 分支事务，该分支事务在用户服务执行新增用户逻辑，并将其纳入 XID 对应全局<br>事务的管辖。</li>
<li>用户服务执行分支事务，向用户表插入一条记录。</li>
<li>逻辑执行到远程调用积分服务时(XID 在微服务调用链路的上下文中传播)。积分服务的RM 向 TC 注册分支事<br>务，该分支事务执行增加积分的逻辑，并将其纳入 XID 对应全局事务的管辖。</li>
<li>积分服务执行分支事务，向积分记录表插入一条记录，执行完毕后，返回用户服务。</li>
<li>用户服务分支事务执行完毕。</li>
<li>TM 向 TC 发起针对 XID 的全局提交或回滚决议。</li>
<li>TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。</li>
</ol>
<hr>
<ul>
<li>详解流程：<ul>
<li>每个RM使用DataSourceProxy连接数据库，其目的是使用ConnectionProxy，使用数据源和数据连接代理的目 的就是在第一阶段将undo_log和业务数据放在一个本地事务提交，这样就保存了只要有业务操作就一定有 undo_log。</li>
<li>在第一阶段undo_log中存放了数据修改前和修改后的值，为事务回滚作好准备，所以第一阶段完成就已经将分 支事务提交，也就释放了锁资源。</li>
<li>TM开启全局事务开始，将XID全局事务id放在事务上下文中，通过feign调用也将XID传入下游分支事务，每个 分支事务将自己的Branch ID分支事务ID与XID关联。</li>
<li>第二阶段全局事务提交，TC会通知各各分支参与者提交分支事务，在第一阶段就已经提交了分支事务，这里各各参与者只需要删除undo_log即可，并且可以异步执行，第二阶段很快可以完成。</li>
<li>第二阶段全局事务回滚，TC会通知各各分支参与者回滚分支事务，通过 XID 和 Branch ID 找到相应的回滚日志，通过回滚日志生成反向的 SQL 并执行，以完成分支事务回滚到之前的状态，如果回滚失败则会重试回滚操作。</li>
</ul>
</li>
</ul>
<ul>
<li><p>实战step：</p>
<ul>
<li>下载，解压并启动seata服务器 /bin/seata-server.bat -p 8888 -m file 。端口和文件方式存储信息。(TC)</li>
<li>添加discover-server子模块，discover-server基于Eureka实现。</li>
<li>添加微服务子模块，子模块pom中引入spring-cloud-alibaba-seata（包含了RM TM），并配置TC的地址：registry.conf、file.conf中：</li>
</ul>
<blockquote>
<p>在file.conf中更改service.vgroup_mapping.[springcloud服务名]-fescar-service-group = “default”，并修改 service.default.grouplist =[seata服务端地址]</p>
</blockquote>
<ul>
<li>创建代理数据源（配置mysql连接）</li>
</ul>
<blockquote>
<p>Seata的RM通过DataSourceProxy才能在业务代码的事务提交时，通过这个切<br>  入点，与TC进行通信交互、记录undo_log等。每个RM使用DataSourceProxy连接数据库，其目的是使用ConnectionProxy，使用数据源和数据连接代理的目 的就是在第一阶段将undo_log和业务数据放在一个本地事务提交，这样就保存了只要有业务操作就一定有 undo_log。</p>
</blockquote>
<ul>
<li>代码部分细节：<code>FeignClient</code>、<code>@GlobalTransactional</code>、<code>@Transactional</code></li>
</ul>
<blockquote>
<p>将@GlobalTransactional注解标注在全局事务发起的Service实现方法上，开启全局事务: GlobalTransactionalInterceptor会拦截@GlobalTransactional注解的方法，生成全局事务ID(XID)，XID会在整个分布式事务中传递。 在远程调用时，spring-cloud-alibaba-seata会拦截Feign调用将XID传递到下游服务。</p>
</blockquote>
</li>
</ul>
<br>

<h3 id="4、TCC"><a href="#4、TCC" class="headerlink" title="4、TCC"></a>4、TCC</h3><h4 id="4-1-TCC相关解决方案"><a href="#4-1-TCC相关解决方案" class="headerlink" title="4.1 TCC相关解决方案"></a>4.1 TCC相关解决方案</h4><p>tcc-transaction、<br>Hmily、<br>ByteTcc、<br>EasyTransaction</p>
<br>

<h4 id="4-2-TCC的三种异常处理"><a href="#4-2-TCC的三种异常处理" class="headerlink" title="4.2 TCC的三种异常处理"></a>4.2 TCC的三种异常处理</h4><ol>
<li><strong>空回滚</strong><br> 没有执行try就执行了cancel</li>
<li><strong>幂等</strong><br> cancel和commit会重试</li>
<li><strong>悬挂</strong><br> RPC 调用分支事务try时，先注册分支事务，再执行RPC调用，如果此时 RPC 调用的网络发生拥堵， 通常 RPC 调用是有超时时间的，RPC 超时以后，TM就会通知RM回滚该分布式事务，可能回滚完成后，RPC 请求 才到达参与者真正执行，而一个 Try 方法预留的业务资源，只有该分布式事务才能使用，该分布式事务第一阶段预 留的业务资源就再也没有人能够处理了，对于这种情况，我们就称为悬挂，即业务资源预留后没法继续处理。</li>
</ol>
<p>转账最终方案：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;A</span><br><span class="line">try: </span><br><span class="line">	try幂等校验</span><br><span class="line">	try悬挂处理 </span><br><span class="line">	检查余额是否够30元 </span><br><span class="line">	扣减30元</span><br><span class="line">confirm: </span><br><span class="line">	空</span><br><span class="line">cancel: </span><br><span class="line">	cancel幂等校验</span><br><span class="line">	cancel空回滚处理 </span><br><span class="line">	增加可用余额30元</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;B</span><br><span class="line">try: </span><br><span class="line">	空</span><br><span class="line">confirm: </span><br><span class="line">	confirm幂等校验</span><br><span class="line">	正式增加30元 </span><br><span class="line">cancel:</span><br><span class="line">	空</span><br></pre></td></tr></table></figure>

<h4 id="4-3-Hmily实现TCC事务"><a href="#4-3-Hmily实现TCC事务" class="headerlink" title="4.3 Hmily实现TCC事务"></a>4.3 Hmily实现TCC事务</h4><ul>
<li><p>新增配置类接收application.yml中的Hmily配置信息，并创建HmilyTransactionBootstrap Bean</p>
</li>
<li><p>A账户</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;DAO 略</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;service </span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">    @Transactional</span><br><span class="line">    @Hmily(confirmMethod &#x3D; &quot;commit&quot;, cancelMethod &#x3D; &quot;rollback&quot;)</span><br><span class="line">	public  void updateAccountBalance(String accountNo, Double amount) &#123;</span><br><span class="line">		&#x2F;&#x2F;事务id</span><br><span class="line">		&#x2F;&#x2F;try幂等校验</span><br><span class="line">		&#x2F;&#x2F;try悬挂处理</span><br><span class="line">		&#x2F;&#x2F;从账户扣减及扣减失败</span><br><span class="line">		&#x2F;&#x2F;增加本地事务try成功记录，用于幂等性控制标识 accountInfoDao.addTry(transId);</span><br><span class="line">		&#x2F;&#x2F;远程调用bank2</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;feign</span><br><span class="line"></span><br><span class="line">	@FeignClient(value &#x3D; &quot;seata‐demo‐bank2&quot;, fallback &#x3D; Bank2Fallback.class)</span><br><span class="line">	public interface Bank2Client &#123;</span><br><span class="line">		@GetMapping(&quot;&#x2F;bank2&#x2F;transfer&quot;)</span><br><span class="line">		@Hmily</span><br><span class="line">		Boolean transfer(@RequestParam(&quot;amount&quot;) Double amount);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;controller 略</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>B账户 略</li>
</ul>
<h4 id="4-4-TCC与2PC对比"><a href="#4-4-TCC与2PC对比" class="headerlink" title="4.4 TCC与2PC对比"></a>4.4 TCC与2PC对比</h4><ol>
<li>2PC通常在DB层面，TCC在应用层面</li>
<li>2PC无侵入性，TCC自定义数据操作粒度，锁冲突降低，提高吞吐，但业务逻辑每个分支都需要实现TCC三个方法，且需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式事务-下</title>
    <url>/2021/02/04/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-%E4%B8%8B/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>摘要</strong></p>
<p>1、可靠消息最终一致性，概念特点、问题、针对问题的方案(本地消息表、Rocketmq)<br>2、本地消息表流程图<br>3、Rocketmq流程、实战<br>4、最大努力通知，流程、</p>
<span id="more"></span>

<h2 id="可靠消息最终一致性"><a href="#可靠消息最终一致性" class="headerlink" title="可靠消息最终一致性"></a>可靠消息最终一致性</h2><p>当事务发起方执行完成本地事务后并发出一条消息，事务参与方(消息消费者)一定能 够接收消息并处理事务成功，此方案强调的是只要消息发给事务参与方最终事务要达到一致。</p>
<ul>
<li><p>特点</p>
<ul>
<li>适合执行周期长且实时性要求不高的场景</li>
<li>引入消息机制后，同步的事务操作变为基于消 息执行的异步操作, 避免了分布式事务中的同步阻塞操作的影响，并实现了两个服务的解耦。</li>
</ul>
</li>
<li><p>网络通信的不确定性会导致分布式事务问题：</p>
</li>
</ul>
<ol>
<li>事务发起方(消息生产方)将消息发给消息中间件</li>
<li>事务参与方从消息中间件接收消息</li>
</ol>
<h3 id="可靠消息最终一致性的问题"><a href="#可靠消息最终一致性的问题" class="headerlink" title="可靠消息最终一致性的问题"></a>可靠消息最终一致性的问题</h3><p>1、本地事务与消息发送的原子性问题</p>
<ul>
<li>发送消息成功，数据库操作失败  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">begin transaction; </span><br><span class="line">		&#x2F;&#x2F;1.发送MQ</span><br><span class="line">		&#x2F;&#x2F;2.数据库操作 </span><br><span class="line">commit transation;</span><br></pre></td></tr></table></figure></li>
<li>如果发送MQ消息失败，就会抛出异常，导致数据库事务回滚。但如果是超时异常，数 据库回滚，但MQ其实已经正常发送了，同样会导致不一致。  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">begin transaction; </span><br><span class="line">	&#x2F;&#x2F;1.数据库操作</span><br><span class="line">	&#x2F;&#x2F;2.发送MQ </span><br><span class="line">commit transation;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>2、事务参与方接收消息的可靠性，需要如果接收消息失败可以重复接收消息</p>
<p>3、消息重复消费的问题，要解决消息重复消费的问题就要实现事务参与方的方法幂等性。 </p>
<h2 id="本地消息表方案"><a href="#本地消息表方案" class="headerlink" title="本地消息表方案"></a>本地消息表方案</h2><p><img src="/2021/02/04/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-%E4%B8%8B/bendixiaoxibiao.png" alt="本地消息表方案"></p>
<ul>
<li>交互流程规范</li>
</ul>
<ol>
<li>用户服务在本地事务新增用户和增加 ”积分消息日志“。(用户表和消息表通过本地事务保证一致)</li>
<li>定时任务扫描日志–&gt;保证将消息发送给消息队列。启动独立的线程，定时对消息日志表中的消息进行扫描并发送至消息中间件，在消息中间件反馈发送成功后删除该消息日志，否则等待定时任务下一周期重试。</li>
<li>MQ的ack(即消息确认)机制–&gt;幂等，收到ack，MQ将不再向消费者推送消息，否则消费者会不断重 试向消费者来发送消息。</li>
</ol>
<h2 id="RocketMQ事务消息方案"><a href="#RocketMQ事务消息方案" class="headerlink" title="RocketMQ事务消息方案"></a>RocketMQ事务消息方案</h2><ul>
<li>交互流程实现</li>
</ul>
<ol>
<li>Producer (MQ发送方)发送事务消息至MQ Server，MQ Server将消息状态标记为Prepared(预备状态)，注意此时这条消息消费者(MQ订阅方)是无法消费到的。</li>
<li>MQ Server接收到Producer 发送给的消息则回应发送成功表示MQ已接收到消息。</li>
<li>Producer 端执行业务代码逻辑，通过本地数据库事务控制。</li>
<li>若Producer 本地事务执行成功则自动向MQServer发送commit消息，MQ Server接收到commit消息后将状态标记为可消费，此时MQ订阅方(积分服务)即正常消费消息。若Producer 本地事务执行失败则自动向MQServer发送rollback消息，MQ Server接收到rollback消息后 将删除消息。</li>
<li>如果执行Producer端本地事务过程中，执行端挂掉或者超时，MQ Server将会不停的询问同组的其他 Producer来获取事务执行状态，这个过程叫事务回查。MQ Server会根据事务回查结果来决定是否投递消息。 </li>
</ol>
<br>

<ul>
<li>以上主干流程已由RocketMQ实现，对用户侧来说，用户需要：</li>
</ul>
<ol>
<li>实现本地事务执行</li>
<li>本地事务回查方法，因此关注本地事务的执行状态</li>
</ol>
<h3 id="RocketMq事务"><a href="#RocketMq事务" class="headerlink" title="RocketMq事务"></a>RocketMq事务</h3><p>RocketMQ主要解决了两个功能:<br>1、本地事务与消息发送的原子性问题。<br>2、事务参与方接收消息的可靠性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;回调</span><br><span class="line"></span><br><span class="line"> public interface RocketMQLocalTransactionListener &#123;</span><br><span class="line">      &#x2F;**</span><br><span class="line">		‐ 发送prepare消息成功此方法被回调，该方法用于执行本地事务</span><br><span class="line">		‐ @param msg 回传的消息，利用transactionId即可获取到该消息的唯一Id</span><br><span class="line">		‐ @param arg 调用send方法时传递的参数，当send时候若有额外的参数可以传递到send方法中，这里能获取到</span><br><span class="line">		‐ @return 返回事务状态，COMMIT:提交 ROLLBACK:回滚 UNKNOW:回调</span><br><span class="line">		*&#x2F;</span><br><span class="line">          RocketMQLocalTransactionState executeLocalTransaction(Message msg, Object arg);</span><br><span class="line"></span><br><span class="line">      	&#x2F;**</span><br><span class="line">		‐ @param msg 通过获取transactionId来判断这条消息的本地事务执行状态</span><br><span class="line">		‐ @return 返回事务状态，COMMIT:提交 ROLLBACK:回滚 UNKNOW:回调</span><br><span class="line">		*&#x2F;</span><br><span class="line">          RocketMQLocalTransactionState checkLocalTransaction(Message msg);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> &#x2F;&#x2F; 发送事务消息API  </span><br><span class="line">TransactionMQProducer producer &#x3D; new TransactionMQProducer(&quot;ProducerGroup&quot;); producer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;);</span><br><span class="line">producer.start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;设置TransactionListener实现 producer.setTransactionListener(transactionListener);</span><br><span class="line">&#x2F;&#x2F;发送事务消息</span><br><span class="line">SendResult sendResult &#x3D; producer.sendMessageInTransaction(msg, null);  </span><br></pre></td></tr></table></figure>


<ul>
<li><p>实践：</p>
<ul>
<li>在application-local.propertis中配置rocketMQ nameServer地址及生产组</li>
<li>service</li>
</ul>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;从event构建消息体</span><br><span class="line">	public void sendUpdateAccountBalance(AccountChangeEvent accountChangeEvent) </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;发送消息并收到回应后执行本地事务</span><br><span class="line">public void doUpdateAccountBalance(AccountChangeEvent accountChangeEvent)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;实现执行本地事务和事务回查两个方法</span><br><span class="line"> 	@RocketMQTransactionListener(txProducerGroup &#x3D; &quot;producer_group_txmsg_bank1&quot;)</span><br><span class="line"> 	public class ProducerTxmsgListener implements RocketMQLocalTransactionListener </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>controller 略</li>
<li>B账户的MQ 监听类</li>
</ul>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;需要实现幂等</span><br><span class="line">@RocketMQMessageListener(topic &#x3D; &quot;topic_txmsg&quot;,consumerGroup &#x3D; &quot;consumer_txmsg_group_bank2&quot;)</span><br><span class="line">public class TxmsgConsumer implements RocketMQListener&lt;String&gt; </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="最大努力通知"><a href="#最大努力通知" class="headerlink" title="最大努力通知"></a>最大努力通知</h2><ul>
<li>流程</li>
</ul>
<ol>
<li></li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2021/05/22/%E5%88%A9%E7%8E%87/</url>
    <content><![CDATA[<h3 id="利率"><a href="#利率" class="headerlink" title="利率"></a>利率</h3><p>1、</p>
]]></content>
  </entry>
  <entry>
    <title>协同过滤</title>
    <url>/2021/04/05/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/</url>
    <content><![CDATA[<ul>
<li><p>user based</p>
<span id="more"></span>
<ul>
<li>有一个行为user，列为item的打分矩阵，且得到每个用户的平均分</li>
<li>对于矩阵中的空值：<br>  通过计算用户a和用户b\c\d等(对itemb有分数的)相似度(皮尔逊相关系数)，然后<br>  根据均分，计算出用户a对itemb的分数</li>
</ul>
</li>
<li><p>item based</p>
<ul>
<li>由一条条访问记录得到一个个user*item的矩阵</li>
<li>上述所有矩阵加和，得到item*item的共现矩阵</li>
<li>根据共现矩阵，计算item和item的相似度，得到相似度的item*item矩阵</li>
<li>item*item的相似度共现矩阵和某用户的浏览序列相乘，得到该用户的打分向量</li>
</ul>
</li>
</ul>
<ul>
<li><a href="https://www.jianshu.com/p/5463ab162a58">协同过滤</a></li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>ml</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>协程</title>
    <url>/2021/04/05/%E5%8D%8F%E7%A8%8B/</url>
    <content><![CDATA[<p>协程、多核与cpu</p>
<span id="more"></span>

<ul>
<li>协程<ul>
<li>从隔离、通信、调度、切换开销角度分析</li>
<li>进程<ul>
<li>资源完全隔离、进程挂了不影响其他进程，通信需要Inter-Process Communication，有七中状态，os进行调度</li>
</ul>
</li>
<li>线程<ul>
<li>每个进程可以有多个线程，共享进程资源、共享全局变量，但会引起多线程并发问题，需要锁机制，通信需要wait signal</li>
<li>切换<ul>
<li>进程切换与线程切换的一个最主要区别就在于进程切换涉及到虚拟地址空间的切换而线程切换则不会。线程共享虚拟地址空间。</li>
<li>和进程一样涉及到内核状态、硬件上下文的切换</li>
</ul>
</li>
<li>线程池里一个线程挂了<ul>
<li>当执行方式是execute时,可以看到堆栈异常的输出。</li>
<li>当执行方式是submit时,堆栈异常没有输出。但是调用Future.get()方法时，可以捕获到异常。</li>
<li>不会影响线程池里面其他线程的正常执行。</li>
<li>线程池会把这个线程移除掉，并创建一个新的线程放到线程池中。</li>
</ul>
</li>
</ul>
</li>
<li>协程<ul>
<li>协程是属于线程的。协程程序是在线程里面跑的，因此协程又称微线程和纤程等.协没有线程的上下文切换消耗</li>
<li>不涉及内核，完全在用户态进行，不用锁</li>
<li>如python中的yield</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>多核的数据传递、总线</p>
<ul>
<li>不管别的CPU私有Cache是否缓存相同的数据，都需要发出一次广播事件。这在一定程度上加重了总线负载，也增加了读写延迟。<br>针对该问题，提出了一种状态机机制降低带宽压力。这就是MESI protocol（协议）。</li>
<li>cache line具有4中状态，分别是Modified、Exclusive、Shared和Invalid。<br>当cache line状态是Modified或者Exclusive状态时，修改其数据不需要发送消息给其他CPU</li>
<li>多核Cache一致性由硬件保证，对软件来说是透明</li>
<li>MOESI Protocol。多一种Owned状态。多出来的状态也是为了更好的优化性能。</li>
</ul>
</li>
<li><p>多cpu和单cpu多核</p>
<ul>
<li><a href="https://www.zhihu.com/question/20998226">cpu</a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>历史中国宪制</title>
    <url>/2021/04/11/%E5%8E%86%E5%8F%B2%E4%B8%AD%E5%9B%BD%E5%AE%AA%E5%88%B6/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>回溯</title>
    <url>/2021/04/13/%E5%9B%9E%E6%BA%AF/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>学习模型的模式</title>
    <url>/2021/02/21/%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<p>伪代码的算法流程 + 数学逻辑 –&gt;才能真的掌握住这个模型的本质。<br>而其他都是表象。所以数学肯定是要啃的。矩阵论和概率统计，微积分，必然是需要的。<br>更多的应用和实践，会深化对于模型的认知。</p>
<span id="more"></span>

<p>基本要求：<br>1、历史背景及演进：解决什么问题而生<br>2、伪代码表述流程<br>3、前提、局限、适用场景<br>4、演化和可优化之处、优化思路<br>5、和其他模型对比</p>
<p>进阶要求：<br>1、根据伪代码思路，看具体实现<br>2、为什么会有这样的前提和局限–&gt;数学证明：唯一性证明、误差上界证明、loss函数证明、模型参数的迭代公式证明</p>
<p>炉火纯青：<br>1、当我问你某个公式，你能够给出，并给出推导过程<br>2、和其他模型相似问题下的解决方案(公式)对比</p>
]]></content>
      <categories>
        <category>模型方法论</category>
      </categories>
      <tags>
        <tag>方法论</tag>
      </tags>
  </entry>
  <entry>
    <title>对偶</title>
    <url>/2021/03/11/%E5%AF%B9%E5%81%B6/</url>
    <content><![CDATA[<br>


<ul>
<li>对于凸形<br>结论1：凸形可行域只有1个峰，只要达到那个峰，我们就达到了最优，是全局最优。<br>结论2：至少有一个顶点是峰。</li>
</ul>
<span id="more"></span>
<ul>
<li><p>单纯形法：</p>
<ol>
<li>单纯形是什么？数学上可以写成一堆线性不等式限制出来的区域。</li>
<li>单纯形法，仅适用于求解线性规划，线性规划又是凸优化的一种。因为线性规划的定义域是单纯形，单纯形是凸的，即线性规划是定义域为凸、目标函数为线性的问题。</li>
<li>先找到一个顶点，然后从这个顶点，沿着某条边线，走到下一个顶点，直到最优。方向的选择可以有很多种，最多使用的是比较短视的方法：沿着最陡峭的那一条，追求当前步上升最快。</li>
<li>单纯形法寻找路线优化目标函数，直至达到一个峰，而该峰就是全局最优。</li>
</ol>
</li>
<li><p>Slater’s condition 根据wiki， </p>
<blockquote>
<p>Slater’s condition (or Slater condition) is a sufficient condition for strong duality to hold for a convex optimization problem, named after Morton L. Slater. Informally, Slater’s condition states that the <strong>feasible region must have an interior point</strong>.<br>即在对偶问题中gap为0要满足的条件，即可行域中必须有内点的条件。</p>
</blockquote>
</li>
<li><p>线性规划的对偶理论没出现的时候，线性规划是不知道能不能解的。也就是说，对偶理论能够证明一个线性规划问题<strong>不</strong>存在解。思路是找到一个跟原问题的对偶问题密切相关的问题，如果这个问题有解，原问题就没解。</p>
</li>
<li><p><strong>那么证明便归为两个主要部分，1 如何转化为对偶问题 2 为什么两个问题的解相关？</strong></p>
</li>
<li><p>首先[问题要满足是凸优化]。对于凸优化来说，在满足constraint qualifications(如上文的slater condition为其中一种，满足该条件，这里涉及到仿射函数即可表示为f=A*w+b 。仿射函数其实就是线性变换liner。)情况下，gap=0，为强对偶。</p>
</li>
<li><p>其次[在求解凸优化时引入乘子以及最优解需要满足的条件]。在凸优化中的非线性优化问题下，该问题满足一些constraint qualifications，当存在不等式约束时(只有等式约束时，即拉格朗日function求偏导(也就是KKT turns into the Lagrange conditions))，我们用KKT引进 <a href="https://en.wikipedia.org/wiki/Lagrange_multipliers" title="Lagrange multipliers">Lagrange multipliers</a>，这些乘子需要满足KKT 的四个条件。注意到这里的目标函数与约束函数一定是<a href="https://en.wikipedia.org/wiki/Smooth_function" title="Smooth function">continuously differentiable</a> at a point x(local optimum点)，如果functions are non-differentiable,<a href="https://en.wikipedia.org/wiki/Subderivative" title="Subderivative">subdifferential</a><br>versions of Karush–Kuhn–Tucker (KKT) conditions are available.</p>
</li>
<li><p>对偶问题， 这里代表Lagrangian dual problem，还有其他对偶问题 <a href="https://en.wikipedia.org/wiki/Wolfe_dual_problem" title="Wolfe dual problem">Wolfe dual problem</a> and the <a href="https://en.wikipedia.org/wiki/Fenchel%27s_duality_theorem" title="Fenchel&#39;s duality theorem">Fenchel dual problem</a>。</p>
</li>
<li><p>Lagrangian dual problem 需要先形成一个L函数，这个原问题的解是，</p>
</li>
</ul>
<ul>
<li>todo</li>
</ul>
<ul>
<li> 用数学表达：</li>
</ul>
<p>凸优化问题：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-0c145b1194ca5146.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="standard form for convex minimization problem.jpg"></p>
<p>reference:<br>[1]<a href="https://cowles.yale.edu/sites/default/files/files/pub/d00/d0080.pdf">https://cowles.yale.edu/sites/default/files/files/pub/d00/d0080.pdf</a><br>[2]<a href="https://en.wikipedia.org/wiki/Convex_optimization">https://en.wikipedia.org/wiki/Convex_optimization</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>常见算法复杂度</title>
    <url>/2021/04/05/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/</url>
    <content><![CDATA[<br>

<ul>
<li><p>复杂度</p>
<span id="more"></span>
<ul>
<li><p>排序</p>
<ul>
<li>冒泡、直选、直插 n^2 – 对每个元素找位置、两个for循环嵌套</li>
<li>快排、堆、希尔、归并 nlogn – 二分思想(树)</li>
<li>计数 n+max – 先遍历放到max长度数组，再遍历数组取数</li>
<li>基数 n*位数 – 每位都来来一遍分配</li>
<li>不稳定算法： “快希选堆”（快牺牲稳定性） </li>
</ul>
</li>
<li><p>查找</p>
<ul>
<li>分块、二分、二叉排序查找  logn</li>
<li>顺序查找 n</li>
<li>哈希  O(1)</li>
</ul>
</li>
<li><p>遍历</p>
<ul>
<li>树遍历：时间复杂度为O(n)，同样空间复杂度也为O(n)，n为结点数。</li>
<li>图遍历： 邻接表O(V+E) 邻接矩阵O(V^2)</li>
</ul>
</li>
<li><p>红黑</p>
<ul>
<li>插入、删除、查找的最坏时间复杂度都为 O(logn)。</li>
</ul>
</li>
<li><p>b树</p>
<ul>
<li>平衡二叉树没能充分利用磁盘预读功能<br>B树是为了充分利用磁盘预读功能来而创建的一种数据结构<br>专门做索引而发明</li>
</ul>
</li>
<li><p>b+</p>
<ul>
<li>查找，(m/2) * log(m)n</li>
</ul>
</li>
<li><p>b b+区别</p>
<ul>
<li>叶子才有数据-&gt;减少磁盘io</li>
<li>利于范围查询</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>庄家</title>
    <url>/2021/02/22/%E5%BA%84%E5%AE%B6/</url>
    <content><![CDATA[<br>
<br>

<p>Q1：庄家坐庄步骤？<br>1吸货 2拉升 3洗盘 4出货✅<br>1吸货 2洗盘 3拉升 4出货❎</p>
<span id="more"></span>
<p>需要先拉升，在于底部吸货不足，卖家少。<br>拉升2倍左右，开始洗盘，洗盘过程中获利盘出货，给庄家吸货机会。这同时也抬高了吸货成本。<br>洗盘时间视情况而定，有时一个月左右。</p>
<p>Q2：庄家吸货方法？<br>跌停板吸货 （亿安科技）<br>熊市顺势低吸（底部但是爆量）<br>抬升吸货 （洗盘但是无量）</p>
<p>Q3：洗盘？<br>洗盘的目的：洗掉大户和集中的持股人。避免在高位卖出砸盘。<br>大户：几万、几十万都不是大户，几百万、上千万股的”人“。这样的人，属于对手。（主力和老庄家(被套的)–对手）</p>
<p>Q4：主力和庄家？</p>
<p>Q5：出货？<br>拉高出货，以变现锁定的获利。<br>成本区在之前的拉升和吸货之间的位置。<br>填权再拉。<br>然后高位震荡也能出货。<br>之前的拉升，净吃入。拉高出货不能吃，要卖。拉两三天，做个平台，拉两三天再做个平台，别人就会认为每次到平台就突破，就会买入。如果大盘面差，要跑的时候，向下钓鱼出货法–开始砸、跌停板打下来，然后巨大买盘，买盘巨大，不停再往上推，就会有买盘跟进，卖盘都是庄的，择机再砸掉买盘，又往上推，再砸，再推。1天可以卖掉好几千万资金。利用追涨杀跌。</p>
<p>Q6：大波段操作<br>胆大心细。有胆量入场，指定止损–买入后预计涨结果跌了、或者预计强势结果未强势–&gt;改错择机出来。<br>买错即卖。<br>国家政策–&gt;板块和强势股(股票强度、涨停板)–&gt;龙头、二三龙头、补涨股等<br>–&gt;F10基本面符合–&gt;看技术面，平台突破点找买点</p>
<p>Q7：二龙头补涨<br>二龙头的突破(某一天放量、十几分钟拉了3%-5%、分时线k线高于原来的平台)时加仓。补涨等不到回调机会。<br>卖出点只和技术面有关。<br>跌破五日线，先卖1/3。五日线十日线高位死叉卖1/3。死叉后又跌则全出。</p>
<p><img src="/2021/02/22/%E5%BA%84%E5%AE%B6/yakj.png" alt="yakj"></p>
]]></content>
      <categories>
        <category>股市理论</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>股市理论</tag>
      </tags>
  </entry>
  <entry>
    <title>微信的GraphTR模型</title>
    <url>/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h4 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h4><p>“迁移学习+多任务”的场景<br>通过将不同领域的节点、关系都建模在一幅图中，通过图卷积，完成知识从数据丰富的领域向数据稀疏领域的迁移，并兼顾两个领域的指标。</p>
<span id="more"></span>

<p>多 域信息的异构图上完成图卷积，每个节点要聚合来自多个领域的异构消息。之前传统的聚合方式，如mean/max pooling，矩阵相乘，可能带来异构消息相互抵销而引入信息损失。</p>
<p>为此微信团队采用了GraphSAGE+FM+Transformer多种手段，从不同粒度来交叉、聚合消息，极大提升了模型的表示能力，这种新的消息聚合方式值得借鉴。</p>
<h4 id="场景及难点"><a href="#场景及难点" class="headerlink" title="场景及难点"></a>场景及难点</h4><p>1、微信团队面临的场景是：</p>
<p>每个视频都打有若干tag（人工标注或由内容理解算法打上的）<br>用户观看视频时，需要有算法从这个视频自带的tag中挑选出与当前用户最相关的若干个tag，展示在视频的下方。<br>用户点击某个tag，会进入一个沉浸式频道，其中展现的全部是与该tag相关的视频</p>
<p>2、难点在于：用户点击视频的行为比较丰富，但是用户点击tag的行为比较稀疏，训练数据不足。</p>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>方案一：<br>训练一个模型，输入视频的多模态信息(标题、封面图、关键帧)，输出是与这个视频最match的tag。训练时，拿人工打标的结果作为label。线上serving时，将预测出来的top-K个标签，展示在视频的下方。</p>
<p>这个方案可行，但是其只利用了视频的静态属性，没有用户的信息，所以推荐出来的tag只有与视频在语义上的相关性，完全没有针对当前用户的个性化，不满足业务需求。</p>
<p>方案二：<br>1、tag embedding用tag的word embedding。<br>2、用户的embedding是其过去有过”正交互”的tag embedding的pooling<br>所谓“正交互”，可以是用户过去一段时间内点击过的tag<br>但是考虑到user-tag的交互太稀疏，因此可以选用户过去点击过视频所携带的tag<br>pooling时，也可以考虑进播放完成度、时间衰减等因素，进行加权平均。<br>3、线上serving时，拿user embedding在当前视频所携带的tag embedding中寻找Top-K近邻，展示在视频下方。</p>
<p>怎么评价这一方案：</p>
<p>1、该方案，考虑了用户的历史，有更强的个性化。<br>2、但是拿word embedding做tag embedding，仍然只考虑了tag的语义信息。用户行为蕴含的信息，要比语义信息更加重要。<br>3、用户与tag的交互行为太少了，很难在“用户点击tag的序列”上套用word2vec来学习到tag embedding</p>
<p>方案三：微信的GraphTR模型</p>
<hr>
<h4 id="优化点"><a href="#优化点" class="headerlink" title="优化点"></a>优化点</h4><p>GraphTR是为了要<em>学习优质tag embedding</em>，为此要注重利用用户的行为信息<br>但是由于user-tag的行为太稀疏，因此GraphTR需要<em>通过user-video的行为学习到tag embedding</em><br>要达成以上目标，也有多种作法。而GraphTR的做法是：</p>
<p>1、将user, video, tag（还加上video的来源media）都放入一个大的异构图<br>通过图卷积，学习到video embedding</p>
<p>2、再建模video与video之间的相关性（比如在同一个session中播放过）</p>
<p>3、因为video embedding融合了tag embedding，因此在优化目标达成之后，一个优质的副产品就是得到tag embedding</p>
<h4 id="GraphTR是如何构建这个异构图的？"><a href="#GraphTR是如何构建这个异构图的？" class="headerlink" title="GraphTR是如何构建这个异构图的？"></a>GraphTR是如何构建这个异构图的？</h4><p>1、node：</p>
<p>图上要包括：user, video, tag, media (视频来源)这 4类节点。<br>因为用户数目太多，而每个用户的行为相对稀疏，GraphTR将用户按照gender-age-location分成84000组，用user group替代user，在图中建模。</p>
<p>2、edge：<br>video-video：同属一个观看session中的两video之间有边<br>user-video：某视频被某user group一周观看超过3次<br>video-tag：video和其携带的tag<br>video-media：video和其来源<br>tag-tag：两个 tag属于同一个视频</p>
<h4 id="如何传递、融合图上异构节点的信息？"><a href="#如何传递、融合图上异构节点的信息？" class="headerlink" title="如何传递、融合图上异构节点的信息？"></a>如何传递、融合图上异构节点的信息？</h4><p>1、为了完成user, video, tag, media这四类节点的信息融合，GraphTR设计了3层卷积结构，称为Heterogeneous field interaction network (HFIN)。</p>
<p>2、最底层Heterogeneous Feature Layer：<br>3-hop的embdding是lookup获得的，分别有四个域(user/video/tag/media域的特征)，相加得到2-hop邻居的embedding。</p>
<p>3、中间层：Multi-field Interaction Layer：<br>这一层的任务是由2-hop邻居的embedding，聚合生成1-hop邻居的embedding。<br>而HFIN采用了GraphSAGE+FM+Transformer三种方式，粒度上从由粗到细，完成聚合。</p>
<h4 id="三种聚合方式"><a href="#三种聚合方式" class="headerlink" title="三种聚合方式"></a>三种聚合方式</h4><p>1、GraphSAGE聚合<br>graphsage聚合<br><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/graphsage%E8%81%9A%E5%90%88.png" alt="graphsage聚合"></p>
<p>这里的hGraph就是1-hop的最终embding。</p>
<p>2、FM 聚合<br>FM聚合，区分各域，因此粒度更细一些。<br><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/fm%E8%81%9A%E5%90%88.png" alt="fm聚合"></p>
<p>hFM2就是1-hop的最终embding。</p>
<p>3、Multi-field transformer aggregator</p>
<p>GraphTR觉得FM聚合时，各域节点（即各域特征）交叉得还不够：1:FM聚合，只有在第2步才做域与域之间的交叉。2:在一个域内部，这n+1个特征之间，只有简单pooling，不存在交叉。3:FM聚合的第1步，每个域average pooling的是，这1+n个节点的原始特征。</p>
<p>Transformer聚合，希望增强各域节点（即各域特征）的交叉。步骤如下：</p>
<p>S1:Transformer决定在第1步引入交叉。具体方式就是，在一个域的1+n个节点之间进行Transformer变换，重新生成1+n个向量，每个新向量是老向量的加权平均，权重是当前老向量相对于其他老向量的attention score。(一套attention恐怕没有代表性，还引入多头机制)</p>
<p><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/transformer%E8%81%9A%E5%90%88.png" alt="transformer聚合"></p>
<p>S2:再拿生成的1+n个新向量，做average pooling。</p>
<p>S3:最后将“域间交叉结果”与”域内交叉结果”拼接在一起返回，作为由Transformer聚合得到的1-hop邻居的embedding。论文的实验结果证明，这个最复杂、最细粒度的聚合，对于模型性能的提升也最大。</p>
<p>4、通过三种聚合方式，我们就可以得到1-hop邻居的最终embedding，是这三种聚合结果的concat。</p>
<p>5、最上层：The Second Aggregation Layer<br>这一层负责由1-hop邻居节点（1个target node自身，m个邻居节点，一共1+m个）的embedding(下边公式中的矩阵H)，生成target node上的embedding。聚合方式也是基于Transformer的。</p>
<p>根据1+m个原向量，生成1+m个新向量，每个新向量是所有老向量的加权平均，权重是当前原向量与其他原向量的attention score<br>再拿这1+m个新向量，取平均，得到target node上的最终向量表示。</p>
<h4 id="如何定义loss"><a href="#如何定义loss" class="headerlink" title="如何定义loss?"></a>如何定义loss?</h4><p>通过以上三层卷积，就能够给图上所有类型的所有节点，都产生一个embedding。接下来的问题就是，如何定义优化目标，使这些节点的embedding得到优化？</p>
<p>这一部分的解决方案比较常规，无非就是建模节点之间的相关性，可以有选择是:</p>
<p>建模user-tag之间的相关性，user与点击过的tag之间的距离要尽可能小。但是user-tag之间交互的数据太少；建模user-video之间的相关性，user与点击过的视频之间，距离应该较近。但是图上建模的不是单个user而是user group，一个user group包含的用户兴趣太复杂，拿user-goup与video训练，可能噪声比较大；建模video-video之间的相关性，在同一个session被观看的视频之间，距离要尽可能小。因为video的点击行为比较多，这方面的数据比较丰富，文中采用的是这种方案。</p>
<p><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/loss.png" alt="loss"></p>
<h4 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h4><p>将这些tag emedding代入上文的”第二个方案”，即拿用户观看过视频携带的tag的embedding加权平均得到user embedding，再拿这个user embedding在当前视频所携带的tag的embedding中寻找出距离最近的top-k个tag，作为推荐结果显示在视频的下方。因为这些tag embedding蕴含了丰富的user-video行为信息，不仅有助于提升用户对tag的点击率，也有助于提升进入沉浸式tag频道后的观看时长。</p>
<h4 id="借鉴"><a href="#借鉴" class="headerlink" title="借鉴"></a>借鉴</h4><p>1、数据少的领域如何借力于数据多的领域，同时要兼顾两个领域的优化目标：<br>通过将不同领域的不同节点、关系建立在一张异构图上，通过图卷积，使得每个节点的embedding都浓缩了多个领域的知识，达成了“知识迁移+目标兼顾”。</p>
<p>2、GraphTR采用了GraphSAGE+FM+Transformer多种手段，粒度上从粗到细，交叉、聚合来自不同领域的异构消息，相比于mean/max pooling、浅层FC等传统聚合方式，极大提升了模型的表达能力。</p>
]]></content>
      <categories>
        <category>模型框架</category>
      </categories>
      <tags>
        <tag>dl</tag>
        <tag>模型框架</tag>
      </tags>
  </entry>
  <entry>
    <title>思想深度</title>
    <url>/2021/02/22/%E6%80%9D%E6%83%B3%E6%B7%B1%E5%BA%A6/</url>
    <content><![CDATA[<br>
<br>

<h4 id="观点"><a href="#观点" class="headerlink" title="观点"></a>观点</h4><ul>
<li><p>计划经济是狂妄、愚蠢。</p>
<span id="more"></span></li>
<li><p>小波动基本是没意义的，一个现象进入分析的视野，已经承认这现象不属于小的波动，这点是必须明确的。</p>
</li>
<li><p>有领涨的并不一定都能最终走出行情，即使是上涨的趋势，行情还可以分大、中、小。</p>
</li>
<li><p>一个股票中的常见现象，就是当前一段领涨个股和大盘走势相背离的时候，往往意味着一个结构性震荡的来临，这一点至少在股票中是通用的，而76年的改变其实也是很符合这一点的。 </p>
</li>
<li><p>汉奸，当然可以有很高的的艺术造诣。所谓坏人就不能有优点了？无所谓好坏吧。</p>
</li>
<li><p>真正的你又何曾生死，生死又与你何干？。而你的心究竟又是哪个心？</p>
</li>
<li><p>所有的现代战争，从根本意义上就是货币战争。</p>
</li>
<li><p>和庄股一样，目前的关键是不能让资本大量逃离，否则就会连续崩盘跳水。</p>
</li>
</ul>
<br>

<h4 id="民族复兴周期-与-世界经济周期-历史性共振下的-国家地缘与货币战略"><a href="#民族复兴周期-与-世界经济周期-历史性共振下的-国家地缘与货币战略" class="headerlink" title="民族复兴周期 与 世界经济周期 历史性共振下的 国家地缘与货币战略"></a>民族复兴周期 与 世界经济周期 历史性共振下的 国家地缘与货币战略</h4><p>框架：</p>
<p>1、从历史大现象规律出发，得到–&gt;当前为强盛时期。</p>
<p>2、由马克思的”五阶段论”，得到–&gt;列宁的社会主义本质是公有制，斯大林的社会主义是资本主义–因此苏东突变后权力或权力资本迅速转为资本主义(本来就同源)，毛泽东的文革，重点放在了人与人的关系而非人与自然，而不能真的实现反资。邓小平的中国特色社会主义和社会主义初级阶段策略同样也应该是一个民族主义的策略。</p>
<blockquote>
<p>社会中一部分人对另一部分人不再存在依附关系，而是全社会的人都毫不例外地依附于一个非自然的身外之物：资本，就叫做资本主义社会。<br>因此人与自然的关系被打破，不再依附自然。</p>
</blockquote>
<p>3、文革的必然失败–&gt;使得面对资本全球化成了无可逃避的现实</p>
<p>4、1得到中国处于强盛期 + 23得到世界资本全球化–&gt;机遇<br>为什么是力量在中国？：从霸业的人口上得到的。大不列颠王国以5000万，美国和苏联在2亿5千万，下一个12亿5千万。<br>时间推定：1929英德老的5千万级别主导循环结束；美苏90年的循环在一半1974年形成了石油危机的中型调整，美苏这两个不同类型的资本主义之间的同级别竞争以美国的胜利结束；然后到2000年美国出现高点，开始调整；2019年中国开始。</p>
<p>5、地缘战略：以环渤海湾地区、珠江三角洲地区、秦川地区建构大的战略三角，成为亚洲之王，其领土（或附庸性质的影响）应该从乌拉尔山往东直到大海与美洲对望，从北冰洋直到太平洋俯视澳洲，形成世界的中轴，让欧洲和美洲成为其两翼</p>
<p>6、货币：<br>最有竞争力的货币将是美元、欧元、卢比和人民币。<br>在目前阶段一定要坚持对美元采取一种不挂钩的挂钩政策，坚决长期地维持人民币对美元的币值稳定。<br>逐步扩大对亚洲区的影响，取代日元的地位，逐步成为实质亚洲货币。<br>利用第一个阶段形成的对美元的极大落差，配合世界经济大循环周期，选择时机释放，将美元在一次精心策划的战役中一次性击毁。最后在一个长期反复、拉锯的过程中，利用新的12亿5千万级别世界经济大循环周期确立中国对美国的领先地位。</p>
<p>7、分析美国：<br>美国操纵汇率–&gt;为了其总体利益服务的。<br>美国危机–&gt;泡沫化，0以下的储蓄率<br>2000年的下跌速度极快–&gt;大规模的资本逃离还没有出现<br>目前的大级别反弹–&gt;构成资本逃离的机会，一旦反弹到位，出现大资本逃离。<br>为了避免大资本逃离–&gt; 大反弹到位前把货币贬值到一个相应的地位，这样才使得美圆资本套现后不能以一个较高的汇率出逃</p>
<p>但，如果有一个容量极大的货币紧贴美圆，则美圆贬值的所有如意算盘将打不响，而人民币正好就是这种货币。人民币与美圆的挂钩使得美圆资产变现以后有了一个顺畅的逃跑渠道。</p>
<p>这也是以前帖子里面预测2019年90年大周期世界经济大危机的现实基础，正确的人民币战略将加快、加深这个进程。</p>
<hr>
<p>综上，</p>
<p>1、够有想象力，三角洲和攻击美元的策略也能想得出，而且还自圆其说<br>2、对社与资(列林斯大林)的理解，我闻所未闻<br>3、历史的宏观视角，让我有大历史之感，得到一些规律，证明论点<br>4、美国现象到本质的精辟概括</p>
<h4 id="我的观点"><a href="#我的观点" class="headerlink" title="我的观点"></a>我的观点</h4><p>1、出得了方案<br>2、给得了完备解释<br>3、理解深刻性或者说独到处，一语成谶<br>4、精辟</p>
<p>=&gt;想象力、知识沉淀、思考本质 =&gt;精辟且自洽</p>
]]></content>
      <categories>
        <category>历史</category>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>历史</tag>
      </tags>
  </entry>
  <entry>
    <title>投资逻辑摘抄</title>
    <url>/2021/05/12/%E6%8A%95%E8%B5%84%E9%80%BB%E8%BE%91%E6%91%98%E6%8A%84/</url>
    <content><![CDATA[<br>
<br>



<h1 id="David-Swensen："><a href="#David-Swensen：" class="headerlink" title="David Swensen："></a>David Swensen：</h1><span id="more"></span>

<p>1、Led by Tobin’s ideas, he stressed<strong>asset allocation rather than stock picking, or attempts to time the market</strong> — beyond the mechanical market timing that came with his policy of regularly rebalancing Yale’s portfolio. At the margin, that entailed <strong>buying more of assets that had done badly and selling some of those that had done well</strong>.</p>
<p>2、With the public markets deeply liquid and exhaustively researched, there was no point in trying to beat them. But <strong>private markets were less efficient</strong>, and he could reasonably hope to find bargains if his team was smart enough. </p>
<p>3、The first was that equities were indubitably better than bonds or cash for the longer term — and that “equities” should not be restricted merely to shares traded on public stock exchanges but should include <strong>any investment with a non-guaranteed upside for the investor</strong>. His second was that diversification was important. </p>
<p>4、 He regularly attacked excessive fees in his later years. If his team couldn’t find any place where Yale’s long-term horizon might give them a chance to beat the market, he might even have left money in index funds.</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统</title>
    <url>/2021/04/05/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<br>

<p>推荐系统</p>
<span id="more"></span>

<ul>
<li>冷启动<ul>
<li>用户冷启动<ul>
<li>热门</li>
<li>内容品类 多样性探索</li>
<li>用户画像</li>
<li>快速探索(各个筛选的选择tab)</li>
<li>主产品的用户迁移</li>
</ul>
</li>
<li>作品冷启动<ul>
<li>标的物跟用户行为的相似性</li>
<li>利用标的物跟标的物的相似性</li>
</ul>
</li>
<li>用户自己搜索、自己筛选、选择主题等</li>
</ul>
</li>
</ul>
<ul>
<li><p>es向量召回</p>
<ul>
<li><p>订单规则太多，导致召回排序后没多少曝光    </p>
<ul>
<li>走es召回。先过滤再取topN</li>
</ul>
</li>
<li><p>订单主页的日活100w 直播间dau30w 聊天室10w</p>
</li>
<li><p>冷门无机会曝光</p>
<ul>
<li>具体哪一个阶段导致，然后加入策略</li>
</ul>
</li>
<li><p>item侧的embding隔天变化。user的emb实时更新<br>  请求来了去es的item里找top</p>
</li>
<li><p>es比fassi的好处</p>
</li>
<li><p>性能</p>
<ul>
<li>qps100左右，128的emb，召回压力不大</li>
</ul>
</li>
</ul>
</li>
<li><p>向量距离方法</p>
<ul>
<li>fassi</li>
<li>kdtree</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>提升方法</title>
    <url>/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<br>

<h4 id="bagging和boosting"><a href="#bagging和boosting" class="headerlink" title="bagging和boosting"></a>bagging和boosting</h4><p>1、区别？<br>采样：随机采样(Bootstrap sampling–有放回)、加大错误样本权重<br>特征的采样</p>
<span id="more"></span>
<p>并行计算上<br>弱学习器的权重</p>
<p>2、从偏差和方差的角度解释bagging和boosting的原理？<br>bagging：<br>重采样、权重也相同–模型的区别性不大，bias小。<br>但如果假设各个子模型独立，则显著降低variance。如果完全相同的子模型，则var和单个模型一样。bagging属于两者之间，一定程度降低了var。RF特征上随机选择，进一步降低了模型的相关性，从而进一步降低了var。</p>
<p>boosting:<br>前向分步学习算法，是sequencial地减少损失函数，loss是逐步地下降的，bias也随之逐步下降。但由于是这种sequence、adaptive地，模型相关性较高，不能显著减少var。</p>
<h4 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h4><p>1平均法；2投票法：多数投票、绝对多数投票、加权投票；3学习法。<br>学习法，代表方法是stacking。stacking是再加上一层学习器，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。</p>
<h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><p>1、Adaboost是模型为加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题。</p>
<p><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E5%8A%A0%E6%B3%95%E5%89%8D%E5%90%91.png" alt="加法前向"></p>
<p><strong>为什么loss是指数函数？证明如下：</strong></p>
<p>————————————————————————————————————————————————————————————<br><br><br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/exp%E8%AF%81%E6%98%8E1.png" alt="exp证明1"><br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/exp%E8%AF%81%E6%98%8E2.png" alt="exp证明2"><br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/exp%E8%AF%81%E6%98%8E3.png" alt="exp证明3"></p>
<!-- <img src=exp证明1.png width=80% height=450>
<img src=exp证明2.png width=80% height=700>
<img src=exp证明3.png width=80% height=300> -->
<br>
——————————————————————————————————————————————————————————--

<p>2、分类器结合时的权重？<br>由于Adaboost中若干个分类器的关系是第N个分类器更可能分对第N-1个分类器没分对的数据，而不能保证以前分对的数据也能同时分对。所以在Adaboost中，每个弱分类器都有各自最关注的点，每个弱分类器都只关注整个数据集的中一部分数据，所以它们必然是共同组合在一起才能发挥出作用。所以最终投票表决时，需要根据弱分类器的权重来进行加权投票，权重大小是根据弱分类器的分类错误率计算得出的，总的规律就是弱分类器错误率越低，其权重就越高。</p>
<p>计算公式：</p>
<p>1 误差<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E8%AF%AF%E5%B7%AE.png" alt="误差"></p>
<!-- <img src=误差.png width=60% height=150>
 -->
<p>2 权重系数<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E5%BC%B1%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%83%E9%87%8D%E7%B3%BB%E6%95%B0.png" alt="弱分类器权重系数"></p>
<!-- <img src=弱分类器权重系数.png width=60% height=100> -->

<p>3 样本权重<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E6%A0%B7%E6%9C%AC%E6%9D%83%E9%87%8D.png" alt="样本权重"></p>
<!-- <img src=样本权重.png width=80% height=300> -->

<p>4 分类器结合及最终分类器<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E5%88%86%E7%B1%BB%E5%99%A8%E7%BB%93%E5%90%88.png" alt="分类器结合"></p>
<!-- <img src=分类器结合.png width=80% height=230> -->


<p>由上面的公式，可得到：<br>每次新增加一个弱分类器的时候，前面的弱分类器分错的样本的权重占总样本权重的0.5，前面弱分类器分对的样本等权重也占总样本权重的0.5。</p>
<p>3、正则化</p>
<p>fn = fn-1 + θ * a * G<br>θ 为正则化项</p>
<p>4、评价</p>
<p>可解释性<br>参数个数<br>performance<br>异常点敏感<br>弱分类器选择<br>可用于特征选择</p>
<h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p>1、区别于adaboost</p>
<p>Adaboost是通过提高错分样本的权重来定位模型的不足，GBDT是通过负梯度来定位模型的不足，因此GBDT可以使用更多种类的损失函数。由于loss可以选择更鲁棒的，对于adaboost存在异常点敏感的问题,gbdt更健壮。</p>
<p>可以灵活处理离散和连续值。</p>
<p>分类的GBDT：是用指数损失函数，此时GBDT退化为Adaboost算法。<br>另一种方法是用类似于逻辑回归的对数似然损失函数的方法。</p>
<p>使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。有很多人对GBDT算法进行了开源代码的开发，比较火的是陈天奇的XGBoost和微软的LightGBM。</p>
<p>2、为什么只能分类树</p>
<p>GBDT的核心在于累加所有树的结果作为最终结果，而分类树的结果显然是没办法累加的，所以GBDT中的树都是回归树，不是分类树。</p>
<p>3、损失函数有哪些？</p>
<p>1指数损失；2对数损失；3均方差(如果我们选择平方损失函数，那么这个差值其实就是我们平常所说的残差。)；4绝对损失；5Huber损失(它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。)；6分位数损失。56主要用于健壮回归，也就是减少异常点对损失函数的影响。</p>
<p>4、例子</p>
<p><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/gbdt%E4%BE%8B%E5%AD%90.png" alt="gbdt例子"></p>
<p>5、SGBDT</p>
<p>子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。</p>
<p>6、构造特征输入LR<br>如果我们想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力。Facebook 在2014年 发表的一篇论文便是这种尝试下的产物，利用gbdt去产生有效的特征组合，以便用于逻辑回归的训练，提升模型最终的效果。如我们 使用 GBDT 生成了两棵树，两颗树一共有五个叶子节点。我们将样本 X 输入到两颗树当中去，样本X 落在了第一棵树的第二个叶子节点，第二颗树的第一个叶子节点，于是我们便可以依次构建一个五纬的特征向量，每一个纬度代表了一个叶子节点，样本落在这个叶子节点上面的话那么值为1，没有落在该叶子节点的话，那么值为 0。于是对于该样本，我们可以得到一个向量[0,1,0,1,0] 作为该样本的组合特征，和原来的特征一起输入到逻辑回归当中进行训练。实验证明这样会得到比较显著的效果提升。</p>
<p>7、CART分类树过程<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/cart%E5%88%86%E7%B1%BB%E8%BF%87%E7%A8%8B.png" alt="cart分类过程"></p>
<p>8、相比于传统的LR，SVM效果为什么好一些</p>
<ul>
<li><p>GBDT基于树模型，继承了树模型的优点 [对异常点鲁棒、不相关的特征干扰性低（LR需要加正则）、可以很好地处理缺失值、受噪音的干扰小]</p>
</li>
<li><p>处理 missing feature</p>
</li>
<li><p>数据规模影响不大，因为我们对弱分类器的要求不高，作为弱分类器的决策树的深 度一般设的比较小，即使是大数据量，也可以方便处理。像 SVM 这种数据规模大的时候训练会比较麻烦。</p>
</li>
<li><p> 通常在给定的不带噪音的问题上，他能达到的最佳分类效果还是不如 SVM，逻辑回归之类的。<br>实际问题中，往往有很大的噪音，使得 Decision Tree 这个弱势就不那么明显了。</p>
</li>
</ul>
<p>9、加速训练？<br>是否预排序,预排序可以加速查找最佳分裂点（不确定）.在样本规模上的并行计算。</p>
<p>10、参数</p>
<ul>
<li><p>第一类Miscellaneous Parameters </p>
</li>
<li><p>第二类：Boosting Parameters:<br>n_estimators 最大弱学习器的个数，太小欠拟合，太大过拟合<br>learning_rate 学习率，太大过拟合，一般很小0.1，和n_estimators一起调<br>subsample 子采样，防止过拟合，太小欠拟合。GBDT中是不放回采样</p>
</li>
<li><p>第三类：Tree-Specific Parameters<br>max_features 最大特征数<br>max_depth 最大树深，太大过拟合<br>min_samples_split 内部节点再划分所需最小样本数，越大越防过拟合<br>min_weight_fraction_leaf 叶子节点最小的样本权重和。如果存在较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。越大越防过拟合<br>max_leaf_nodes:最大叶子节点数 ，太大过拟合<br>min_impurity_split:节点划分最小不纯度<br>presort:是否对数据进行预分类，以加快拟合中最佳分裂点的发现。默认False。非稀疏数据则预排序，若稀疏数据则不预排序。小规模数据预排序。</p>
</li>
</ul>
<p>11、调参思路</p>
<p>1、首先使用默认的参数，进行数据拟合；<br>2、从步长(learning rate)和迭代次数(n_estimators)入手；一般来说,开始选择一个较小的步长来网格搜索最好的迭代次数。这里，可以将步长初始值设置为0.1。对于迭代次数进行网格搜索；<br>3、接下来对决策树的参数进行寻优<br>4、首先我们对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索。【min_samples_split暂时不能一起定下来，因为这个还和决策树其他的参数存在关联】<br>5、接着再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参；做到这里，min_samples_split要做两次网格寻优，一次是树的最大深度max_depth，一次是叶子节点最少样本数min_samples_leaf。<br>【具体观察min_samples_split的值是否落在边界上，如果是可以进一步寻优】<br>6、继续对最大特征数max_features进行网格搜索。做完这一步可以看看寻找出的最优参数组合给出的分类器的效果。<br>7、可以进一步考虑对子采样的比例进行网格搜索，得到subsample的寻优参数<br>8、回归到第2步调整设定的步长(learning rate)和迭代次数(n_estimators)，注意两者的乘积保持不变，这里可以分析得到：通过减小步长可以提高泛化能力，但是步长设定过小，也会导致拟合效果反而变差，也就是说，步长不能设置的过小。</p>
<hr>
<p>mind:</p>
<p><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E6%8F%90%E5%8D%87.png" alt="提升"></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>整体思维</title>
    <url>/2021/04/15/%E6%95%B4%E4%BD%93%E6%80%9D%E7%BB%B4/</url>
    <content><![CDATA[<br>

<p>人生怎么走，成为什么人</p>
<span id="more"></span>



<p>时间，大多被浪费。各种事情打散了力量。</p>
<p>能量如果不凝聚起来，和热量有什么大区别？</p>
<p>深度。体系地思考决定深度。而小聪明，片段化地推理，没有温故和反复的逻辑，无法体现深度。</p>
<p>体系，需要书籍、积累、重复构建。需要阅历、经验和磨炼。有质量地思考。</p>
<p>力求成为什么样的人，就去照着那样行动。不要太过顾忌，在意别人的想法。甚至有时候你觉得，这样是不是太故作姿态了。但是也要做。不去模仿，没有可能性成为。</p>
<p>理性和思考，让你自信。大家都能想到的思路和方案，不是你的特殊性。要在你自己的特殊性上发力，要进一步去思考和抽象。而不是停留在表象，或者前滩。其实这样的随笔也不过是警戒。不必要写太多，起到作用就行。更多还是要自己去创造。走到最前方。关注着有思想的人。</p>
<p>去解决问题。</p>
<p>研究生博士生上不了，你能够有创造出产品的能力吗。研究生博士生最终还是为了创造和能力的提升。</p>
<p>你能到达的优秀必须有厚重的基底。否则还是走不远。</p>
<p>我需要去读。因为我需要时间再去沉淀。我看得到自己的智慧，我希望能够发光。而不是在局部里挣扎。</p>
<p>不过始终还是为了站得更高。</p>
<p>语言、倾听、拒绝、让人信赖……</p>
<p>都是要你去反思和提升深度广度。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>机器</title>
    <url>/2021/04/05/%E6%9C%BA%E5%99%A8/</url>
    <content><![CDATA[<p>物理cpu、逻辑cpu、内存、硬盘、gpu</p>
<span id="more"></span>

<p>命令：<br>cat /proc/cpuinfo | grep “physical id” | uniq | wc -l<br>cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c<br>cat /proc/cpuinfo | grep physical | uniq -c<br>grep MemTotal /proc/meminfo<br>df -h<br>nvidia-smi<br>lspci | grep -i nvidia</p>
<p>1    prodbigdata-aitm-distributed-job001 10.111.20.180    [DATACOMMAND]           1<em>4c  8g 200g                                                                                 <br>2    prodbigdata-aitm-distributed-job002 10.111.20.181    [DATACOMMAND]              1</em>4c  8g 200g                                                                                 <br>3    prodbigdata-algo001                 10.111.4.208     [DATACOMMAND]                      1<em>16c  16g 200g                                                                         <br>4    prodbigdata-algo002                 10.111.51.113    [DATACOMMAND]                       1</em>8c  16g 256g                                                                      <br>5    prodbigdata-algo004                 10.111.4.204     [DATACOMMAND]                          1<em>16c  16g 200g                                                                      <br>6    prodbigdata-algo005                 10.111.4.205     [DATACOMMAND]                          1</em>16c  16g 200g                                                                      <br>7    prodbigdata-rec-algo-service001     10.111.106.61    [DATACOMMAND]                                                                                            <br>8    prodbigdata-rec-algo-train001       10.111.106.62    [DATACOMMAND]                       1<em>54c 500g   PB级别     8gpu 每个显存16g                                                              <br>9    prodbigdata-youtube-dnn001          10.111.22.113    [DATACOMMAND]                      1</em>16c 120g  1T             2gpu 每个显存16g                                                   <br>10   prodbigdata-ypp-keras-frcnn         10.111.3.220     [DATACOMMAND]                        1*16c 120g  1T             2gpu 每个显存16g                                                                          <br>11   testbigdata-kafka-streaming-job001  10.111.51.50     [DATACOMMAND]            </p>
<p>Mon Mar 22 12:53:07 2021       <br>+—————————————————————————–+<br>| NVIDIA-SMI 430.40       Driver Version: 430.40       CUDA Version: 10.1     |<br>|——————————-+———————-+———————-+<br>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>|===============================+======================+======================|<br>|   0  Tesla P100-PCIE…  Off  | 00000000:00:09.0 Off |                    0 |<br>| N/A   29C    P0    32W / 250W |   1419MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   1  Tesla P100-PCIE…  Off  | 00000000:00:0A.0 Off |                    0 |<br>| N/A   28C    P0    25W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   2  Tesla P100-PCIE…  Off  | 00000000:00:0B.0 Off |                    0 |<br>| N/A   29C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   3  Tesla P100-PCIE…  Off  | 00000000:00:0C.0 Off |                    0 |<br>| N/A   34C    P0    27W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   4  Tesla P100-PCIE…  Off  | 00000000:00:0D.0 Off |                    0 |<br>| N/A   32C    P0    31W / 250W |   2384MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   5  Tesla P100-PCIE…  Off  | 00000000:00:0E.0 Off |                    0 |<br>| N/A   35C    P0    34W / 250W |    263MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   6  Tesla P100-PCIE…  Off  | 00000000:00:0F.0 Off |                    0 |<br>| N/A   27C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   7  Tesla P100-PCIE…  Off  | 00000000:00:10.0 Off |                    0 |<br>| N/A   29C    P0    31W / 250W |   1301MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+</p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>样本</title>
    <url>/2021/04/05/%E6%A0%B7%E6%9C%AC/</url>
    <content><![CDATA[<ul>
<li><p>正样本稀疏</p>
<span id="more"></span>
<ul>
<li>欠采样、过采样</li>
<li>集成<ul>
<li>负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据<br>  然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。</li>
<li>这种解决问题的思路类似于随机森林</li>
</ul>
</li>
<li>权重<ul>
<li>不同样本数量的类别分别赋予不同的权重</li>
</ul>
</li>
<li>一分类<ul>
<li>把它看做一分类（one class learning） 或异常检测问题，这类方法的重点不在于捕捉类间的差别，而是为其中一类进行建模，比较有代表性的是 one-class-SVM。</li>
<li>符合这些图像特征的就属于人脸，反之则不是。对比二分类，显著的区别就是，二分类不但能的出来这个图片不是人脸，他还能告诉你这个图片是猪脸。</li>
</ul>
</li>
</ul>
</li>
<li><p>推荐之样本</p>
<ul>
<li><p>避免高度活跃用户对loss的影响</p>
<ul>
<li>训练集中对每个用户提取相同数量的训练样本</li>
</ul>
</li>
<li><p>根据用户最后一次点击行为的位置，过滤掉最后一次点击之后的展示，可以人为认为用户没有看到。</p>
</li>
<li><p>一个用户对同一个内容点击与不点击并存的情况，如果多次曝光的间隙非常短，考虑只使用其中的一次曝光数据。</p>
</li>
<li><p>考虑去除只有曝光但没有点击操作的用户的样本（也就是说有的用户只有负样本，没有正样本），不过去除的话，那模型就只能够学习到活跃用户或者有意向用户的行为习惯</p>
</li>
<li><p>要求当线上模型在预测时，需要将喂给模型的特征做一次落地，比如传到kafka，后续再由相应程序进行解析生成之后的的训练样本。</p>
</li>
<li><p>同一个request中，如果收到某样本后面样本的展示或者点击事件，5min后还没有收到该样本的点击事件，则作为负样本进行训练；如果在作为负样本训练之后，在一段时间之后又收到该样本的正例行为，则需要作出补偿。</p>
</li>
<li><p>专家样本</p>
</li>
<li><p>坏样本</p>
<ul>
<li>样本偏差、模型敏感、无法代表全体、</li>
</ul>
</li>
<li><p>在信用卡欺诈模型中，对于召回率的要求比较高（不希望漏掉一个欺诈用户），并且预测出来的数据还会经过人工审核，相对的对于准确率要求就低一些</p>
</li>
<li><p>但是在我们的原始数据中，正样本的比例本身就占比非常小了，或者正样本本身就是正太分布部分，但是在预测的时候，连长尾分布的部分也不能放过，（尽量的把所有欺诈用户召回），比如信用卡欺诈里有的超级用户虽然数量小，但是一次违约就是几十万，比几百个普通用户还严重，这种时候是否要用权值设置或者复制正样本的方式，来做识别增强。</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度</title>
    <url>/2021/02/04/%E6%A2%AF%E5%BA%A6/</url>
    <content><![CDATA[<h2 id="梯度消失和爆炸"><a href="#梯度消失和爆炸" class="headerlink" title="梯度消失和爆炸"></a>梯度消失和爆炸</h2><ul>
<li><p>deep后带来的信息传递/梯度传递问题</p>
<ul>
<li><p>层数过多导致？sigmoid和tanh为什么会导致梯度消失？</p>
<ol>
<li>直观解释：从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。</li>
<li>反向传播角度解释：由于反向传播过程中，前面网络权重的偏导数的计算是逐渐从后往前累乘的，如果使用激活函数如sigmoid，导数小于一，因此累乘会逐渐变小，导致梯度消失，前面的网络层权重更新变慢；如果权重 本身比较大，累乘会导致前面网络的参数偏导数变大，产生数值上溢。<br>    </li>
</ol>
</li>
<li><p>梯度消失</p>
<ol>
<li>原因：层数过多，学习率的大小，网络参数的初始化，激活函数的边缘效应</li>
<li>在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。<br>
<span id="more"></span></li>
</ol>
</li>
<li><p>梯度爆炸</p>
<ol>
<li>原因：1）隐藏层的层数过多；2）<strong>权重的初始化值过大</strong></li>
<li>在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为$NaN$值，再也无法更新。</li>
<li>解决：1）用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。（2）用Batch Normalization。（3）<strong>LSTM的结构设计也可以改善RNN中的梯度消失问题。</strong>（4）进行梯度裁剪(clip), 如果梯度值大于某个阈值，我们就进行梯度裁剪，限制在一个范围内.（5）使用正则化，这样会限制参数 的大小，从而防止梯度爆炸。（6）设计网络层数更少的网络进行模型训练</li>
</ol>
</li>
</ul>
</li>
</ul>
<h4 id="LSTM为什么有助于解决梯度消失和爆炸问题？"><a href="#LSTM为什么有助于解决梯度消失和爆炸问题？" class="headerlink" title="LSTM为什么有助于解决梯度消失和爆炸问题？"></a>LSTM为什么有助于解决梯度消失和爆炸问题？</h4><p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
<p>RNN 中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。</p>
<p>其一是遗忘门接近 1（例如模型初始化时会把 forget bias 设置成较大的正数，让遗忘门饱和），这时候远距离梯度不消失；其二是遗忘门接近 0，但这时模型是故意阻断梯度流的，这不是 bug 而是 feature（例如情感分析任务中有一条样本 “A，但是 B”，模型读到“但是”后选择把遗忘门设置成 0，遗忘掉内容 A，这是合理的）。当然，常常也存在 f 介于 [0, 1] 之间的情况，在这种情况下只能说 LSTM 改善（而非解决）了梯度消失的状况。</p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>1、优秀激活函数的性质<br>非线性–多层后能够逼近所有函数<br>可导–优化器大多用梯度下降更新参数<br>单调–能够保证单层网络的损失函数是凸函数<br>近似恒等性– f(x)近似=x，参数初始化为随机小值时，神经网络更稳定<br>输出均值为0 – 能够加速收敛</p>
<p>2、激活函数输出范围<br>有限 – 基于梯度更新参数更稳定<br>无限 – 调小学习率</p>
<p>3、sigmoid<br>导数范围在0-1/4，梯度消失问题<br>输出均值不为0，收敛慢<br>幂运算耗时高</p>
<p>4、tanh<br>梯度消失<br>幂运算耗时高</p>
<p>5、 relu<br>缺点：<br>输出均值非0<br>dead relu – 某些神经元永远不被激活，导致相应的参数无法更新</p>
<p>6、dead relu改进<br>负数输入过多导致，改变参数初始化避免过多的负数特征送入relu，设置更小的学习率，避免参数分布巨大变化<br>leaky – 虽然能解决dead，但实际效果中没有证明出比relu更好</p>
<h4 id="初始化建议"><a href="#初始化建议" class="headerlink" title="初始化建议"></a>初始化建议</h4><p>均值为0<br>标准差为 sqrt(2/当前层输入特征个数)的正太分布</p>
<h4 id="指数加权平均（Exponentially-weighted-average）"><a href="#指数加权平均（Exponentially-weighted-average）" class="headerlink" title="指数加权平均（Exponentially weighted average）"></a>指数加权平均（Exponentially weighted average）</h4><p>V_t=β*V_t−1+(1−β)*θ_t</p>
<p>1、当 β 较大时（β = 0.98 相当于每一点前50天的平均气温)。曲线波动相对较小更加平滑，因为对很多天的气温做了平均处理，正因为如此，曲线还会右移。</p>
<p>较小，0.5时，曲线波动相对激烈，但是它可以更快的适应温度的变化。</p>
<p>2、当 β = 0.9时，我们可以近似的认为当前的数值是过去10天的平均值，但是显然如果我们直接计算过去10天的平均值，要比用指数加权平均来的更加准确。但是如果直接计算过去10天的平均值，我们要存储过去10天的数值，而加权平均只要存储V_t−1</p>
<p>3、指数加权平均 不能很好地拟合前几天的数据，因此需要 偏差修正<br>在机器学习中，多数的指数加权平均运算并不会使用偏差修正。因为大多数人更愿意在初始阶段，用一个捎带偏差的值进行运算。不过，如果在初试阶段就开始考虑偏差，指数加权移动均值仍处于预热阶段，偏差修正可以做出更好的估计。<br>​<br>V_t = V_t / (1 - β_t)</p>
<h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>GD 到 BGD 到 SGD–即minibatch的SGD(现在说SGD一般都指MBGD)，<br>BGD即每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型参数。这样就会大大加快训练速度。<br>SGD即每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。</p>
<p>步骤：<br>1、计算 t时刻损失函数关于当前参数的梯度 g_t = ▽loss<br>2、计算 t时刻的一阶动量m_t和二阶动量 v_t<br>3、计算 t时刻下降梯度 n_t = lr * m_t/sqrt(v_t)<br>4、计算t+1时刻的梯度即 w_t+1 =  w_t - n_t</p>
<p>其中一阶动量是，梯度相关的函数<br>二阶动量是，梯度平方相关的函数</p>
<p>1、SGD<br>m_t = g_t<br>v_t = 1</p>
<p>SGD每次都会在当前位置上沿着负梯度方向更新（下降，沿着正梯度则为上升），并不考虑之前的方向梯度大小等。<br>动量（moment）通过引入新的变量去积累之前的梯度（通过指数衰减平均得到），得到加速学习过程的目的。</p>
<p>若当前的梯度方向与累积的历史梯度方向一致，则当前的梯度会被加强，从而这一步下降的幅度更大。若当前的梯度方向与累积的梯度方向不一致，则会减弱当前下降的梯度幅度。如下图</p>
<p>2、SGDM 含有momentum的SGD</p>
<p>m_t = θ * m_t-1 + (1 - θ) * g_t   – 指数加权平均<br>v_t = 1 </p>
<p>初始化，m_t = 0</p>
<p><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/momentum.png" alt="momentum"></p>
<p>3、Ada 引入了二阶动量<br>m_t = g_t<br>v_t = ∑ g_t^2</p>
<p>优点：对于梯度较大的参数，意味着学习率会变得较小。而对于梯度较小的参数，则效果相反。这样就可以使得参数在平缓的地方下降的稍微快些，不至于徘徊不前。<br>缺点：由于是累积梯度的平方，到后面累积的比较大，会导致梯度消失。</p>
<p>在凸优化中，AdaGrad算法具有一些令人满意的理论性质。但是，在实际使用中已经发现，对于训练深度神经网络模型而言，从训练开始时累积梯度平方会导致学习率过早过量的减少。AdaGrad算法在某些深度学习模型上效果不错，但不是全部。</p>
<p>Adadelta<br>Adadelta是对Adagrad的改进，主要是为了克服Adagrad的两个缺点（摘自Adadelta论文《AdaDelta: An Adaptive Learning Rate Method》）：<br>the continual decay of learning rates throughout training<br>the need for a manually selected global learning rate</p>
<p>4、RMSProp<br>m_t = g_t<br>v_t = θ * v_t-1 + (1 - θ) * g_t^2</p>
<p>RMSprop也是对Adagrad的扩展，以在非凸的情况下效果更好。和Adadelta一样，RMSprop使用指数加权平均（指数衰减平均）只保留过去给定窗口大小的梯度，使其能够在找到凸碗状结构后快速收敛。</p>
<p>在实际使用过程中，RMSprop已被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习人员经常采用的优化算法之一。keras文档中关于RMSprop写到：This optimizer is usually a good choice for recurrent neural networks.</p>
<p><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/rmsprop.png" alt="rmsprop"></p>
<p>5、Adam</p>
<p>Adam实际上是把momentum和RMSprop结合起来的一种算法<br><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/adam.png" alt="adam"><br><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/adamformula.png" alt="adamformula"></p>
<p>reference:</p>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html">https://ruder.io/optimizing-gradient-descent/index.html</a></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>统计角度看ml</title>
    <url>/2021/03/11/%E7%BB%9F%E8%AE%A1%E8%A7%92%E5%BA%A6%E7%9C%8Bml/</url>
    <content><![CDATA[<br>

<p>一些思考，从整体上看模型的思路，进行比较。</p>
<span id="more"></span>
<p>极大似然？就是后验、大量样本的整体出现概率值最大。样本之间独立。可应用乘法原理。</p>
<p>条件概率，即某(些)条件下某(些)事件出现的概率。</p>
<p>决策树则是求其极大值，局部选择当前条件概率最大。条件概率越大，不确定性越低，条件熵越小。整体熵未必减小。考虑整体熵值的是最大熵模型和logistics模型。logistics是可以通过假设先验概率，转化为这样的问题：推导出满足极大似然极值而得到的w参数。都可以通过拉格朗日将问题转化为无约束求极值，偏导为0，算法里有迭代尺度、梯度下降、牛顿插值。牛顿插值迭代收敛快些。</p>
<p>特征选择无论是熵、增益(比)还是基尼系数都体现出不确定性的思想。增益大则说明特征对结果的影响力大，因为说明条件熵越小，即条件概率越大，该条件下不确定性越低。增益或比越大，而基尼越小越好。因为前者都有-号，类似相反数的关系。本质还条件概率的问题。</p>
<p>决策生成树可以用动归提高算法效率。</p>
<p>感知器从loss func出发，求最小。距离如何定义还要看具体应用场景。主要算法里有<a href="https://blog.csdn.net/greenyang5277/article/details/104270803">Gram矩阵</a>，对偶算法。</p>
<p>k邻很简单，经验风险最小，就是多数为胜，即最简单的频率派概率最大思路，k不一样结果可能瞬间不同了，参数也少。</p>
<p>朴素贝叶斯主要是独立性假设，在类确定下特征条件独立，才能将公式分子简化，否则不那么容易求最大值，后验概率最大，还是极大似然思路，后验概率这是可用简单的乘法原理表达。如果有0，这里提出可以平滑的思想。</p>
<p>无论是熵、条件熵、基尼系数，还是贝叶斯、条件概率，还是感知器的loss func，还是c4.5里的loss(在剪枝时通过熵建立的loss，加入了模型复杂度因子，通过比较剪枝前后大小来判断是否剪枝)，还是logistics(初始分布进行求对数几率，求极大似然最大的参数)、最大熵(公式也化为求极大似然最大)两个对数线性模型的经验分布推导出来的无约束最优化公式，都是对初始概率进行包装，要么转为极大似然问题或者说转为条件概率问题，要么转为loss最小问题。概率问题则为监督学习中的两种，生成和判别。非概率则自定义的一些代数loss。</p>
<p>对给定输入判别输出，判别要么是f(x),要么是P(Y|X)。前者会出现loss func，定义距离如感知器、LDA，后者则决策树、k邻、贝叶斯、GMM。对数线性模型也是P(Y|X)概率分布公式表达的分类模型。都可以说是在对这两种函数求最优的问题。但如决策树、k邻，没有什么公式，也就没有什么参数需要调整，大多重点在算法，如决策树里主要是三个算法里剪枝过程应用到熵之类问题，而kmeans主要kd树解决高维搜索问题。理论框架虽然可以和统计通过加条件等方式相关联，但是更多是另一种思路。</p>
<p>生成模型，由学习数据得到原始分布，再来求P(Y|X)，如贝叶斯。判别模型，学习数据不从分布入手，而直接对条件概率进行假设，如LR。</p>
<p>具体细节，如收敛性证明，拉格朗日转化，最大熵的约束公式，包括泛化误差利用切比雪夫求上界的前提条件，感知器中正负，logistics里对数几率特征空间是n+1维，都需落在数学上去一步步转化推证。</p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>编译tensorflow</title>
    <url>/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/</url>
    <content><![CDATA[<br> 

<p>记录编译过程</p>
<span id="more"></span>

<br> 

<h4 id="前序安装"><a href="#前序安装" class="headerlink" title="前序安装"></a>前序安装</h4><p>1、bazel 2.0.0 通过sh安装，需要chmod，然后export环境变量。我的路径在/var/root/bin，添加到source ~/.bash_profile。把这句source添加到 vi .zshrc ，然后source ~/.zshrc ，重启也有bazel了。</p>
<p>2、tensorflow v2.2.0 </p>
<p>3、conda环境，选择了python2.7，然后安装requirements (tensorflow-2.2.0/tensorflow/tools/pip_package/setup.py )如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">future</span><br><span class="line">absl-py &gt;&#x3D; 0.7.0</span><br><span class="line">astunparse &#x3D;&#x3D; 1.6.3</span><br><span class="line">backports.weakref &gt;&#x3D; 1.0rc1</span><br><span class="line">enum34 &gt;&#x3D; 1.1.6</span><br><span class="line">gast &#x3D;&#x3D; 0.3.3</span><br><span class="line">google_pasta &gt;&#x3D; 0.1.8</span><br><span class="line">h5py &#x3D;&#x3D; 2.10.0</span><br><span class="line">keras_preprocessing &gt;&#x3D; 1.1.0</span><br><span class="line">numpy &#x3D;&#x3D; 1.16.0</span><br><span class="line">opt_einsum &gt;&#x3D; 2.3.2</span><br><span class="line">protobuf &gt;&#x3D; 3.8.0</span><br><span class="line">tensorboard &#x3D;&#x3D; 2.1.0</span><br><span class="line">tensorflow_estimator &#x3D;&#x3D; 2.2.0</span><br><span class="line">termcolor &gt;&#x3D; 1.1.0</span><br><span class="line">wrapt &gt;&#x3D; 1.11.1</span><br><span class="line"></span><br><span class="line"># python3 requires wheel 0.26</span><br><span class="line">wheel &gt;&#x3D; 0.26</span><br><span class="line"></span><br><span class="line"># mock comes with unittest.mock for python3 need to install for python2</span><br><span class="line">mock &gt;&#x3D; 2.0.0</span><br><span class="line"></span><br><span class="line"># functools comes with python3 need to install the backport for python2</span><br><span class="line">functools32 &gt;&#x3D; 3.2.3</span><br><span class="line">six &gt;&#x3D; 1.12.0</span><br><span class="line"></span><br><span class="line"># scipy &lt; 1.4.1 causes segfaults due to pybind11</span><br><span class="line"># Latest scipy pip for py2 is scipy&#x3D;&#x3D;1.2.2</span><br><span class="line">scipy &#x3D;&#x3D; 1.2.2</span><br></pre></td></tr></table></figure>



<p>4、如果需要gpu，则还要安装cuda 10.1 和 cudnn 7.6.5</p>
<br> 

<h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><p>1、配置基本选择N，需要gpu则在CUDAsupport选择y</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd tensorflow-2.2.0</span><br><span class="line">.&#x2F;configure</span><br></pre></td></tr></table></figure>

<p>2、bazel</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bazel build --noincompatible_do_not_split_linking_cmdline --local_ram_resources&#x3D;2048  &#x2F;&#x2F;tensorflow&#x2F;tools&#x2F;pip_package:build_pip_package</span><br><span class="line"></span><br><span class="line">bazel build --noincompatible_do_not_split_linking_cmdline --local_ram_resources&#x3D;2048 &#x2F;&#x2F;tensorflow:libtensorflow_cc.so</span><br></pre></td></tr></table></figure>



<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/inprogress.jpg" alt="过程"></p>
<br>



<p>编译成功，花了12个小时多。</p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/success.jpg" alt="success"></p>
<br>

<h4 id="生成whl"><a href="#生成whl" class="headerlink" title="生成whl"></a>生成whl</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo .&#x2F;tensorflow&#x2F;tools&#x2F;pip_package&#x2F;build_pip_package.sh &#x2F;tmp&#x2F;tensorflow_pkg</span><br><span class="line">#接着我切了conda环境</span><br><span class="line">pip install --upgrade pip</span><br><span class="line">pip uninstall tensorflow</span><br><span class="line">pip uninstall tensorboard</span><br><span class="line">pip uninstall tensorflow-tensorboard</span><br><span class="line">#pip3 install tensorboard&#x3D;&#x3D;2.2.1  #有个巨坑。pip3才能安装2.2.1，但是我们环境用的都是python2.7</span><br><span class="line">#要么把之前的切到3，要么下载下来site-package 装到目录里。我是选择粘贴到目录 #&#x2F;Users&#x2F;wyq&#x2F;anaconda2&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;tensorboard</span><br><span class="line">pip install -U --ignore-installed wrapt  </span><br><span class="line">#又是坑。wrapt：ERROR: Cannot uninstall &#39;wrapt&#39;. It is a distutils installed project and thus #we cannot accurately determine which files belong to it which would lead to only a partial #uninstall.</span><br><span class="line">pip3 install &#x2F;tmp&#x2F;tensorflow_pkg&#x2F;tensorflow-2.2.0-cp27-cp27m-macosx_10_14_x86_64.whl</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/whl0.jpg" alt="whl0"></p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/whl.jpg" alt="whl"></p>
<h4 id="python验证"><a href="#python验证" class="headerlink" title="python验证"></a><br>python验证</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#打开ipython </span><br><span class="line">#如果遇到这个问题 ImportError: cannot import name pywrap_tensorflow</span><br><span class="line">#看这个issue </span><br><span class="line"># https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;35953210&#x2F;error-running-basic-tensorflow-example</span><br><span class="line">import tensorflow as tf</span><br><span class="line">tf.__version__</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></table></figure>

<br>

<p>终于成功了</p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/whlsuccess.jpg" alt="whlsuccess"></p>
<br>

<h4 id="c-验证"><a href="#c-验证" class="headerlink" title="c++验证"></a>c++验证</h4><p>选择了vscode，下次clion可以也试试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#在tf根目录下运行</span><br><span class="line">bazel build &#x2F;&#x2F;tensorflow&#x2F;cc:tutorials_example_trainer</span><br><span class="line">bazel build &#x2F;&#x2F;tensorflow&#x2F;cc:client_client_session_test￼</span><br><span class="line"></span><br><span class="line">#然后运行程序</span><br><span class="line">.&#x2F;bazel-bin&#x2F;tensorflow&#x2F;cc&#x2F;client_client_session_test￼</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/csuccess.jpg" alt="csuccess"></p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/clienttest.jpg" alt="demo"></p>
<br>

<h4 id><a href="#" class="headerlink" title></a></h4><h4 id="c-demo"><a href="#c-demo" class="headerlink" title="c++demo"></a>c++demo</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;main.cpp</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &quot;tensorflow&#x2F;cc&#x2F;client&#x2F;client_session.h&quot;</span><br><span class="line">#include &quot;tensorflow&#x2F;cc&#x2F;ops&#x2F;standard_ops.h&quot;</span><br><span class="line">#include &quot;tensorflow&#x2F;core&#x2F;framework&#x2F;tensor.h&quot;</span><br><span class="line"></span><br><span class="line">using namespace tensorflow;</span><br><span class="line">using namespace tensorflow::ops;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    Scope root &#x3D; Scope::NewRootScope();</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Matrix A &#x3D; [3 2; -1 0]</span><br><span class="line">    auto A &#x3D; Const(root, &#123; &#123;3.f, 2.f&#125;, &#123;-1.f, 0.f&#125; &#125;);</span><br><span class="line">    &#x2F;&#x2F; Vector b &#x3D; [3 5]</span><br><span class="line">    auto b &#x3D; Const(root, &#123; &#123;3.f, 5.f&#125; &#125;);</span><br><span class="line">    &#x2F;&#x2F; v &#x3D; Ab^T</span><br><span class="line">    auto v &#x3D; MatMul(root.WithOpName(&quot;v&quot;), A, b, MatMul::TransposeB(true));</span><br><span class="line"></span><br><span class="line">    std::vector&lt;Tensor&gt; outputs;</span><br><span class="line">    ClientSession session(root);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Run and fetch v</span><br><span class="line">    TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs));</span><br><span class="line">    std::cout &lt;&lt; &quot;tensorflow session run ok&quot; &lt;&lt; std::endl;</span><br><span class="line">    &#x2F;&#x2F; Expect outputs[0] &#x3D;&#x3D; [19; -3]</span><br><span class="line">    std::cout &lt;&lt; outputs[0].matrix&lt;float&gt;();</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;BUILD file</span><br><span class="line"></span><br><span class="line">load(&quot;&#x2F;&#x2F;tensorflow:tensorflow.bzl&quot;, &quot;tf_cc_binary&quot;)</span><br><span class="line">tf_cc_binary(</span><br><span class="line">     name &#x3D; &quot;main&quot;, #目标文件名</span><br><span class="line">     srcs &#x3D; [&quot;main.cpp&quot;], #源代码文件名</span><br><span class="line">     deps &#x3D; [</span><br><span class="line">        &quot;&#x2F;&#x2F;tensorflow&#x2F;cc:cc_ops&quot;,</span><br><span class="line">         &quot;&#x2F;&#x2F;tensorflow&#x2F;cc:client_session&quot;,</span><br><span class="line">         &quot;&#x2F;&#x2F;tensorflow&#x2F;core:tensorflow&quot;</span><br><span class="line">         ],</span><br><span class="line"> )</span><br><span class="line"> </span><br><span class="line"> &#x2F;&#x2F;执行</span><br><span class="line"> bazel build &#x2F;&#x2F;tensorflow&#x2F;demo:main</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/demo.jpg" alt="demo"></p>
<br>

<p>来bili 看跳转~</p>
<p><a href="https://www.bilibili.com/video/BV1Q54y1b74n">https://www.bilibili.com/video/BV1Q54y1b74n</a></p>
<h4 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h4><p>相关编译、过程中error issue、cmake、bazel以及用c++ api构建线上服务</p>
<p><a href="https://github.com/tensorflow/tensorflow/issues/43654">https://github.com/tensorflow/tensorflow/issues/43654</a></p>
<p><a href="https://xugaoxiang.com/2020/05/22/compile-tensorflow2-with-gpu/">https://xugaoxiang.com/2020/05/22/compile-tensorflow2-with-gpu/</a></p>
<p><a href="https://www.tensorflow.org/install/errors">https://www.tensorflow.org/install/errors</a></p>
<p><a href="https://www.javatt.com/p/43568">https://www.javatt.com/p/43568</a></p>
<p><a href="https://www.jianshu.com/p/72b228223804">https://www.jianshu.com/p/72b228223804</a></p>
<p><a href="http://sixerwang.github.io/2018/12/24/cpp-call-tf/">http://sixerwang.github.io/2018/12/24/cpp-call-tf/</a></p>
<p><a href="https://github.com/hemajun815/tutorial/blob/master/tensorflow/compilling-tensorflow-source-code-into-C%2B%2B-library-file.md">https://github.com/hemajun815/tutorial/blob/master/tensorflow/compilling-tensorflow-source-code-into-C%2B%2B-library-file.md</a></p>
<p><a href="https://cmake.org/cmake/help/latest/guide/tutorial/index.html">https://cmake.org/cmake/help/latest/guide/tutorial/index.html</a></p>
]]></content>
      <categories>
        <category>tensorflow源码</category>
      </categories>
      <tags>
        <tag>tensorflow源码</tag>
      </tags>
  </entry>
  <entry>
    <title>编辑距离</title>
    <url>/2021/03/11/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/</url>
    <content><![CDATA[<br>
<br>


<h2 id="编辑距离"><a href="#编辑距离" class="headerlink" title="编辑距离"></a>编辑距离</h2><span id="more"></span>

<br>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">static int editDist(String str1 , String str2 , int m ,int n)&#123;</span><br><span class="line"></span><br><span class="line">    if (str1.charAt(m-1) &#x3D;&#x3D; str2.charAt(n-1))</span><br><span class="line">        return editDist(str1, str2, m-1, n-1);</span><br><span class="line"></span><br><span class="line">    return 1 + min ( editDist(str1,  str2, m, n-1),    &#x2F;&#x2F; Insert</span><br><span class="line">                     editDist(str1,  str2, m-1, n),   &#x2F;&#x2F; Remove</span><br><span class="line">                     editDist(str1,  str2, m-1, n-1) &#x2F;&#x2F; Replace                     </span><br><span class="line">                   ); &#x2F;&#x2F;三个子问题</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;s: beforeediting,length&#x3D;n  t: afterediting,length&#x3D;m</span><br><span class="line">&#x2F;&#x2F;d: recording the distance</span><br><span class="line">        d &#x3D; new int[n + 1][m + 1];  </span><br><span class="line">              </span><br><span class="line">        for (i &#x3D; 0; i &lt;&#x3D; n; i++) &#123;  </span><br><span class="line">            d[i][0] &#x3D; i;  </span><br><span class="line">        &#125;  </span><br><span class="line">        for (j &#x3D; 0; j &lt;&#x3D; m; j++) &#123;  </span><br><span class="line">            d[0][j] &#x3D; j;  </span><br><span class="line">        &#125;  </span><br><span class="line">          </span><br><span class="line">         </span><br><span class="line">        for (i &#x3D; 1; i &lt;&#x3D; n; i++) &#123;  </span><br><span class="line">            s_i &#x3D; s.charAt(i - 1);  </span><br><span class="line">              </span><br><span class="line">            for (j &#x3D; 1; j &lt;&#x3D; m; j++) &#123;  </span><br><span class="line">                t_j &#x3D; t.charAt(j - 1);  </span><br><span class="line">                 </span><br><span class="line">                cost &#x3D; (s_i &#x3D;&#x3D; t_j) ? 0 : 1;  </span><br><span class="line">                 </span><br><span class="line">                d[i][j] &#x3D; min(d[i - 1][j] + 1, d[i][j - 1] + 1,  </span><br><span class="line">                        d[i - 1][j - 1] + cost);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if x &#x3D;&#x3D; y, then d[i][j] &#x3D;&#x3D; d[i-1][j-1]</span><br><span class="line">if x !&#x3D; y, 插入y则 d[i][j] &#x3D; d[i][j-1] + 1</span><br><span class="line">if x !&#x3D; y, 删除x则 d[i][j] &#x3D; d[i-1][j] + 1</span><br><span class="line">if x !&#x3D; y, x改变为y则 d[i][j] &#x3D; d[i-1][j-1] + 1</span><br><span class="line">When x!&#x3D;y, d[i][j] 取三中编辑方式最小代价。</span><br><span class="line">初始化条件 ： d[i][0] &#x3D; i, d[0][j] &#x3D; j</span><br></pre></td></tr></table></figure>
<ul>
<li><p>为什么不同操作就是对应与d的左移上移或左上移？<br>这个问题递归角度较易理解。<br>DP角度，d记录的是目前最小编辑距离。左、上、左上为子问题，即儿子。若为插入，则j需+1得到t的下一个字符，而x继续与此字符比较，i不变。</p>
</li>
<li><p>由递归到DP，实质就是找到递归中子问题。<br>找到后，将子问题结果记录在数组即可。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯网络</title>
    <url>/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<br>

<h2 id="概率图"><a href="#概率图" class="headerlink" title="概率图"></a>概率图</h2><ul>
<li><p>概率图模型分为贝叶斯网络（Bayesian Network）和马尔可夫网络（Markov Network）两大类。</p>
<span id="more"></span></li>
<li><p>贝叶斯网络可以用一个有向图结构表示，马尔可夫网络可以表示成一个无向图的网络结构。若随机变量Y构成一个无向图 G=(V,E)表示的马尔科夫随机场（MRF），则条件概率分布P(Y|X)称为条件随机场（Conditional Random Field, 简称CRF。</p>
</li>
<li><p>更详细地说，概率图模型包括了朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型等，在机器学习的诸多场景中都有着广泛的应用。</p>
</li>
</ul>
<p><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E6%A6%82%E7%8E%87%E5%9B%BE.png" alt="概率图"></p>
<h2 id="频率派与贝叶斯派"><a href="#频率派与贝叶斯派" class="headerlink" title="频率派与贝叶斯派"></a>频率派与贝叶斯派</h2><p>频率派与贝叶斯派各自不同的思考方式：</p>
<p>频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值，同时，样本X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X 的分布；</p>
<p>而贝叶斯派的观点则截然相反，他们认为参数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布。</p>
<p>贝叶斯派既然把概率看做是一个随机变量，所以要计算概率的分布，便得事先知道的无条件分布，即在有样本之前（或观察到X之前），有着怎样的分布呢？这种在实验之前定下的属于基本前提性质的分布称为先验分布，或着无条件分布。</p>
<p>其中，先验信息一般来源于经验跟历史资料。而后验分布π（θ|X）一般也认为是在给定样本X的情况下的θ条件分布，而使π（θ|X）达到最大的值，称为最大后验估计，类似于经典统计学中的极大似然估计。</p>
<h2 id="判别和生成"><a href="#判别和生成" class="headerlink" title="判别和生成"></a>判别和生成</h2><p>常见的概率图模型有朴素贝叶斯、最大熵模型、贝叶斯网络、隐马尔可夫模<br>型、条件随机场、pLSA、LDA等。</p>
<p>朴素贝叶斯、贝叶斯网络、pLSA、LDA等模型都是先对联合概率分布进行建模，然后再通过计算边缘分布得到对变量的预测，所以它们都属于生成式模型；而最大熵模型是直接对条件概率分布进行建模，因此属于判别式模型。隐马尔可夫模型和条件随机场模型是对序列数据进行建模的方法，将在后面的章节中详细介绍，其中隐马尔可夫模型属于生成式模型，条件随机场属于判别式模型。</p>
<p><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E5%88%A4%E5%88%AB%E4%B8%8E%E7%94%9F%E6%88%90.png" alt="判别与生成"></p>
<p>我的视频讲解(<a href="https://www.bilibili.com/video/BV16y4y187pE">https://www.bilibili.com/video/BV16y4y187pE</a>)</p>
<h2 id="解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？"><a href="#解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？" class="headerlink" title="解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？"></a>解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？</h2><p>先验概率：就是因变量（二分法）在数据集中的比例。这是在你没有任何进一步的信息的时候，是对分类能做出的最接近的猜测。<br>似然估计：似然估计是在其他一些变量的给定的情况下，一个观测值被分类为1的概率。例如，“FREE”这个词在以前的垃圾邮件使用的概率就是似然估计。<br>边际似然估计：边际似然估计就是，“FREE”这个词在任何消息中使用的概率。</p>
<h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><p>贝叶斯网络(Bayesian network)，又称信念网络(Belief Network)，或有向无环图模型(directed acyclic graphical model)</p>
<p>例子：<br><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C.png" alt="贝叶斯网络"></p>
<h3 id="结构形式"><a href="#结构形式" class="headerlink" title="结构形式"></a>结构形式</h3><p>1、 a-&gt;c b-&gt;c</p>
<p>P(a,b,c) = P(a)P(b)P(c|a,b)成立，即在c未知的条件下，a、b被阻断(blocked)，是独立的，称之为head-to-head条件独立。</p>
<p>2、c-&gt;a c-&gt;b</p>
<p>考虑c未知，跟c已知这两种情况：</p>
<p>在c未知的时候，有：P(a,b,c)=P(c)P(a|c)P(b|c)，此时，没法得出P(a,b) = P(a)P(b)，即c未知时，a、b不独立。</p>
<p>在c已知的时候，有：P(a,b|c)=P(a,b,c)/P(c)，然后将P(a,b,c)=P(c)P(a|c)P(b|c)带入式子中，得到：P(a,b|c)= P(a|c)*P(b|c)，即c已知时，a、b独立。</p>
<p>3、a-&gt;c-&gt;b</p>
<p>还是分c未知跟c已知这两种情况：</p>
<p>c未知时，有：P(a,b,c)=P(a)P(c|a)P(b|c)，但无法推出P(a,b) = P(a)P(b)，即c未知时，a、b不独立。</p>
<p>c已知时，有：P(a,b|c)=P(a,b,c)/P(c)，且根据P(a,c) = P(a)P(c|a) = P(c)P(a|c)，可化简得到：<br><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/headtail.png" alt="headtail"><br>所以，在c给定的条件下，a，b被阻断(blocked)，是独立的，称之为head-to-tail条件独立。<br>这个head-to-tail其实就是一个链式网络。</p>
<h3 id="因子图"><a href="#因子图" class="headerlink" title="因子图"></a>因子图</h3><p>wikipedia上是这样定义因子图的：将一个具有多变量的全局函数因子分解，得到几个局部函数的乘积，以此为基础得到的一个双向图叫做因子图（Factor Graph）。</p>
<p>通俗来讲，所谓因子图就是对函数进行因子分解得到的一种概率图。一般内含两种节点：变量节点和函数节点。我们知道，一个全局函数通过因式分解能够分解为多个局部函数的乘积，这些局部函数和对应的变量关系就体现在因子图上。</p>
<p>根据贝叶斯网络的例子，</p>
<p float="left">
  <img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/factor.png" width="48%">
  <img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/推导.png" width="48%"> 
</p>


<h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><p>1、求某个变量的边缘分布是常见的问题：这问题有很多求解方法，其中之一就是把贝叶斯网络或马尔科夫随机场 转换成 因子图，然后用sum-product算法求解。换言之，基于因子图可以用sum-product 算法高效的求各个变量的边缘分布。</p>
<p>2、</p>
<p>reference：<br><a href="https://blog.csdn.net/v_july_v/article/details/40984699">https://blog.csdn.net/v_july_v/article/details/40984699</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>通货膨胀-下</title>
    <url>/2021/02/21/%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80-%E4%B8%8B/</url>
    <content><![CDATA[<p><strong>摘要</strong></p>
<p>定义、相关概念及现象、分类、成因、影响、相应政策。</p>
<h2 id="货币幻觉"><a href="#货币幻觉" class="headerlink" title="货币幻觉"></a>货币幻觉</h2><span id="more"></span>
<ul>
<li><p>货币幻觉是新凯恩斯主义的代表人物之一阿克洛夫再次提出的。<br>简单说是100元的钱，被认为有100元的购买力。但实际只有50，另一半则为“铸币税”被征收。<br>而幻觉的存在，因为我们不能够完全理性，价格的传导存在时滞，我们私心喜欢虚幻的“富裕”。</p>
</li>
<li><p>货币本质<br>一般等价物？资产？负债？债券？税票？<br>一般等价物是在交换中起的作用。并不能表现出货币的真实所值，或者说购买力。<br>劳动或资产所得的个人财富，通过货币形式，用于购置资产则为资产。而借来的货币，用以购置资产则为债务。<br>对国家而言，可以将美元看做债券，人民币看做税票，这样的视角更能看出货币所值的变化。</p>
</li>
<li><p>成本<br>借贷的利息，意味着货币资源的机会成本。</p>
</li>
<li><p>货币是一种符号。意味着涨涨跌跌的可能性。货币的乘数效应既能放大资产，也能放大负债。能熬得住，承受得了时间成本，则货币将沉淀为资产。</p>
</li>
</ul>
<h2 id="影响"><a href="#影响" class="headerlink" title="影响"></a>影响</h2><ul>
<li>微观</li>
</ul>
<p>1、货币幻觉导致财富再分配：银行存款名义利率往往不能随通胀充分调整以保证实际利率不变，短期内会出现挤兑， 长期内会造成负储蓄、负投资、负就业、负产出。<br>2、实体企业投资:由长期投资转向短期投资，由生产性投资转向非生产性投资<br>3、累进税制下税收扭曲:通胀税<br>4、商品之间相对价格的变动，价格信号配置资源的效率下降– 模糊了工作效果和报酬的关系、使得投机赌博盛行。</p>
<ul>
<li>宏观<br>1、对于产出的影响<ul>
<li>促进论:凯恩斯名义工资刚性理论、新凯恩斯粘性工资价格模型;弗里德曼和卢卡斯的预 期错误模型;</li>
<li>促退论:通胀造成微观效率损失;</li>
<li>中性论:“二分法”</li>
</ul>
</li>
</ul>
<p>2、对于就业的影响:菲利普斯曲线，附加预期菲利普斯曲线的运动 &amp; 长期垂直的菲利普斯曲线<br>货币政策的短期有效性及长期无效性、中央银行通胀预期管理的重要性;</p>
<p>3、对利率、汇率等重要经济变量的影响</p>
<h2 id="成因"><a href="#成因" class="headerlink" title="成因"></a>成因</h2><ul>
<li>需求拉上：大量的货币发行，导致过多的货币和商品供给的增长不平衡，供给弹性不高不能及时地跟上货币发行。</li>
<li>成本推动：劳动力市场的不完全或产品市场的不完全等造成的，主要包括工资推动型、利润推动型、进口成本推动型。如70s两次石油价格上涨和次贷危机后的大宗商品价格上涨。</li>
<li>预期：当期高通胀率带来市场主体的高通胀预期，进而导致下期高通胀。</li>
<li>结构：在供求总量基本相同的情况下，由于某些结构性因素，如本国产业结构老化，资源流动效率较低等造成的通胀–本质是新部门的供给未能及时跟上。</li>
</ul>
<br>
而各种政策环境，如金本位下的铸币成色、白银涌入、转型为纸币制、战争等历史聚变、计划经济、金融秩序等都通过间接影响到以上四个成因因素而导致通胀。



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>1、货币在流通过程中的乘数效应导致的通胀是几何级数的，与通常讲的CPI所标示的通胀不是一个概念。CPI是有欺骗性的，最真实的通胀就是M2与GDP的比值。</p>
<p>2、对偏离正常现象的分析，会带来更深刻的认知。</p>
<p>3、价格的绝对值无意义，房价回不到过去。</p>
<p>4、一般来说，批发价上涨幅度高于零售。</p>
]]></content>
      <categories>
        <category>货银</category>
        <category>货币政策</category>
      </categories>
      <tags>
        <tag>货银</tag>
        <tag>货币政策</tag>
      </tags>
  </entry>
  <entry>
    <title>通货膨胀</title>
    <url>/2021/02/17/%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>摘要</strong></p>
<p>通胀的基础知识、历史</p>
<h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><h4 id="古罗马的铸币成色下降"><a href="#古罗马的铸币成色下降" class="headerlink" title="古罗马的铸币成色下降"></a>古罗马的铸币成色下降</h4><p>当铸币被减少成色时，首先感知的是发行者，最终才是到物价上。最终感知的人的财富被转移到发行者手里。在刚减少、甚至未减少前感知到时，购入资产–窖藏黄金。此时流通的多为劣币。随着铸币数量增多，价值越跌。而执政者采取的冻结物价的措施，结果却是，规则越来越细、维护规则而设立的官员数量越来越多，触犯规则而死亡的人越来越多，市场越发萧条、充斥恐惧。</p>
<span id="more"></span>
<p>=&gt;<br>无法用立法(价格控制)取代经济规律。通胀是更深问题的表象。</p>
<h4 id="西班牙的白银涌入"><a href="#西班牙的白银涌入" class="headerlink" title="西班牙的白银涌入"></a>西班牙的白银涌入</h4><p>特殊的是，大量涌入没有带来恶性通胀。</p>
<p>1白银涌入<br>2西班牙物资缺乏，多进口<br>3白银官价低于真实价值<br>4人们没有认识到价格上涨的原因在于白银本身供给</p>
<p>=&gt;<br>物价上涨，人们窖藏白银，市场中白银少，流通的为劣币。拥有白银的人以为富有、白银价值不变，不进行生产投资。在舶来品和军事冒险上消耗白银，却未能将白银投入促进生产。流通货币成色降低导致物价进一步上涨。但因为滞后的工资上涨、军事冒险、皇室奢靡、文艺的黄金时代，西班牙一度凭借美洲白银输入和通胀，而使得其王室贵族强极一时。富有者的财富得到巩固，农奴制进一步加强。北欧的革命、宗教革命都未能影响西班牙。</p>
<p>而当美洲白银输入回落，西班牙落后。</p>
<p>=&gt;<br>货币更多不等于财富更多；西班牙最终并未能将货币转为资产、留住涌入的白银的价值；白银收入减缓了制度变革的压力，但并非一件好事；通胀具有传导性，通过贸易传导到其他国家；温和的通胀有助于短期繁荣；虽然金属货币铸造需要成本和受采矿影响，但同样会产生通胀，而纸币受发行者–中央政府官员的良心影响，与通胀的结合就更为天然。</p>
<p>=&gt;<br>工资上浮，并不意味这购买力增加，需要警惕货币幻觉；<br>财富如果不投资、自己不进行变革成长，则长期自然落后，受货币幻觉影响而躺在舒适里，并非好事。<br>大水漫灌的虚假繁荣。</p>
<h4 id="约翰劳的纸币"><a href="#约翰劳的纸币" class="headerlink" title="约翰劳的纸币"></a>约翰劳的纸币</h4><p>法国政府财政赤字重,政府用纸币支付、削减债务<br>1716/05，通用银行成立<br>10月，银行券可支付税收</p>
<p>1717/8，西印度公司成立</p>
<p>开始疯狂:</p>
<blockquote>
<p>1718/12，承诺12%-40%股息回报，垄断烟草业，获得铸币权，接手包税公司(交税给国家、从民众手里征税)，银行大量发行纸币，将国债成功转为股票。<br>出售更多的股票来支付股息。高于市场价33%购入自己公司股票期货。向原始股东限售股票。继续发行大量纸币。</p>
</blockquote>
<blockquote>
<p>1720/1，投放一年前9倍的纸币，新股发行价为40倍，保证金交易出现。</p>
</blockquote>
<p>开始破灭:</p>
<blockquote>
<p>1720上半年，亲王要求兑成硬通货。和东印度合并、高股价回购都不能挽回局势。<br>1720/05，发布规定6个月内纸币贬值，股票降价各50%。</p>
</blockquote>
<p>工薪和小店主受打击小–物价和工资同步地涨跌、且幅度不算太高，投机者有的丧失了所有财富。<br>疯狂到破灭，也就一年时间。</p>
<h4 id="大陆币"><a href="#大陆币" class="headerlink" title="大陆币"></a>大陆币</h4><p>1、战争时期，货币发行来获得物资，导致事后的物价抬升、货币贬值。通过通胀转移私人财富资源用以战备。多种手段，通胀是其一。<br>2、战后的纸币贬值，此时纸币退出流通，或者限制物价，导致通货紧缩。则债务人负担被动加重、物价骤降、经济萧条，价格秩序又被流动性减少而打破。因此，更好的策略是维持战时的物价水平，并保证流动性，促进经济复苏。<br>3、工薪阶层希望价格恢复到战前水平，但是价格本身就是相对的，之前价格的绝对值并不具有意义。战后的价格本应是绝对值高于战前的，而和货币发行相适应。</p>
<p>=&gt;<br>如果流动性和通胀不断地上升或者继续，房价水涨船高，并不会跌回十年前。除非极大萧条，利率极高，央行官员不热爱财富、国家不追求GDP。价格的绝对值，无意义。</p>
<h4 id="法国大革命和分配券"><a href="#法国大革命和分配券" class="headerlink" title="法国大革命和分配券"></a>法国大革命和分配券</h4><p>1788，旱灾导致物价上涨<br>1789，法国政府严重赤字<br>1789/07，攻占巴士底监狱<br>1790/04，革命者没收教会财产，发行分配券，首次利率3%<br>1792，反革命战争爆发<br>1793，路易十六上断头台，9月价格管制出台，开始了恐怖统治；没收有钱人的不动产<br>1794，罗伯斯庇尔遇害，开始了白色恐怖，直到1799拿破仑掌权<br>1795，限价法被废，通胀、经济复苏，粮食价格飞涨，开始了反革命行动 – 烧纸币，恢复铸币，经济秩序(价格)迅速恢复稳定<br>1796，距离首次发行分配券6年不到，分配券面值已经为最为保证的教会财产的20倍。</p>
<p>1770-1787通缩；89-1796通胀；97-1870通缩。</p>
<blockquote>
<p>发行分配券后很快开始贬值，金银开始窖藏，劣币再次驱逐良币。<br>当政府宣布废弃分配券时，大量的分配券沉淀在普通公民手中，他们没有将财富及时转换为永久价值的物品。<br>而工资滞后于物价上涨，工薪和固定收入者丧失了购买力。<br>社会里都是暴发户、投机者、穷人。<br>民众抢掠商人。商业活动萧条，易货贸易、违法交易层出不穷。</p>
</blockquote>
<blockquote>
<p>起初，企业主和大众即布尔乔亚和工人，联合起来对付王室。王室倒台后，开始内斗。最高限价法标志这个马克思阶级斗争的开始。企业主获得了胜利，管制被解除。</p>
</blockquote>
<p>=&gt;<br>历史再度重演。和当初古罗马的通胀没什么区别，都是价格管制来控制，结果无效。和大陆币也区别不大，通胀都是在战争时的手段(通过抬升物价，降低人民生活水平而获得资源，人物力投入到战争)，并以管制解决，然后无效。战争和革命，虽然让社会进步，但不止以士兵生命为代价，还有穷人和富人们。投机者却大发横财。</p>
<p>而人们却没有察觉。</p>
<h4 id="美国内战时期北方"><a href="#美国内战时期北方" class="headerlink" title="美国内战时期北方"></a>美国内战时期北方</h4><p>1836，各州私立银行涌现<br>1861的联邦开支占GDP2%，1865则为26%。联邦政府债务高达28亿美元，33倍于战前，占GDP一半<br>1861年底，纸币不再兑付黄金，18年后才恢复兑换<br>1862，纽交所开设黄金交易</p>
<p>1860-1864，<br>教师工资上涨20-30%，物价上涨到两倍多。则实际工资下降了40%。</p>
<p>1864-1896，<br>由于铸币拥护者的政策导致的绿币升值(与黄金的比对由61%上升到战前的100%)+贸易里其他国家物价下跌，导致物价下跌。最大跌幅65%。30年物价回到世界平均水平。<br>其他国家物价下跌，由于黄金增长低于商品增长。</p>
<blockquote>
<p>白银黄金比价：1867，16：1；1896，31：1；1989，71：1。<br>如果在1867年实行白银本位，则劣币驱良。黄金被窖藏。而流通的白银(因其贬值)，反而可能会促使价格回升。</p>
</blockquote>
<p>=&gt;<br>温和通胀有效，而通缩不利于经济。<br>一国货币贬值，则物价相对于其他货币稳定的国家是上涨的。<br>硬通货的拥护者占了上风，要求按照战前水平兑付黄金，使得货币升值，通缩产生。</p>
<h4 id="德国马克"><a href="#德国马克" class="headerlink" title="德国马克"></a>德国马克</h4><p><strong>1、事实：</strong></p>
<p>每月价格上涨50%–&gt;恶性通胀；马克的恶通胀认为导致了希特勒。</p>
<ul>
<li><p>通胀从小跑进入狂奔<br>一战后的德国，物价是战前的2.5倍，一年后上涨3倍，一年后又上涨2倍多，后来的几个月停顿了下。<br>1921年11月，为战前的40倍。<br>1922年11月开始进一步加速。23年年中开始恐怖狂奔。年末停止。<br>算下爬行阶段耗时2年多，小跑1年半多，狂奔半年多。最高能一个月涨二十多倍。小跑中间也有几个月算温和。<br>23年1月，法国进入鲁尔盆地，8月政府宣布征税，11月与法国战败。此种种不满终于带来了希特勒发动的啤酒馆暴动。</p>
</li>
<li><p>现象<br>民间的借贷利息是政府的120倍。<br>政府和企业主精英重建了工厂，巩固了权力，而对各个家庭生活或者不同经济部门，何等不平等不公正。<br>低利息时大肆借贷购入不动产和商品，马克贬值再用马克归还，产生了新富。<br>政府注销了所有内债。<br>农民享受了高的粮食卖出价格，抵押贷款偿债压力减弱。<br>实际工资下降，真实工资已经低于维持生活的需要。<br>工会要求工资同生活费用指数挂钩，但是消费在一周后进行，当前的指数仍然无法避免手里的钱一周后失去价值。<br>死亡、移民、犯罪上升。<br>耐用品的价格不再取决于需求，而是取决于一周后获得的成本，一周后可能需要双倍的当前价格。<br>汇率贬值-&gt; 国内物价上升-&gt; 增发纸币-&gt; 进一步贬值-&gt; 物价进一步上升  的恶性循环。<br>但，<br>20-23年间国GDP表现不错。钢铁产量稳定。</p>
</li>
<li><p>1923年<br>通胀狂奔后，马克失去信用。11月，失业23%，煤产量44%等经济毁灭。<br>对策：财政部领导人变更，确定结束通胀的新方案，严格削减开支、建立新的税种，德国回复了预算平衡。引入新马克作为临时货币。</p>
</li>
</ul>
<p>=&gt;<br>通胀到了极限，就停顿了。</p>
<p><strong>2、原因探究：</strong></p>
<ul>
<li><p>赔款<br>巨额赔款需要金马克支付，国际市场美元升值高于国内，外国银行购入马克，而使得德国免费获得大量的食品和原材料。但22年7月停止以外汇支付赔款，通胀却加速了。</p>
</li>
<li><p>国际收支<br>出口商品以支付赔款的压力(需要大量出口来获得用以赔偿的外汇，主动性地贬值来刺激出口) 导致马克贬值，国内物价上涨，为维持国内商贸不得不继续印发纸币。</p>
</li>
<li><p>马克投机<br>卖空马克者</p>
</li>
<li><p>避免革命<br>失业和商品短缺会带来俄国一样革命。通胀至少有表面的繁荣。</p>
</li>
<li><p>通胀获利团体的助力<br>马克贬值，外国人希望用马克购买德国艺术品(马克需求增多、而增发的马克进一步大于需求？)；担心受损，资产纷纷被转移到国外。这些都进一步导致货币进一步超发。</p>
</li>
<li><p>政府赤字</p>
</li>
</ul>
<p>=&gt;美元升值和出口压力，为超发找到了借口。本质是巨额赔款成为超发货币的借口。而且政府是通胀获利的最大团体，政府不想看到革命，而马克投机者里，更多是否是政府中人呢。经济现象的解释里，很多结果会反作用于现象，造成现象的强化，而这样的结果被认为成”原因”。这并非根因。</p>
<p><strong>3、问题</strong></p>
<ul>
<li>如果是超发，那么当时真的减少发行，又会出现什么样子呢，会不会紧缩–失业和短缺呢。会不会赔款付不了呢。受害的会否比这样的通胀好？为什么这样的恶性通胀，竟然伴随着就业和GDP的稳定甚至增长呢？</li>
</ul>
<p>(减少发行，如果控制得好能够不会恶性通胀，但是亦不能因此通缩。需要把握节奏。并且，大量的赔款如果不发行足够的货币，除非经济的增长速度快，能够产生相应的流动性需要，否则赔款就是突发的流动性需求。会付不了，或者时间很长。就业和GDP稳定因为，也是算温和的。23年狂奔后导致了大量失业和降产，失业大幅增多现象比通胀发生地慢一些。也可能是失业统计的数据，发生在通胀之后。)</p>
<ul>
<li>新马克和分配券有什么不同呢，都是以土地作为抵押保证。新马克的背景：赔款和政治经济都还是老样子，而新马克却使得恶性通胀恢复地惊人。劣币无法流通后良币取而代之。–海温斯坦为啥能取得稳定货币的成就？</li>
</ul>
<p>(本质是，新马克保证了其价值、控制了发行；而分配券还是超发。似乎根本不在于表面的这些相似，而在于对欲望的态度。)</p>
<ul>
<li>为什么通胀是旧价格的延续而通缩不是？</li>
</ul>
<p>(如果货币价值✖️价格 = 商品劳务价值，那么通胀不改变商品劳务价值，则旧的价格是由于货币价值而反向变动。通缩则是流动性不足，当人们不愿意拿钱买、不愿意出钱投资，认为商品都不值得买，并不是因为觉得货币更值钱，更多可能出于谨慎因素，回报低或者对未来悲观。商品也会慢慢退出市场。由于悲观，影响到投资和产出，变化来自等式右边。)</p>
<ul>
<li>为什么会小跑到狂奔？</li>
</ul>
<p>=&gt;<br>惊人的数字让人记忆犹新。德国现在仍会谨慎。</p>
<h4 id="俄国计划经济"><a href="#俄国计划经济" class="headerlink" title="俄国计划经济"></a>俄国计划经济</h4><p>todo                    </p>
<br>
]]></content>
      <categories>
        <category>货银</category>
        <category>货币政策</category>
      </categories>
      <tags>
        <tag>货银</tag>
        <tag>货币政策</tag>
      </tags>
  </entry>
  <entry>
    <title>银行拨备</title>
    <url>/2021/02/17/%E9%93%B6%E8%A1%8C%E6%8B%A8%E5%A4%87/</url>
    <content><![CDATA[<p><br> <br></p>
<p>拨备有一个重要的功能，就是“以丰补歉”，以平滑利润。</p>
<span id="more"></span>

<blockquote>
<p>为什么要平滑利润？<br><br>如果银行全部股东一直没有变动，那么其实要不要以丰补歉都是一样的，反正赚和亏都是这些股东的。但是，因为股东每个交易日有进有出，所以得考虑利润跨期平滑问题。<br><br>假设有一家银行，过去每年盈利100元，非常稳定地持续了5年以上。突然有一年，发生了一笔不良资产，当年会亏损200元。如果这家银行没有以丰补歉，而是前5年把利润全分红了，然后有股票换手，有新股东进来，然后到了第6年亏损200元，那么新股东怎么都不会开心的。他们会觉得，前面的股东赚走了贷款的利息，而这笔贷款发生违约时，亏损让自己承担，这太不公平了。<br><br>这跟信用债交易有点类似。有些信用债在违约发生前发生交易，新买入的投资者成了接盘侠……</p>
</blockquote>
<p>如果按照要求计提拨备，也很做到平滑利润（不良太多，把超额拨备消耗完毕之后还不够）。那么，还有一个方法，就是在不良贷款的确认上动手脚。因为不良资产和非不良资产之间，并没有清晰、客观的边界，边界划分在哪，是有一定的主观性的。</p>
<p>基于拨备覆盖率指标，我们可以有这样一个假设（确实存在例外的情况）：<code>银行不太可能一边宽松认定不良（甚至隐藏不良），一边又计提大量超额拨备，保持很高的拨备覆盖率。</code></p>
<blockquote>
<p>（1）如果一家银行拨备覆盖率远超监管标准（比如150%），并且还在持续提升，那么很明显是处于丰的阶段。很显然，市场上，这样的银行股，估值一般不低，而且主升浪是从它们拨备覆盖率开始显著上升开始的。<br><br>（2）如果一家银行拨备覆盖率在较长时期内仅维持在监管标准附近，那么有可能是：每年的营业收入用于消化存量不良之后，无能力留存额外的拨备。甚至，它是每年收入能消化多少不良就确认多少不良，并且还有存量不良还未消化完毕（存量不良有可能还未确认到报表中），还在补欠。<br><br>（3）如果一家银行拨备覆盖率高位回落，可能也是处于补欠的阶段，即每年收入已经不足以消化新发生不良，而是需要拿过去的超额拨备去消化新发生的不良。<br><br>（4）一家银行的拨备覆盖率从监管标准开始起飞，则有可能是存量不良处置完毕了，开始进入丰的阶段，积累超额拨备。</p>
</blockquote>
]]></content>
      <categories>
        <category>货银</category>
        <category>商业银行</category>
      </categories>
      <tags>
        <tag>货银</tag>
        <tag>商业银行</tag>
      </tags>
  </entry>
  <entry>
    <title>笛卡尔积</title>
    <url>/2021/03/11/%E9%9B%86%E5%90%88%E7%9A%84%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF/</url>
    <content><![CDATA[<br>
<br>


<h2 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h2><span id="more"></span>

<br>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static String cartesianProduct(final String[][] inputs) &#123;</span><br><span class="line">        if (inputs &#x3D;&#x3D; null) &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        final StringBuilder sb &#x3D; new StringBuilder();</span><br><span class="line">        </span><br><span class="line">        product(&quot;&quot;, 0, inputs, sb);</span><br><span class="line">        </span><br><span class="line">        return sb.toString();</span><br><span class="line">    &#125;</span><br><span class="line">public void product(String prefix,int index, String[][] input,StringBuilder sb)&#123;</span><br><span class="line">        for (int i &#x3D; 0; i &lt; input[index].length; i++) &#123;</span><br><span class="line">            if (index &gt;&#x3D; input.length - 1) &#123;</span><br><span class="line">                sb.append(prefix + input[index][i]);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                product(prefix + input[index][i], index + 1, input, sb);</span><br><span class="line">            &#125;</span><br><span class="line">            if (i &lt; input[index].length - 1) &#123;</span><br><span class="line">                sb.append(&quot;, &quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>两个变量关键prefix和index，作为递归方法的参数时进行变化<code>prefix + input[index][i]</code>和<code>index+1</code>。</p>
<p>从数列角度看，sb(j) = sb(j-1) + charAt(i) ,charAt(i)需要一个for循环即<code>for (int i = 0; i &lt; input[index].length; i++)</code>。sb(j-1)为<code>prefix</code>，下一步需要<code>prefix + input[index][i]</code>；其中<code>j-1</code>为<code>index</code>，进入下一步需要<code>index+1</code>。</p>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>零散问题repo</title>
    <url>/2021/04/05/%E9%9B%B6%E6%95%A3%E9%97%AE%E9%A2%98repo/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>无中心、碎片知识点</strong></p>
<span id="more"></span>


<ul>
<li><p>get post</p>
<ul>
<li>GET请求是通过URL直接请求数据，而POST请求是放在请求头中</li>
<li>GET提交有数据大小的限制，POST请求在HTTP协议中也没有做说明，一般来说是没有设置限制的，但是实际上浏览器也有默认值</li>
<li>登录操作的时候，尽量使用HTTPS请求，安全性更好</li>
</ul>
</li>
<li><p>gmv gtv</p>
<ul>
<li>总销售额，总交易额</li>
</ul>
</li>
</ul>
<ul>
<li><p>感受野<br>如果两个3<em>3和一个5</em>5对原始输入后卷积都得到了1<em>1的输出<br>那么哪个filter好呢？<br>从计算量和参数个数来看，选择多层卷积，而每个卷积为3</em>3的更好。</p>
</li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>零散知识点</category>
      </categories>
      <tags>
        <tag>零散知识点</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM</title>
    <url>/2021/02/04/SVM/</url>
    <content><![CDATA[<br>


<ul>
<li><p>reference：[非常好的两本书。再加上libsvm的源码与调参的论文。]<br>[1]<a href="http://files2.syncfusion.com/Downloads/Ebooks/support_vector_machines_succinctly.pdf">http://files2.syncfusion.com/Downloads/Ebooks/support_vector_machines_succinctly.pdf</a><br>[2]An Introduction to Support Vector Machines and Other Kernel-based Learning Methods<br>[3]<a href="https://pan.baidu.com/share/link?shareid=262520779&amp;uk=1378138793">https://pan.baidu.com/share/link?shareid=262520779&amp;uk=1378138793</a></p>
</li>
<li><p>干货</p>
</li>
<li><p>首先，SVM是解决supervised learning 中classification问题。有两种情况，看是否linearly separable。线性不可分则引入kernel，想法为先做transformation到其他空间进而转为可分问题。</p>
</li>
<li><p>对于线性可分的监督分类问题，SVM的目标是什么呢? find  the optimal separating hyperplane which maximizes the margin of the training data</p>
</li>
<li><p>为什么以最大化间隔为目标？因为it correctly classifies the training data and because it is the one which will generalize better with unseen data</p>
</li>
</ul>
<span id="more"></span>
<ul>
<li><p>这里的 间隔 指？关于间隔涉及到两种分类，一种分类为几何间隔与函数间隔；一种为软、硬间隔。几何间隔在二维则为点线距离，三维空间就是我们学习的点面距离。函数间隔二维中可以理解为<em>几个</em>点没有在线上，三维则为<em>几个</em>点没有在面上；或者结合几何间隔可理解为，是将几何间隔求解的分母去掉了，没有归一化(也因此SVM中不能选择以函数间隔衡量，否则maximizes是没有意义的)。关于软硬，是看噪声，There will never be any data point inside the margin.  If data is noisy, we need soft margin classifier.</p>
</li>
<li><p>继上面的目标，假设该超平面的公式为W*X=0，这里会有疑惑：</p>
<ul>
<li>Why do we use the hyperplane equation W<em>X instead of   Y=a</em>x+b? –&gt; the vector W will always be normal to the hyperplane</li>
</ul>
</li>
<li><p>澄清下要做的步骤：<br>  1 数据集<br>  2 选择两个超平面能够分类数据并在两平面间没有其他点<br>  3 最大化超平面间隔</p>
</li>
<li><p>将步骤整理成数学过程</p>
<ul>
<li><p>设两个超平面， W<em>X+b = -θ  和  W</em>X+b = +θ。这里，为什么我们需要两个超平面？我们设想的是，假定最佳的超平面在这两个超平面的中间。我们求得两个超平面即可求得最佳分类超平面。</p>
</li>
<li><p>θ取值无关，直接设为1。 即得W<em>X+b = -1  和  W</em>X+b = +1。这里要想明白W与b到底是什么关系？b依赖还是独立于W？显然，是独立的，可以想象为，我们需要求得W与b两个变量，能够最大化间隔。</p>
</li>
<li><p>需要满足两个约束: 1. 任何&gt;=1的为class 1 2.任何&lt;=-1的为class -1 –&gt;这个限制使在两平面间没有其他点</p>
</li>
<li><p>将两个约束写为一个式子即： y*(w*x+b)&gt;=1</p>
</li>
<li><p>最大化间隔 ？对于这个问题，目前我们已知条件是两个。一个是两个平面 W<em>X+b = -1  和  W</em>X+b = +1。一个是有一个点x在平面  W<em>X+b = -1 上</em>。得：<br><code>w*(x + m*w/||w||)+b=1</code><br>化简得 <code> m = 2/||w||</code></p>
</li>
<li><p>得到的公式意味着：如果||w||没有限制，那么m我们可以取得任意大的值。</p>
</li>
<li><p>现在自然就面临optimization problem。所有的点subject to  y*(w*x+b)&gt;=1, 在此条件下如何minimize ||w||?先引入<strong>理论1</strong>，该理论为两个条件，在两个条件满足的情况下，可以说我们得到了一个scalar function的local minimum。</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2021/02/04/SVM/theorem1.png" alt="theorem1"></p>
<ul>
<li>f为从集合σ(其元素为vector)到实数集(其元素为值)的映射，且在x处 连续、可二阶导。这里涉及到两个的概念：<ol>
<li>**gradient ：a generalization of the usual concept of derivative of  a function in one dimension to a function in several dimensions  ( the gradient of a function is a vector containing each of its partial derivatives.)**注意符号为 nabla,图中倒三角。 </li>
<li><strong>scalar function：A scalar valued function is a function that takes one or more values but returns a single value. In our case f is a scalar valued function.</strong></li>
</ol>
</li>
<li>positive definite：<br>A symmetric matrix A  is called positive definite if x.T<em>A</em>x&gt;0 , for all n维的实数向量x。</li>
<li><strong>theorem 2</strong>中的四个条件是等价的。因此可以通过其他三种情况来判断是否为正定。这里选择主子式来判断Hessian正定，涉及到三个概念：</li>
</ul>
<p><img src="/2021/02/04/SVM/theorem2.png" alt="theorem2"></p>
<ol>
<li>Minors： 删除某行和某列的所有值再计算行列式。remove the ith line and the jth column</li>
<li>Principal minors ：删除的行、列号一致。remove the ith line and the jth column and i=j</li>
<li>Leading principal minor ：The leading principal minor of A of order k is the minor of order k obtained by <strong>deleting the last n−k rows and columns</strong>.（这里包含一个正三角符号，标注删除哪些行列）栗子看图Leading principal minor</li>
</ol>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-42fb025afc8f45d5.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="leading principal minor.jpg"></p>
<ul>
<li>得到了local minimum，How can we find a global minimum?两步走， 1. Find all the local minima 2. Take the smallest one; it is the global minimum. 另一个思路是看我们的f是否是<strong>convex</strong> ,是then we are sure its local minimum is a global minimum.</li>
</ul>
<p><strong>Theorem: A local minimum of a convex function is a global minimum</strong> 这里又涉及到convex function, convex set的定义。</p>
<ul>
<li>What is a <a href="http://mathworld.wolfram.com/ConvexFunction.html">convex function</a>? A function is convex if you can trace a line between two of its points without crossing the function line.<br><img src="http://upload-images.jianshu.io/upload_images/8716089-fd9cfea722d77a1d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="convex  0&lt;λ&lt;1.jpg"><blockquote>
<p>A function is convex if its epigraph (the set of points on or above the graph of the function) is a convex set. In Euclidean space, a convex set is the region such that, for every pair of points within the region, every point on the straight line segment that joins the pair of points is also within the region. </p>
</blockquote>
</li>
</ul>
<p>栗子看图convex set</p>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-b484ebf0cfdf222d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="convex set.jpg"></p>
<ul>
<li>同样根据Hessian判断是否convex，这里需要看是否是<strong>positive semidefinite</strong>,而semi有对应三个条件是与之等价。看 theorem 3<blockquote>
<p>**More generally, a continuous, twice differentiable function of several variables is convex on a convex set if and only if its Hessian matrix is positive semidefinite on the interior of the convex set.**The difference here is that we need to check all the principal minors, not only the leading principal minors. </p>
</blockquote>
</li>
</ul>
<p>  <img src="http://upload-images.jianshu.io/upload_images/8716089-744d4e22dac7d5aa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="theorem 3.jpg"></p>
<ul>
<li>其中on the interior of the convex set是什么意思呢？定义：the domain of a convex function is a convex set，那么 a function is convex on a convex set意思就是在domain上是convex function，而interior只是意味着为两边开区间。</li>
<li><a href="http://www.math.cmu.edu/~ploh/docs/math/mop2013/convexity-soln.pdf">其他证明是convex function的方法</a></li>
<li>这里就谈convex function的optimization问题求解。涉及对偶概念，根据wiki，<blockquote>
<p>Duality :duality means that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem (the duality principle). The solution to the dual problem <strong>provides a lower bound</strong> to the solution of the primal (minimization) problem. </p>
</blockquote>
</li>
</ul>
<p>  给最小值以下限。lower bound中有一个值为<strong>infimum</strong> (即 greatest lower bound)。补充，相对而言</p>
<blockquote>
<p>The same logic apply with the relation “greater than or equal” and we have the concept of upper-bound and supremum.</p>
</blockquote>
<ul>
<li>对偶，在求最小值时求对应的最大值，求出的最大值将是=&lt;最小值，两者之差即为<strong>duality gap**。对应来说，在求最大值时求对应最小值，求出的最小值将是&gt;=最大值即upper bound。</strong>duality gap<strong>为正，我们称之</strong>weak duality holds<strong>，为0则为</strong>strong duality holds**。</li>
<li>拉格朗日乘子 ： <blockquote>
<p>In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to <strong>equality</strong> constraints. </p>
</blockquote>
</li>
</ul>
<p>  <img src="http://upload-images.jianshu.io/upload_images/8716089-59b1f2660a4869b6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Lagrange_portrait.jpg"></p>
<ul>
<li><p>如何将3D图在2D平面表示：Contour lines  两个要点：1. 线上的点z值不变，for each point on a line, the function returns the same value 2. 颜色扮演标识，the darker the area is, the smallest the value of the function is<br><img src="http://upload-images.jianshu.io/upload_images/8716089-271a7e44b55a0424.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="contours.png"></p>
</li>
<li><p>那么梯度就可以用<strong>向量场</strong>进行可视化。箭头指向函数增长的方向。与Contour lines 图有什么关系呢？在Contour lines 图中，gradient vectors非常容易画出，1 垂直于Contour lines 2.指向增加的方向。<img src="http://upload-images.jianshu.io/upload_images/8716089-8ee3efb4b3646c71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="gradient_and_contour.png"></p>
</li>
<li><p>将约束函数和优化目标函数以contour lines 画在一幅图中，并画出两者的gradient vector。可得到最优点。图中的优化目标函数为x^2+y^2, 约束函数为 y=1-x。 <img src="http://upload-images.jianshu.io/upload_images/8716089-fc1f3051bbf27c8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="function_and_constraint.png"></p>
</li>
<li><p>▽f(x,y) = λ ▽g(x,y)<br>λ 这就是拉格朗日乘子。根据图中，当两个gradient vector平行时，我们得到最优解。无论是否同向。更无论是否等长。乘以λ 即意味着不必等长。即求▽L(x,y,λ )=0时的x，y。现在我们需要列出L并求解。</p>
</li>
<li><p>由于我们需要求f(w)=1/2*||w||^2的最小值，将每个约束函数乘以的 λ需要取正数。<img src="http://upload-images.jianshu.io/upload_images/8716089-76c3fc3fc413375d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="L.jpg"></p>
</li>
<li><p>又面临一个问题，求L(x,y,λ )=0， the problem can only be solved <strong>analytically when the number of examples is small</strong> (Tyson Smith, 2004 即只有当约束函数数量比较小的时候，λ 个数不多，我们才能用分析的方法求解). So we will once again rewrite the problem using the duality principle–&gt;we need to minimize with respect to w and b, and to maximize with respect to a at the same time.我们在上一步需要最小化<br><img src="http://upload-images.jianshu.io/upload_images/8716089-4ca65da333340058.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="duality_after_L.jpg"></p>
</li>
<li><p>这里需要讲清楚三个问题。1. 拉格朗日是对约束函数是等式的情况，那么我们在这里是不等式约束的问题，也用拉格朗日乘子解决，需要满足什么条件吗？(KKT)2.之前说了，对偶问题有强与弱，只有当强时，gap才为0，我们才能将对偶问题的最大值作为原问题的最小值。那么，这里是否满足是strong duality holds? （强对偶 即下文Slater’s condition）3.或许你对为什么能够是对w b求min，对a求max还是留有疑问。(拉格朗日到对偶问题这两个之间的转化过程)</p>
</li>
<li><p>仍需要引入两个理论。1.  duality principle 2.Slater’s condition 3.KKT<br>首先，L对w与b求偏导，令为0(这里两个等式)，再将这两个等式带入到L中，消去了w、b，只剩下变量a，即得L(a)。于是将问题转化为 Wolfe dual Problem<br><img src="http://upload-images.jianshu.io/upload_images/8716089-67cdfbf09b820630.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="wolfe dual.jpg"></p>
</li>
<li><p>慢着，这里又出现一个问题。</p>
<blockquote>
<p>Traditionally the Wolfe dual Lagrangian problem is constrained by the gradients being equal to zero. In theory, we should add the constraints θL/θw=0  and  θL/θb=0 . However, we only added the latter, because it is necessary for removing   b from the function. However, we can solve the problem without the constraint   θL/θw=0.</p>
</blockquote>
<p>这里就会不明白为什么不需要加上θL/θw=0约束仍能够solve the problem？暂且保留疑问。</p>
</li>
<li><p>Karush-Kuhn-Tucker conditions :<strong>first-order necessary conditions</strong> for a solution of an optimization problem to be optimal<br>除了KKT还需要满足一些regularity conditions，其中之一为Slater’s condition。</p>
<blockquote>
<p>Because the primal problem we are trying to solve is a convex problem, the KKT conditions are also sufficient for the point to be primal and dual optimal, and there is zero duality gap.</p>
</blockquote>
<p>这里说的，即只要为convex问题，KKT也满足，即可说得出的结果是原问题或对偶问题的最优解，因为Slater’s condition是一定满足了的，gap=0。对于SVM，如果结果满足KKT，那么即可说是最优解。(详细证明过程[2] ch5)</p>
<p>   <img src="http://upload-images.jianshu.io/upload_images/8716089-62f19539ba8a88bc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="KKT.jpg"></p>
</li>
<li><p>可见，1. stationary 即为偏导为0即为驻点,若无约束函数则直接gradient是0，有了约束则gradient of the Lagrangian为0。2. primal feasibility为原问题约束函数  3. dual feasibility 为我们对L求解时使用对偶理论时的约束函数  4.complementary slackness含义是，要么a=0，要么y*(w*x+b)-1=0.这里与<strong>Support vectors</strong>相关，</p>
<blockquote>
<p>Support vectors are examples having a positive Lagrange multiplier. They are the ones for which the constraint y*(w<em>x+b)-1&gt;=0  is active. (We say the constraint is active when y</em>(w*x+b)-1=0 )</p>
</blockquote>
<p>这里，是否会疑惑为什么不能同时为0？为什么multiplier一定是正数？在KKT中，我们只选取支持向量，即将不等号约束改为等号约束，其他的点不考虑。</p>
<blockquote>
<p>Solving the SVM problem is equivalent to finding a solution to the KKT conditions.” (Burges, 1988)</p>
</blockquote>
</li>
<li><p>现在有了L(a),求导即可。得到了a。再根据偏导为0的公式回代得到w 。再根据prime problem中的约束函数y*(w*x+b)-1&gt;=0，计算b</p>
</li>
<li><p>用QP solver来解对偶问题。用python CVXOPT包。将wolfe dual.jpg中我们需要求解的公式转化到下面CVXOPT支持的形式。这里引入了一个Gram matrix - The matrix of all possible inner products of X.</p>
</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-907dcfe22f62860e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="CVXOPT.jpg"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-ac41318c2c79efb1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="转化过程.jpg"></p>
<p>这个图里有问题，minimize部分最后一项需要<code>q.T*a</code>, 详见代码部分，需要q = cvxopt.matrix(-1 * np.ones(m))。</p>
<ul>
<li>code部分：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cvxopt.solvers</span><br><span class="line">X, y &#x3D; 这里获取到数据</span><br><span class="line">m &#x3D; X.shape[0]  #data有多少</span><br><span class="line"># Gram </span><br><span class="line">K &#x3D; np.array([np.dot(X[i], X[j]) for j in range(m) for i in range(m)]).reshape((m, m)) </span><br><span class="line">P &#x3D; cvxopt.matrix(np.outer(y, y) * K)</span><br><span class="line">q &#x3D; cvxopt.matrix(-1 * np.ones(m))</span><br><span class="line"># 等式约束</span><br><span class="line">A &#x3D; cvxopt.matrix(y, (1, m))</span><br><span class="line">b &#x3D; cvxopt.matrix(0.0)</span><br><span class="line"># 不等式约束</span><br><span class="line">G &#x3D; cvxopt.matrix(np.diag(-1 * np.ones(m))) h &#x3D; cvxopt.matrix(np.zeros(m))</span><br><span class="line"># 求解</span><br><span class="line">solution &#x3D; cvxopt.solvers.qp(P, q, G, h, A, b)</span><br><span class="line"># 拉格朗日乘子</span><br><span class="line">multipliers &#x3D; np.ravel(solution[&#39;x&#39;])</span><br><span class="line"># 支持向量</span><br><span class="line">has_positive_multiplier &#x3D; multipliers &gt; 1e-7 </span><br><span class="line">sv_multipliers &#x3D; multipliers[has_positive_multiplier]</span><br><span class="line">support_vectors &#x3D; X[has_positive_multiplier] </span><br><span class="line">support_vectors_y &#x3D; y[has_positive_multiplier]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#计算w，b</span><br><span class="line">def compute_w(multipliers, X, y):</span><br><span class="line">    return np.sum(multipliers[i] * y[i] * X[i]  for i in range(len(y)))</span><br><span class="line">def compute_b(w, X, y):</span><br><span class="line">    return np.sum([y[i] - np.dot(w, X[i]) for i in range(len(X))])&#x2F;len(X)</span><br><span class="line"></span><br><span class="line">w &#x3D; compute_w(multipliers, X, y)</span><br><span class="line">w_from_sv &#x3D; compute_w(sv_multipliers, support_vectors, support_vect</span><br><span class="line">b &#x3D; compute_b(w, support_vectors, support_vectors_y)</span><br><span class="line"> </span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<p>We saw that the original optimization problem can be rewritten using a Lagrangian function. Then, thanks to duality theory, we transformed the Lagrangian problem into the Wolfe dual problem. We eventually used the package CVXOPT to solve the Wolfe dual.</p>
</blockquote>
<ul>
<li>为什么需要将拉格朗日函数转化为对偶问题到wolfe dual？<br>这里还差对偶原则及Slater’s condition 概念。</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>股市整体价值理论</title>
    <url>/2021/02/22/%E8%82%A1%E5%B8%82%E6%95%B4%E4%BD%93%E4%BB%B7%E5%80%BC%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[<br>
<br>

<h4 id="大牛股"><a href="#大牛股" class="headerlink" title="大牛股"></a>大牛股</h4><ul>
<li>现在没涨，一年内会涨；<code>现在涨了，预计继续上涨（至少在未来3个月内）</code>；曾经大涨，但现在已经停止上涨或者开始下跌或者已经大跌，且在短期内股价不存在大涨（创新高）的可能性。</li>
<li>“买大牛股，抓主升浪”，这两句话是不可分的。</li>
<li>高确定性的操作与盈利模式</li>
<li>只关注那9～10类强势股</li>
<li>股票投资要记住两句话：第一句话是不要赔钱，第二句话是记住第一句。<span id="more"></span></li>
<li>绝大多数赔钱者，其赔钱的主要原因是缘于<code>选股问题——选错了股票，或者选中了非牛股，甚至是大熊股</code>，而操作技巧方面的不当只占赔钱的次要因素。</li>
<li>短期暴涨型主升浪：<code>在短期内股价涨幅达到80%～100%的上涨波段，或者是股价涨幅能够在一年内至少翻倍的股票</code>。<br>
<br></li>
</ul>
<p><img src="/2021/02/22/%E8%82%A1%E5%B8%82%E6%95%B4%E4%BD%93%E4%BB%B7%E5%80%BC%E7%90%86%E8%AE%BA/tqly.png" alt="63.28/19.42=3.26倍"><br>63.28/19.42=3.26倍，11/06-01/09，<br>10年50.22-&gt;488.57，年收益0.8729<br>PEG=[44.7 79]/87=[0.506 0.908]</p>
<br>

<ul>
<li>上涨动力：一是内在价值低估–&gt;估值修复力，二是内在价值成长–&gt;内在增长力，三是投机价值增长–&gt;投机价值增长力。</li>
<li><code>从低估值股票挖掘，再看其成长性，最后看其有无热门概念，这就是选股的逻辑。</code></li>
<li>一是信息系统，包括信息搜集、整理、归类与处理；二是分析系统，分析信息对于股价的作用，特别是对于主升浪的作用，这需要在事前建立相关的评价标准；三是操作系统，在“五面整体顺势”的情况下加仓，在“五面整体逆市”的情况下减仓，即，在判断正确的情况下持仓或者加仓，在判断错误的情况下不增仓或者减仓止损。</li>
<li>一个运动的物体受到的力可以分为四种：一是原动力，二是持续推动力，三是阻力，四是运动过程中新加入的作用力。</li>
<li>炮射导弹可以利用大炮而获得极高的初速度与相当远的运动距离，可以大量节省火箭发动机的燃料，这对于降低导弹成本、提高导弹射程是很重要的。</li>
<li>洲际导弹的变轨技术</li>
<li>主升浪是十分罕见的，主升浪所占的时间一般不到该股票的整体交易时间的30%。也就是说，大多数股票在**70%**以上的时间里是处于非主升浪状态，其中的多数又是处于振荡状态，或者说无明显的趋势状态。无趋势状态，是很难预测的，故可以归为随机波动范畴。</li>
<li>小市值低价股现在被公告注入热门资产后，这是一个巨大的利好题材，该利好题材带来“一字涨停板”方式暴涨，我们经常看到这类股票会以<strong>连续5～6个甚至是十几个“一字涨停板”暴涨</strong>。这样的暴涨过程，就是一个惯性运动，即，以巨大的初速去克服股价上涨的所有阻力，当股价运动速度无法抵消阻力的反作用时，股价的向上运动就会停止，甚至会因前期的过度上涨而发生走势逆转，导致短期大跌，最终形成暴涨暴跌之态。</li>
<li>股价原动力的爆发力度：估值法，就是对股票进行估值，看看股价是否被低估。绝对估值法，就是评估股票的绝对投资价值。相对估值法。</li>
</ul>
<br>

<p>综上，<br>1、不能选错股票、不能赔钱，核心：大牛股的主升浪，核心：低估值股票挖掘，再看其成长性，最后看其有无热门概念。<br>2、短期暴涨型主升浪<br>3、连续5～6个甚至是十几个“一字涨停板”暴涨<br>4、一是原动力，二是持续推动力，三是阻力，四是运动过程中新加入的作用力。<br>5、股价原动力的爆发力度：估值法</p>
<br>
<br>

<h4 id="绝对估值法"><a href="#绝对估值法" class="headerlink" title="绝对估值法"></a>绝对估值法</h4><p>评估股票的内在价值与股价的差值，当内在价值与股价的比值越大，则股价的上涨潜力越大，这种潜力一旦遇到合适的市场时机，就有可能爆发出来，转变为引发股价暴涨的原动力。</p>
<ul>
<li><p>投资价值估值法<br>1、目标股票的长期投资收益率与长期国债收益率进行比较，两者差值越大，则长期投资价值越高，这里的长期应该是指5～10年以上。巴菲特的标准更高一些，他的长期价值投资收益率基本定在10年内的年均复合收益率达到10%以上。<br>2、在考虑到风险溢价的情况下，择股标准应该趋于保守，所选择的长期价值股应该是低市盈率的成长股。按照价值投资的基本标准，对于长期成长股来说，其合理的股价范围应该满足PEG＜1（PEG=市盈率/收益增长率），且越低越好。当PEG＜0.5时，股票就存在足够的安全边际。<br>3、若一只股票未来的5～10年的年均复合收益增长率能够达到20%，则当该股的市盈率跌到10倍时，就具有极好的投资价值。应该注意的是，此时的股价虽然不高估，但也不是<strong>明显低估</strong>，还难以产生爆发性很强的估值修复力。假若此时，股市暴跌，该股股价随着股市暴跌而意外地跌到5倍市盈率，那么，此时该股股价就被明显低估了。</p>
</li>
<li><p>资产价值估值法<br>1、比较股票净资产值与总市值的比值，比值越大，则股价低估越多，股价上涨空间越大。<br>2、这类股票主要体现在隐蔽资产股上面。自2006年来，隐蔽资产股经常走出大牛股。<br>3、一隐蔽资产的大幅升值不被知晓，二隐蔽资产的增值过程是公开的、渐进式的，但隐蔽资产股的股价涨幅在起初滞后于隐蔽资产的增值幅度，最终股价会出现报复性补涨，使得隐蔽资产增值的这一事实成为了股价暴涨的原动力。</p>
<blockquote>
<p>2012年7月的罗顿发展：当《二十一世纪经济报道》披露该股在10年前曾花了2亿元在博鳌的黄金地段买下了1800亩土地，这些土地现值100多亿元，以罗顿发展总股本4.4亿股计算，仅这些土地价值就足以支撑该股股价到达15～20元。在该信息公告时，罗顿发展的股价只有3.9元。该信息公告后，罗顿发展的股价就出现了连续涨停板，不到一个月股价涨幅达到170%！在此期间，上证指数还是下跌的。<br><br>2013年1月28日至2月6日的西水股份就属此例。西水股份持有1.3亿股兴业银行，当兴业银行股价暴涨后，西水股份持有的兴业银行价值大幅增值，几乎等于西水股份的总市值，由于西水股份还持有未上市的天安保险11亿股，以及其它资产，这等于说除去兴业银行外，西水股份持有的其它资产被忽略不计了，这是明显不合理的，于是，在2013年1月28日至2月6日，该股就出现报复性的补涨，股价从6.6元涨到了11.8元，几乎翻倍。</p>
</blockquote>
</li>
<li><p>绝对套利估值法<br>1、上市公司被以高出市场价一大截全额要约收购时，其要约收购价格就是股票的真实内在价值，这个内在价值与现价的差值就是股价的无风险套利空间。假若市场是有效的，那么，股价就会几乎一步到位地涨到那个要约收购价格，于是，这个要约收购公告就是造成股价暴涨的原动力。<br>2、前几年被要约收购退市的石油大明、辽河股份等，就属此例。<br>3、当然，在有些时候，要约收购并非是全额的，而是部分的，如去年的重庆啤酒，那么，这类股票的上涨就可能不是一步到位的，而是渐进式的，二级市场就存在套利空间。在渐进式的情况下，要约收购公告虽然还是股价上涨的原动力，但它却会受到当时的市场运行状态、该股同板块股票运行状态等因素的影响，因此，股价运行就不仅受到原动力的作用，还受到其它新的作用力的作用。<br>3、这种情况还出现在某些上市公司大股东回购公司部分股票时，回购部分股票的股价并非是股票的真实内在价值，该价格只是大股东认可的价格而已，不能与完全要约收购价格相提并论，这是需要搞清楚的。</p>
</li>
</ul>
<p>综上，<br>1、按照逻辑，估值存在潜力时，股价未能反应的原因为，存在其他阻力，如市场运行状态、同版股票运行状态等。<br>2、概括，三个绝对估值方法为，PEG&lt;0.5的长期投资、隐蔽资产(所持股票或投资的公司或土地等)、全额要约收购与部分要约收购。</p>
<br>

<h4 id="相对估值法"><a href="#相对估值法" class="headerlink" title="相对估值法"></a>相对估值法</h4><p>一是基本面预期类，二是市场面预期类，三是技术面预期类，四是大盘面预期类。</p>
<p>1、就是领涨的市场、板块或者个股，它们是那些拥有比价效应、等待补涨的市场、板块或者个股的追赶的目标。<br>2、在补涨者开始补涨时，领涨者可以暂时停止上涨；在补涨者开始补涨时，领涨者还可以继续上涨，这对补涨者的领涨作用更大。<br>3、随着价格等自变量的变化，心理预期这个因变量也会随之变化。可见，若用一句话表述心理预期类投机价值，那就是–&gt;一切处于变化之中。<br>4、戴维斯双击，是指因股票收益持续成长，投资者会对股票未来的收益增长产生更高的预期，在未来收益还未实现的情况下，就以未来的高收益定位其市盈率，从而提升其市盈率定位水平，最终导致股价过度上涨。<br>5、<code>提升了股票的市盈率，也就是提升了股票的估值，这相当于产生了新的投机价值</code><br>6、戴维斯双击原理： 由于 <code>股价=每股收益*市盈率</code> ，当每股收益持续增长时，即使市盈率保持不变，股价也会同比上涨，这属于戴维斯双击中的第一击；当每股收益持续增长时，若还提升市盈率，则股价会更上一层楼，涨幅会更大，这属于戴维斯双击中的第二击。戴维斯双击能够导致乘数效应，使得股价涨幅加倍。由于戴维斯双击提升了股票的市盈率，也就是提升了股票的估值，这相当于产生了新的投机价值。戴维斯双击效应产生的根源，就是投资者依据历史数据的惯性，顺势推导、过度预期的结果。<br>7、更重要的是<code>因比价关系引发的股价跟涨而造成的股价虚高</code>，但这并不妨碍我们得出因基本面过度预期而产生新的投机价值的结论。<br>8、主流热点是市场热钱追逐的对象，处于热点中的板块与股票一定是短期内最牛的，基本上属于短线暴涨型品种。<br>9、市场面预期，就是对于市场未来炒作热点的预判。一旦判断某个板块或者个股刚刚成为或者在未来会成为市场炒作热点，那么，该板块或者个股就具有新的投机价值，股价就具有投机价值所赋予的上涨空间。<br>10、比价关系是领涨者已经给出了方向，给出了大致的投机价值空间，但市场面预期是市场才刚刚开始启动热点，或者还未开始热点，但投机者只是预见到了炒作热点将出现，但投机者一般还难以确定投机价值到底有多大，上涨空间有多大，这要走一步看一步。<br>11、在2012年12月中旬，当媒体披露北斗系统将投入商业应用时，超级主力立即抢进了北斗星通等龙头股，造成北斗概念股暴涨。我认为，这次对于北斗概念股的炒作，属于市场面预期主导下的“自我增强”型炒作模式。其基本逻辑是：超级主力判断北斗概念也许会成为市场热点，但到底能形成多大的热点，他们也许并没有很大把握，因为北斗概念是一个全新的概念，人们很陌生，超级主力担心投资者不认同；但超级主力知道，要让投资者认识且认同北斗概念，最佳的方法就是让北斗概念股暴涨，因为股票暴涨一定会吸引全市场的眼光，所有人都会好好研究的，这就是一种“胡干胡有理，越干越有理”的江湖思维。当然，平心而论，毕竟北斗导航属于国家级的概念，超级主力知道，即使胡干的风险也不大，最终，他们成功了。<br>12、赚钱的境界是这样的：最低的境界是打工，用自己的身体赚钱；次低的境界是做实业，让别人的身体为自己赚钱；较高的境界是玩钱（做金融），让钱生钱；而最高的境界是玩规则，让所有的人与所有的钱为自己赚钱。这就是赚钱的金字塔模型。<br>13、以二级市场来论，也存在一个投资者的金字塔模型：<br>最低的境界是交易，次低的境界是跟庄，较高的境界是坐庄（单只股票），而最高的境界是引领市场、发动行情（玩板块甚至玩整个市场）。北斗概念主力就属于最高境界的，不仅是北斗概念，去年市场的几乎所有的市场热点——3D打印、手游、传媒等，都是不同的超级主力们发动的行情。这些主力们“敢为天下先”的底气何在？有人可能认为是他们的资金实力雄厚，此言谬矣，你若有钱，去发动钢铁板块、水泥板块试一试？我认为，这些超级主力的过人之处，还是<strong>对于未来市场主流热点的预期、研判的功力深厚</strong>，任何人要是有这个本事，就一定会走在市场曲线的前面。<br>14、价量时空。<br>15、基本面分析公司未来价值，关注的是现在的股价，以及未来的股价定位，不太关心过去股价。<br>16、基本面分析的核心就是定价理论，即，对于现在股价是否合理进行评判——若低估了就可以买进。公司未来价值的预测，以期判断股价的上涨空间。巴菲特的办公室里是没有电脑的，但他非常关心现在的股票报价与其内在价值的关系，至于股票过去是什么价格或者什么走势，他也从来不看股价走势图。<br>17、市场面分析是通过比价关系来推测股价定位。所以，市场面分析主要是看现在股价与未来股价，对于过去股价也不是太关心。这很容易理解，若某个板块因热门概念而出现暴涨时，在依据比价关系挖掘该板块内的补涨股时，看绝对价格要比看历史走势重要得多，只要股价被低估，任何形态的股价走势都可以出现补涨。<br>18、技术分析的目的是为了预测股价未来走势，侧重于研究过去价格。它将股价走势图从基本面、市场面中抽提出来，让股价走势图完全独立了。走势图一旦独立，技术分析也就自成一统。技术分析将股价走势图看成一个活物留下的足迹，通过研究这些历史足迹，去预测这个活物下一步或者未来将走向何处。技术分析就是分析与预测<strong>这三种趋势的产生、持续与转换关系</strong>。<br>19、首先，技术分析要研究<strong>趋势的产生</strong>。<strong>一个新趋势的产生，一定是原有的其它两类趋势中的某一个终结的结果。</strong>如，一个新的上涨趋势的产生，就一定意味着一个原有的下跌趋势，或者横盘趋势的结束。所以，研究新趋势的产生，意味着要同时研究旧趋势的结束，两个相关趋势要同时研究。<br>20、其次，技术分析要研究趋势的持续。<strong>一个趋势一旦产生后，就会持续</strong>，这种持续就相当于趋势产生了顺势发展的惯性。很显然，假若能够认识到一个上涨趋势是处于持续发展中，或者惯性上涨中，那么，投资或者投机就变成了很简单的事情——买入持有，随着趋势惯性上涨就可以了；假若…<br>21、再次，技术分析还要研究趋势的结束。与研究趋势产生，本质上是一回事，只是研究的对象不同罢了<br>22、所有的技术分析方法，万变不离其宗，就是<strong>为了揭示趋势的产生、持续与结束</strong>，或者简言之，就是为了揭示趋势惯性。投机价值就是预期价值，而趋势惯性本身就具有预期性质，所以，趋势惯性能够提升或者降低投机价值。<br>23、波浪理论有某一浪的预期上涨高度、形态理论有形态突破后的预期量度涨幅、量价理论有放量突破后的惯性上涨高度，等等。224、股市有谚：横有多长，竖有多长。说的是一只股票若长期做底或者横盘，股价一旦启动向上突破，那么，涨幅机会十分惊人。这个惊人的涨幅，就是技术面预期类赋予的投机价值，因为这个投机价值的存在，使得股价最终具有了预期中的上涨空间。这是一个自我实现的预言——因为技术面给出了预期涨幅，那么，买方就会在涨幅到顶之前持续买入，以获取预期中的那个投机价值收益。<br>25、技术面预期类投机价值对于短线交易，特别是对于挖掘主升浪启动点或者启动阶段、主跌浪的启动点与启动阶段都是至关重要的。<br>26、对于短线交易来说，趋势、形态、量价关系是最重要的三个因素，而均线、指标等，就属于次级重要的因素了。<br>27、“济安金信价值分析系统”，该系统的主开发人杨健教授，说<strong>均线系统属于滞后指标</strong>，对于操作意义不大，而该系统运用了许多自创的新指标。<br>28、巴菲特说：“投资只需要学习两门课程就可以了，一门是如何评估企业价值，一门是如何看待股市波动。” </p>
<p>综上，<br>1、基本面预期<br>2、热点等市场面预期研判<br>3、戴维斯双击<br>4、领涨、补涨、跟涨虚高</p>
<br>

<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>大体说的，也是已知的，但是给了个分析框架。<br>基本面向好，为什么不能反映在股价。为什么8个月涨了4倍多。涨停的逻辑在哪。等问题在框架内还是有一定程度解释力。</p>
<p>框架基本操作思路是牛股的大波段，选股思路为低估值+成长+题材。<br>并由力学中四种力，四种力的不同合力模式构成物体运动。<br>股市中不同合力模式构成股价上涨，主要三方面估值修复+价值成长+投机价值。<br>转而对原动力进行估值法定量衡量。<br>绝对估值法包含：PEG、隐蔽资产、要约收购。<br>比价效应带来领涨、补涨、跟涨虚高。而预期无参照物，不同于比价，即一切处于变化之中。<br>相对估值法包含：基本面预期、市场面预期(热点)、技术面预期、大盘面预期。</p>
<p>再统一，即为投资价值和投机价值。</p>
<p>补充：<br>基本面的信息主要包括政策性信息、行业性信息、经营信息（产品价格信息、重大合同信息、新项目与新产品信息）、财务信息（营收信息、净利润增减信息）等。<br>技术面信息主要包括价格图表、技术指标与交易信息，其中，价格图表是技术分析的核心要件，不同的交易者关注不同的图表；技术指标更是五花八门，多达数百种，只能是各取所需了；而交易信息主要是各种交易数据的排行榜，包括涨幅榜、换手率榜、成交量榜、量比榜、成交金额榜等。</p>
]]></content>
      <categories>
        <category>股市理论</category>
        <category>数女-谷</category>
      </categories>
      <tags>
        <tag>股市理论</tag>
      </tags>
  </entry>
</search>
