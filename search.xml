<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2021.05</title>
    <url>/2021/07/28/2021-05/</url>
    <content><![CDATA[<br>

<h3 id="0523"><a href="#0523" class="headerlink" title="0523"></a>0523</h3><p>1、时间管理</p>
<p>!头脑里的问题和复盘，所思所想才能所悟</p>
<span id="more"></span>

<p>!没有计划和纪律就无主心骨</p>
<p>!不回溯自己走过的路径，出现的问题，会失去很多。</p>
<p>!功能繁杂就需要拆分，一个个解决。</p>
<p>!答案的找寻，需要自己去看具体的case</p>
<p>!要一次做到高质量 而不要缝缝补补</p>
<p>!<code>理性</code>，而<code>自信</code>– 对自我和信念坚持的信心。内心告诉自己不能的时候，就没有动力去研究如何可能，没有动力去找到正解</p>
<p>!思考的深度，让你行动时自信有纪律</p>
<p>!知识点的琐碎，在于理论框架的缺乏。自我体系的缺乏。</p>
<p>!工具不行，还是尽量得解决，不然只会拖延问题的最终效率</p>
<p>!不只看代码 还是得去了解思路后再看代码， 一个个看变量含义实在没有意义</p>
<p>!大部分时间的消耗，在于零散。零散的生活琐碎，零散的知识积累。没有理解到本质</p>
<p>!模型 是一个对问题的思考框架。如果没有细节上的关键点，或者局限或者突破，或者思路的逻辑性和数学上严谨性，那就漏洞百出了</p>
<p>!人文底蕴、语言的精炼、关注点的<code>深度</code>广度，以及<code>谦卑</code>，让他如此有魅力。</p>
<p>!要看放弃了什么</p>
<p>!所有以上的过程都不加判断地行动 愚昧</p>
<p>!头脑中对问题的预演，多个备选方案</p>
<p>!流畅的理解所形成的的记忆，一连串地因果链条</p>
<p>!每一个视角都可以深挖到许多细节和展开</p>
<p>!信号。而不是具体内容更关键</p>
<p>!大多抄来抄去</p>
<p>!写时思考</p>
<hr>
<br>

<p>2、零碎</p>
<p>是什么：</p>
<p>旁氏骗局，明斯基：拆东补西，夸张宣传，诱鱼上钩</p>
<p>幸存者偏差：成功者上总结的trait未必是必要条件 – 因为失败者同样可能有相同的trait然而并未成功。飞机弹孔的例子。</p>
<p>数字货币：纯数字化的 现实技术困难，想象OR设计实现的效果</p>
<p>国债轧空：</p>
<br>

<p>所有这些与我有什么关联？</p>
<p>我看到武去拆解一个概念的思路，以及材料来源。第二，和理论相结合和印证。</p>
<br>

<p>stl：deepcopy 重复地构造析构 异常</p>
<p>测试框架、过程监控</p>
<br>

<hr>
<p>3、tips</p>
<p>理性 信心 深度– 体现在细节 谦卑 精炼。慢慢地锤炼。</p>
<p>先预演、设计、想象、框架性、多角度、多方案，再行动（思考后看代码看资料等）。即计划（保证专注）与纪律（保证发力）。有所放弃。</p>
<p>方法论：拆解、温故、问题盘旋推敲 – 追求本质 、流畅地本质 口述逻辑链条、悬空观察、一次性高质量-不要修补、琐碎收束到体系中、陷入practice–case study、框架性思维问题、好工具事半功倍、语言或者事件背后的信号。</p>
<p>思考的角度可以带来很多不一样的感受。关注在运营、产品、变现、技术、手段、模式… …</p>
<hr>
<p>进一步：</p>
<br>

<p><strong>复盘细节与case、框架性与角度 –( 细节的上升 问题的抽象)、三思而行、悬空与慢、质量与效率、工具与程序与规范的重要性。</strong></p>
<br>

<blockquote>
<p>Case\ trick \ abstract \ detached \ one-time success\ tools \ steps \ time \  …</p>
</blockquote>
<br>

<p>记忆：</p>
<p>为了本质和质量，可以牺牲效率。但不能无节制。</p>
<p>本质即抽象、维度角度、细节等的层次。</p>
<p>5年后，10年后，20年后？</p>
<p>金融的10年后，会更加精彩。算法就没有10年后了么。需要对金融、互联网、产品、运营以及技术都有一定自己的理解和框架。世界是丰富的。如果是金融，从事研究、交易，一样的学习路线，一样去体悟。有一定的胜率。能够把握住精髓么。金融科班的含金量和算法，都高。都有价值和背书。都可以生成价值。</p>
<p>国家国际都缺乏天才。天才尚且无法解决一些问题。理论的局限性，还需要时间发展。</p>
<p>落脚到现实。还是不要选择从头开始。时间来不及。有余力，再去做吧。</p>
<br>

]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>Apriori_fptree</title>
    <url>/2021/03/11/Apriori-fptree/</url>
    <content><![CDATA[<h2 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h2><ul>
<li>input<br>{ID1:[item1,item2…],ID2:[item3,item2…],…}</li>
</ul>
<span id="more"></span>
<ul>
<li><p>output<br>item1-&gt;item2,item3…</p>
</li>
<li><p>math<br>频率及条件概率的简单应用<br>支持度: 频数/样本总量<br>置信度: 条件概率<br>如果子集不为频繁集则超集一定不为；反之如果超集是频繁集则子集均为频繁集。</p>
</li>
<li><p>parameters<br><code>minSupport</code><br><code>minConf</code></p>
</li>
<li><p>过程<br>1-4完成了频繁集的搜寻，5及之后产生规则。</p>
</li>
</ul>
<p>1、<code>list</code>存放每个用户的  <code>itemset</code>，生成[[],[]]存放所有item并排序，<code>map(frozenset, C1)</code><br>2、第一轮遍历，生成一个<code>list</code>存放所有支持度大于<code>minSupport</code>，大于则<code>insert(0,key)</code>。遍历的函数<code>scanD</code>所生成<code>list</code>的元素是<code>frozenset</code>。如果<code>can.issubset(tid)</code>且<code>！ssCnt.has_key(can)</code>则加入到<code>supportData</code>。<br>3、list存放所有频繁物品集，<code>supportData</code>存放所有计算过的物品集的支持度，以<code>frozenset</code>作为<code>key</code>。两者更新的过程是在<code>apriori</code>中循环调用<code>aprioriGen</code>和<code>scanD </code>，再更新。直到<code>scanD</code>得到的频繁物品集为空。<br>4、<code>aprioriGen</code>的过程，两层循环，遍历上一个(即物品集中个数为目前需要产生的个数-1)频繁物品集，<code>L1 = list(Lk[i])[:k-2];L2 = list(Lk[j])[:k-2]</code>取前k-2项比较，若相同则<code>retList.append(Lk[i] | Lk[j])</code>将物品集union。是一个merging过程。</p>
<p>5、如果频繁集<code>frozenset</code>中item多于2，先<code>rulesFromConseq</code>再<code>calcConf</code>。<code>rulesFromConseq</code>是个递归过程，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rulesFromConseq(freqSet, H, supportData, brl, minConf&#x3D;0.7):</span><br><span class="line">    m &#x3D; len(H[0])</span><br><span class="line">    if (len(freqSet) &gt; (m + 1)): #停止条件</span><br><span class="line">        Hmp1 &#x3D; aprioriGen(H, m+1)</span><br><span class="line">        Hmp1 &#x3D; calcConf(freqSet, Hmp1, supportData, brl, minConf)</span><br><span class="line">        if (len(Hmp1) &gt; 1):   #至少需要两个元素才能够merge</span><br><span class="line">            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)</span><br></pre></td></tr></table></figure>
<p>这里一个问题是，如果<code>len(Hmp1) =1</code>，但是<code>len(freqSet) &gt; (len(Hmp1[0]) + 1)</code>仍然成立，因此修改为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rulesFromConseq(freqSet, H, supportData, brl, minConf&#x3D;0.7):</span><br><span class="line">    m &#x3D; len(H[0])</span><br><span class="line">    if (len(freqSet) &gt; (m + 1)): #停止条件</span><br><span class="line">        if(len(H) &gt; 1):</span><br><span class="line">            Hmp1 &#x3D; aprioriGen(H, m+1)</span><br><span class="line">        Hmp1 &#x3D; calcConf(freqSet, Hmp1, supportData, brl, minConf)</span><br><span class="line">        rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)</span><br></pre></td></tr></table></figure>

<br>


<h2 id="fptree"><a href="#fptree" class="headerlink" title="fptree"></a>fptree</h2><ul>
<li><p>概念<br>闭项：超集的支持度为s，子集的支持度都不超过s。<br>条件模式基： 即一个item追溯到root的所有路径<br>条件fp树 ： 即根据条件模式基创建的tree<br><code>myCondTree, myHead = createTree(condPattBases, minSup)</code></p>
</li>
<li><p>准备<br><code>headertable</code> : <code>dict</code>，<code>value</code>为<code>[count,treeNode]</code><br><code>treeNode</code> : <code>class</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def __init__(self, nameValue, numOccur, parentNode):</span><br><span class="line">     self.name &#x3D; nameValue</span><br><span class="line">     self.count &#x3D; numOccur</span><br><span class="line">     self.nodeLink &#x3D; None</span><br><span class="line">     self.parent &#x3D; parentNode      </span><br><span class="line">     self.children &#x3D; &#123;&#125; </span><br></pre></td></tr></table></figure>
<p>主要方法：<code>createTree</code> <code>updateTree</code>  <code>mineTree</code><br>辅助方法：<code>updateHeader</code> <code>findPrefixPath</code> <code>ascendTree</code></p>
</li>
<li><p>过程</p>
</li>
</ul>
<ol>
<li><code>createTree</code>首先第一次遍历计算<code>count</code>，<code>headerTable[item] = headerTable.get(item, 0) + dataSet[trans]</code>将<code>count</code>简单赋值为1。注意到这里的dataSet类型。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def createInitSet(dataSet):</span><br><span class="line">    retDict &#x3D; &#123;&#125;</span><br><span class="line">    for trans in dataSet:</span><br><span class="line">        retDict[frozenset(trans)] &#x3D; 1</span><br><span class="line">    return retDict</span><br></pre></td></tr></table></figure>
获得<code>freqItemSet = set(headerTable.keys())</code>后第二次遍历。先对item排序，<code>orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)] </code>，再<code>updateTree(orderedItems, retTree, headerTable, count)</code>。</li>
<li><code>updateTree</code>，这里就是普通的对树的操作，也是递归。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if len(items) &gt; 1:#call updateTree() with remaining ordered items</span><br><span class="line">        updateTree(items[1::], inTree.children[items[0]], headerTable, count)</span><br></pre></td></tr></table></figure>
唯一注意之处，需要<code>updateHeader</code>。</li>
<li><code>mineTree</code>和<code>Apriori</code>中的<code>rulesFromConseq </code>一样较复杂，包含递归。先生成条件模式基，再生成conditional-fptree，再递归。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if myHead !&#x3D; None:                  </span><br><span class="line">   mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)</span><br></pre></td></tr></table></figure>
循环递归mineTree的过程即生成规则过程，直到频繁2项集都得到。</li>
</ol>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>2021.07</title>
    <url>/2021/07/28/2021-07/</url>
    <content><![CDATA[<br>



<h4 id="0708"><a href="#0708" class="headerlink" title="0708"></a>0708</h4><p>爷爷离开了。最后的阶段我也没有做点什么。没有陪伴，甚至没有电话，总觉得还有机会。</p>
<span id="more"></span>

<p>我以为自己会悲痛。然而没有。还是在追寻快乐。没有什么阻挡对快感的追求。有种抓紧时间的感觉。抓紧时间感受和热爱。</p>
<p>想离婚了。初心是什么呢。我贪恋的是陪伴和港湾，而不是这个人么。我心疼他累，心疼他在张总面前的委屈，心疼他为我做的一切。但是我还是不满足。不满足于他认知的一些空白，不满足于他不能再带给我见识和思想的冲击。不满足于，他对我而言似乎只是为了避免孤独。不满足于他时常只考虑我，而不够果断。不满足于激情的程度。我想要更多。</p>
<p>比如，他能够更自信，更聪明，更直接。更有魅力。</p>
<p>然而的确，我没有缺点么。为什么他能够包容。</p>
<p>爷爷和我说的最多就是好好工作。我不记得更多教诲了。死亡的肉身，只是皮包骨，枯瘦地。</p>
<p>大姑说爷爷前几个晚上，自己玩被子，玩很久，像个孩子。抓着大姑的手臂，把腿跷在大姑腿上。</p>
<p>回家那天晚上还和大姑吵了起来。一句没有过脑子的话脱口而出。这是我的毛病。但是我道歉了，也是我没想到的，也是我自己的确做的太少了。爷爷为我做的，远远多于我为他所做的。大姑会在背后说不好听的话，但是我也能接受，毕竟我和那些人的交集几乎为零，而大姑在意她的形象也是好的。没什么坏处。</p>
<p>是我太谦让了还是我没认识到利害。怪在没过脑子。</p>
<p>时间过得太快了。而根本记不住什么，也没能滚雪球地让时间给自己带来沉淀。似乎是这样的。感知不到早上和明天，后天的变化。</p>
<p>不要再继续这样消极没有激情的自己。</p>
<br>

<h4 id="0710"><a href="#0710" class="headerlink" title="0710"></a>0710</h4><p>昨天熬夜了，今天状态不太好。</p>
<p>1、整理每个点 – 提取和重复</p>
<p>2、熟悉后建立框架</p>
<p>3、视频输出、白纸上提取 </p>
<p>4、英文重要性</p>
<br>

<p>今天，1 “学习去学习”，讲了两种思维模式，专注和发散，在学习新事物的时候，对模式不够清晰的情况下，在两者之间进行切换反复。 2记忆分为工作记忆和长期记忆，对于长期记忆需要 多次少量的重复。根据时间间隔来重复多次。 3经济学人每天一篇 </p>
<br>



<p>4vue的router，从单html页面的跳转(to)，到工程里按钮点击后跳转到另一个link。后面是我需要的。</p>
<p>还有一些技巧，比如flex 和content，以及elementui和viewui有区别，以及es6的语法–缩写，以及scss，.+类名，scope不影响其他页面的样式。</p>
<p>5计网的视频</p>
<p>6猫的一篇讲私立幼儿园，结论是钱不够也没必要。说到底还是英语问题。</p>
<br>

<h4 id="0711"><a href="#0711" class="headerlink" title="0711"></a>0711</h4><p>吵架。去了奉贤、闵行。没有阅读。</p>
<p>1、不要吵架，非常耽误精力和时间。用沟通来解决问题。</p>
<p>2、利他思维、独立思考和判断</p>
<br>

<h4 id="0712"><a href="#0712" class="headerlink" title="0712"></a>0712</h4><p>1、经济学人</p>
<p>2、看看宁德时代和隆基股份、上海机场和美的的有没有印证</p>
<p>3、猫的文章，除了感受的部分，“基础知识” + “信息源” + “个股”</p>
<p>4、几个基础概念整理</p>
<p>5、打压热门item</p>
<p>6、sql统计标签</p>
<p>7、计网到ch4的第三个视频</p>
<br>

<h4 id="0713"><a href="#0713" class="headerlink" title="0713"></a>0713</h4><p>1、经济学人</p>
<p>2、计网到ch4的第9个</p>
<p>3、高数多元微分和定积分</p>
<p>4、整理昨天的</p>
<p>5、做个算法题、看篇论文-gpt</p>
<br>

<h4 id="0714"><a href="#0714" class="headerlink" title="0714"></a>0714</h4><p>1、ch4结束</p>
<p>2、极限结束</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF</title>
    <url>/2021/03/09/CRF/</url>
    <content><![CDATA[<br>

<p>本文主要介绍 线性链的CRF。应用于标注问题。<br>介绍：概率图、MEMM问题、势函数、CRF的定义和表示方法，以及基本问题。</p>
<span id="more"></span>

<br>



<h3 id="00概率无向图模型"><a href="#00概率无向图模型" class="headerlink" title="00概率无向图模型"></a>00概率无向图模型</h3><p>1、无向图表示的随机变量之间：成对马尔科夫性、局部马尔科夫性、全局马尔科夫性。</p>
<p>2、概率无向图模型定义：<br>联合概率满足上面的三个马尔科夫性，则称此联合概率分布为概率无向图模型或者马尔科夫随机出。</p>
<p>3、团与最大团</p>
<p>4、因子分解<br>联合分布表示为 最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解。</p>
<p>这个函数，就是 势函数。要求是正的。一般为指数函数。</p>
<p>5、条件随机场</p>
<p>给定随机变量x的条件下，随机变量y的马尔科夫随机场。</p>
<p>学习时，学习条件概率模型。预测时利用条件概率，求得最大输出序列y。</p>
<p>定义：</p>
<p><img src="/2021/03/09/CRF/crf.png" alt="crf"></p>
<!-- <img src=crf.png width=80% height=300> -->

<p><img src="/2021/03/09/CRF/lccrf.png" alt="lccrf"></p>
<!-- <img src=lccrf.png width=80% height=300> -->

<br>

<br>



<h3 id="01引入"><a href="#01引入" class="headerlink" title="01引入"></a>01引入</h3><p>MEMM有的缺陷是标签偏向。</p>
<p>标签偏向问题的原因是 – MEMM在每一步都用softmax进行了归一化</p>
<p>CRF通过全局归一化解决了标签偏向问题。</p>
<p>这样讲是比较费解的。论文里的例子容易理解。两个词，rib rob。</p>
<p>概率图如下(来自论文)</p>
<p><img src="/2021/03/09/CRF/bias.png" alt="img"></p>
<p>这里隐状态1到2只有一条路，4到5也只有一条路，所以整个模型没有考虑观测值i 和 o不同，所以导致词性标注过程中，不考虑观测变量。这个就不符合逻辑了。</p>
<p>因此局部的归一化，容易受隐变量影响，而不去考虑观测变量。这个带来的问题是：</p>
<p>如果training数据，3个都是rib，一个是rob。那么在预测过程中，求max P(y1y2y3|rob)时，viterbi算法走出来的，将会是0-&gt;1-&gt;2-&gt;3。</p>
<p>即，<strong>如果上一个状态是低熵的，那么很容易忽略观测变量，而在预测时，受样本影响较大。</strong></p>
<p>即，无论观测值，当State1比State2转移少，State1会一直转移到State1，即使全局看转移到State2概率更高。即有更少转移的状态、拥有的转移概率普遍偏高，概率最大路径更容易出现转移少的状态。</p>
<h3 id="02CRF解决label-bias"><a href="#02CRF解决label-bias" class="headerlink" title="02CRF解决label bias"></a>02CRF解决label bias</h3><p>一，crf将输入序列和输出标注映射为一个d维实数向量，而MEMM的特征函数拥有的信息只是输入序列和当前状态以及上一个状态，也就是说CRF的特征函数掌握信息量更多，从而表达能力更强。</p>
<p>第二个的改进是它不再每一次状态转移进行归一化，而是在全局进行归一化，这样完美解决Label Bias问题。即CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。</p>
<p>有得必有失，注意到模型的分母需要罗列所有的状态序列，对于序列长度为n的输入序列，状态序列的个数为S^n ，对于这种指数增长问题，在实际应用中一般都是intractable的，只能付诸于近似求解，比如我们之前提过的Variational Bayes或者Gibbs Sampling等等。</p>
<p>不过有一种特殊结构的CRF – linear chain crf，精确快速求解的方式是存在的。</p>
<p>(注：马尔科夫随机场就是概率无向图，而crf是马尔科夫随机场特殊一种，而线性链是crf的特殊一种。隐状态是随机场，而观测变量是条件。条件随机场还有其他多种形式。)</p>
<br>

<br>



<h3 id="03最大图、势函数"><a href="#03最大图、势函数" class="headerlink" title="03最大图、势函数"></a>03最大图、势函数</h3><h4 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h4><p>例子更清楚：<br><img src="/2021/03/09/CRF/%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E4%BE%8B%E5%AD%901.png" alt="矩阵形式例子1"><br><img src="/2021/03/09/CRF/%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E4%BE%8B%E5%AD%902.png" alt="矩阵形式例子2"></p>
<h4 id="linear-crf-是如何定义特征函数的？如何将原来intractable的问题变为tractable？"><a href="#linear-crf-是如何定义特征函数的？如何将原来intractable的问题变为tractable？" class="headerlink" title="linear crf 是如何定义特征函数的？如何将原来intractable的问题变为tractable？"></a>linear crf 是如何定义特征函数的？如何将原来intractable的问题变为tractable？</h4><p>linear让最大团的个数为序列长度-1，并且，最大团内部的特征函数为 t1 个状态特征函数 和 t2个转移特征函数 之和的形式。</p>
<h4 id="那，无向图的条件概率怎么表达呢？"><a href="#那，无向图的条件概率怎么表达呢？" class="headerlink" title="那，无向图的条件概率怎么表达呢？"></a>那，无向图的条件概率怎么表达呢？</h4><p>从MEMM可见，本质上有两点，一是x到y的函数，即自定义的特征函数，二是loss。和MEMM不同的是，CRF是无向图。无向图的条件概率怎么表达呢？</p>
<p>来自google图片</p>
<p><img src="/2021/03/09/CRF/crftu.png" alt="img"></p>
<p>这里就引入了最大团概念。由上文介绍。得到</p>
<p><img src="/2021/03/09/CRF/crfp.jpg" alt="img"></p>
<p>这个公式，本质是求 观测变量下隐状态序列的概率。c为最大团个数，k为自定义的k个特征函数。</p>
<p>对于CRF，可以为他定义两款特征函数：转移特征&amp;状态特征。 即将建模总公式展开，见下面👇的参数化表达。</p>
<p>省得写了，下面大量贴图。</p>
<br>

<br>





<h3 id="04三种表达形式"><a href="#04三种表达形式" class="headerlink" title="04三种表达形式"></a>04三种表达形式</h3><p>1、参数化形式</p>
<p>公式：<br><img src="/2021/03/09/CRF/%E5%8F%82%E6%95%B0%E5%8C%96%E5%85%AC%E5%BC%8F.png" alt="参数化公式"></p>
<p>解释如下：<br><img src="/2021/03/09/CRF/%E5%8F%82%E6%95%B0%E5%8C%96%E5%85%AC%E5%BC%8F%E8%A7%A3%E9%87%8A2_1.png" alt="参数化公式解释2_1"></p>
<p><img src="/2021/03/09/CRF/%E5%8F%82%E6%95%B0%E5%8C%96%E5%85%AC%E5%BC%8F%E8%A7%A3%E9%87%8A2_2.png" alt="参数化公式解释2_2"></p>
<p>即：<br><img src="/2021/03/09/CRF/%E8%A7%92%E5%BA%A6.png" alt="角度"></p>
<p>把两种特征合在一起：</p>
<p><img src="/2021/03/09/CRF/hebing1.jpg" alt="1"></p>
<p>合并的公式中，我们并没有显示将边和节点区分开来，而只是写出了边的特征函数，因为从某种程度上边包含的信息已经涵盖了节点所拥有的信息，将两者统一起来可以有利于我们数学公式表达的方便性，另一方面，将边和节点进行单独讨论，从理论上可能有一点冗余，但是从实际效果中，节点信息可以充当一种backoff，起到一定的平滑效果(Smoothing)。</p>
<p>特征函数部分和MEMM一样记做score：</p>
<p><img src="/2021/03/09/CRF/hebingscore.jpg" alt="2"></p>
<p>可见：</p>
<p>我们为 token(隐状态) 打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值。对数线性的思路。</p>
<p>这个的推导在视频里去看。<a href="https://www.bilibili.com/video/BV19t411R7QU?p=4">https://www.bilibili.com/video/BV19t411R7QU?p=4</a></p>
<br>

<p>2、参数化简化的形式推导<br>略</p>
<br>

<p>3、矩阵形式</p>
<p>矩阵形式推导：<br><img src="/2021/03/09/CRF/%E7%9F%A9%E9%98%B5%E6%8E%A8%E5%AF%BC.png" alt="矩阵推导"></p>
<!-- <img src=矩阵推导.png width=70% height=500> -->

<br>
<br>


<h3 id="05三个问题"><a href="#05三个问题" class="headerlink" title="05三个问题"></a>05三个问题</h3><h4 id="模型基本问题"><a href="#模型基本问题" class="headerlink" title="模型基本问题"></a>模型基本问题</h4><p>模型问题包含 learning 和 inference。<br>learning即 parameter estimation。求 θ。<br>inference：<br>(以下 y 为隐变量)<br>1 marginal prob ： 当建模对象为joint distribution，求p(y_t|x)即求边缘概率问题。<br>2 conditional prob ： 求 p(x|y)  –此为生成模型才有的问题，在crf中不考虑<br>3 MAP inference ： decoding ， 求  y_pred = argmax p(y|x)          其中y_pred = y_1y_2y_3 …</p>
<p><img src="/2021/03/09/CRF/wenti.jpg" alt="问题定义"></p>
<br>

<h4 id="CRF-learning"><a href="#CRF-learning" class="headerlink" title="CRF learning"></a>CRF learning</h4><p>1、问题定义<br><img src="/2021/03/09/CRF/learning%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89.png" alt="learning问题定义"></p>
<p>CRF也是极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。能用在log-linear models上的求参方法都可以用过来。</p>
<p>2、todo – 数学推导</p>
<br>




<h4 id="CRF-marginal-prob"><a href="#CRF-marginal-prob" class="headerlink" title="CRF marginal prob"></a>CRF marginal prob</h4><p>1、即求 P(yt = i | x)<br>给定x条件下的 隐变量yt 的边缘概率。<br>即 词性标注中，给定一句话，判断出 y1(第一个词)是动词(i)的概率是多少。</p>
<p>2、硬算，复杂度高。指数级计算不可行。<br>简化：</p>
<p>数学转化。<br>用到了变量消除法， sum+product，又叫信念传播，belief propagate。<br>HMM的前向 后向传播也就是belief propagate。</p>
<p><img src="/2021/03/09/CRF/sumproduct.png" alt="sumproduct"><br><img src="/2021/03/09/CRF/sumproduct2.png" alt="sumproduct2"></p>
<h4 id><a href="#" class="headerlink" title></a><br></h4><h4 id="CRF-decoding"><a href="#CRF-decoding" class="headerlink" title="CRF decoding"></a>CRF decoding</h4><p>求 y的序列，使得 argmax p(y|x)<br>– 类似于 HMM 问题。vertbi，既然是dp，核心要梳理清楚dp的转移函数。</p>
<p>我们就定义i处的局部状态为 f(I) ,表示在位置i处的隐状态的各种取值可能为 I，然后递推位置i+1处的隐状态，写出来DP转移公式。</p>
<br>

<p>reference</p>
<p><img src="https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico" alt="img"><a href="https://arxiv.org/abs/1011.4088">https://arxiv.org/abs/1011.4088</a> </p>
<p><a href="http://www.ai.mit.edu/courses/6.891-nlp/ASSIGNMENT1/t11.1.pdf">http://www.ai.mit.edu/courses/6.891-nlp/ASSIGNMENT1/t11.1.pdf</a></p>
<p><a href="https://www.cnblogs.com/en-heng/p/6201893.html">https://www.cnblogs.com/en-heng/p/6201893.html</a></p>
<p><a href="https://blog.csdn.net/aws3217150/article/details/68935789">https://blog.csdn.net/aws3217150/article/details/68935789</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF前续知识</title>
    <url>/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<br>

<p>从HMM MEMM介绍概率图、生成和判别模型</p>
<br>

<span id="more"></span>

<h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>我理解，机器学习，本质是一种反向求解过程。数据的具象世界里找到抽象的规律。比由规律推演出具象要复杂。</p>
<p>并且反向求解还有各种现实的局限，数据上、算力上、可行性上、性能约束上。就会有各种trick。就像工程中缓存击穿穿透问题，通过小trick来避免。而其根基还是在于统计。</p>
<p>由机器学习到深度学习，根基和理论框架都没有变。</p>
<p>CRF即是一种对序列数据的建模。放松了HMM的1-gram依赖假设和观测独立假设。转移和发射矩阵也进一步抽象为函数。因此就比HMM更实际些。也由于放松， 需要各种理论(和SVM一样涉及到了许多定理，更多是概率统计里分布相关的定理)支持，就比HMM 更加复杂。</p>
<p>很容易从概率图，看到机器学习为何向深度转化。本质都是对现实世界里的学习任务进行建模。</p>
<h2 id="00-生成和判别模型"><a href="#00-生成和判别模型" class="headerlink" title="00 生成和判别模型"></a>00 生成和判别模型</h2><p>概率图为什么那么难。核心在 是否理解 生成和判别模型的区别。</p>
<p>神经网络模型、SVM、perceptron、LR、DT……</p>
<p>NB、LDA ……</p>
<p>核心区别在于： 对 联合概率  还是对 条件概率 建模。</p>
<p><strong>角度一</strong>用inference过程举例：假设我知道P(X) P(X,Y) ，那我就根据条件概率公式预测出来y_pred 了。</p>
<p>这个是 生成模型。</p>
<p>而判别 ： 假设我知道P(Y|X) ，那我直接代入X就可得到y_pred</p>
<p><strong>角度二</strong>用learning过程举例：生成我要对 P(X,Y) 建模，判别我要对P(Y|X) 建模。</p>
<p>其实是很简单的。但是我们 对于不同模型去整体把握的时候 ，忽略了top down思维方式。bottom up的确是不容易想清楚。</p>
<p>进一步： 你能从这里思考出，生成和判别的各自局限吗？</p>
<p>判别边界和(结果的)生成过程、先验假设、数据量和性能</p>
<h2 id="01-概率图"><a href="#01-概率图" class="headerlink" title="01 概率图"></a>01 概率图</h2><p>序列数据 – 标注 分类 走势分析 –&gt;其中概率图解决标注和分类</p>
<p>概率图的话，需要去学习基本的有向图表示含义，图画出来(如下)则能判断，给出隐状态c，a\b之间是否独立。</p>
<p>来自知乎某文：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/tu.png" alt="img"></p>
<p>focus在HMM MEMM CRF –&gt;如何解决标注问题</p>
<h2 id="02-马尔科夫随机场定义-概率无向图定义"><a href="#02-马尔科夫随机场定义-概率无向图定义" class="headerlink" title="02 马尔科夫随机场定义(概率无向图定义)"></a>02 马尔科夫随机场定义(概率无向图定义)</h2><p>马尔科夫假设 – 马尔科夫模型的前提与局限</p>
<p>马尔科夫性质 – a. 成对，b. 局部，c. 全局。</p>
<p>这个性质主要有助于无向图里势函数的推导。不在乎数学，可忽略。势函数见下04。</p>
<h2 id="03-有向图和无向图"><a href="#03-有向图和无向图" class="headerlink" title="03 有向图和无向图"></a>03 有向图和无向图</h2><p>图模型 – 有向(贝叶斯网络)和无向(马尔科夫网络)  </p>
<p>HMM、 Karman filter是有向，boltzman 、CRF是无向</p>
<p>核心区别：在于建模过程如何求 node 代表的token ，该随机变量的联合概率</p>
<h2 id="04-无向图的联合概率-–-势函数"><a href="#04-无向图的联合概率-–-势函数" class="headerlink" title="04 无向图的联合概率 – 势函数"></a>04 无向图的联合概率 – 势函数</h2><p>势函数哪里来的or为啥这个样子？ – 或者说为啥无向图的联合概率是这个样子的因子分解？</p>
<p>： 由 Hammersley-Clifford定理 保证。</p>
<h2 id="05-HMM"><a href="#05-HMM" class="headerlink" title="05 HMM"></a>05 HMM</h2><h3 id="HMM之前的NB"><a href="#HMM之前的NB" class="headerlink" title="HMM之前的NB"></a>HMM之前的NB</h3><p>来自google图片：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/nb.png" alt="img"></p>
<h3 id="HMM五要素"><a href="#HMM五要素" class="headerlink" title="HMM五要素"></a>HMM五要素</h3><p>第一个隐状态的概率分布、转移矩阵、发射矩阵、状态集、观测集</p>
<p>来自google图片：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/hmm.png" alt="img"></p>
<p>进一步： 你能从这里思考出，HMM是生成还是判别？</p>
<p>这也是HMM痛点。假设不合实际、生成自身局限、没有更多特征信息。</p>
<h3 id="HMM问题"><a href="#HMM问题" class="headerlink" title="HMM问题"></a>HMM问题</h3><p>1、学习过程 ，即计算出五要素(建模过程)</p>
<p>监督学习：</p>
<p>MLE – 本质就是频率来估计概率</p>
<p>非监督学习：</p>
<p> Baum-Welch– 本质即EM</p>
<p>2、概率计算过程 ，即建好的模型下(五要素已知下)，当前观测序列的联合概率</p>
<p>直接计算 – 本质就是个暴力穷举</p>
<p>前向算法、后向算法  – 本质和viterbi一样也是DP优化</p>
<p>3、预测过程 ，即预测出当前时刻，哪个隐状态概率最大</p>
<p>近似算法 – 复杂度高且是局部最优：即使当转移概率为0时，仍可能出现在预测出来的状态序列里</p>
<p>viterbi算法 – 本质就是个动态规划</p>
<p>4、filter问题</p>
<p>略</p>
<p>5、smoothing问题</p>
<p>略</p>
<p>这些算法并不需要太复杂的证明，都比较直观。这也是HMM好理解的原因。</p>
<p>感兴趣来bili  一键三连。</p>
<h2 id="06-MEMM最大熵马尔科夫模型"><a href="#06-MEMM最大熵马尔科夫模型" class="headerlink" title="06 MEMM最大熵马尔科夫模型"></a>06 MEMM最大熵马尔科夫模型</h2><p>整个模型的框架和HMM一样，不同之处在于：</p>
<p>1、观测独立的假设被打破</p>
<p>2、局部采用LR(最大熵模型)</p>
<p>3、因为2，建模过程就通过梯度下降</p>
<h3 id="MEMM的概率图"><a href="#MEMM的概率图" class="headerlink" title="MEMM的概率图"></a>MEMM的概率图</h3><p>来自google图片：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memm.png" alt="img"></p>
<p>从图中可见：</p>
<p>1、根据01概率图的基础知识，MEMM中，当y_t已知，这种是个v字形的有向图，则x_t和y_t-1 、x_t-1 是相关的–即路径是连通的。因此打破了HMM的观测独立假设。</p>
<p>2、这里是判别模型，因此对P(Y|X,θ)建模</p>
<p>3、“x_t-&gt;y_t” 每个这个箭头的过程，都是一个逻辑回归LR。因此，每一个这样的局部都需要进行一次softmax，而softmax中e的指数是：w*f( h, y_t) ，也叫做质量分数(mass score)(这个mass来自pmf的m)，其中h是 (y_t-1, x_1:t)</p>
<p>4、概率计算：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memmp.jpg" alt="img"></p>
<p>这里，就是softmax得到的隐状态y的概率，加上log就是最大似然</p>
<p>其中，t为第几个隐状态，i为隐状态值，o为观测值，a为不同特征函数个数–自定义，Z为softmax的归一化。</p>
<p>特征函数是自己定义的，可求导即可。(本质即为，从某个词到某个词性的函数)。特征函数权重是可训练参数。</p>
<p>相当于dfm的子网、graphsage或者GCN的聚合子网。</p>
<p>又找到了一个图，这个更清楚：👈🏻左边为HMM 右边👉🏻为MEMM</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memm3.png" alt="img"></p>
<h3 id="MEMM问题"><a href="#MEMM问题" class="headerlink" title="MEMM问题"></a>MEMM问题</h3><p>和HMM一样，涉及到建模、预测。</p>
<p>区别的地方是：</p>
<p>建模：MEMM的loss是-log极大似然，然后梯度下降去求。</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memmloss.png" alt="img"></p>
<p>这里，对n个样本，每个样本长度为mi，的softmax结果求log，然后加了正则。</p>
<p>最大熵推导出softmax。整个loss的本质框架是MLE。</p>
<h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2><p>补充：</p>
<p>来自google图片：</p>
<p>NB在sequence建模下拓展到了HMM；LR在sequence建模下拓展到了CRF。</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/more.png" alt="img"></p>
<p>reference：</p>
<p><a href="http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf">http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf</a></p>
<p><a href="http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf">http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf</a></p>
<p><a href="https://blog.csdn.net/taoqick/article/details/102672110">https://blog.csdn.net/taoqick/article/details/102672110</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/37163081">https://zhuanlan.zhihu.com/p/37163081</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/33397147">https://zhuanlan.zhihu.com/p/33397147</a></p>
<p><a href="https://www.quora.com/What-is-the-best-resource-to-understand-Conditional-Random-Fields">https://www.quora.com/What-is-the-best-resource-to-understand-Conditional-Random-Fields</a></p>
<p>谷歌图片</p>
<p><a href="https://zhuanlan.zhihu.com/p/113187662">https://zhuanlan.zhihu.com/p/113187662</a></p>
<p><a href="https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://en.wikipedia.org/&amp;httpsredir=1&amp;article=1162&amp;context=cis_papers">https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://en.wikipedia.org/&amp;httpsredir=1&amp;article=1162&amp;context=cis_papers</a></p>
<p><a href="https://www.bilibili.com/video/BV19t411R7QU?p=3">https://www.bilibili.com/video/BV19t411R7QU?p=3</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF后续</title>
    <url>/2021/04/14/CRF%E5%90%8E%E7%BB%AD/</url>
    <content><![CDATA[<br>

<p>介绍下BiLSTM+CRF吧</p>
<br>

<span id="more"></span>



<h4 id="Q1、为啥搞一个LSTM-CRF的hybrid-model"><a href="#Q1、为啥搞一个LSTM-CRF的hybrid-model" class="headerlink" title="Q1、为啥搞一个LSTM+CRF的hybrid model?"></a>Q1、为啥搞一个LSTM+CRF的hybrid model?</h4><p>用LSTM，整体的预测accuracy是不错indeed, 但是会出现上述的错误：在B之后再来一个B。这个错误在CRF中是不存在的，因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。</p>
<p>那就把CRF接到LSTM上面，把LSTM在time<em>step上把每一个hidden</em>state的tensor输入给CRF，让LSTM负责在CRF的特征限定下，依照新的loss function，学习出一套新的非线性变换空间。</p>
<h4 id="Q2、CRF-源码"><a href="#Q2、CRF-源码" class="headerlink" title="Q2、CRF++源码"></a>Q2、CRF++源码</h4><ul>
<li>输入</li>
</ul>
<p>He reckons the current account deficit will narrow to only #1.8 billion in September .”代表一个训练句子xx，而CRF++要求将这样的句子拆成 <strong>每一个词一行并且是固定列数的数据</strong>，其中列除了原始输入，还可以包含一些其他信息，比如第二列包含了POS信息，最后一列是Label信息。而不同的训练序列与序列之间的相隔，就靠一个空白行来区分。</p>
<ul>
<li>特征模版</li>
</ul>
<p>1、“%x[row, column]” 代表获得<strong>当前指向位置向上或向下偏移|row|行</strong>，并指向第column列的值。</p>
<p>2、CRF++中主要有两种特征模版，<strong>Unigram和Bigram 模版</strong>，注意Unigram和Bigram是<strong>相对于输出序列</strong>而言，而不是相对于输入序列。对于”U01:%x[0,1]”这样一个模版，上面例子的输入数据会产生如下的特征函数：</p>
<p>3、</p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>Decorator</title>
    <url>/2021/02/04/Decorator/</url>
    <content><![CDATA[<h2 id="functional-programming-concepts"><a href="#functional-programming-concepts" class="headerlink" title="functional programming concepts"></a><strong>functional programming concepts</strong></h2><p>分两类：</p>
<ul>
<li>Function decorators</li>
<li>Class decorators<span id="more"></span></li>
</ul>
<blockquote>
<p>A decorator in Python is any callable Python object that is used to <strong>modify a function or a class</strong>. A reference to a function “func” or a class “C” is passed to a decorator and the decorator returns a modified function or class. The modified functions or classes usually contain calls to the original function “func” or class “C”. </p>
</blockquote>
<h2 id="show-me-the-code"><a href="#show-me-the-code" class="headerlink" title="show me the code"></a><strong>show me the code</strong></h2><ul>
<li><p><em>参数检查</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def argument_test_natural_number(f):</span><br><span class="line">    def helper(x):</span><br><span class="line">        if type(x) &#x3D;&#x3D; int and x &gt; 0:</span><br><span class="line">            return f(x)</span><br><span class="line">        else:</span><br><span class="line">            raise Exception(&quot;Argument is not an integer&quot;)</span><br><span class="line">    return helper</span><br><span class="line">    </span><br><span class="line">@argument_test_natural_number</span><br><span class="line">def factorial(n):</span><br><span class="line">    if n &#x3D;&#x3D; 1:</span><br><span class="line">        return 1</span><br><span class="line">    else:</span><br><span class="line">        return n * factorial(n-1)</span><br><span class="line"></span><br><span class="line">for i in range(1,10):</span><br><span class="line">	print(i, factorial(i))</span><br><span class="line"></span><br><span class="line">print(factorial(-1))</span><br></pre></td></tr></table></figure></li>
<li><p><em>统计次数</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def call_counter(func):</span><br><span class="line">    def helper(*args, **kwargs):</span><br><span class="line">        helper.calls +&#x3D; 1</span><br><span class="line">        return func(*args, **kwargs)</span><br><span class="line">    helper.calls &#x3D; 0</span><br><span class="line"></span><br><span class="line">    return helper</span><br><span class="line"></span><br><span class="line">@call_counter</span><br><span class="line">def succ(x):</span><br><span class="line">    return x + 1</span><br><span class="line"></span><br><span class="line">@call_counter</span><br><span class="line">def mul1(x, y&#x3D;1):</span><br><span class="line">    return x*y + 1</span><br><span class="line"></span><br><span class="line">print(succ.calls)</span><br><span class="line">for i in range(10):</span><br><span class="line">    succ(i)</span><br><span class="line">mul1(3, 4)</span><br><span class="line">mul1(4)</span><br><span class="line">mul1(y&#x3D;3, x&#x3D;2)</span><br><span class="line">    </span><br><span class="line">print(succ.calls)</span><br><span class="line">print(mul1.calls)</span><br></pre></td></tr></table></figure></li>
<li><p><em>含参数的decorator</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def greeting(expr):</span><br><span class="line">    def greeting_decorator(func):</span><br><span class="line">        def function_wrapper(x):</span><br><span class="line">            print(expr + &quot;, &quot; + func.__name__ + &quot; returns:&quot;)</span><br><span class="line">            func(x)</span><br><span class="line">        return function_wrapper</span><br><span class="line">    return greeting_decorator</span><br><span class="line"></span><br><span class="line">@greeting(&quot;-wyq-&quot;)</span><br><span class="line">def foo(x):</span><br><span class="line">    print(42)</span><br><span class="line"></span><br><span class="line">foo(&quot;Hi&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p><em>using import 要注意变量的域</em></p>
</li>
</ul>
<p>greeting_decorator.py  没用functools的版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def greeting(func):</span><br><span class="line">    def function_wrapper(x):</span><br><span class="line">        &quot;&quot;&quot; function_wrapper of greeting &quot;&quot;&quot;</span><br><span class="line">        print(&quot;Hi, &quot; + func.__name__ + &quot; returns:&quot;)</span><br><span class="line">        return func(x)</span><br><span class="line">    function_wrapper.__name__ &#x3D; func.__name__</span><br><span class="line">    function_wrapper.__doc__ &#x3D; func.__doc__</span><br><span class="line">    function_wrapper.__module__ &#x3D; func.__module__</span><br><span class="line">    return function_wrapper</span><br></pre></td></tr></table></figure>
<p>anotherfile.py:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from greeting_decorator import greeting</span><br><span class="line"></span><br><span class="line">@greeting</span><br><span class="line">def f(x):</span><br><span class="line">    &quot;&quot;&quot; just some silly function &quot;&quot;&quot;</span><br><span class="line">    return x + 4</span><br><span class="line"></span><br><span class="line">f(10)</span><br><span class="line">print(&quot;function name: &quot; + f.__name__)</span><br><span class="line">print(&quot;docstring: &quot; + f.__doc__)</span><br><span class="line">print(&quot;module name: &quot; + f.__module__)</span><br></pre></td></tr></table></figure>
<p>greeting_decorator.py</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from functools import wraps</span><br><span class="line"></span><br><span class="line">def greeting(func):</span><br><span class="line">    @wraps(func)</span><br><span class="line">    def function_wrapper(x):</span><br><span class="line">        &quot;&quot;&quot; function_wrapper of greeting &quot;&quot;&quot;</span><br><span class="line">        print(&quot;Hi, &quot; + func.__name__ + &quot; returns:&quot;)</span><br><span class="line">        return func(x)</span><br><span class="line">    return function_wrapper</span><br></pre></td></tr></table></figure>
<p>否则将返回</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function name: function_wrapper</span><br><span class="line">docstring:  function_wrapper of greeting </span><br><span class="line">module name: greeting_decorator</span><br></pre></td></tr></table></figure>
<ul>
<li><em><code>__call__</code></em><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Fibonacci:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.cache &#x3D; &#123;&#125;</span><br><span class="line">    def __call__(self, n):</span><br><span class="line">        if n not in self.cache:</span><br><span class="line">            if n &#x3D;&#x3D; 0:</span><br><span class="line">                self.cache[0] &#x3D; 0</span><br><span class="line">            elif n &#x3D;&#x3D; 1:</span><br><span class="line">                self.cache[1] &#x3D; 1</span><br><span class="line">            else:</span><br><span class="line">                self.cache[n] &#x3D; self.__call__(n-1) + self.__call__(n-2)</span><br><span class="line">        return self.cache[n]</span><br><span class="line"></span><br><span class="line">fib &#x3D; Fibonacci()</span><br><span class="line"></span><br><span class="line">for i in range(15):</span><br><span class="line">    print(fib(i), end&#x3D;&quot;, &quot;)</span><br></pre></td></tr></table></figure></li>
<li><em>using class</em><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class decorator2:</span><br><span class="line">    </span><br><span class="line">    def __init__(self, f):</span><br><span class="line">        self.f &#x3D; f</span><br><span class="line">        </span><br><span class="line">    def __call__(self):</span><br><span class="line">        print(&quot;Decorating&quot;, self.f.__name__)</span><br><span class="line">        self.f()</span><br><span class="line"></span><br><span class="line">@decorator2</span><br><span class="line">def foo():</span><br><span class="line">    print(&quot;inside foo()&quot;)</span><br><span class="line"></span><br><span class="line">foo()</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepFm</title>
    <url>/2021/03/08/DeepFm/</url>
    <content><![CDATA[<br>

<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在推荐领域较流行的深度模型方案：</p>
<p>实战：阿里MLR、阿里DIN、阿里ESSM、京东强化学习推荐、facebook个性化推荐dlrm、Deep Neural Networks for YouTube Recommendations、华为DeepFM、google2016 wide&amp;deep learning、googleDeep&amp;Cross Network等。</p>
<span id="more"></span>
<p>DeepFm模型及进化：XDeepFM(deep进化)、AFM(加入attention)、FFM(field-aware)、PNN、FNN、NFM等。</p>
<p>其中，DeepFM在论文中通过大量实验证明，DeepFM的AUC和Logloss都优于目前的最好效果。效率上，DeepFM和目前最优的效果的深度模型相当。在Benchmark数据集和商业数据集上，DeepFM效果超过目前所有模型。</p>
<h3 id="Q1：FM解决什么问题？"><a href="#Q1：FM解决什么问题？" class="headerlink" title="Q1：FM解决什么问题？"></a>Q1：FM解决什么问题？</h3><p>1、普通的线性模型，我们都是将各个特征独立考虑的，并没有考虑到特征与特征之间的相互关系。<br>为了表述特征间的相关性，我们采用多项式模型。</p>
<p>在多项式模型中，特征xi与xj的组合用xixj表示。为了简单起见，我们讨论二阶多项式模型。<br>2、与线性模型相比，FM的模型就多了后面特征组合的部分。<br><img src="/2021/03/08/DeepFm/%E4%BA%8C%E9%A1%B9%E5%BC%8F.png" alt="二项式"></p>
<h3 id="Q2：FM参数求解？"><a href="#Q2：FM参数求解？" class="headerlink" title="Q2：FM参数求解？"></a>Q2：FM参数求解？</h3><p>1、公式中，组合部分的特征相关参数共有n(n−1)/2个。在数据很稀疏的情况下xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。</p>
<p>为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解。</p>
<p><img src="/2021/03/08/DeepFm/%E5%BC%95%E5%85%A5V.png" alt="引入V"></p>
<p>2、如何求解V？</p>
<p>推导公式网上到处都有。</p>
<p>3、得到公式推导结果后，对w求导，梯度下降进行训练。</p>
<h4 id="Q3：FFM？"><a href="#Q3：FFM？" class="headerlink" title="Q3：FFM？"></a>Q3：FFM？</h4><p>公式：</p>
<p><img src="/2021/03/08/DeepFm/ffm_gs.png" alt="ffm_gs"></p>
<p>举例：</p>
<p><img src="/2021/03/08/DeepFm/ffm.png" alt="ffm"></p>
<h4 id="Q4：why-DeepFm？"><a href="#Q4：why-DeepFm？" class="headerlink" title="Q4：why DeepFm？"></a>Q4：why DeepFm？</h4><p>1、因子分解机(Factorization Machines, FM)通过对于每一维特征的隐变量内积来提取特征组合。最终的结果也非常好。<br>但是，虽然理论上来讲FM可以对高阶特征组合进行建模，但实际上因为计算复杂度的原因一般都只用到了二阶特征组合。<br>那么对于高阶的特征组合来说，通过多层的神经网络即DNN去解决。</p>
<p>2、One-hot类型的特征输入到DNN中，会导致网络参数太多。<br>如何解决这个问题呢，类似于FFM中的思想，将特征分为不同的field：让Dense Vector进行组合，来表示高阶特征。</p>
<p><img src="/2021/03/08/DeepFm/field.png" alt="field"></p>
<p>3、但是低阶和高阶特征组合隐含地体现在隐藏层中，如果我们希望把低阶特征组合单独建模，然后融合高阶特征组合。<br>=&gt;就得到了DeepFm。</p>
<h4 id="Q5：what-DeepFm？"><a href="#Q5：what-DeepFm？" class="headerlink" title="Q5：what DeepFm？"></a>Q5：what DeepFm？</h4><p>1、有两种融合方式，分别为串行和并行的结构。<br>这里介绍并行结构。</p>
<p><img src="/2021/03/08/DeepFm/structure.png" alt="structure"><br><img src="/2021/03/08/DeepFm/deepfm.png" alt="deepfm"></p>
<p>2、emb部分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#embeddings</span></span><br><span class="line"><span class="comment">#weights[&#x27;feature_embeddings&#x27;] 存放的每一个值其实就是FM中的vik</span></span><br><span class="line">weights[<span class="string">&#x27;feature_embeddings&#x27;</span>] = tf.Variable(</span><br><span class="line">    tf.random_normal([self.feature_size,self.embedding_size],<span class="number">0.0</span>,<span class="number">0.01</span>),</span><br><span class="line">    name=<span class="string">&#x27;feature_embeddings&#x27;</span>)</span><br><span class="line"></span><br><span class="line">weights[<span class="string">&#x27;feature_bias&#x27;</span>] = tf.Variable(tf.random_normal([self.feature_size,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>),name=<span class="string">&#x27;feature_bias&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">self.embeddings = tf.nn.embedding_lookup(self.weights[<span class="string">&#x27;feature_embeddings&#x27;</span>],self.feat_index) <span class="comment"># N * F * K</span></span><br><span class="line">feat_value = tf.reshape(self.feat_value,shape=[-<span class="number">1</span>,self.field_size,<span class="number">1</span>])</span><br><span class="line">self.embeddings = tf.multiply(self.embeddings,feat_value)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果是离散值，则embedding lookup之后，每个维度✖️1，连续值则✖️连续值。<br>✖️后的结果就是公式里的 Vi,f · Xi</p>
<p>3、dnn部分<br>为了更好的发挥DNN模型学习high-order特征的能力，文中设计了一套子网络结构，将原始的稀疏表示特征映射为稠密的特征向量。<br>子网络设计时的两个要点：</p>
<p>不同field特征长度不同，但是子网络输出的向量需具有相同维度；<br>利用FM模型的隐特征向量V作为网络权重初始化来获得子网络输出向量。文中将FM的预训练V向量作为网络权重初始化替换为直接将FM和DNN进行整体联合训练，从而实现了一个端到端的模型。 （即lookup）</p>
<p>4、fm</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># second order term</span></span><br><span class="line"><span class="comment"># sum-square-part</span></span><br><span class="line">self.summed_features_emb = tf.reduce_sum(self.embeddings,<span class="number">1</span>) <span class="comment"># None * k</span></span><br><span class="line">self.summed_features_emb_square = tf.square(self.summed_features_emb) <span class="comment"># None * K</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># squre-sum-part</span></span><br><span class="line">self.squared_features_emb = tf.square(self.embeddings)</span><br><span class="line">self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, <span class="number">1</span>)  <span class="comment"># None * K</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#second order</span></span><br><span class="line">self.y_second_order = <span class="number">0.5</span> * tf.subtract(self.summed_features_emb_square,self.squared_sum_features_emb)</span><br><span class="line">self.y_second_order = tf.nn.dropout(self.y_second_order,self.dropout_keep_fm[<span class="number">1</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="Q6：扩展，自定义算子？"><a href="#Q6：扩展，自定义算子？" class="headerlink" title="Q6：扩展，自定义算子？"></a>Q6：扩展，自定义算子？</h4><p>比如我们输入是每个item的idor一些离散特征时候。需要对离散特征的各个值–&gt;index构建一个table。然后每次输入转换成对应的index，再根据index去tf.nn.embedding_lookup。离散特征对应的weight dict大小，需要给个预估值，大一些，囊括各种离散、连续的取值总数。</p>
<p>这里就可以对tensorflow里的hashtable算子进行扩展。构建一个table，动态增长index，来了一个新的取值，就对应index++。</p>
<p>实现：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">Status <span class="title">Find</span><span class="params">(OpKernelContext* ctx, <span class="keyword">const</span> Tensor&amp; key, Tensor* value,</span></span></span><br><span class="line"><span class="function"><span class="params">              <span class="keyword">const</span> Tensor&amp; default_value)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Grab the input tensor</span></span><br><span class="line">	<span class="keyword">const</span> V default_val = default_value.flat&lt;V&gt;()(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">auto</span> A = key.flat&lt;K&gt;();</span><br><span class="line">    <span class="comment">// Create an output tensor</span></span><br><span class="line">	<span class="keyword">auto</span> output_flat = value-&gt;flat&lt;V&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set all but the first element of the output tensor to 0.</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> N = A.size();</span><br><span class="line">    <span class="keyword">int</span> pos = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">    K find_key;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function">tf_shared_lock <span class="title">sl</span><span class="params">(mu_)</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (; pos &lt; N; pos++) &#123;</span><br><span class="line">            find_key = SubtleMustCopyIfIntegral(A(pos));</span><br><span class="line">            <span class="keyword">auto</span> it = table_.find(find_key);</span><br><span class="line">            <span class="keyword">if</span>(it == table_.end()) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                output_flat(pos) = it-&gt;second;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(pos &lt; N) &#123;</span><br><span class="line">        <span class="keyword">int64_t</span> hsize;</span><br><span class="line">        <span class="function">mutex_lock <span class="title">l</span><span class="params">(mu_)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(; pos &lt; N; pos++) &#123;</span><br><span class="line">            find_key = SubtleMustCopyIfIntegral(A(pos));</span><br><span class="line">            <span class="keyword">auto</span> it = table_.find(find_key);</span><br><span class="line">            <span class="keyword">if</span>(it == table_.end()) &#123;</span><br><span class="line">               hsize = table_.size();</span><br><span class="line">               <span class="keyword">if</span>(hsize &gt;= max_size_) &#123;</span><br><span class="line">                <span class="keyword">return</span> errors::ResourceExhausted(<span class="string">&quot;max size limit&quot;</span>);</span><br><span class="line">               &#125;</span><br><span class="line">               table_.insert(&#123;find_key, hsize&#125;);</span><br><span class="line">               output_flat(pos) = hsize;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                output_flat(pos) = it-&gt;second;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Status::OK();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> dynamic_unique_table <span class="keyword">import</span> DynamicUniqueTable</span><br><span class="line"></span><br><span class="line">table = DynamicUniqueTable(key_dtype=tf.string,</span><br><span class="line">                     value_dtype=tf.int64,</span><br><span class="line">                     default_value=-<span class="number">1</span>,</span><br><span class="line">                     max_ids=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(table.lookup(<span class="string">&quot;asdfetryuijkn&quot;</span>)))</span><br><span class="line">    print(sess.run(table.lookup(<span class="string">&quot;hahahahah&quot;</span>)))</span><br><span class="line">    print(sess.run(table.export()))</span><br><span class="line">    tf.train.Saver().save(sess, <span class="string">&#x27;index_test_model2/&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p>相关命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">本地：</span><br><span class="line">python pai&#x2F;main.py --train_tables&#x3D;.&#x2F;dub_train.csv --test_tables&#x3D;.&#x2F;dub_test.csv --data_type&#x3D;file --discrete_size&#x3D;15 --continue_size&#x3D;14 --epoch&#x3D;30 --buckets&#x3D;.&#x2F; --save_predict&#x3D;True</span><br><span class="line">pai:</span><br><span class="line">组件无法支持多张表输入， 直接在odps的sql页面执行：</span><br><span class="line"></span><br><span class="line">PAI-name tensorflow1120_ext</span><br><span class="line"></span><br><span class="line">-project algo_public</span><br><span class="line">-Dscript&#x3D;&#39;oss:&#x2F;&#x2F;datadrive.oss-cn-shanghai-internal.aliyuncs.com&#x2F;tmp&#x2F;tl_deepfm.tar.gz&#39;</span><br><span class="line">-DentryFile&#x3D;&#39;pai&#x2F;main.py&#39;</span><br><span class="line">-DgpuRequired&#x3D;100</span><br><span class="line">-Dtables&#x3D;&#39;odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_fit_normal,odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_eval_normal&#39;</span><br><span class="line">-Dbuckets&#x3D;&#39;oss:&#x2F;&#x2F;datadrive.oss-cn-shanghai-internal.aliyuncs.com&#x2F;tmp&#x2F;&#39;</span><br><span class="line">-Darn&#x3D;&#39;acs:ram::1872928906167841:role&#x2F;aliyunodpspaidefaultrole&#39;</span><br><span class="line">-DossHost&#x3D;&quot;oss-cn-shanghai-internal.aliyuncs.com&quot;</span><br><span class="line">-DuserDefinedParameters&#x3D;&quot;--train_tables&#x3D;odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_fit_normal--test_tables&#x3D;odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_eval_normal--discrete_size&#x3D;20 --continue_size&#x3D;49 --data_type&#x3D;odps --input_need_hash&#x3D;True --use_input_bn&#x3D;False --epoch&#x3D;100&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>




]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT</title>
    <url>/2021/07/14/GPT/</url>
    <content><![CDATA[<br>

<br>

<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><span id="more"></span>

<ol>
<li>GPT 1.0采取预训练+FineTuning两个阶段，它采取<strong>Transformer作为特征抽取器</strong>。</li>
<li>预训练阶段采用<strong>“单向语言模型”作为训练任务</strong>，把语言知识编码到Transformer里。第二阶段，在第一阶段训练好的模型基础上，通过Finetuning来做具体的NLP任务。</li>
<li>Bert基本就是GPT 1.0的结构，除了预训练阶段采取的是“双向语言模型”之外，它们并没什么本质差异。</li>
<li>首先把Transformer模型参数扩容，常规的Transformer Big包含24个叠加的Block，扩容到48层。其次，大量的不同领域的数据，以及数据筛选。之后，GPT 2.0用这些网页做“单向语言模型”。最后，第二阶段的finetune，2.0采用的是无监督地进行下游任务的学习(只是GPT作者想说明在第一阶段Transformer学到了很多通用的包含各个领域的知识)。</li>
<li>BPE输入改动</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>LPR</title>
    <url>/2021/05/09/LPR/</url>
    <content><![CDATA[<br>

<h3 id="LPR"><a href="#LPR" class="headerlink" title="LPR"></a>LPR</h3><br>

<p>目的，替换贷款利率在贷款定价中发挥基准作用。</p>
<br>

<h4 id="影响"><a href="#影响" class="headerlink" title="影响"></a>影响</h4><p>1】利率市场化改革</p>
<p>​    开始LPR的13年，也取消了贷款利率的下限。</p>
<span id="more"></span>

<p>​    没有起到作用：</p>
<p>​        1、政策属性强，盯住的是贷款基准利率而不是货币市场利率 – 盯住MLF 投放规模大、期限合适、使用频率高、央行选定的中期政策利率</p>
<p>​        2、中小行参与度低 ，报价和lpr的使用方面都只有少量银行参与 – 加入银行</p>
<p>​        3、只有1年期限的报价，对其他期限的利率无法提供参照 – 增加5y期</p>
<br>    

<p>意义-新增贷款盯住LPR，LPR盯住MLF。MLF由央行市场化招标形成，兼具灵活(相对于贷款基准利率)和稳定性(相对于货币市场利率)。</p>
<p>提高了LPR的市场化程度，淡化了贷款基准利率影响，促进解决货币市场化利率和贷款基准利率并存的 利率双轨制问题。推动贷款利率市场化改革。</p>
<br>

<p>2】提高利率传导效率</p>
<p>18年通过MLF等扩张性操作投放了流动性后，货币市场利率下降后，贷款利率并没有随之下降。价格型传导机制受阻。</p>
<p>欧美经验中，市场化利率传导路径：</p>
<p>央行制定政策利率（欧元区利率走廊、降准、MLF、美国公开市场操作、隔夜逆回购等操作）-&gt;货币市场利率（同业拆借、回购）-&gt;存款利率（银行的负债分为存款性和借款性，两者具有替代性。如货币市场利率低时瑞银对存款征收年费）-&gt;货币市场利率和存款利率导致负债成本降低，决定了银行的贷款利率（贷款的成本加成定价模型FTP 贷款利率=资金成本+风险成本+业务成本+合理利润）</p>
<br>

<p>传导成立条件：1存款和 同业负债的替代性（监管机构对银行负债结构的考核使得替代性弱，有专门针对存款的考核如存贷比&lt;=3/4、同业负债占比&lt;1/3。因为借款占比更高时则有流动性和交叉风险。处于对稳定性考虑，银行人才稀缺，放开这样的约束不太现实。） 2银行内部完善的FTP（我国存在双轨制，银行内部存在资产负债部和金融市场部，资产负债部进行存贷款定价，参考存贷款基准利率，然后将多余的资金给金融市场部交易，金融市场参考货币市场利率。货币市场作为后发一方对于存贷款利率影响较小）。我国不满足。</p>
<br>

<p>在我国不太可行的欧美路线，央行考虑跳过欧美传导的路径。通过MLF影响LPR，然后LPR传导给贷款利率。”子午谷奇谋“。1、增加了利率的可控性，货币调控向价格型转变创造空间 2增加了MLF中期货币政策利率的影响力，完善了短期利率走廊+中期政策利率的调控框架</p>
<br>

<p>3】降低实体经济融资成本</p>
<p>货币政策传导受阻，中小企业融资问题仍然突出；贷款基准利率环境下，协同保留贷款利率0.9倍隐性下限。</p>
<p>改革后的LPR报价方式，相较于4.35%，新增贷款的基准利率的确有所下降。</p>
<p>基准利率是贷款客户和银行讨价还价的锚，基准利率下降使得企业议价能力上升。并使得银行难以再设置隐形下限。</p>
<br>

<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>1、期限 2、参与度与报价广泛性 3、mlf的政策属性强 4、央行借款占比低，对整体负债成本影响小，传导有限。5、融资贵更多贵在风险成本和业务成本。而非资金成本。需要其他配套措施解决融资问题。6、存款利率不下，仍然参考基准存款利率。存贷差收缩，压低银行利润，扩大基差风险。银行需要下沉资质，增强风控能力。</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM</title>
    <url>/2021/01/31/LSTM/</url>
    <content><![CDATA[<ul>
<li>篇幅稍长，分为四个部分</li>
</ul>
<ol>
<li>background</li>
<li>step-by-step</li>
<li>show me the code</li>
<li>deep thinking</li>
</ol>
<p>codes<br><a href="https://github.com/satyrswang/blog-jianshu/blob/master/LSTM.lua">https://github.com/satyrswang/blog-jianshu/blob/master/LSTM.lua</a></p>
<h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><ul>
<li><p>what?<br>rnn和feedforward network有嘛不同？</p>
<blockquote>
<p>It’s the easiest to implement an RNN just as a feedforward network with some parts of the input feeding into the middle of the stack, and a bunch of outputs coming out from there as well. There is no magic internal state kept in the network. It’s provided as a part of the input!</p>
</blockquote>
<span id="more"></span>
<p>只是把隐层有拎出来作为下一个隐层的input。=_=<br>然而，理论支持吗？</p>
</li>
<li><p>Problem</p>
<ul>
<li>视频那么多帧，前一帧连着后一帧，间隔又短，那么是否可用前一帧来预测后一帧？<br>看情况。</li>
<li>完形填空 the clouds are in the ___<br>I grew up in France… I speak fluent <em>French</em>.<br>当gap变大，France和<em>French</em>距离那么远，RNN没用了。</li>
<li>为什么gap大了，就没用了？理论证明如下：<br><a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf">Bengio, et al. (1994)</a><br><a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">Hochreiter (1991) German</a></li>
</ul>
</li>
<li><p>然而，LSTMs 不会因为gap惹事儿。<br>Long Short Term Memory networks<br><img src="/2021/01/31/LSTM/chain.webp" alt="chain"></p>
</li>
</ul>
<h2 id="step-by-step"><a href="#step-by-step" class="headerlink" title="step-by-step"></a>step-by-step</h2><ul>
<li>图第二个干嘛了？你先别看图，听我讲：<br>注意这里横着看，看的是chain中第t个</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#过程1 --名字是 input transform</span><br><span class="line">输入 ：input中的x(t)，chain中前一个x输出的结果h(t-1)</span><br><span class="line">参数 ：x的权重w1，h的权重w2，加一个bias</span><br><span class="line">激活函数 ：tanh</span><br></pre></td></tr></table></figure>
<p>以上得到一个结果记为c_in</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#过程2 --名字是 三个gates，每个gate如下</span><br><span class="line">输入 ：input中的x(t)，chain中前一个x输出的结果h(t-1)</span><br><span class="line">参数 ：x的权重w1，h的权重w2，加一个bias</span><br><span class="line">激活函数 ：g</span><br></pre></td></tr></table></figure>
<p>得到三个结果记为i , f , o<br>先保留一个问题： 过程1、2的输入虽然都是x h变量，但是是一样的吗？还是x h这两个向量的部分值呢？<br>有了c_in,i , f , o 之后干嘛，我怎么得到这一层的h？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#过程3 -- 名字是 state update </span><br><span class="line">输入 ：c_in,i , f , o ,c_out(t-1)</span><br><span class="line">输出 ：新的h(t)， c_out(t)</span><br></pre></td></tr></table></figure>
<p>c_out(t-1) 是chain中前一个的输出呗。h、c_out怎么计算的？<br>c_out(t)  =  f * c_out(t-1) + i * c_in<br>h(t)  =  o * tanh(c_out(t))</p>
<ul>
<li>就这么简单？<br>是的。为什么能这样呢？<blockquote>
<p>Because of the <strong>gating mechanism</strong> the cell can keep a piece of information for long periods of time during work and <strong>protect the gradient inside the cell from harmful changes during the training</strong>. Vanilla LSTMs don’t have <strong>a forget gate</strong> and add unchanged cell state during the update (it can be seen as a recurrent connection with a constant weight of 1), what is often referred to as a Constant Error Carousel (CEC). It’s called like that, because <strong>it solves a serious RNN training problem of vanishing and exploding gradients</strong>, which in turn makes it possible to learn long-term relationships.</p>
</blockquote>
</li>
</ul>
<p>原来，因为有个<strong>gating mechanism</strong> 就是 过程2 嘛，解决了RNN的gradient的问题。为什么能解决<strong>vanishing and exploding gradients</strong>的问题呢？理论支持去看论文。</p>
<h2 id="show-me-the-code"><a href="#show-me-the-code" class="headerlink" title="show me the code"></a>show me the code</h2><p>基于 Torch7</p>
<ul>
<li>snippet1: inputs<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local inputs &#x3D; &#123;&#125;</span><br><span class="line">table.insert(inputs, nn.Identity()())   -- x(t)</span><br><span class="line">table.insert(inputs, nn.Identity()())   -- c_out(t-1)</span><br><span class="line">table.insert(inputs, nn.Identity()())   -- h(t-1)</span><br><span class="line">local input &#x3D; inputs[1]</span><br><span class="line">local prev_c &#x3D; inputs[2]</span><br><span class="line">local prev_h &#x3D; inputs[3]</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li><p>  想想看我们要什么？你回答完了之后，听我讲：<br>三个变量 ：过程1、2要的x(t) h(t-1)和过程3还要的c_out(t-1)</p>
</li>
<li><p>  怎么得到？<br>这里用到了<code>nn.Identity()()</code> 和 <code>table.insert</code></p>
<blockquote>
<p>The array-like objects in lua are called tables.<br>nn.Identity() - passes on the input (used as a placeholder for input)</p>
</blockquote>
</li>
</ol>
<p>如果你用tf，那么nn.Identify就是placeholder</p>
<ul>
<li>snippet2: Computing gate values<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local i2h &#x3D; nn.Linear(input_size, 4 * rnn_size)(input) </span><br><span class="line">local h2h &#x3D; nn.Linear(rnn_size, 4 * rnn_size)(prev_h)   </span><br><span class="line">local preactivations &#x3D; nn.CAddTable()(&#123;i2h, h2h&#125;)    </span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li> <code>4 * rnn_size</code>什么鬼？<br>过程1、2在激活前是不是都是x(t) h(t-1)的线性变换？即<code>nn.Linear</code>。<br><code>preactivations</code>将i2h, h2h作加法运算返回一个vector。<br>我们将线性变换的结果分成4份，每份<code>rnn_size</code>多个值。为什么分为4份？记得我们有三个gates吗 ，得到i,f,o？<blockquote>
<p>The first will be used for <strong>i</strong>n gates, second for <strong>f</strong>orget gates, third for <strong>o</strong>ut gates and the last one <strong>as a cell input</strong> .</p>
</blockquote>
</li>
</ol>
<p>就跟玩儿似的。这里<strong>as a cell input</strong>就是直赋值给了h(t)，作为chain下一个的输入。也解释了之前的保留问题，即输入并不是一样的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local pre_sigmoid_chunk &#x3D; nn.Narrow(2, 1, 3 * rnn_size)(preactivations)</span><br><span class="line">local all_gates &#x3D; nn.Sigmoid()(pre_sigmoid_chunk)</span><br><span class="line">local in_chunk &#x3D; nn.Narrow(2, 3 * rnn_size + 1, rnn_size)(preactivations)</span><br><span class="line">local in_transform &#x3D; nn.Tanh()(in_chunk)</span><br><span class="line">local in_gate &#x3D; nn.Narrow(2, 1, rnn_size)(all_gates)</span><br><span class="line">local forget_gate &#x3D; nn.Narrow(2, rnn_size + 1, rnn_size)(all_gates)</span><br><span class="line">local out_gate &#x3D; nn.Narrow(2, 2 * rnn_size + 1, rnn_size)(all_gates)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p> <code>nn.Narrow</code>什么鬼？</p>
<blockquote>
<p>select appropriate parts of the preactivation vector.</p>
</blockquote>
</li>
<li><p>  其他很简单啊，前3份传入gates要<code>nn.Sigmoid()</code>激活。3另一份只需要<code>nn.Tanh()</code>激活。</p>
</li>
</ol>
<ul>
<li><p>snippet3: Cell and hidden state<br>gates结果i f o也有了。进入过程3了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local c_forget &#x3D; nn.CMulTable()(&#123;forget_gate, prev_c&#125;)</span><br><span class="line">local c_input &#x3D; nn.CMulTable()(&#123;in_gate, in_transform&#125;)</span><br><span class="line">local next_c &#x3D; nn.CAddTable()(&#123; c_forget, c_input&#125;)</span><br><span class="line">local c_transform &#x3D; nn.Tanh()(next_c)</span><br><span class="line">local next_h &#x3D; nn.CMulTable()(&#123;out_gate, c_transform&#125;)</span><br></pre></td></tr></table></figure>
<p>按公式计算。没说的。得到<code>next_c</code>和<code>next_h</code></p>
</li>
<li><p>snippet4: define module</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">outputs &#x3D; &#123;&#125;</span><br><span class="line">table.insert(outputs, next_c)</span><br><span class="line">table.insert(outputs, next_h)</span><br><span class="line">return nn.gModule(inputs, outputs)</span><br></pre></td></tr></table></figure></li>
<li><p>手残党的snippet5: 栗子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">require &#39;nn&#39;</span><br><span class="line">require &#39;nngraph&#39;</span><br><span class="line">LSTM &#x3D; require &#39;LSTM.lua&#39;  --以上snippet</span><br><span class="line">--创建3层LSTM，输入3输出3</span><br><span class="line">network &#x3D; &#123;LSTM.create(3, 4), LSTM.create(4, 4), LSTM.create(4, 3)&#125;</span><br><span class="line">--准备</span><br><span class="line">local x &#x3D; torch.randn(1, 3)</span><br><span class="line">local previous_state &#x3D; &#123;</span><br><span class="line">  &#123;torch.zeros(1, 4), torch.zeros(1,4)&#125;,</span><br><span class="line">  &#123;torch.zeros(1, 4), torch.zeros(1,4)&#125;,</span><br><span class="line">  &#123;torch.zeros(1, 3), torch.zeros(1,3)&#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#x3D; nil</span><br><span class="line">next_state &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line">--feed数据</span><br><span class="line">local layer_input &#x3D; &#123;x, table.unpack(previous_state[1])&#125;</span><br><span class="line">for l &#x3D; 1, #network do</span><br><span class="line">  local layer_output &#x3D; network[l]:forward(layer_input)</span><br><span class="line">  table.insert(next_state, layer_output)</span><br><span class="line">  local layer_h &#x3D; layer_output[2]</span><br><span class="line">  if l &lt; #network then</span><br><span class="line">    layer_input &#x3D; &#123;layer_h, table.unpack(previous_state[l + 1])&#125;</span><br><span class="line">  else</span><br><span class="line">    output &#x3D; layer_h</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">print(next_state)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="deep-thinking"><a href="#deep-thinking" class="headerlink" title="deep thinking"></a>deep thinking</h2><p>尽管已经很长了。还是要写理解。这时你可以看图了。</p>
<blockquote>
<p>what information we’re going to throw away from the cell state<br> what new information we’re going to store in the cell state</p>
</blockquote>
<p>1、 什么是forget gate？</p>
<ul>
<li>其实就是将x h线性变换后做一个sigmoid， 如果结果是0，代表forget  c_out(t-1)。</li>
<li>这个例子非常好：<blockquote>
<p>the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p>
</blockquote>
</li>
</ul>
<p>2、 i 和 c_in?</p>
<ul>
<li>两步，第一步i，i = 1相当于是确定哪些值我们需要update或者说需要更新输入的多大成分，想象为将c_in scale了i倍；而tanh相当于为需要更新的值确定了更新成什么c_in。</li>
<li>相乘，则确定了新的候选值，再与f相加，我们便确定了新的状态。</li>
</ul>
<blockquote>
<p>we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.</p>
</blockquote>
<p>3、 那么输出什么？</p>
<ul>
<li>首先我们需要确定哪些更新后的状态需要输出，用sigmoid，得到的o就是我们想要输出的部分。 </li>
<li> 然后 基于更新好的状态c_out(t)，将其tanh控制在[-1,1]之间。乘以o，输出我们要输出的。<blockquote>
<p>since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output <strong>whether the subject is singular or plural</strong>, so that we know what form a verb should be conjugated into if that’s what follows next.</p>
</blockquote>
</li>
</ul>
<p>4、 各类变种<br> <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf">Gers &amp; Schmidhuber (2000)</a><br><a href="http://arxiv.org/pdf/1406.1078v3.pdf">Cho, et al. (2014)</a><br><a href="http://arxiv.org/pdf/1508.03790v2.pdf">Yao, et al. (2015)</a><br><a href="http://arxiv.org/pdf/1402.3511v1.pdf">Koutnik, et al. (2014)</a></p>
<p>5、 比对各类变种的结论<br><a href="http://arxiv.org/pdf/1503.04069.pdf">Greff, et al. (2015)</a><br><a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz, et al. (2015)</a></p>
<p>欢迎补充材料。<br>reference:<br><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>News投资逻辑摘抄</title>
    <url>/2021/05/12/News%E6%8A%95%E8%B5%84%E9%80%BB%E8%BE%91%E6%91%98%E6%8A%84/</url>
    <content><![CDATA[<br>
<br>



<h1 id="David-Swensen："><a href="#David-Swensen：" class="headerlink" title="David Swensen："></a>David Swensen：</h1><span id="more"></span>

<p>1、Led by Tobin’s ideas, he stressed<strong>asset allocation rather than stock picking, or attempts to time the market</strong> — beyond the mechanical market timing that came with his policy of regularly rebalancing Yale’s portfolio. At the margin, that entailed <strong>buying more of assets that had done badly and selling some of those that had done well</strong>.</p>
<p>2、With the public markets deeply liquid and exhaustively researched, there was no point in trying to beat them. But <strong>private markets were less efficient</strong>, and he could reasonably hope to find bargains if his team was smart enough. </p>
<p>3、The first was that equities were indubitably better than bonds or cash for the longer term — and that “equities” should not be restricted merely to shares traded on public stock exchanges but should include <strong>any investment with a non-guaranteed upside for the investor</strong>. His second was that diversification was important. </p>
<p>4、 He regularly attacked excessive fees in his later years. If his team couldn’t find any place where Yale’s long-term horizon might give them a chance to beat the market, he might even have left money in index funds.</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>Report读后感</title>
    <url>/2021/05/09/Report%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<br>

<br>

<h3 id="Financial-Stability-Report读后感"><a href="#Financial-Stability-Report读后感" class="headerlink" title="Financial Stability Report读后感"></a>Financial Stability Report读后感</h3><p>1、Asset Valuations</p>
<p>Size of Selected Asset Markets各类资产价格涨幅</p>
<span id="more"></span>

<p>Yields on Nominal Treasury Securities 2、10年名义国债利率</p>
<p>Term Premium on 10-Year Nominal Treasury Securities 10年国债期限溢价</p>
<p>Implied volatility of 10-Year Swap rate 110互换利率隐含波动率</p>
<p>Treasury market Depth –</p>
<p>1、时限，给定价格和数量，多快成交 </p>
<p>2、深度，给定时间和价格，多大数量</p>
<p> 3、宽度，给定时限和数量，多低价格</p>
<p><strong>多样性</strong>是指两个队伍中的人不是因为同一原因而排在这个队的，否则整个队伍会一起出现或者一起离开，对流动性不利<br><strong>恢复性</strong>是指对如果有一个队伍的人突然被清空，那么重新排成类似的队伍需要的时间，时间越短流动性越好。</p>
<p>Corporate Bond Yields</p>
<p>Corporate Bond Spreads to Similar-maturity Treasury Securities   Source: ICe Data Indices, LL</p>
<p>低评级公司债和可比到期国债利率差价变低。</p>
<p>excess Bond Premium</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow源码解读01</title>
    <url>/2021/04/16/TensorFlow%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB01/</url>
    <content><![CDATA[<br>

<p>架构</p>
<span id="more"></span>



<p>1、在网络通信、硬件上开发变量、基础op和一些函数。在之上有执行器、分布式。然后是api接口和应用。</p>
<p>2、核心为，通信、设备管理、op对数据操作、图计算。</p>
<p>3、core目录包含了核心模块。public和client，一个是api接口头文件、一个是api接口实现。platform为和os相关接口文件。protobuf为数据传输的结构化序列。framework包含log、tensor、memory、registey等。graph，distributed，都好理解。kernels是核心的op，ops为梯度、io等op。lib为公共库，如hash等。stream_executor是并行计算框架。common_runtime有session、threadpool、executor管理等。contrib是contributor贡献。gpus封装了cuda cudnn编程库。</p>
<p>4、tensor相关操作，slice add reshape reduce shuffle 等。<code>tensorbuffer</code>指针，指向Eigen::Tensor</p>
<p>5、</p>
<p>Python所构建好的graph模型，会在底下悄悄地生成一个由GraphDef表示的图结构来。然后我们使用Python等语言里的Session具体去分配内存，初使化参数，运行计算图时，TF的后端会将我们前一步所构建的GraphDef转化为一个可执行的Graph。</p>
<p>构建scope，即new graph，包含着op registry。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//static OpRegistry* global_op_registry = new OpRegistry; graph里包含 registry</span></span><br><span class="line">Graph* graph = <span class="keyword">new</span> Graph(OpRegistry::Global());</span><br><span class="line"></span><br><span class="line"><span class="comment">// ShapeRefiner performs shape inference for TensorFlow Graphs.  It is</span></span><br><span class="line"><span class="comment">// responsible for instantiating InferenceContext objects for each</span></span><br><span class="line"><span class="comment">// Node in the Graph, and providing/storing the &#x27;input_tensor&#x27; Tensors</span></span><br><span class="line"><span class="comment">// used by Shape Inference functions, when available at graph</span></span><br><span class="line"><span class="comment">// construction time.</span></span><br><span class="line">ShapeRefiner* refiner =</span><br><span class="line">      <span class="keyword">new</span> ShapeRefiner(graph-&gt;versions(), graph-&gt;op_registry());</span><br><span class="line">  <span class="keyword">return</span> Scope(<span class="keyword">new</span> Impl(graph, <span class="keyword">new</span> Status, <span class="keyword">new</span> Impl::NameMap, refiner,</span><br><span class="line">                        <span class="comment">/* disable_shape_inference */</span> <span class="literal">false</span>));</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>tensorflow源码</category>
      </categories>
      <tags>
        <tag>tensorflow源码</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow源码解读02</title>
    <url>/2021/04/16/TensorFlow%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB02/</url>
    <content><![CDATA[<br>

<p>Tensor、op相关</p>
<span id="more"></span>



<p>从api开始挖。然后再上升到框架。然后再topdown地整合。</p>
<p>友好小白。</p>
<br>

<p>1、Tensor\TensorShape</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建tensor - constructor包含TensorShape</span></span><br><span class="line"><span class="comment">//TensorShape继承自Base ，constructor传入一个initializer_list</span></span><br><span class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">TensorShapeBase</span><span class="params">(gtl::ArraySlice&lt;int64&gt; dim_sizes)</span></span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<br>

<p>2、DeepCopy</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//主要两个步骤 ，在tensor_utils中</span></span><br><span class="line"><span class="comment">//1 数据的memcpy -- 这里用到了string_view，use StringPiece as a convenient map over the tensor buffer</span></span><br><span class="line"><span class="keyword">using</span> StringPiece = absl::string_view;</span><br><span class="line">StringPiece(<span class="keyword">static_cast</span>&lt;<span class="keyword">char</span>*&gt;(buf_-&gt;data()), TotalBytes());</span><br><span class="line"><span class="built_in">memcpy</span>(<span class="keyword">const_cast</span>&lt;<span class="keyword">char</span>*&gt;(output_data.data()), input_data.data(), input_data.size());</span><br><span class="line"></span><br><span class="line"><span class="comment">//2 类型 和 维度</span></span><br><span class="line">output-&gt;unaligned_flat&lt;Variant&gt;() =input.unaligned_flat&lt;Variant&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//这里unaligned_flat，为unaligned_shaped&lt;T, 1&gt;(&#123;NumElements()&#125;);</span></span><br><span class="line"><span class="comment">//主要检查type ，用span来填充维度：FillDimsAndValidateCompatibleShape，添入Eigen的dims并返回Eigen的tensor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> ArraySlice = absl::Span&lt;<span class="keyword">const</span> T&gt;;</span><br><span class="line"><span class="keyword">typedef</span> Eigen::TensorMap&lt;Eigen::Tensor&lt;T, NDIMS, Eigen::RowMajor, IndexType&gt; &gt;</span><br><span class="line">      UnalignedTensor;</span><br></pre></td></tr></table></figure>

<p>类似的操作如 slice concat split等不再赘述。</p>
<br>

<p>3、gtest</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">   <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values_to_test = LoadValuesFromConfig();</span><br><span class="line">   RegisterMyTests(values_to_test);</span><br><span class="line">   ...</span><br><span class="line">   <span class="keyword">return</span> RUN_ALL_TESTS();</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//写法如</span></span><br><span class="line">TEST(TensorProtoUtil, CompressTensorProtoInPlaceTooSmall) &#123;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> kLength = <span class="number">63</span>;</span><br><span class="line">  TensorProto tensor_proto =</span><br><span class="line">      tensor::CreateTensorProto(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;(kLength), &#123;kLength&#125;);</span><br><span class="line">  EXPECT_FALSE(tensor::CompressTensorProtoInPlace(&amp;tensor_proto));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//CreateTensorProto的测试</span></span><br><span class="line"><span class="keyword">auto</span> proto = tensor::CreateTensorProto(values, shape);</span><br><span class="line">EXPECT_EQ(proto.DebugString(),</span><br><span class="line">          <span class="string">&quot;dtype: DT_STRING\n&quot;</span></span><br><span class="line">          <span class="string">&quot;tensor_shape &#123;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  dim &#123;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;    size: 1\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  &#125;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  dim &#123;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;    size: 3\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  &#125;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;&#125;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;string_val: \&quot;a\&quot;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;string_val: \&quot;b\&quot;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;string_val: \&quot;c\&quot;\n&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>tensorflow 的test非常多，都是基于gtest框架。</p>
<br>

<p>4、CreateTensorProto</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//同样两个部分，shape和value进行create。tensor_util.h中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//shape部分是protobuf相关操作</span></span><br><span class="line">internal::SetTensorProtoShape(shape, &amp;tensor_shape_proto);</span><br><span class="line"><span class="comment">//set的方式主要是：循环获得shapeproto中的RepeatedPtrField&lt; ::tensorflow::TensorShapeProto_Dim &gt;</span></span><br><span class="line"><span class="comment">// RepeatedPtrField is like RepeatedField, but used for repeated strings or Messages.</span></span><br><span class="line"><span class="comment">//获得后，再调用其Add方法，将shape的每一维 生成一个element并且set值</span></span><br><span class="line"><span class="function">Element* <span class="title">Add</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//value用internal::TensorProtoHelper&lt;Type&gt; 中的AddValues这个static方法，把value中的值一个个加进proto</span></span><br><span class="line">tensor.set_dtype(TypeHelper::GetDataType());</span><br><span class="line">tensor.mutable_tensor_shape()-&gt;Swap(&amp;tensor_shape_proto);</span><br><span class="line">TypeHelper::AddValues(values.begin(), values.end(), &amp;tensor);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>还有诸如CompressTensorProtoInPlace（主要是CompressRepeatedField，核心两个阈值<code> static const int64 kDefaultMinNumElements = 64;static const float kDefaultMinCompressionRatio = 2.0f;</code>）。</p>
<br>

<p>5、protobuf</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//tensor、op、graph等的proto 涉及到很多protobuf操作，不赘述</span></span><br><span class="line"><span class="comment">//比如Arena中</span></span><br><span class="line"><span class="keyword">auto</span>* p = CreateMaybeMessage&lt;::tensorflow::TensorShapeProto&gt;(GetArenaNoVirtual());</span><br></pre></td></tr></table></figure>

<br>

<p>6、OpRegistry</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// The standard implementation of OpRegistryInterface, along with a</span></span><br><span class="line"><span class="comment">// global singleton used for registering ops via the REGISTER</span></span><br><span class="line"><span class="comment">// macros below.  Thread-safe.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Example registration:</span></span><br><span class="line"><span class="comment">//   OpRegistry::Global()-&gt;Register(</span></span><br><span class="line"><span class="comment">//     [](OpRegistrationData* op_reg_data)-&gt;Status &#123;</span></span><br><span class="line"><span class="comment">//       // Populate *op_reg_data here.</span></span><br><span class="line"><span class="comment">//       return Status::OK();</span></span><br><span class="line"><span class="comment">//   &#125;);</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>









<br>
总结：
1、A `Span<T>` is somewhat analogous to an `absl::string_view`, but for an array of elements of type `T`. A user of `Span` must ensure that the data being pointed to outlives the `Span` itself.

<p>2、string_view </p>
<p>3、# ##  </p>
<p>4、gtest</p>
<p>5、initializer_list</p>
</T>]]></content>
      <categories>
        <category>tensorflow源码</category>
      </categories>
      <tags>
        <tag>tensorflow源码</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow源码解读03</title>
    <url>/2021/05/02/TensorFlow%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB03/</url>
    <content><![CDATA[<p>1、</p>
]]></content>
  </entry>
  <entry>
    <title>batchnorm</title>
    <url>/2021/04/05/batchnorm/</url>
    <content><![CDATA[<br>

<p>bn相关问题</p>
<span id="more"></span>
<ul>
<li><p>白化</p>
<ul>
<li><p>目的</p>
<ul>
<li><p>1）去除特征之间的相关性 —&gt; 独立；</p>
</li>
<li><p>2）使得所有特征具有相同的均值和方差 —&gt; 同分布。</p>
</li>
</ul>
</li>
<li><p>PCA白化保证了所有特征分布均值为0，方差为1</p>
</li>
<li><p>ZCA白化则保证了所有特征分布均值为0，方差相同；</p>
</li>
<li><p>白化操作，固定了每一层网络输入分布，加速网络训练过程的收敛</p>
</li>
</ul>
</li>
<li><p>Internal Covariate Shift</p>
<ul>
<li><p>内容</p>
<ul>
<li><p>covariate shift 就是分布不一致假设之下的一个分支问题<br>它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同</p>
</li>
<li><p>对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大</p>
</li>
<li><p>可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由</p>
</li>
</ul>
</li>
<li><p>问题</p>
<ul>
<li><p>简而言之，每个神经元的输入数据不再是“独立同分布”。</p>
</li>
<li><p>学习速度、饱和区(早停)、影响其他层</p>
</li>
</ul>
</li>
<li><p>解决</p>
<ul>
<li>bn框架<ul>
<li>h = f ( g * {(x-μ)/θ} + b)</li>
</ul>
</li>
<li>经过这么的变回来再变过去，会不会跟没变一样<ul>
<li>再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数</li>
<li>但是新参数有不同的学习动态。</li>
<li>在旧参数中，x的均值取决于下层神经网络的复杂关联；<br>但在新参数中， 仅由 b 来确定，去除了与下层计算的密切耦合。</li>
</ul>
</li>
<li>这样的 Normalization 与标准的白化区别<ul>
<li>标准白化操作的目的是“独立同分布”</li>
<li>变换为均值为 b  、方差为 g^2 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>BN</p>
<ul>
<li><p>分类</p>
<ul>
<li>Batch Normalization<ul>
<li>每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的</li>
<li>分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性</li>
<li>但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。</li>
</ul>
</li>
<li>Layer Normalization </li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>防止模型梯度爆炸</li>
<li>为什么加速收敛<ul>
<li>梯度的方向为垂直等高线的方向而走之字形路线，这样会使迭代很 </li>
</ul>
</li>
</ul>
</li>
<li><p>常见的方法有</p>
<ul>
<li>min-max标准化（min-max normalization）</li>
<li>归一化</li>
<li>log函数转换</li>
<li>atan函数转换</li>
<li>z-score标准化（zero-mena normalization，此方法比较常用）- 模糊量化法</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>bean解析及注册源码</title>
    <url>/2021/03/11/bean%E8%A7%A3%E6%9E%90%E5%8F%8A%E6%B3%A8%E5%86%8C%E6%BA%90%E7%A0%81/</url>
    <content><![CDATA[<br>

<p>S1<code>xml文件等资源类 </code> – 对各种资源类的封装+encode</p>
<span id="more"></span>
<p><img src="https://upload-images.jianshu.io/upload_images/8716089-f03da4d523f26bbb.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="resource.jpg"></p>
<p>S2<code>读取xml文件</code> – 对xml文件的校验、load、read(这里的read调用BeanDefinitionDocumentReader)<br>主要在<code>XmlBeanDefinitionReader</code>中</p>
<p>S3<code>解析属性(xml中标签)</code> –从xml到Bean<br>实现在<code>DefaultBeanDefinitionDocumentReader</code><br><code>doRegisterBeanDefinitions()</code> 中解析了<code>profile</code>属性，并且其中的<code>parseBeanDefinitions()</code>是解析xml的开始。</p>
<p>根据不同的<code>namespace</code>和<code>nodename</code>，分别不同处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123;</span><br><span class="line">	importBeanDefinitionResource(ele);</span><br><span class="line">&#125;</span><br><span class="line">else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123;</span><br><span class="line">	processAliasRegistration(ele);</span><br><span class="line">&#125;</span><br><span class="line">else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123;</span><br><span class="line">	processBeanDefinition(ele, delegate);</span><br><span class="line">&#125;</span><br><span class="line">else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123;</span><br><span class="line">	&#x2F;&#x2F; recurse</span><br><span class="line">	doRegisterBeanDefinitions(ele);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>处理过程涉及到<code>BeanDefinitionParserDelegate BeanDefinitionHolder BeanDefinitionReaderUtils XmlReaderContext</code></p>
<blockquote>
<p>delegate中对元素(属性)进行解析，结果放入holder中，此时holder已经包含了各种属性。再由Utils中将holder进行注册。最后由context将注册结果通知监听器。</p>
</blockquote>
<p>S4<code>注册</code></p>
]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>c++tips上</title>
    <url>/2021/04/11/c-tips/</url>
    <content><![CDATA[<br>
<br>


<p>基础汇总</p>
<span id="more"></span>

<br>

<p>c++总的思想角度是，告诉编译器要干嘛。以及内存上的管理。</p>
<p>相关关键词都在指示，将内存如何安排。</p>
<ul>
<li><p>extern</p>
<p> means it is only a declaration, its definition is later or external to this file, asking the compiler not to assign memory or generate code</p>
</li>
</ul>
<ul>
<li><p>内存对齐</p>
</li>
<li></li>
<li><p>构造与析构函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、是否在析构函数抛出异常：</span><br><span class="line">	1异常点之后的程序如果有释放资源，会造成诸如资源泄漏</span><br><span class="line">	2通常异常发生时，c++的机制会调用已经构造对象的析构函数来释放资源，此时若析构函数本身也抛出异常，则前一个异常尚未处理，又有新的异常，会造成程序崩溃的问题。</span><br><span class="line">	3把异常完全封装在析构函数内部，决不让异常抛出函数之外</span><br><span class="line">	</span><br><span class="line">2、构造函数是否可以抛出异常：</span><br><span class="line">	1构造函数可以抛出异常。构造函数中尽量不要抛出异常。</span><br><span class="line">	2既需要分配内存，又需要抛出异常时要特别注意防止内存泄露的情况发生。因为在构造函数中抛出异常，在概念上将被视为该对象没有被成功构造，因此当前对象的析构函数就不会被调用，就会造成内存泄漏。</span><br><span class="line">	3同时，由于构造函数本身也是一个函数，在函数体内抛出异常将导致当前函数运行结束，并释放已经构造的成员对象，包括其基类的成员，即执行直接基类和成员对象的析构函数</span><br><span class="line">	</span><br><span class="line">3、构造函数和析构函数可以调用虚函数吗：</span><br><span class="line">	1当创建某个派生类的对象时，如果在它的基类的构造函数中调用虚函数，那么此时派生类的构造函数并未执行，所调用的函数可能操作还没有被初始化的成员，将导致灾难的发生。</span><br><span class="line">	2即使想在构造函数中实现动态联编，在实现上也会遇到困难。这涉及到对象虚指针（vptr）的建立问题。一个类的构造函数在执行时，并不能保证该函数所能访问到的虚指针就是当前被构造对象最后所拥有的虚指针，因为后面派生类的构造函数会对当前被构造对象的虚指针进行重写，因此无法完成动态联编</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>堆栈</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、堆栈和缓存的区别</span><br><span class="line">	1栈使用的是一级缓存， 他们通常都是被调用时处于存储空间中，调用完毕立即释放；</span><br><span class="line">	2堆是存放在二级缓存中，堆的首地址放在一级缓存缓存中，分配和释放会产生系统调用，由用户态进入内核态，所以速度会慢一些</span><br></pre></td></tr></table></figure></li>
<li><p>安全漏洞、内存越界</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  </span><br></pre></td></tr></table></figure></li>
<li><p>野指针</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">“野指针”不是NULL指针，是指指向“垃圾”内存的指针。即指针指向的内容是不确定的。</span><br><span class="line">产生的原因：</span><br><span class="line">1）指针变量没有初始化。因此，创建指针变量时，该变量要被置为NULL或者指向合法的内存单元。</span><br><span class="line">2）指针p被free之后，没有置为NULL，让人误以为p是个合法的指针。</span><br><span class="line">3）指针跨越合法范围操作。不要返回指向栈内存(非静态局部变量）的指针或引用。</span><br><span class="line"></span><br><span class="line">可能后果：</span><br><span class="line">若操作系统将这部分已经释放的内存重新分配给另外一个进程，而原来的程序重新引用现在的迷途指针，向其中写入数据，则这部分程序内容将被破坏，而导致程序错误。这种类型的程序错误，通常会导致segment fault和一般的保护错误。</span><br><span class="line">其他常见错误：</span><br><span class="line">返回一个基于栈分配的局部变量的地址时，一旦调用的函数返回，分配给这些变量的空间将回收，此时它们拥有的是垃圾值，如return &amp;num，如果要使它的生命周期加长，应该将其声明为static</span><br></pre></td></tr></table></figure></li>
<li><p>STL容器及常见算法</p>
</li>
</ul>
<blockquote>
<p><a href="http://vernlium.github.io/2019/12/29/C-STL%E5%B8%B8%E7%94%A8%E5%AE%B9%E5%99%A8API%E6%80%BB%E7%BB%93/">http://vernlium.github.io/2019/12/29/C-STL%E5%B8%B8%E7%94%A8%E5%AE%B9%E5%99%A8API%E6%80%BB%E7%BB%93/</a></p>
<p><a href="https://blog.csdn.net/weixin_43150428/article/details/82469933">https://blog.csdn.net/weixin_43150428/article/details/82469933</a></p>
</blockquote>
<ul>
<li><p>tuple</p>
</li>
<li><p>array container</p>
</li>
<li><p>range-base for</p>
</li>
<li><p>initializer lists</p>
</li>
<li><p>delegate/inheriting constructors</p>
</li>
<li><p>nullptr</p>
</li>
<li><p>inline</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、内联展开避免函数中断调用开销</span><br><span class="line">2、宏：内联函数在运行时可调试，而宏定义不可以；编译器会对内联函数的参数类型做安全检查或自动类型转换（同普通函数），而宏定义则不会；内联函数可以访问类的成员变量，宏定义则不能；在类中声明同时定义的成员函数，自动转化为内联函数</span><br><span class="line"></span><br><span class="line">inline一般只用于如下情况：</span><br><span class="line">一个函数不断被重复调用函数只有简单的几行，且函数不包含for、while、switch语句，递归。</span><br></pre></td></tr></table></figure></li>
<li><p>unordered_table</p>
</li>
<li><p>shared_ptr/weak_ptr</p>
</li>
<li><p>regexp</p>
</li>
<li><p>cost of exception handling</p>
</li>
<li><p>type trait</p>
</li>
<li><p>strong exception gaurantee</p>
</li>
<li><p>CRTP</p>
</li>
<li><p>smart pointer</p>
</li>
<li><p>std::function and function pointer</p>
</li>
<li><p>runtime cost of lambda function</p>
</li>
<li><p>虚继承</p>
</li>
<li><p>Rvalue reference</p>
</li>
</ul>
<ul>
<li>function/bind 适配器、取反器等</li>
</ul>
<p>call operator被重写了，即()被重写。避免函数调用开销，作为inline。可以加入全局变量，以面向对象的角度考虑。支持泛型。</p>
<p>bind即将某个参数固定为，第一个或第二个操作数。</p>
<p>函数适配器，就像复合函数，将各个类型的函数进行组合。进行一些自定义来能够将自己的函数可以用在适配器里。</p>
<ul>
<li><p>Lambda expression and closure</p>
</li>
<li><p>auto/decltype</p>
</li>
<li><p>static_assert</p>
</li>
<li><p>可变模板参数</p>
</li>
<li><p>指针和引用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、指针可以是null，是对象</span><br><span class="line">2、引用必须初始化，不是对象，且不能重新赋值</span><br><span class="line"></span><br><span class="line">Using Reference when you find </span><br><span class="line">1、It always represents a non-null object</span><br><span class="line">2、And it will not represent any other objects</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>全局作用域</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">::</span><br></pre></td></tr></table></figure></li>
<li><p>static</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">1、定义一个全局变量有许多缺点，最明显的缺点是破坏了此变量的访问范围（使得在此函数中定义的变量，不仅仅只受此函数控制）。static 关键字则可以很好的解决这个问题。</span><br><span class="line"></span><br><span class="line">2、在 C++ 中，需要一个数据对象为整个类而非某个对象服务,同时又力求不破坏类的封装性,即要求此成员隐藏在类的内部，对外不可见时，可将其定义为静态数据。</span><br><span class="line"></span><br><span class="line">即，需要class享有且隐藏在类内，或者，只受某个函数控制而不是受类控制。</span><br><span class="line"></span><br><span class="line">3、</span><br><span class="line">全局（静态）存储区：分为 DATA 段和 BSS 段。这段数据在程序刚开始运行时就完成初始化。</span><br><span class="line"></span><br><span class="line">DATA 段（全局初始化区）存放初始化的全局变量和静态变量；</span><br><span class="line">BSS 段（全局未初始化区）存放未初始化的全局变量和静态变量。程序执行之前已经为0。</span><br><span class="line"></span><br><span class="line">4、面向过程：</span><br><span class="line">静态全局变量 -- 只能在本文件中访问，不能在其它文件中访问，即便是 extern 外部声明也不可以</span><br><span class="line">静态局部变量 -- 程序运行结束以后才释放</span><br><span class="line">静态函数 -- 只能在本文件中调用，不能被其他文件调用</span><br><span class="line"></span><br><span class="line">面向对象：</span><br><span class="line">静态成员变量</span><br><span class="line">静态成员函数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">static 修饰的变量存放在全局数据区的静态变量区，包括全局静态变量和局部静态变量，都在全局数据区分配内存。初始化的时候自动初始化为 0。</span><br><span class="line">（4）不想被释放的时候，可以使用static修饰。比如修饰函数中存放在栈空间的数组。如果不想让这个数组在函数调用结束释放可以使用 static 修饰。</span><br><span class="line">（5）考虑到数据安全性（当程序想要使用全局变量的时候应该先考虑使用 static）。</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>malloc free 和new delete</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、malloc只分配指定大小的堆内存空间，而new可以根据对象类型分配合适的堆内存空间，当然还可以通过重载operator new 自定义内存分配策略，其次还能够构造对象</span><br><span class="line">2、free释放对应的堆内存空间，delete,先执行对象的析构函数，在释放对象所占空间。</span><br><span class="line">3、malloc与free是C++&#x2F;C 语言的标准库函数，new&#x2F;delete 是C++的运算符。</span><br><span class="line">4、malloc返回类型是void*,使用时需要类型转换，而new在分配时，编译器能够根据对象类型自动计算出大小，返回类型是指向对象类型的指针，其封装了sizeof和类型转换功能，实际上new分为两步，第一步是通过调用operator new函数分配一块合适，原始的，未命名的内存空间，返回类型也是void *,而且operator new可以重载，可以自定义内存分配策略，甚至不做内存分配，甚至分配到非内存设备上，而malloc无能为力，第二步，调用构造函数构造对象，new将调用constructor，而malloc不能；delete将调用destructor，而free不能</span><br><span class="line"></span><br><span class="line">总结：1、大小or分配策略是否可自定义 2、析构和构造函数 3、库函数和运算符(重载) </span><br></pre></td></tr></table></figure></li>
<li><p>const</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、变量 初始化后，不被改变</span><br><span class="line"></span><br><span class="line">2、函数 </span><br><span class="line">1. 修饰返回值 const int func() -- 不能改变返回值</span><br><span class="line">2. 修饰参数 int func(const ) -- 函数体内不能改变参数</span><br><span class="line">3. 修饰成员函数 int func() const -- 函数体内不能改变成员变量的值</span><br><span class="line"></span><br><span class="line">3、指针</span><br><span class="line">1. const int* 、int const*-- 常量指针，pointer指向的变量的值不能变，pointer可指向其他对象</span><br><span class="line">2. int* const -- 指针常量，可以改变指针指向变量的值，pointer不能指向其他对象</span><br><span class="line"></span><br><span class="line">4、对象 const对象只能调用const成员函数，不能调用普通函数</span><br><span class="line"></span><br><span class="line">const int* const p&#x3D;&amp;i;</span><br></pre></td></tr></table></figure></li>
<li><p>多态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、虚函数的类 至少有一个(多继承会有多个)一维的虚函数表叫做虚表(virtual table)，属于类成员，虚表的元素值是虚函数的入口地址，在编译时就已经为其在数据端分配了空间</span><br><span class="line"></span><br><span class="line">2、确定的虚函数对应virtual table中一个固定位置n，n是一个在编译时期就确定的常量，所以，使用vptr加上对应的n，就可以得到对应的函数入口地址。C++采用的这种绝对地址+偏移量的方法调用虚函数，查找速度快执行效率高，时间复杂度为O(1)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/11/c-tips/vptr.jpg" alt="vptr"></p>
</li>
<li><p>c++ 与java .NET</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一，C++的难不仅仅在于其静态结构体系，还有很多源于语言设计上的包袱，比如对C的兼容，比如没有垃圾收集机制，比如对效率的强调，等等。一旦把这些包袱丢掉，设计的难度确实可以大大下降。</span><br><span class="line"></span><br><span class="line">第二，Java和.NET的核心类库是在C++十几年成功和失败的经验教训基础之上，结合COM体系优点设计实现的，自然要好上一大块。事实上，在Java和.NET核心类库的设计中很多地方，体现的是基于接口的设计，和真正的基于对象的设计。有了这两个主角站台，“面向类的设计”不能喧宾夺主，也能发挥一些好的作用。</span><br><span class="line"></span><br><span class="line">第三，Java和.NET中分别对C++最大的问题——缺少对象级别的delegate机制做出了自己的回应，这就大大弥补了原来的问题。</span><br><span class="line"></span><br><span class="line">尽管如此，Java还是沾染上了“面向类设计”的癌症，基础类库里就有很多架床叠屋的设计，而J2EE&#x2F;Java EE当中，这种形而上学的设计也很普遍，所以也引发了好几次轻量化的运动。</span><br><span class="line"></span><br><span class="line">至于.NET，我听陈榕介绍过，在设计.NET的时候，微软内部对于是否允许继承爆发了非常激烈的争论。很多资深高人都强烈反对继承。至于最后引入继承，很大程度上是营销需要压倒了技术理性。尽管如此，由于有COM的基础，又实现了非常彻底的delegate，所以 .NET 的设计水平还是很高的。它的主要问题不在这，在于太急于求胜，更新速度太快，基础不牢。当然，根本问题还是微软没有能够在Web和Mobile领域里占到多大的优势，也就使得.NET没有用武之地。</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>命名规范</p>
</li>
</ul>
<p>指针：p；指针指针：pp；句柄：h；引用：ref；array rg</p>
<p>成员变量：m_ ；全局变量 ：g_  ；静态变量：s_  ；类class：C ；接口：I</p>
<p>i fl ui ch ul(ULONG) dw fn w(USHORT, SHORT, WORD) b v d f(BOOL)  hr(HRESULT)</p>
<ul>
<li><p>c++ 与 c</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、C面向过程，C++面向对象--Support data abstraction</span><br><span class="line">2、Protect legacy codes </span><br><span class="line">3、More safe</span><br><span class="line">4、c++支持面向对象、基于对象、基于过程、泛型编程</span><br></pre></td></tr></table></figure></li>
</ul>
<p>reference：<br><a href="https://blog.csdn.net/myan/article/details/5928531">https://blog.csdn.net/myan/article/details/5928531</a></p>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>c++感受</title>
    <url>/2021/06/19/c-%E6%84%9F%E5%8F%97/</url>
    <content><![CDATA[<br>

<br>

<h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><span id="more"></span>

<br>

<p>1、面向过程、面向对象、泛型编程。</p>
<p>过程是topdown思维，像递归，或者动态规划里的转移方程，将问题拆解成子问题，直到可以解决。而问题规模是从大问题到子问题的唯一区别，那么通过递归解决问题规模上的转化。</p>
<p>过程里，有 模块 对应为class 、 调用 对应为message–通信 、records 对应为 objects 、 procedure对应着成员函数。关于数据，过程编程里的数据，直到需要才有，而对象编程里，数据是已经初始化，存放在内存。 </p>
<p>对象则是需要将系统拆分考虑。对象自身，也就需要考虑构造、析构、new&amp;delete、this、成员变量、成员函数，以及一些operator、重载和重写。访问级别、static、以及const。别名 – typedef。对象之间的关系，则需要考虑继承多态、以及关系，是is 还是 has。</p>
<p>有意思的是，function object。其实也只是正常的class，重载operator ()，可以传入到泛型函数里。函数相关还有成员函数指针，也算语法糖。而引用和指针、深浅拷贝的问题，也都是语法层面的问题，本质从内存管理上去考虑就很容易理解。</p>
<p>函数有几种，一种是只有一个实现，一种是多种实现方式，但是input不变，另一种是input的类型也在变化。对应就可以通过默认参数、重写以及template模板，来实现。</p>
<p>2、</p>
]]></content>
      <categories>
        <category>c++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>cmake</title>
    <url>/2021/04/13/cmake/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>cuda编程</title>
    <url>/2021/04/12/cuda%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<br>

<p>cuda c的hello world</p>
<span id="more"></span>

<br> 

<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">*hello_world.cu</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">hello_world</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;GPU: Hello world!\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;CPU: Hello world!\n&quot;</span>);</span><br><span class="line">  hello_world&lt;&lt;&lt;<span class="number">1</span>,<span class="number">10</span>&gt;&gt;&gt;();</span><br><span class="line">  cudaDeviceReset();<span class="comment">//if no this line ,it can not output hello world from gpu</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<h4 id="异构计算"><a href="#异构计算" class="headerlink" title="异构计算"></a>异构计算</h4><p>1、一台 intel i7-4790 CPU加上两台Titan x GPU构成的工作站，GPU插在主板的PCIe卡口上，运行程序的时候，CPU像是一个控制者，指挥两台Titan完成工作后进行汇总，和下一步工作安排，所以CPU我们可以把它看做一个指挥者，主机端，host，而完成大量计算的GPU是我们的计算设备，device。</p>
<p>2、一个四核CPU一般有四个ALU，ALU是完成逻辑计算的核心，也是我们平时说四核八核的核，控制单元，缓存也在片上，DRAM是内存，一般不在片上，CPU通过总线访问内存。</p>
<p>3、GPU，排列成行的一组ALU 公用一个Control单元和Cache，这个部分相当于一个完整的多核CPU。但是不同的是ALU多了，control部分变小，可见计算能力提升了，控制能力减弱了。</p>
<p>4、CPU和GPU之间通过PCIe总线连接，用于传递指令和数据，这部分也是后面要讨论的性能瓶颈之一。</p>
<p>5、低并行逻辑复杂的程序适合用CPU；高并行逻辑简单的大数据计算适合GPU</p>
<h4 id="衡量GPU计算能力"><a href="#衡量GPU计算能力" class="headerlink" title="衡量GPU计算能力"></a><br>衡量GPU计算能力</h4><p>容量：CUDA核心数量（越多越好）；内存大小（越大越好）</p>
<p>性能：峰值计算能力；内存带宽</p>
<br>

<h4 id="CPU和GPU线程的区别："><a href="#CPU和GPU线程的区别：" class="headerlink" title="CPU和GPU线程的区别："></a>CPU和GPU线程的区别：</h4><p>1、CPU线程是重量级实体，操作系统交替执行线程，线程上下文切换花销很大</p>
<p>2、GPU线程是轻量级的，GPU应用一般包含成千上万的线程，多数在排队状态，线程之间切换基本没有开销。</p>
<p>3、CPU的核被设计用来尽可能减少一个或两个线程运行时间的延迟，而GPU核则是大量线程，最大幅度提高吞吐量</p>
<br>

<h4 id="cuda-c"><a href="#cuda-c" class="headerlink" title="cuda c"></a>cuda c</h4><p>1、CUDA C 是标准ANSI C语言的扩展，扩展出一些语法和关键字来编写设备端代码，而且CUDA库本身提供了大量API来操作设备完成计算。</p>
<p>2、驱动API是低级的API，使用相对困难，运行时API是高级API使用简单，其实现基于驱动API。<br>这两种API是互斥的，也就是你只能用一个，两者之间的函数不可以混合调用，只能用其中的一个库。</p>
<p>3、一个CUDA应用通常可以分解为两部分</p>
<p>CPU 主机端代码和GPU 设备端代码</p>
<p>4、nvcc 是从LLVM开源编译系统为基础开发的。CUDA nvcc编译器会自动分离你代码里面的不同部分，如图中主机代码用C写成，使用本地的C语言编译器编译，设备端代码，也就是核函数，用CUDA C编写，通过nvcc编译，链接阶段，在内核程序调用或者明显的GPU设备操作时，添加运行时库。</p>
<h4 id="cuda程序步骤"><a href="#cuda程序步骤" class="headerlink" title="cuda程序步骤"></a><br>cuda程序步骤</h4><ol>
<li>分配GPU内存</li>
<li>拷贝内存到设备</li>
<li>调用CUDA内核函数来执行计算</li>
<li>把计算完成数据拷贝回主机端</li>
<li>内存销毁</li>
</ol>
<br>

<h4 id="CUDA抽象了硬件实现"><a href="#CUDA抽象了硬件实现" class="headerlink" title="CUDA抽象了硬件实现"></a>CUDA抽象了硬件实现</h4><ol>
<li>线程组的层次结构</li>
<li>内存的层次结构</li>
<li>障碍同步</li>
</ol>
<br>

<h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul>
<li>Nvidia Nsight集成开发环境</li>
<li>CUDA-GDB 命令行调试器</li>
<li>性能分析可视化工具</li>
<li>CUDA-MEMCHECK工具</li>
<li>GPU设备管理工具</li>
</ul>
]]></content>
      <categories>
        <category>gpu</category>
      </categories>
      <tags>
        <tag>gpu</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>cuda编程模型</title>
    <url>/2021/04/12/cuda%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>gpu</category>
      </categories>
      <tags>
        <tag>gpu</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>debug训诫</title>
    <url>/2021/05/26/debug%E8%AE%AD%E8%AF%AB/</url>
    <content><![CDATA[<br>

<br>

<p>1、感觉很难，错误都在细节。</p>
<span id="more"></span>

<p>2、error好好看。</p>
<p>3、并不是难。而是繁复。改动处有很多。但少量。</p>
<p>4、改了一处，考虑是否对其他地方有影响。等到发现问题，又难以和之前的改动联系起来。导致锁定时间不能更快。</p>
<p>5、除了业务逻辑，还有 nil 数组越界 除以0 之类的小错误</p>
<p>6、还有你用的接口，虽然是别人提供的，但未必正确 – check功能即可。mock数据。即业务数据是否准确。</p>
<p>7、能配置的 是否都尽量配置化了。方便线上出问题不用发版。</p>
<p>8、日志和性能监控的，时间打点</p>
<p>9、kibana用法：性能上、error上</p>
<p>10、更多时间消耗在：定位，不仔细地排查到，而是根据感觉、设想找到错误的地方然后改动，结果又需要进一步排查。这个使得时间成倍地被消耗。</p>
]]></content>
      <categories>
        <category>训诫</category>
      </categories>
      <tags>
        <tag>训诫</tag>
      </tags>
  </entry>
  <entry>
    <title>dubbo-spi</title>
    <url>/2021/01/31/dubbo-spi/</url>
    <content><![CDATA[<h2 id="spi"><a href="#spi" class="headerlink" title="spi"></a>spi</h2><p><img src="/2021/01/31/dubbo-spi/jdbc.png" alt="jdbc实现"></p>
<p><strong>SPI 的缺点</strong><br>JDK 标准的 SPI 会一次性加载实例化扩展点的所有实现，JDK 启动的时候会一次性全部加载。<br>1如果有的扩展点实现初始化很耗时或者如果有些实现类并没有用到， 会很浪费资源。<br>2如果扩展点加载失败，会导致调用方报错，而且这个错误很难定位到。</p>
<span id="more"></span>

<p><strong>dubbo spi</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Protocol  p &#x3D; ExtensionLoader.getExtensionLoader(xxx.class).getAdaptiveExtension();</span><br><span class="line">ExtensionLoader.getExtensionLoader(xxx.class).getExtension(name);</span><br><span class="line">ExtensionLoader.getExtensionLoader(xxx.class).getActivateExtension(url, key);</span><br></pre></td></tr></table></figure>
<p><code>protocol</code>会在运行的时候判断一下应该选用这个Protocol接口的哪个实现类来实例化对象。<br>动态的根据配置去找到对应的实现类。如果你没有配置，那就走默认的实现类。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@SPI(&quot;dubbo&quot;)  </span><br><span class="line">public interface Protocol &#123;  </span><br><span class="line">      </span><br><span class="line">    int getDefaultPort();  </span><br><span class="line">  </span><br><span class="line">    @Adaptive  </span><br><span class="line">    &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException;  </span><br><span class="line">  </span><br><span class="line">    @Adaptive  </span><br><span class="line">    &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException;  </span><br><span class="line"></span><br><span class="line">    void destroy();  </span><br><span class="line">  </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">dubbo&#x3D;org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol</span><br></pre></td></tr></table></figure>
<p><code>@SPI(“dubbo”)</code>：通过 SPI 机制来提供实现类，实现类是通过 dubbo 作为默认 key 去配置文件里找到的，配置文件名称与接口全限定名一样的，通过 dubbo 作为 key 可以找到默认的实现类就是 org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol</p>
<p><code>@Adaptive</code>：如果想要动态替换掉默认的实现类，需要使用 @Adaptive 。表示动态代理实现。在运行的时候会针对 Protocol 生成<code>代理类</code>，这个代理类的那俩方法里面会有<code>代理代码</code>，代理代码会在运行的时候动态根据 url 中的 protocol 来获取那个 key，默认是 dubbo，自己指定则获取相应的实现。</p>
<h2 id="扩展dubbo组件"><a href="#扩展dubbo组件" class="headerlink" title="扩展dubbo组件"></a>扩展dubbo组件</h2><p><strong>step1</strong><br><img src="/2021/01/31/dubbo-spi/implement.png" alt="自定义方法"></p>
<p><strong>step2</strong><br><img src="/2021/01/31/dubbo-spi/properties.png" alt="添加配置"></p>
<p><strong>step3</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class App </span><br><span class="line">&#123;</span><br><span class="line">    public static void main( String[] args )</span><br><span class="line">    &#123;</span><br><span class="line">&#x2F;&#x2F;        调用方代码</span><br><span class="line">    	Protocol protocol &#x3D; ExtensionLoader.getExtensionLoader(Protocol.class).getExtension(&quot;myProtocol&quot;);</span><br><span class="line">    	System.out.println(protocol.getDefaultPort());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="实现原理探析"><a href="#实现原理探析" class="headerlink" title="实现原理探析"></a>实现原理探析</h2><p>getExtension方法获取一个SPI接口的扩展类实例的流程: 分为解析配置文件、加载并缓存扩展类、创建并加工(属性注入与层层包装)扩展类实例 几个步骤。</p>
<p>通过getAdaptiveExtension方法的流程可以发现，要想获得一个SPI接口的自适应扩展类实例，有2种方式：<br>1在SPI接口的配置文件中配置具有@Adaptive注解的扩展类，在执行解析SPI接口配置文件方法getExtensionClasses时，它会调用loadClass方法，该方法判断扩展类是否具有@Adaptive注解，如果有，则将该类Class缓存到ExtensionLoader的字段“cachedAdaptiveClass”中，然后直接实例化该Class的实例并进行自动装配；<br>2如果未配置@Adaptive修饰的扩展类，则Dubbo会使用字节码技术创建一个自适应扩展类，前提是SPI接口上至少有一个被@Adaptive注解的方法；</p>
<p>todo</p>
]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title>dssm</title>
    <url>/2021/03/08/dssm/</url>
    <content><![CDATA[<h4 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h4><ul>
<li><p>任务<br>用来预测两个句子的语义相似度，又可以获得某句子的低维语义Embedding向量。</p>
<span id="more"></span></li>
<li><p>场景<br>DSSM 模型的最大特点就是 Query 和 Document 是两个独立的子网络，后来这一特色被移植到推荐算法的召回环节，即对用户端（User）和物品端（Item）分别构建独立的子网络塔式结构。<br>两个子网络产生的 Embedding 向量可以独自获取及缓存。</p>
</li>
</ul>
<p>当模型训练完成时，物品的 Embedding 是可以保存成词表的，线上应用的时候只需要查找对应的 Embedding 即可。因此线上只需要计算 （用户，上下文） 一侧的 Embedding，基于 Annoy 或 Faiss 技术索引得到用户偏好的候选集。</p>
<ul>
<li>word hashing<br>word hashing方法是用来减少输入向量的维度，该方法基于字母的n-gram。给定一个单词（good），我们首先增加词的开始和结束部分（#good#），然后将该词转换为字母 [公式] -gram的形式（假设为trigrams：#go，goo，ood，od#）。最后该词使用字母 n-gram的向量来表示。</li>
</ul>
<p>这种方法的问题在于有可能造成冲突，因为两个不同的词可能有相同的n-gram向量来表示。与原始的ont-hot向量表示的词典大小相比，word hashing明显降低了向量表示的维度。</p>
<ul>
<li>优点</li>
</ul>
<p>1、解决了LSA、LDA、Autoencoder等方法存在的一个最大的问题：字典爆炸（导致计算复杂度非常高），因为在英文单词中，词的数量可能是没有限制的，但是字母n-gram的数量通常是有限的.<br>2、基于词的特征表示比较难处理新词，字母的 n-gram可以有效表示，鲁棒性较强<br>3、使用有监督方法，优化语义embedding的映射问题<br>4、省去了人工的特征工程</p>
<ul>
<li>缺点</li>
</ul>
<p>1、word hashing可能造成冲突<br>2、DSSM采用了词袋模型，损失了上下文信息<br>3、在排序中，搜索引擎的排序由多种因素决定，由于用户点击时doc的排名越靠前，点击的概率就越大，如果仅仅用点击来判断是否为正负样本，噪声比较大，难以收敛<br>4、对于中文而言，处理方式与英文有很多不一样的地方。中文往往需要进行分词，但是我们可以仿照英文的处理方式，将中文的最小粒度看作是单字（在某些文献里看到过用偏旁部首，笔画，拼音等方法）</p>
<ul>
<li><p>扩展<br>对DSSM的优化出现了很多的变种，有CNN-DSSM，LSTM-DSSM，MV-DSSM等。</p>
</li>
<li><p>trick<br><img src="/2021/03/08/dssm/trick.png" alt="trick"></p>
</li>
</ul>
<hr>
<ul>
<li>架构<br><img src="/2021/03/08/dssm/dssm.png" alt="dssm"></li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>dubbo</title>
    <url>/2021/04/05/dubbo/</url>
    <content><![CDATA[<p>dubbo–rpc框架相关</p>
<span id="more"></span>

<ul>
<li><p>分层</p>
<ul>
<li>service、config、cluster、 monitor、protocol、exchange、registry、proxy、transport、serialize等</li>
</ul>
</li>
<li><p>网络通信协议、序列化协议</p>
<ul>
<li>dubbo 长连接、nio、高并发、数据量小</li>
<li>hessian </li>
<li>还有其他 json 、java二进制、rmi等</li>
</ul>
</li>
<li><p>负载均衡</p>
<ul>
<li>权重、轮询、自动感知、一致性hash</li>
<li>方法级别配置</li>
</ul>
</li>
<li><p>spi</p>
<ul>
<li>插件扩展</li>
</ul>
</li>
</ul>
<ul>
<li><p>动态代理</p>
<ul>
<li><p>静态代理</p>
<ul>
<li>编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件</li>
<li>代理类中包含具体代理的接口类</li>
</ul>
</li>
<li><p>动态</p>
<ul>
<li><p>可以不需要针对每个目标类都创建一个代理类，静态代理中接口一旦新增加方法，目标对象和代理对象都要进行修改，非常麻烦</p>
</li>
<li><p>jdk动态</p>
<ul>
<li> InvocationHandler 接口和 Proxy 类 (InvocationHandler里面会用到Proxy类)</li>
<li><code>SmsService smsService = (SmsService) JdkProxyFactory.getProxy(new SmsServiceImpl());</code></li>
<li>JDK 动态代理只能只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类</li>
</ul>
</li>
<li><p>cglib动态代理</p>
<ul>
<li>自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法</li>
<li><code>AliSmsService aliSmsService = (AliSmsService) CglibProxyFactory.getProxy(AliSmsService.class);</code></li>
<li> CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/basis/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3.md">代理模式详解.</a></p>
</li>
</ul>
</li>
<li><p>服务治理</p>
<ul>
<li>链路、压力、时长、可用性(成功率)、服务分层(循环依赖)</li>
<li>服务降级<ul>
<li>dubbo中的mock<ul>
<li>类加上Mock后缀，作为降级策略</li>
</ul>
</li>
</ul>
</li>
<li>失败重试、超时重试<ul>
<li>200ms超时，重试3次</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title>flink</title>
    <url>/2021/03/08/flink/</url>
    <content><![CDATA[<br>


<h1 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h1><h2 id="state"><a href="#state" class="headerlink" title="state"></a>state</h2><ul>
<li><p>场景</p>
<ul>
<li>有状态的逻辑是因为数据之间存在关联，单条数据是没有办法把所有的信息给表现出来。<span id="more"></span>
<ul>
<li>去重</li>
<li>窗口计算</li>
<li>机器学习参数</li>
<li>访问历史数据</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么要管理状态</p>
<ul>
<li>内存<ul>
<li>流式作业：24 小时的数据都放到内存，可能会出现内存不足。</li>
</ul>
</li>
<li>高可用<ul>
<li>机器若出现故障或者宕机，需要考虑如何备份及从备份中去恢复，</li>
</ul>
</li>
<li>扩展性<ul>
<li>单节点无法处理全部访问数据，增加几个节点进行横向扩展，这时数据的状态如何平均分配到新增加的节点。</li>
</ul>
</li>
</ul>
</li>
<li><p>方案</p>
<ul>
<li><p>Managed State &amp; Raw State</p>
<ul>
<li>自定义operator用 raw</li>
<li>raw 必须能够转成字节数组</li>
<li>managed，flink自动存储和恢复，并进行内存优化<br>支持已知的数据结构，如 Value、List、Map</li>
</ul>
</li>
<li><p>Managed State</p>
<ul>
<li><p>Keyed State </p>
<ul>
<li><p>每个 Key 对应一个 State</p>
</li>
<li><p>整个程序中没有 keyBy 的过程就没有办法使用KeyedStream。</p>
</li>
<li><p>并发改变时状态重新分配：内置了 2 种分配方式</p>
</li>
<li><p>Keyed State 通过 RuntimeContext 访问，这需要 Operator 是一个 Rich Function。</p>
</li>
<li><p>几种 Keyed State 的差异</p>
<ul>
<li><p>ReducingState 和 AggregatingState 与 ListState 都是同一个父类，但状态数据类型上是单个值</p>
</li>
<li><p> AggregatingState 输入的 IN，输出的是 OUT。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Operator State </p>
<ul>
<li>可以用于所有算子，常用于 Source</li>
<li>一个 Operator 实例对应一个 State</li>
<li>Operator  State 需要自己实现 CheckpointedFunction 或 ListCheckpointed 接口。</li>
<li>支持的数据结构相对较少 </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>使用示例</p>
<ul>
<li><p><a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/StateMachineExample.java">写状态机是如何实现</a></p>
</li>
<li><p>实现的是：首先下订单，订单生成后状态为待付款，当再来一个事件状态付款成功，则事件的状态将会从待付款变为已付款，待发货…</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>状态的保存和恢复</p>
<ul>
<li><p>保存</p>
<ul>
<li>Checkpoint 会定时制作分布式快照</li>
</ul>
</li>
<li><p>恢复</p>
<ul>
<li><p>checkpoint</p>
<ul>
<li>数据源需要支持数据重新发送</li>
<li>两种一致性语义，一种是恰好一次，一种是至少一次</li>
<li>1、把进程或者线程移到 active 的 其他台机器上<br>2、整个作业的所有 Task 都回滚到最后一次成功 Checkpoint 中的状态</li>
</ul>
</li>
<li><p>savepoint</p>
<ul>
<li>手动调整并发，必须要重启作业并会提示 Checkpoint 已经不存在–&gt; 此时savepoint</li>
<li>比较持久，以标准格式存储</li>
<li>允许代码或配置发生改变，恢复需要启动作业手动指定一个路径恢复</li>
</ul>
</li>
</ul>
</li>
<li><p>checkpoint实现</p>
<ul>
<li>运行环境 env.enableCheckpointing 传入间隔时间。越频繁，恢复时追数据就会相对减少，IO 消耗增加。</li>
<li>设置了 Exactly_Once 语义，并且需要 Barries 对齐，这样可以保证消息不会丢失也不会重复。</li>
<li>setMinPauseBetweenCheckpoints 防止 Checkpoint 太过于频繁</li>
<li>setCheckpointTimeout 表示做 Checkpoint 多久超时</li>
<li>setMaxConcurrentCheckpoints</li>
<li>enableExternalizedCheckpoints。默认 Checkpoint 会在整个作业 Cancel 时被删除。Checkpoint 是作业级别的保存点。</li>
</ul>
</li>
<li><p>checkpoint可选的状态存储方式</p>
<ul>
<li><p>MemoryStateBackend</p>
<ul>
<li>构造方法是设置最大的 StateSize，选择是否做异步快照</li>
<li>且需要注意 maxStateSize &lt;= akka.framesize 默认 10 M</li>
<li>Checkpoint 存储在 JobManager 内存中，因此总大小不超过 JobManager 的内存。- 本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。不推荐在生产场景使用。</li>
</ul>
</li>
<li><p>FsStateBackend</p>
<ul>
<li>需要传一个文件路径和是否异步快照</li>
<li>State 依然在 TaskManager 内存中- Checkpoint 存储在外部文件系统（本地或 HDFS）</li>
<li>常规使用状态的作业、例如分钟级窗口聚合或 join、需要开启 HA 的作业。</li>
</ul>
</li>
<li><p>RocksDBStateBackend</p>
<ul>
<li>key/value 的内存存储系统</li>
<li>不支持同步的 Checkpoint</li>
<li>支持增量的 Checkpoint</li>
<li>存储在外部文件系统（本地或 HDFS）</li>
<li>单个 TaskManager 上 State 总量不超过它的内存+磁盘，单 Key 最大 2G，总大小不超过配置的文件系统容量即可</li>
<li>超大状态的作业，例如天级窗口聚合、需要开启 HA 的作业、最好是对状态读写性能要求不高的作业。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h2><ul>
<li>todo</li>
</ul>
<h2 id="watermark"><a href="#watermark" class="headerlink" title="watermark"></a>watermark</h2><ul>
<li>todo</li>
</ul>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><ul>
<li>todo</li>
</ul>
<h2 id="CET"><a href="#CET" class="headerlink" title="CET"></a>CET</h2><ul>
<li>todo</li>
</ul>
<h2 id="概念-amp-角色"><a href="#概念-amp-角色" class="headerlink" title="概念&amp;角色"></a>概念&amp;角色</h2><ul>
<li>TaskManager &amp; slot<ul>
<li>每一个 TaskManager 都是一个JVM进程</li>
<li>每个task slot表示TaskManager拥有资源的一个固定大小的子集</li>
<li>将其管理的内存均分给各个slot</li>
<li>一个TaskManager一个slot时，那么每个task group运行在独立的JVM中</li>
<li>多个slot时，多个subtask可以共同享有一个JVM</li>
<li>在同一个JVM进程中的task将共享TCP连接和心跳消息，也可能共享数据集和数据结构，从而减少每个task的负载。</li>
</ul>
</li>
</ul>
<ul>
<li>todo</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li><a href="https://www.infoq.cn/article/vgkza-s9fmbgabp71pgh">状态管理及容错机制</a></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>流处理</tag>
      </tags>
  </entry>
  <entry>
    <title>elasticsearch</title>
    <url>/2021/04/27/elasticsearch/</url>
    <content><![CDATA[<br>

<h2 id="相关笔记"><a href="#相关笔记" class="headerlink" title="相关笔记"></a>相关笔记</h2><p>1、搜索引擎或者elk(es、logstash、kibana)系统，都是根据内容中的关键字建立倒排索引即反向索引，便于搜索。</p>
<span id="more"></span>



<p>2、 在 Lucene 的基础上进行封装，实现了分布式搜索引擎。</p>
<p>3、es的索引、类型和文档的概念比较重要，类似于 MySQL 中的数据库、表和行。</p>
<p>4、Elasticsearch 也是 Master-slave 架构，也实现了数据的分片和备份。写入index和type时是和master打交道，然后同步到slave，但是写入doc数据，不需要如此。为了提高性能，写数据是采取routing，将写入压力分散。</p>
<p>5、</p>
<p>Reference:</p>
<p><a href="https://zhuanlan.zhihu.com/p/62892586">https://zhuanlan.zhihu.com/p/62892586</a></p>
]]></content>
      <categories>
        <category>es</category>
      </categories>
      <tags>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title>graphsage</title>
    <url>/2021/03/08/graphsage/</url>
    <content><![CDATA[<h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h4><ul>
<li>卷积<br>数学上卷积的两个例子：</li>
</ul>
<p>一个对象（吃冰淇凌）对一个系统（体重）的作用效果满足线性原理、累加原理。该对象对这个系统连续作用了一段时间后，求该系统的状态。这个时候，一个卷积就可以求出来了！</p>
<span id="more"></span>

<br>
第二个例子：
![卷积](卷积.jpg)


<hr>
<p>DL中的卷积：</p>
<p>CNN卷积本质是，共享参数的filter过滤器，像素点加权构成feature map 实现特征提取。<br>a）平滑滤波 b）边缘提取，很容易通过设计特定的“卷积核”，然后将其与像素矩阵的对应元素（不进行旋转）相乘得到。<br>a）就是将中心像素值与周围临近的像素进行平均，自然就能“削峰填谷”，实现平滑处理<br>b) 中心像素复制n份，减去周围n个临近的像素值。相近的则减为0，边缘才被留下。<br>卷积神经网络中“卷积”，是为了提取图像的特征，其实只借鉴了数学卷积中“加权求和”的特点。</p>
<ul>
<li><p>为什么需要GCN<br>CNN LSTM等 对非欧几里得空间数据(eg：社交网络、信息网络等)进行处理上却存在一定的局限性。<br>用GCN：拓扑图中每个node相邻的个数不同，不能用同样大小的filter进行平移提取feature。任何数据在赋范空间内都可以建立拓扑关联，如谱聚类。GCN是区别于CV NLP的任务的模型。</p>
</li>
<li><p>图学习任务<br>1、图节点分类任务：图中每个节点都有对应的特征，当我们已知一些节点的类别的时候，可以设计分类任务针对未知节点进行分类。我们接下来要介绍的 GCN、GraphSAGE、GAT模型都是对图上的节点分类。<br>2、图边结构预测任务：图中的节点和节点之间的边关系可能在输入数据中能够采集到，而有些隐藏的边需要我们挖掘出来，这类任务就是对边的预测任务，也就是对节点和节点之间关系的预测。<br>3、图的分类：对于整个图来说，我们也可以对图分类，图分类又称为图的同构问题，基本思路是将图中节点的特征聚合起来作为图的特征，再进行分类。</p>
</li>
</ul>
<p>如：<br>1、节点分类—反欺诈：因为图中每个节点都拥有自己的特征信息。通过该特征信息，我们可以构建一个风控系统，如果交易节点所关联的用户 IP 和收货地址与用户注册 IP 和注册地址不匹配，那么系统将有可能认为该用户存在欺诈风险。<br>2、边结构预测—商品推荐：图中每个节点都具有结构信息。如果用户频繁购买某种类别商品或对某种类别商品评分较高，那么系统就可以认定该用户对该类商品比较感兴趣，所以就可以向该用户推荐更多该类别的商品。</p>
<ul>
<li>拉普拉斯矩阵<br><img src="/2021/03/08/graphsage/laplacian.png" alt="laplacian"></li>
</ul>
<hr>
<ul>
<li><p>GCN主要贡献<br>这篇文章的主要贡献是为图半监督分类任务设计了一个简单并且效果好的神经网络模型，这个模型由谱图卷积(spectral graph convolution)的一阶近似推导而来，具有理论基础。</p>
</li>
<li><p>GCN学习策略</p>
</li>
</ul>
<p><img src="/2021/03/08/graphsage/GCN%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5.png" alt="GCN学习策略"></p>
<ul>
<li><p>多层图卷积网络(Graph Convolutional Network, GCN)的逐层传播公式<br><img src="/2021/03/08/graphsage/%E9%80%90%E5%B1%82%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F.png" alt="逐层传播公式"></p>
</li>
<li><p>谱图卷积(Spectral Graph Convolutions)</p>
</li>
<li><p>逐层线性模型</p>
</li>
<li><p>半监督学习节点分类</p>
</li>
<li><p>传播公式解释</p>
</li>
</ul>
<p>todo</p>
<h4 id="graphsage"><a href="#graphsage" class="headerlink" title="graphsage"></a>graphsage</h4><p>1、比GCN进步之处<br>GCN的训练方式需要将邻接矩阵和特征矩阵一起放到内存或者显存里，在大规模图数据上是不可取的。其次，GCN在训练时需要知道整个图的结构信息(包括待预测的节点), 这在现实某些任务中也不能实现(比如用今天训练的图模型预测明天的数据，那么明天的节点是拿不到的)。GraphSAGE的出现就是为了解决这样的问题。</p>
<p>GraphSAGE采用了采样的机制，使得图模型可以应用到大规模的图结构数据中，是目前几乎所有工业上图模型的雏形。<br>进一步：每个节点这么多邻居，采样能否考虑到邻居的相对重要性呢，或者我们在聚合计算中能否考虑到邻居的相对重要性? </p>
<p>2、inductive 还是 transductive<br>如果训练时用到了测试集或验证集样本的信息(或者说，测试集和验证集在训练的时候是可见的), 我们把这种学习方式叫做transductive learning, 反之，称为inductive learning. 显然，我们所处理的大多数机器学习问题都是inductive learning, 因为我们刻意的将样本集分为训练/验证/测试，并且训练的时候只用训练样本。然而，在GCN中，训练节点收集邻居信息的时候，用到了测试或者验证样本，所以它是transductive的。</p>
<p>3、简单过程<br>思路一个网络里，我们知道部分点的分类，我们希望通过各种方法，知道其他未知点的属性。解决对未知节点的泛化问题。</p>
<p>GraphSAGE是一个inductive框架，在具体实现中，训练时它仅仅保留训练样本到训练样本的边。inductive learning 的优点是可以利用已知节点的信息为未知节点生成Embedding. GraphSAGE 取自 Graph SAmple and aggreGatE, SAmple指如何对邻居个数进行采样。aggreGatE指拿到邻居的embedding之后如何汇聚这些embedding以更新自己的embedding信息。<br><br></p>
<p><img src="/2021/03/08/graphsage/visual_graphsage.webp" alt="visual_graphsage"></p>
<hr>
<p>4、具体步骤及伪代码</p>
<p>步骤：<br>1.对邻居采样<br>2.采样后的邻居embedding传到节点上来，并使用一个聚合函数聚合这些邻居信息以更新节点的embedding<br>3.根据更新后的embedding预测节点的标签</p>
<p><img src="/2021/03/08/graphsage/graphsage.png" alt="graphsage"></p>
<br>


<p>描述：初始化各个节点emb，对每个节点emb采样邻居的emb，对邻居进行聚合，将自己的emb和聚合后的emb做一个非线性变换，更新为自己的emb。</p>
<p>5、K的解释</p>
<p>K：聚合器数量，权重矩阵数量，层数。</p>
<p><img src="/2021/03/08/graphsage/K%E8%A7%A3%E9%87%8A.jpg" alt="K解释"></p>
<br>

<p>6、采样<br>定长抽样。定义邻居个数，进行有放回的重采样/负采样达到个数。每个node采样个数一致，为了把多个邻居拼成tensor放入gpu批量训练。</p>
<p>7、聚合器<br>平均效果最好。<br>也有用lstm聚合器和pooling聚合器，</p>
<p>8、学习过程<br>有监督：交叉熵<br>无监督：学习出来的相邻node的emb应该尽可能接近。此时的loss如下。<br><img src="/2021/03/08/graphsage/loss.jpg" alt="loss"><br><img src="/2021/03/08/graphsage/loss%E8%A7%A3%E9%87%8A.jpg" alt="loss解释"></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>hive</title>
    <url>/2021/03/03/hive/</url>
    <content><![CDATA[<br>


<ul>
<li><p>组件架构：<br>hiveserver2（beeline）,hive,metadb</p>
<span id="more"></span>
<blockquote>
<p>Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.</p>
</blockquote>
</li>
<li><p>连接hiveserver2  <br>GUI CLI JDBC (beeline)</p>
</li>
<li><p>数据源<br>用kafka，sqoop等获得data，放入hdfs，这些数据各种结构都有。<br>关系数据库的表，MongoDB 或json数据，或日志</p>
</li>
<li><p>执行hql<br>背后运行的是mapreduce or Tez jobs(类似于pig latin脚本执行pig)<br><code>insert into test values(&quot;wangyuq&quot;,&quot;123&quot;);</code><br>查看tracking url</p>
</li>
<li><p>stage<br>将你的数据移到目的位置之前，将会staing 那儿一段时间。staging文件最终丢弃。</p>
</li>
<li><p>比对<br>pig是对非结构化数据处理的好的etl。<br>hive不是关系数据库，只是维护存储在HDFS的数据的metadata，使得对大数据操作就像sql操作表一样，只不过hql和sql稍有出入。使我们能用sql来执行mr。可以对hdfs数据进行query。<br>hive使用metastore存表。hive默认derby但是可自定义更换。</p>
</li>
<li><p>劣<br>hive不能承诺优化，只是简单，因此hive不能支持实时，性能差<br>index view有限制（partition bucket 弥补）<br>和sql 的datatype不完全一样</p>
</li>
<li><p>与hdfs关系<br>hdfs里有hive，data在hdfs上，schema在metastore里。<br>load语句： 将hdfs搬运到hive，hdfs不再有该数据。只是将真正的data转到了hive目录下。</p>
</li>
</ul>
<ul>
<li> Making Multiple Passes over the Same Data<blockquote>
<p>Hive has a special syntax for producing multiple aggregations from a single pass through a source of data, rather than rescanning it for each aggregation. This change can save considerable processing time for large input data sets. </p>
</blockquote>
</li>
</ul>
<p>因此如下方式更加高效,并且可开启并行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM pv_users</span><br><span class="line">    INSERT OVERWRITE TABLE pv_gender_sum</span><br><span class="line">        SELECT pv_users.gender, count_distinct(pv_users.userid)</span><br><span class="line">        GROUP BY pv_users.gender</span><br><span class="line"></span><br><span class="line">    INSERT OVERWRITE DIRECTORY &#39;&#x2F;user&#x2F;data&#x2F;tmp&#x2F;pv_age_sum&#39;</span><br><span class="line">        SELECT pv_users.age, count_distinct(pv_users.userid)</span><br><span class="line">        GROUP BY pv_users.age;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.exec.parallel&#x3D;true;   &#x2F;&#x2F;打开任务并行执行</span><br><span class="line">set hive.exec.parallel.thread.number&#x3D;16; &#x2F;&#x2F;同一个sql允许最大并行度，默认为8。</span><br></pre></td></tr></table></figure>

<ul>
<li><p>日期处理<br>查看N天前的日期：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select from_unixtime(unix_timestamp(&#39;20111102&#39;,&#39;yyyyMMdd&#39;) - N*86400,&#39;yyyyMMdd&#39;) from t_lxw_test1 limit 1;  </span><br></pre></td></tr></table></figure>
<p>获取两个日期之间的天数/秒数/分钟数等等：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select ( unix_timestamp(&#39;2011-11-02&#39;,&#39;yyyy-MM-dd&#39;)-unix_timestamp(&#39;2011-11-01&#39;,&#39;yyyy-MM-dd&#39;) ) &#x2F; 86400  from t_lxw_test limit 1; </span><br></pre></td></tr></table></figure></li>
<li><p>left outer join</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--query 1</span><br><span class="line">select count(id) from  </span><br><span class="line">(select id  from   a  left outer join   b  </span><br><span class="line">on a.id&#x3D;b.id and  b.date&#x3D;&#39;2017-10-27&#39;    </span><br><span class="line">where to_date(a.adate) &gt;&#x3D; &#39;2017-10-27&#39;   and a.date&#x3D;&#39;2017-07-24&#39;  </span><br><span class="line">) a </span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--query 2</span><br><span class="line">select count(id) from  </span><br><span class="line">(select id  from   a  left outer join   b  </span><br><span class="line">on a.id&#x3D;b.id and  b.date&#x3D;&#39;2017-10-27&#39;  and a.date&#x3D;&#39;2017-07-24&#39;  </span><br><span class="line">where to_date(a.adate) &gt;&#x3D; &#39;2017-10-27&#39;  </span><br><span class="line">) a </span><br></pre></td></tr></table></figure>
<p>区别？where 后面跟的是过滤条件，query 1 中的a.date=’2017-07-24’, 在table scan之前就会Partition Pruner 过滤分区，所以只有’2017-07-24’下的数据会和b进行join。<br>而query 2中会读入所有partition下的数据，再和b join，并且根据join的关联条件只有a.date=’2017-07-24’  的时候才会真正执行join，其余情况下又由于是left outer join, 右面会留NULL</p>
</li>
<li><p><a href="http://beadooper.com/?page_id=313">配置文件</a></p>
</li>
<li><p> <a href="http://superlxw1234.iteye.com/blog/1751216">正则</a><br>java中的正则匹配即可:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name rlike &#39;^[\\u4e00-\\u9fa5]+$&#39;</span><br><span class="line">select mobile from phone where mobile rlike &#39;^\\d+$&#39; ;  </span><br></pre></td></tr></table></figure></li>
<li><p><a href="http://superlxw1234.iteye.com/blog/1582880">控制hive任务中的map数和reduce数</a></p>
</li>
<li><p><a href="https://github.com/hbutani/SQLWindowing"> SQLWindowing</a></p>
</li>
<li><p><a href="http://blog.csdn.net/yeweiouyang/article/details/42082663">hdfs目录创建hive表,指定分区</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE if not exists push_log(</span><br><span class="line">     hostid STRING, dayid STRING</span><br><span class="line">     plmn STRING)</span><br><span class="line"> COMMENT &#39; log table&#39;</span><br><span class="line"> PARTITIONED BY (hostid STRING, dayid STRING) </span><br><span class="line"> ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\001&#39;</span><br><span class="line"> STORED AS TEXTFILE</span><br><span class="line"> LOCATION &#39;&#x2F;user&#x2F;data&#x2F;push&#39;;</span><br><span class="line">alter table push_log add partition(hostid&#x3D;&#39;$hostid&#39;, dayid&#x3D;&#39;$dayid&#39;) location &#39;&#x2F;user&#x2F;data&#x2F;push&#x2F;$hostid&#x2F;$dayid&#39;;</span><br></pre></td></tr></table></figure>
<p>testtext 数据<code>wer 46 weree   78 wer 89 rr  89</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table d_part(name string)  partitioned by(value string) row format delimited fields terminated by &#39;\t&#39;  lines terminated by &#39;\n&#39; stored as textfile;</span><br><span class="line"></span><br><span class="line">set hive.exec.dynamic.partition&#x3D;true;</span><br><span class="line">set hive.exec.dynamic.partition.mode&#x3D;nonstrick;</span><br><span class="line"></span><br><span class="line">insert overwrite table d_part partition(value) select name,addr as value from testtext;</span><br><span class="line"></span><br><span class="line">select * from d_part;</span><br><span class="line">show partitions d_part;</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; create table d_part2(</span><br><span class="line">    &gt; name string</span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; partitioned by(value string,dt string)</span><br><span class="line">    &gt; row format delimited fields terminated by &#39;\t&#39; </span><br><span class="line">    &gt; lines terminated by &#39;\n&#39;</span><br><span class="line">    &gt; stored as textfile;</span><br><span class="line">hive&gt; insert overwrite table d_part2 partition(value,dt)</span><br><span class="line">    &gt; select &#39;test&#39; as name,  </span><br><span class="line">    &gt; addr as value,</span><br><span class="line">    &gt; name as dt</span><br><span class="line">    &gt; from testtext;</span><br><span class="line">show partitions d_part2;</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="http://superlxw1234.iteye.com/blog/1568739">hive中转义特殊字符</a></li>
<li>schema tool<br><a href="https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive_schema_tool.html">https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive_schema_tool.html</a></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>juc</title>
    <url>/2021/04/05/juc/</url>
    <content><![CDATA[<p>线程池、各种锁、高并发解决方案相关</p>
<span id="more"></span>


<ul>
<li><p>线程池</p>
<ul>
<li>阻塞队列<ul>
<li>任务队列中没有任务时阻塞获取任务的线程，使得线程进入wait状态，释放cpu资源。</li>
<li>有任务时才唤醒对应线程从队列中取出消息进行执行。</li>
</ul>
</li>
<li>任务类型<ul>
<li>CPU密集型任务<ul>
<li>尽量使用较小的线程池，一般为CPU核心数+1</li>
</ul>
</li>
<li>IO密集型任务<ul>
<li>可以使用稍大的线程池，一般为2*CPU核心数。</li>
</ul>
</li>
<li>混合<ul>
<li>分别用不同的线程池去处理</li>
</ul>
</li>
</ul>
</li>
<li>4类<ul>
<li>newCachedThreadPool, newFixedThreadPool, newScheduledThreadPool, newSingleThreadExecutor</li>
</ul>
</li>
</ul>
</li>
<li><p>乐观锁 悲观锁</p>
<ul>
<li>悲观<ul>
<li>db里的行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。</li>
</ul>
</li>
<li>乐观<ul>
<li>在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制</li>
<li>乐观锁适用于多读的应用类型，这样可以提高吞吐量</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
        <category>java</category>
        <category>并发</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>lexicons</title>
    <url>/2021/07/20/lexicons/</url>
    <content><![CDATA[<br>

<br>

<p>spared by幸免于</p>
<p>upheaval</p>
<span id="more"></span>

<p>maintain its work- force at full force</p>
<p>help us maintain our level of excellence through this complicated time despite the new challenges that confront us all</p>
<p> our current economic woes 经济困难</p>
<p>we wish you well as we all plot our courses through the current difficulties.</p>
<p>an abiding influence 持续的影响</p>
<p>commenced the day-long event</p>
<p>delivering the keynote address</p>
<p>attends to the problems facing practitioners </p>
<p>be caricatured as  an Ivory Tower academic被讽刺为象牙塔学术</p>
<p>Consul General of Egypt 埃及总领事</p>
<p>reiterate 重申</p>
<p>crowd out 挤出</p>
<p>cabinet内阁</p>
<p>upper eche-lons of 高层</p>
<p> wonk 专家</p>
<p>newsletter 时事通讯</p>
<p>liquidate 清盘</p>
<p>scrutinize</p>
<p>the July Politburo meeting</p>
<p>catalyst</p>
<p>economy tilting over</p>
<p>extradition request</p>
<p>stiff competition</p>
<p>coerce强迫</p>
<p>The comments sit awkwardly with China’s existing diplomatic disputes</p>
<p>State Department 国务院</p>
<p>span from</p>
]]></content>
      <categories>
        <category>英文积累</category>
      </categories>
      <tags>
        <tag>英文积累</tag>
      </tags>
  </entry>
  <entry>
    <title>linux&amp;mac命令</title>
    <url>/2021/04/13/linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<br> 

<p>记录常见linux mac命令</p>
<span id="more"></span>

<br> 

<h4 id="端口、进程"><a href="#端口、进程" class="headerlink" title="端口、进程"></a>端口、进程</h4><p>1、查看进程号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef | grep 进程名</span><br></pre></td></tr></table></figure>



<p>2、查看端口被哪个进程监听</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo lsof -i :端口</span><br></pre></td></tr></table></figure>



<p>3、查看进程监听的端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo lsof -nP -p 进程号 | grep LISTEN</span><br><span class="line">sudo lsof -nP | grep LISTEN | grep 进程号</span><br></pre></td></tr></table></figure>



<p>4、查看监听端口的进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo lsof -nP | grep LISTEN | grep 端口号</span><br></pre></td></tr></table></figure>



<p>5、看到一个新的方法（MacOS统计TCP/UDP端口号与对应服务）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;### TCP LISTEN ###&quot;</span><br><span class="line">lsof -nP -iTCP -sTCP:LISTEN</span><br></pre></td></tr></table></figure>



<p>6、关于环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Mac系统的环境变量，加载顺序为：</span><br><span class="line">&#x2F;etc&#x2F;profile &#x2F;etc&#x2F;paths ~&#x2F;.bash_profile ~&#x2F;.bash_login ~&#x2F;.profile ~&#x2F;.bashrc</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;profile和&#x2F;etc&#x2F;paths是系统级别的，系统启动就会加载，后面几个是当前用户级的环境变量。后面3个按照从前往后的顺序读取，如果&#x2F;.bash_profile文件存在，则后面的几个文件就会被忽略不读了，如果&#x2F;.bash_profile文件不存在，才会以此类推读取后面的文件。~&#x2F;.bashrc没有上述规则，它是bash shell打开的时候载入的。</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;bashrc （一般在这个文件中添加系统级环境变量）</span><br><span class="line">全局（公有）配置，bash shell执行时，不管是何种方式，都会读取此文件</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;acb1f062a925</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>mac</tag>
        <tag>cheetsheet</tag>
      </tags>
  </entry>
  <entry>
    <title>macd</title>
    <url>/2021/06/20/macd/</url>
    <content><![CDATA[<br>
<br>



<h4 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h4><p><strong>DIF=12日EMA-26日EMA。</strong></p>
<span id="more"></span>

<p><strong>DIF绝对值大小，代表着长短期均线距离（开口）的大小。</strong>当12日均线在26日均线之上时，股价处于上升状态，DIF在0轴之上；当12日均线在26日均线之下时，股价处于下跌状态，DIF在0轴之下。</p>
<p><strong>当DIF上穿0轴时，即12均线与26日均线金叉；当DIF下穿0轴时，即12日均线与26日均线死叉。在趋势行情中，金叉与死叉是有效的买卖信号，但在震荡行情中，金叉与死叉基本上都是假信号。</strong></p>
<p>股价在上涨，但DIF却在不断下跌，即<strong>随着股价上涨，两条均线之间的距离未能创出新高。</strong>也就意味着上涨的势头越来越弱，虽然从股价上看还是上涨的，但随时都可能开始下跌。</p>
<p><strong>DEA：DIF值的移动平均线，一般是软件默认是9日平均线。</strong></p>
<p>如果DIF上穿DEA（金叉），意味着最近的DIF正在变大；如果DIF下穿DEA（死叉），意味着最近的DIF正在变小。</p>
<p>当DIF在零轴之上时：</p>
<p><strong>DIF与DEA金叉，意味着DIF正在变大，即股价的长短期均线距离在变大，股价上涨势头越来越猛。</strong></p>
<p><strong>DIF与DEA死叉，意味着DIF正在变小，即股价的长短期均线的距离正在变小，股价目前上涨势头正在变弱。</strong></p>
<p>当DIF在零轴之下时：</p>
<p><strong>DIF与DEA金叉，此时DIF是负值，也就是说DIF的绝对值在变小，即股价的长短期均线距离在变小，股价下跌势头正在变弱。</strong></p>
<p><strong>DIF与DEA死叉，此时DIF是负值，也就是说DIF的绝对值在变大，即股价的长短期均线距离在变大，股价下跌势头越来越强。</strong></p>
<p><strong>红柱与绿柱：（DIF-DEA）*2，即是柱子的数值，红柱为正值，绿柱为负值。</strong></p>
<p>当红柱变为绿柱时，对应着DIF与DEA的死叉；当绿柱变为红柱时，对应着DIF与DEA的金叉。</p>
<p><br><br></p>
<h4 id="均值系统"><a href="#均值系统" class="headerlink" title="均值系统"></a>均值系统</h4><p>1、三种情况，一是交织缠绕，二是女上，三是男上。</p>
<p>2、响应地，就有 从女上减弱进入缠绕，男上减弱进入缠绕，以及由缠绕突破成女上或者男上。</p>
<p>3、而形状按照力量分有，走平后按照原来的情况继续、靠近而不突破后按照原来的情况继续、突破后缠绕。</p>
<p>4、买卖点：男上位最后一次缠绕后背弛构成的空头陷阱抄底进入，这是第一个值得买入的位置，而第二个值得买入或加码的位置，就是女上位后第一次缠绕形成的低位。</p>
<p>5、买尽量买男上的转折，卖尽量卖女上的缠绕背驰。  </p>
<p>6、</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><p>1、本质上都是一个评价系统，也就是告诉你在这个系统的标准下，评价对象的强弱</p>
<p>2、</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>ml基础</title>
    <url>/2021/04/05/ml%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<br>

<p>ml知识点</p>
<span id="more"></span>

<ul>
<li><p>softmax和logistic</p>
<ul>
<li>sigmoid: 导数在(0,1/4)，当x&gt;1/4或者&lt;-1/4时，导数非常小 1/1+e^(-x)</li>
<li>当分类数为2时，可以推导出两个一致</li>
<li>softmax建模使用的分布是多项式分布，而logistic则基于二项式分布</li>
<li>softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类</li>
<li>多个logistic回归进行多分类，”苹果”这个词语既属于”水果”类也属于”3C”类别。</li>
</ul>
</li>
<li><p>二项式和多项式分布</p>
<ul>
<li>二项<ul>
<li>二项分布是n重伯努利试验中正例发生次数的离散概率分布</li>
<li><code>import numpy; a = numpy.random.binomial(n=10, p=0.7, size = 1)</code></li>
</ul>
</li>
<li>多项<ul>
<li>二项分布是单变量分布，而多项分布是多变量分布</li>
<li>多项分布的例子是扔骰子，每次试验有多种可能，进行多次试验，多项分布描述的是每种可能发生次数的联合概率分布</li>
<li><code>a = numpy.random.multinomial(n=10, pvals=[0.2,0.4,0.4], size = 1)</code></li>
</ul>
</li>
</ul>
</li>
<li><p>逻辑回归梯度下降伪代码</p>
<ul>
<li><a href="https://blog.csdn.net/qq_33391629/article/details/108711228">https://blog.csdn.net/qq_33391629/article/details/108711228</a></li>
</ul>
</li>
<li><p>最小均方误差和最小二乘</p>
<ul>
<li>算术平均数可以让均方误差误差最小</li>
<li>如果误差的分布是正态分布，那么最小二乘法得到的就是最有可能的值。即：最小二乘法是概率密度(误差的概率密度)是高斯分布的最大似然估计的状况</li>
<li><a href="https://www.zhihu.com/question/396712527/answer/1243682221">线性回归模型用最小二乘法得到的估计的均方误差怎么得到</a></li>
</ul>
</li>
<li><p>并行直方图</p>
<ul>
<li>分裂节点时，数据在block中按列存放，而且已经经过了预排序，因此可以并行计算，即同时对各个属性遍历最优分裂点</li>
</ul>
</li>
<li><p>特征点分割</p>
<ul>
<li>树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点</li>
<li>当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，XGBoost采用了一种近似的算法</li>
<li>近似的算法对每维特征加权分位进行分桶，具体的算法利用到了损失函数关于待求树的二阶导数。</li>
</ul>
</li>
<li><p>偏导数与残差</p>
<ul>
<li>偏导数的计算可以确定每个相关数据对最终结果的局部/全局最小值的一个梯度。<br>顺着梯度进行调整，会让结果逐步趋向局部/全局最小值。</li>
<li>gbdt是用梯度来近似残差：损失函数L对当前所学模型F预测值的负梯度，所以模型的更新是沿着梯度下降方向的。</li>
</ul>
</li>
</ul>
<ul>
<li><p>auc计算</p>
<ul>
<li>(auc计算)[<a href="https://blog.csdn.net/qq_22238533/article/details/78666436]">https://blog.csdn.net/qq_22238533/article/details/78666436]</a></li>
<li>为啥auc<ul>
<li>随机抽出一对样本（一个正样本，一个负样本），然后用训练得到的分类器来对这两个样本进行预测，预测得到正样本的概率大于负样本概率的概率。</li>
</ul>
</li>
</ul>
</li>
<li><p>模型评估</p>
<ul>
<li>精确率<ul>
<li>分类正确的正样本 占 判定为正样本的总数 。</li>
</ul>
</li>
<li>召回率<ul>
<li>分类正确的正样本 占 真正正样本总数 。</li>
</ul>
</li>
<li>问题<ul>
<li>提高精确率，会更倾向于 “更有把握才把样本判定为正” ，此时保守而降低了召回率</li>
<li>即排序的TOPN，精确率很高，但是会漏掉 很多其他正样本。导致用户找不到自己想要的。</li>
<li>综合考虑<ul>
<li>PR 曲线<ul>
<li>x召回，y精确。</li>
</ul>
</li>
<li>f1 score<ul>
<li>调和平均值</li>
</ul>
</li>
<li>ROC 曲线</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql性能优化</title>
    <url>/2021/04/05/mysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<br>

<p>background知识、性能优化</p>
<span id="more"></span>
<ul>
<li><p>MyISAM 与 InnoDB </p>
<ul>
<li>InnoDB 支持事务、能回滚，MyISAM 不支持</li>
<li>MyISAM 适合查询以及插入为主的应用，修改多或者要安全则innodb</li>
<li>InnoDB 中必须包含只有自增长字段的索引，MyISAM可以和其他字段一起建立联合索引</li>
<li>清空整个表时，InnoDB 是一行一行的删除，效率非常慢。MyISAM 则会重建表</li>
<li>InnoDB 支持外键，MyISAM 不支持   <ul>
<li>外键<ul>
<li>定义外键约束，关系数据库可以保证无法插入无效的数据</li>
<li>可以把数据与另一张表关联起来，这种列称为外键</li>
<li>外键约束会降低数据库的性能，追求速度并不设置外键约束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>多对多关系：通过一个中间表，关联两个一对多关系</p>
</li>
</ul>
<ul>
<li><p>分库分表</p>
<ul>
<li><p>原因</p>
<ul>
<li>单表数据大，sql效率低</li>
<li>并发高，单库撑不住；磁盘使用高</li>
</ul>
</li>
<li><p>分表</p>
<ul>
<li>经常读取和不经常读取的字段分开</li>
<li>把一个大的用户表分拆为用户基本信息表user_info和用户详细信息表user_profiles。大部分时候，只需要查询user_info表，提高了查询速度。</li>
</ul>
</li>
<li><p>中间件：proxy方案 or client层方案 sharding-jdbc mycat</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>crud</p>
</li>
<li><p>分布式id生成</p>
</li>
</ul>
<ul>
<li><p>垂直、水平拆分<br>中间件解决对某个字段值的自动路由，路由到相应的库、表<br>range、hash</p>
</li>
<li><p>迁移方案<br>数据库中间件的调研学习、设计分库分表方案、测试环境分库代码的正常读写<br>开始迁移：<br>1部署分库策略 2双写 3导数的时候，判断最后修改时间，确保只能新数据覆盖老数据 4自动校验，反复读写，直到新旧库数据一致 5停掉老库，部署分库分表的代码</p>
</li>
</ul>
<ul>
<li>动态扩容缩容分库分表<br>一般都够用：32库，每个库1500个写并发，qps可以承受4.8w。加上一个MQ削峰，qps接受8w，每秒消费5w。一个库32张表，1024表，每个表500w，一个mysql放50亿条数据。</li>
</ul>
<p>1确定机器数量、每台机器多少个库、多少表 2路由规则 3机器上装好mysql 4导数工具导数 5修改配置调整数据库服务器地址 6重发系统上线</p>
<ul>
<li><p>分库分表后的全局id<br>1并发不高，qps几百–直接往一个库写入，获得自增的id；起一个服务，专门地获取最大的id，返回一批新的id<br>2sequence+固定步长<br>3uuid –不具有有序性，b+树过多随机读写<br>4系统时间 + 业务数据<br>5雪花算法<br>第一个bit是0。41bit-时间，10bit是工作机器id，12bits序列号<br>同一ms内有12bits可表达的id</p>
</li>
<li><p>mysql读写分离<br>Case1在binlog没有被从拉到本地的中继日志中、串行地执行relaylog中的命令时，master宕机<br>Solution半同步，至少一个从返回ack才认为写操作完成。<br>Case2由于串行重发， 会有延迟几十到百毫秒不等<br>Solution2并行同步，从开启多线程，并行重放日志，库级别并行<br>mysql中查看延迟，严重解决：<br>分库、并行复制、新插入的数据查询不到时处理、查询直连主库</p>
</li>
<li><p>单库单表和分库分表的join</p>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql索引</title>
    <url>/2021/04/05/mysql%E7%B4%A2%E5%BC%95/</url>
    <content><![CDATA[<br>

<p>性能优化之 索引</p>
<span id="more"></span>


<ul>
<li><p>优点</p>
<ul>
<li>分组、排序操作</li>
<li>加速表与表之间的连接</li>
</ul>
</li>
<li><p>分类</p>
<ul>
<li><p>普通索引</p>
<ul>
<li>允许重复值和空值</li>
</ul>
</li>
<li><p>唯一索引</p>
<ul>
<li>必须唯一，允许空值</li>
<li>组合索引，则列值的组合必须唯一</li>
</ul>
</li>
<li><p>主键索引</p>
<ul>
<li>不允许值重复或者值为空</li>
</ul>
</li>
<li><p>空间索引</p>
<ul>
<li>空间数据类型的字段建立的索引</li>
</ul>
</li>
<li><p>全文索引</p>
<ul>
<li> CHAR、VARCHAR 或 TEXT 类型的列上创建</li>
<li>只有 MyISAM 存储引擎支持全文索引</li>
</ul>
</li>
</ul>
</li>
<li><p>位图索引</p>
<ul>
<li>　“select * from table where Gender=‘男’ and Marital=“未婚”;”<br>  首先取出男向量10100…，然后取出未婚向量00100…，<br>  将两个向量做and操作，这时生成新向量0010</li>
</ul>
</li>
<li><p>B与B+ </p>
<ul>
<li>B：树内的每个节点都存储数据；叶子节点之间无指针连接</li>
<li>B+：数据只出现在叶子节点；所有叶子节点增加了一个链指针</li>
</ul>
</li>
<li><p>聚集索引、非聚集索引 </p>
<ul>
<li>区别<ul>
<li>聚集：行中数据的物理顺序与键值的逻辑（索引）顺序相同</li>
<li>非聚集索引叶子节点上存储了索引字段自身值和主键索引</li>
</ul>
</li>
<li>聚集相关<ul>
<li>聚簇索引默认就是主键索引；第一个唯一非空索引被作为聚集索引；内部会生成一个隐藏的主键</li>
<li>uuid作为主键，写入性能低了</li>
</ul>
</li>
<li>非聚集<ul>
<li>索引不能随意增加。在做写库操作的时候，需要同时维护这几颗树的变化，导致效率降低</li>
<li>回表或者二次查询<ul>
<li>使用聚集索引查询可以直接定位到记录，而普通索引通常需要扫描两遍索引树</li>
<li>解决<ul>
<li>联合索引</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>联合索引</p>
<ul>
<li>最左匹配</li>
</ul>
</li>
<li><p>查询失效</p>
<ul>
<li>条件<ul>
<li>最左</li>
<li>OR条件中的每个列都加上索引</li>
<li>not in</li>
<li>前置通配符</li>
<li>is null</li>
</ul>
</li>
<li>函数<ul>
<li>where age-1=17</li>
<li>内置函数，索引失效 </li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title>node2vec</title>
    <url>/2021/04/05/node2vec/</url>
    <content><![CDATA[<ul>
<li><p>node2vec的随机游走</p>
<span id="more"></span>
<ul>
<li><p><a href="https://blog.csdn.net/rover2002/article/details/106760664">Alias Method: 非均匀随机抽样算法</a></p>
<ul>
<li>空间换时间的方法，在常数时间内，完成非均匀到均匀采样的映射。</li>
<li>主要思想： 概率转换为面积，然后把面积填到其他地方，总体长方形面积最小。由于有填到其他地方的，就有alias。这样之后再掷筛子，随机。</li>
</ul>
</li>
<li><p>跳转概率</p>
<ul>
<li><p>Node2vec 的跳转概率为 自定义的权重*有向边权重。</p>
</li>
<li><p>自定义权重，考虑了当前点vi的所有相关点 和 vi前一个node的距离。距离为0则概率1/p(即重复访问刚刚访问过的node)，为1则1，为2则1/q。</p>
</li>
<li><p>根据论文做法，对p q在{0.25,0.5,1,2,4}中进行grid search。</p>
</li>
<li><p>参数p控制重复访问刚刚访问过的顶点的概率。若p较大，则访问刚刚访问过的顶点的概率会变低。</p>
</li>
<li><p>参数q控制着游走是向外还是向内：<br>  若q&gt;1，随机游走倾向于访问和上一次的t接近的顶点(偏向BFS)；<br>  若q&lt;1，倾向于访问远离t的顶点(偏向DFS)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>python</title>
    <url>/2021/04/05/python/</url>
    <content><![CDATA[<br>

<p>语言、语法糖相关</p>
<span id="more"></span>


<ul>
<li><p>python2 3 区别</p>
<ul>
<li><p>map filter，由function变为class，前者返回列表，后者为object</p>
</li>
<li><p>print由命令变为函数</p>
</li>
<li><p>编码由ASCII变为utf8</p>
</li>
<li><p>xrange</p>
<ul>
<li>要生成很大的数字序列的时候，用xrange会比range性能优很多，因为不需要一上来就开辟一块很大的内存空间。</li>
<li>range创建列表，xrange是生成器</li>
<li>3只保留了生成器的方式并命名为range</li>
</ul>
</li>
<li><p>除法 1/2 = 0 0.5</p>
</li>
</ul>
</li>
<li><p>滑动窗口算法 </p>
<ul>
<li>单调栈</li>
<li><a href="https://leetcode-cn.com/problems/sliding-window-maximum/">https://leetcode-cn.com/problems/sliding-window-maximum/</a></li>
</ul>
</li>
<li><p>abc</p>
<ul>
<li>注解abstractmethod，抽象基类</li>
</ul>
</li>
<li><p>标记清除</p>
<ul>
<li>对执行删除（-1）后的每个引用-1，为0的放到死亡容器，否则放到存活容器</li>
<li>循环存活容器，复活死亡中的变量放到存活容器内</li>
<li>删除死亡容器内的所有对象</li>
</ul>
</li>
<li><p>分代</p>
<ul>
<li>新创建的对象做为0代。每执行一个【标记-删除】，存活的对象代数就+1</li>
<li>代数越高的对象（存活越持久的对象），进行【标记-删除】的时间间隔就越长</li>
<li>一个对象10次检测都没给它干掉, 就认定这个对象一定很长寿, 就减少这货的”检测频率”</li>
</ul>
</li>
<li><p>触发垃圾回收</p>
<ul>
<li>调用gc.collect()</li>
<li>GC达到阀值时</li>
<li>程序退出时</li>
</ul>
</li>
<li><p>intern机制</p>
</li>
<li><p>copy deepcopy</p>
<ul>
<li>复制的值是不可变对象（数值，字符串，元组）,对象的id值与等式左边的id值相同。</li>
<li>复制的值是可变对象（列表和字典）<ul>
<li>浅拷贝两种情况：<ul>
<li>复制的 对象中无 复杂 子对象，浅复制的值改变也并不会影响原来的值反之也不影响</li>
<li>复制的对象中有 复杂 子对象， 改变原来的值 中的复杂子对象的值 ，会影响浅复制的值</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>cookie和session</p>
<ul>
<li>session 在服务器端，cookie 在客户端（浏览器）</li>
<li>session id 是存在 cookie 中</li>
<li>浏览器禁用了 cookie ，同时 session 也会失效</li>
<li>cookie安全性比session差</li>
</ul>
</li>
<li><p>正则</p>
<ul>
<li>正则表达式匹配中，（.<em>）和（.</em>?）匹配区别？<ul>
<li>贪婪、非贪婪</li>
</ul>
</li>
<li>正则re.complie<ul>
<li>re.compile是将正则表达式编译成一个对象，加快速度，并重复使用</li>
</ul>
</li>
</ul>
</li>
<li><p>动态类型dynamic typing</p>
<ul>
<li>将string传入mod，compilier并不会报错，仍然是原来compile出来的步骤。</li>
<li>mod(“%s%s”, (“py”,”thon”) )结果将是python，在c中，遇到BINARY_MOULO指令操作，如果是string类型则直接调用PyString_Format()。</li>
</ul>
</li>
</ul>
<ul>
<li><p>一切发生在运行时、命名空间。   </p>
<ul>
<li><p>命名空间分模块、类、方法。</p>
</li>
<li><p>编译只是从将代码在运行时转成成code object，当执行时转成function object。</p>
</li>
<li><p>def是一个assignment function(将return 的结果assign给变量)，遇到def 的时候实则就是call这个function并对参数进行assign。</p>
</li>
<li><p>类在创建时就开始执行，在一个字典所表示的命名空间中执行。这个命名空间用来创建类对象。</p>
</li>
<li><p>都是 对象/引用。变量只是名称，不是容器。字典中将名称映射到对象上。</p>
</li>
<li><p>三个scopes，local，global:module，builtin</p>
</li>
<li><p>not pthon object hooks：1，赋值，赋值将改变命名空间，而非对象自身 2，类型检查 3，is运算 4，and or not 都是布尔运算 5，调用，需要实现getattr来返回属性对象，该属性对象在call时执行某方法</p>
</li>
<li><p>如果要对object 移动，需要delete并清楚所有的references。</p>
</li>
<li><p>弱引用是当object删除之后被通知，即callback。</p>
</li>
<li><p>互相引用的reference cycle–&gt;需要cycle gc</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Cython PyPy</p>
<ul>
<li>Cpython中，PyObject都是以struct实现，包含type pointer、reference counting或其他C type。PyType struct有对type的一些描述信息。</li>
</ul>
</li>
<li><p>常用到的文本提取，任意字符串</p>
<ul>
<li>.* .*?</li>
</ul>
</li>
<li><p>编译 解释</p>
<ul>
<li>compiler部分很少，interpreter工作量远远多。</li>
<li>Python interpreter是一个virtual machine(模拟物理机)，是个stack machine(操作stacks)，和register machine(从内存不同位置读写)不同。</li>
<li>且是byte interpreter，input是instruction sets，元素为byte codes，lexing&amp;parsing将python codes 转为byte codes。相当于C codes和assembly codes。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>redis_mindroad</title>
    <url>/2021/04/05/redis-mindroad/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>汇总</strong></p>
<span id="more"></span>


<ul>
<li><p>基础数据结构5种及场景</p>
</li>
<li><p>高可用</p>
<ul>
<li>持久化</li>
<li>集群<ul>
<li>主从</li>
<li>cluster</li>
<li>sentinel</li>
</ul>
</li>
</ul>
</li>
<li><p>高级数据结构</p>
<ul>
<li>bitmap</li>
<li>geo</li>
<li>hyperloglog</li>
</ul>
</li>
<li><p>并发竞争</p>
</li>
<li><p>分布式锁</p>
</li>
<li><p>内存淘汰机制</p>
</li>
<li><p>读写，cache aside pattern</p>
</li>
<li><p>单线程</p>
</li>
<li><p>选型</p>
</li>
<li><p>事务</p>
</li>
<li><p>分区</p>
</li>
<li><p>性能</p>
<ul>
<li>expire时间</li>
<li>pipeline</li>
<li>keys scan</li>
</ul>
</li>
<li><p>缓存</p>
<ul>
<li>缓存击穿</li>
<li>缓存穿透</li>
<li>缓存雪崩</li>
</ul>
</li>
<li><p>新特性</p>
</li>
<li><p>底层</p>
</li>
<li><p>ziplist、skiplist…</p>
</li>
</ul>
<p><img src="/2021/04/05/redis-mindroad/redis.png" alt="脑图"></p>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>redis</title>
    <url>/2021/01/31/redis/</url>
    <content><![CDATA[<h2 id="相关笔记"><a href="#相关笔记" class="headerlink" title="相关笔记"></a>相关笔记</h2><p>1、从自己拉还是定期拉 还是master发<br>第一次全量复制的时候，从发命令后主发rdb和写缓存<br>续传的时候：master维护backlog里有offset、master run id,从发送offset给主，如果没有则全量<br>其他情况，master会异步发送</p>
<span id="more"></span>
<p>2、单机写 读的并发量、集群并发量<br>几万、十万qps、几十万<br>单个value 1g</p>
<p>3、哨兵没有检测到主failure， 主自动重启，导致数据清空</p>
<p>4、通信及复制：<br>启动slave时：PSYNC给master<br>第一次连接—全量复制中，两个点：1rdb快照 2写命令缓存<br>全量参数：1时间60s 2内存缓冲区持续消耗和一次性超过<br>断点续传：网络故障的部分复制，offset</p>
<p>5、哨兵+主从复制<br>不保证0丢失，保证高可用</p>
<p>6、heartbeat<br>互相发送，主每10s、从每1s</p>
<p>7、主备切换前提、选举算法、哨兵master信息同步<br>选举前提 quorum和majority,至少满足max(quorum,majority)<br>选举算法：四个参数。<br>切换后其他的哨兵更新master配置：通过监听的channel中的version号，version号是负责切换的哨兵从新的master中获得的configuration epoch</p>
<p>8、持久化方式、优缺点、实现<br>Rdb：定期地冷备数据 — fork子进程进行磁盘io，不影响高性能，但如果rdb文件特别大则会影响服务，每隔5min宕机丢数据<br>aof：每1s后台fsync，最多丢1s数据。append写入文件，无磁盘寻址时间，文件尾部破损容易修复(?)、命令基于内存的数据重新构建而不是基于旧的指令日志 – rewrite log（指令压缩、旧的仍然提供服务，新的好了后替换），灾难性误删除.</p>
<p>9、并发竞争<br>redis的cas？zookeeper分布式锁？<br>采用CAS协议，则是如下的情景。<br> •第一步，A取出数据对象X，并获取到CAS-ID1；<br>•第二步，B取出数据对象X，并获取到CAS-ID2； <br>•第三步，B修改数据对象X，在写入缓存前，检查CAS-ID与缓存空间中该数据的CAS-ID是否一致。结果是“一致”，就将修改后的带有CAS-ID2的X写入到缓存。<br> •第四步，A修改数据对象Y，在写入缓存前，检查CAS-ID与缓存空间中该数据的CAS-ID是否一致。结果是“不一致”，则拒绝写入，返回存储失败。<br>这样CAS协议就用了“版本号”的思想，解决了冲突问题。（乐观锁概念）</p>
<p>在使用redis的setnx方法和memcace的add方法时，如果指定的key已经存在，则返回false。利用这个特性，实现全局锁<br>每次生成全局id前，先检测指定的key是否存在，如果不存在则使用redis的incr方法或者memcache的increment进行加1操作。这两个方法的返回值是加1后的值，如果存在，则程序进入循环等待状态。循环过程中不断检测key是否还存在，如果key不存在就执行上面的操作。</p>
<p>10、数据恢复<br>放到指定目录，然后重启redis，redis会恢复内存中的数据然后继续提供服务</p>
<p>11、哨兵–分布式<br>监控、修改地址(确保slave连接正确的master)、主从切换（确保潜在master的slave复制了所有的数据）、故障通知<br>两个配置：quorum、majority<br>quorum是至少多少哨兵认为宕机，才是master真的宕机<br>majority是必须满足大多数的哨兵是运行，才能进行故障转移。<br>=&gt;主备切换至少满足max(quorum,majority)<br>sdown odown</p>
<p>12、丢失：<br>case1slave没有同步完master的数据，master宕机<br>case2master机器脱离了集群，但是仍然运行，哨兵又选举了新的master。但是client还是在旧的写，旧的成为slave去新的master更新数据。这部分写丢失。<br>解决：两个参数。master宕机控制在丢失数据10s内。一点超过10s的数据复制，则master停止写请求。</p>
<p>13、哨兵自动发现<br>1往自己监控的channel里发消息 ，包括： runid、master监控配置、hostip<br>2监听channel，感知其他的哨兵<br>3监控配置的同步</p>
<p>14、cluster<br>Cluster bus通信、gossip协议</p>
<p>15、集群元数据维护方式<br>集中式：zookeeper作为实现，时效性高，存储更新有压力<br>gossip：分散更新，滞后</p>
<p>16、一致性hash、hash slot<br>hash的值空间组成一个环，将master的ip进行hash，确定在环的位置。数据找到位置后，存入顺时针走的第一个遇到的master。<br>当master宕机，则master和前一个master之间的数据受到影响。<br>热点问题：master计算多个hash值，在环中增加虚拟节点<br>slot：每个key计算crc16值，然后对16384取模，对应到相应的hash slot。每个master持有部分的slot</p>
<p>17、cluster中的选举、复制和哨兵<br>大于一半的master投票给该slave则该slave可以替换为master<br>cluster直接集成了复制和哨兵功能</p>
<p>18、缓存雪崩<br>所有的请求在redis都没有命中<br>解决：<br>redis高可用(主从+哨兵)、hystrix限流+本地ehcache缓存、redis持久化<br>先查本地、再redis、再限流，未通过的请求则降级</p>
<p>19、穿透<br>没查到则写一个空值到redis</p>
<p>20、击穿<br>热点key失效时缓存被瞬时击穿<br>1不用更新则永不过期 2更新频率高或者时间长则定时线程提前主动重新构建缓存或者延后过期时间 3更新频率低或者时间短，则分布式互斥锁或者本地锁保证少量请求能重新构建缓存，其余则锁释放后再访问新的缓存</p>
<p>21、双写一致性<br>Cache aside：读-先读缓存、再读数据库、再写入缓存，更新则先更新数据库、再删除缓存<br>为什么是删除不是更新？lazy<br>删除缓存失败？–先删除缓存再更新数据库<br>先删除缓存再更新数据库–还是会有不一致，每秒并发几万：当更新没完成，一个请求来了，写入缓存的是旧数据，然后又更新了数据库。</p>
<p>解决：串行化。<br>更新的数据都附带上数据标识，如果不在缓存中，则将读和更新发到一个队列中，线程从队列中串行地拿操作执行。<br>注意：1请求操作的超时（过多的写操作积压，一般单机20个队列，qps500的写可以支持，200ms里100个写，每个队列5个，每个20ms完成，那么读请求也可以在200ms内返回。否则扩容机器） 2读并发高 3热点商品的请求倾斜 4路由到同一台机子</p>
<p>22、线上部署情况<br>cluster模式，10台机器，5台master，一主一从<br>每个master高峰的qps 5w<br>master配置：32g8core+1T，分给redis内存最多10g<br>内存中放商品数据，每条数据大概10kb，10w条则1g，一般200w条，占20g。目前的qps高峰是3500左右请求量。</p>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>spark</title>
    <url>/2021/04/05/spark/</url>
    <content><![CDATA[<br>

<p>spark job相关</p>
<span id="more"></span>

<ul>
<li><p>groupby reduceby区别</p>
<ul>
<li>reduceByKey 会进行分区内聚合，函数内部调用了combineByKey，然后再进行网络传输</li>
<li>groupByKey 不会进行局部聚合</li>
</ul>
</li>
<li><p>依赖</p>
<ul>
<li>NarrowDependency分为OneToOneDependency和RangeDependency两种</li>
<li>ShuffleDependency：子依赖多个父RDD</li>
</ul>
</li>
<li><p>stage &amp; task</p>
<ul>
<li><p>DAGScheduler将Stage划分-&gt;把Stage转换为TaskSet-&gt;TaskScheduler将计算任务最终提交到集群</p>
</li>
<li><p>最后的Stage包含了一组ResultTask</p>
</li>
<li><p>task</p>
<ul>
<li>task和partition是一对一。一组Task就组成了一个Stage</li>
<li>Task<ul>
<li>ShuffleMapTask<ul>
<li>根据Task的partitioner将计算结果放到不同的bucket</li>
</ul>
</li>
<li>ResultTask<ul>
<li>计算结果发送回Driver</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>如何将stage划分为taskset</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Spark Standalone-cluster</p>
<ul>
<li>流程<ul>
<li>集群启动后worker向Master汇报资源，Master掌握集群资源</li>
<li>client提交sparkApplication，向Master申请启动Driver</li>
<li>Master收到请求后随机找一台节点启动Driver，Driver向Master申请executor进程资源</li>
<li>driver 里scheduler将DAG划分为stage，taskscheduler划分stage为taskset</li>
<li>Master找到满足资源的worker节点，启动Excutor，反向注册给Driver</li>
<li>Driver的context taskscheduler再把task发给executor，监控task，回收结果（collect方法）</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>缓存</p>
<ul>
<li>persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中</li>
<li>触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用</li>
<li>StorageLevel</li>
<li>shuffle会涉及到网络传输，可能会丢失数据，shuffle之前做persist，框架默认将数据持久化到磁盘</li>
</ul>
</li>
<li><p>checkpoint</p>
<ul>
<li>在checkpoint的时候强烈建议先进行cache</li>
<li>当你checkpoint执行成功了,那么前面所有的RDD依赖都会被销毁</li>
<li>区别persist<ul>
<li>persist可以将 RDD 的 partition 持久化到磁盘，但该 partition 由 blockManager 管理, blockManager stop，文件夹被删除</li>
<li>checkpoint 将 RDD 持久化到 HDFS 或本地文件夹，是一直存在的，也就是说可以被下一个 driver program 使用</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>rdd</p>
<ul>
<li>本质上是多个Partition组成的List。一个RDD可以包含多个分区，每个分区就是一个dataset片段。分区数决定了并行计算的数量<br>默认情况下，HDFS上面一个Block就是一个Partition。</li>
<li>RDD如何保障数据处理效率<ul>
<li>通过persist与patitionBy函数来控制RDD的分区与持久化</li>
<li>底层接口则是基于迭代器的</li>
</ul>
</li>
</ul>
</li>
<li><p>容错</p>
<ul>
<li><p>rdd记住构建它的操作图（Graph of Operation），Worker失败时重新计算，无需replication</p>
</li>
<li><p>需要恢复执行过程的中间状态：通过Spark提供的checkpoint</p>
</li>
<li><p>lineAge：每次更新都会记录下来，比较复杂且比较耗费性能。适用于DAG中重算太耗费时间的</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>数据倾斜<ul>
<li>先局部聚合，再全局聚合。</li>
<li>并行度太少了，导致个别Task的压力太大</li>
<li>自定义partition，分散key的分布，使其更加均匀</li>
</ul>
</li>
</ul>
<ul>
<li><p>累加器和广播变量</p>
<ul>
<li><p>累加器</p>
<ul>
<li>累计计数等场景</li>
<li>Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新。</li>
</ul>
</li>
<li><p>广播变量</p>
<ul>
<li>每个executor一个副本，普通变量每个task一个副本</li>
<li>driver定义和修改</li>
<li>节点间高效分发大对象</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>序列化</p>
<ul>
<li><p>java序列化</p>
<ul>
<li>static和transient修饰的变量不会被序列化</li>
<li>readObject()方法和writeObject()自定义实现序列化</li>
</ul>
</li>
<li><p>spark序列反序列化过程</p>
<ul>
<li>代码中对象在driver本地序列化；</li>
<li>对象序列化后传输到远程executor节点；</li>
<li>远程executor节点反序列化对象</li>
</ul>
</li>
<li><p>SerializationDebugger在日志中出问题的类和属性</p>
</li>
<li><p>kryo </p>
<ul>
<li>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好</li>
<li><code>.registerKryoClasses(Array(classOf[Student])) // 将自定义的类注册到Kryo</code></li>
<li>Spark 2.0.0以来,sparkcontext初始化时某些类型已被注册进去</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>repartition和coalesce的区别、partitionby</p>
<ul>
<li>coalesce<ul>
<li>repartition底层为coalesce</li>
<li>一般增大rdd分区用repartition，减小用coalesce</li>
<li>repartition一定涉及shuffle，coalesce根据传入参数判断是否发生shuffle</li>
</ul>
</li>
<li>partitionby<ul>
<li>repartition 和 partitionBy 都是对数据进行重新分区，默认都是使用 HashPartitioner，区别在于partitionBy 只能用于 PairRDD</li>
<li>repartition 随机生成的数来当做 Key，partitionby是自己的key</li>
</ul>
</li>
</ul>
</li>
<li><p>map()和mapPartition()的区别</p>
<ul>
<li>map()：每次处理一条数据；mapPartition()：每次处一个分区的数据，可能导致OOM。 </li>
<li>当内存空间较大的时候建议使用mapPartition()，以提高处理效率。</li>
<li>mapPartitionsWithIndex类似mapPartitions，但func带有一个整数参数表示分片的索引值</li>
</ul>
</li>
<li><p>glom</p>
<ul>
<li>glom会把每个批次每个分区的数据从Iterator类型转换为Array类型，所以如果每个分区的数据非常大的话会出现OOM的情况。</li>
</ul>
</li>
<li><p>pipe</p>
<ul>
<li>每个分区一个脚本</li>
</ul>
</li>
<li><p>比hive快</p>
<ul>
<li>HQL 引擎还比 Spark SQL 的引擎更快</li>
<li>Hadoop 每次 shuffle 操作后，必须写到磁盘，spark内存</li>
<li>消除了冗余的 MapReduce 阶段</li>
<li>Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM<br> Spark 基于线程，只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。</li>
</ul>
</li>
<li><p>aggregateByKey foldByKey combineByKey </p>
<ul>
<li>aggregateByKey <ul>
<li><code>rdd.aggregateByKey(0)(math.max(_,_),_+_)</code></li>
<li>每个分区的各个key的value和初始值0，进行max，获得到每个key对应value的max<br>  然后每个分区进行combine即相加</li>
</ul>
</li>
<li> 计算相同key对应值的相加结果：<code>rdd.foldByKey(0)(_+_)</code></li>
<li>combineByKey<ul>
<li><code>input.combineByKey((_,1), (acc:(Int,Int),v)=&gt;(acc._1+v,acc._2+1),//v为当前值 (acc1:(Int,Int),acc2:(Int,Int))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))</code></li>
<li>key对应的value先映射成一个二元组(value,1)，同一个分区内的相同key将二元组相加<br>再将不同分区的相同key的二元组相加。</li>
<li>相加时，元组的第一位和第二位都分别累加</li>
</ul>
</li>
</ul>
</li>
<li><p>cogroup</p>
<ul>
<li>第一个RDD元素是(1,”Allen”)，(2,”Bob”)，(3,”Carl”)，第二个RDD元素是(1,10000)，(1,5000)，(2,11000)，(2,6000)，(3,12000)，(3,6000)</li>
<li>join的结果是： (1,(Allen,10000)) (1,(Allen,5000)) (2,(Bob,11000)) (2,(Bob,6000)) (3,(Carl,12000)) (3,(Carl,6000)) </li>
<li>cogroup的结果是： (1,([Allen],[10000, 5000])) (2,([Bob],[11000, 6000])) (3,([Carl],[12000, 6000])) </li>
</ul>
</li>
<li><p>mapValues </p>
<ul>
<li>mapValues(_+”|||”)–每个value后加上”|||”</li>
</ul>
</li>
</ul>
<ul>
<li><p>T级别数据</p>
<ul>
<li><p>单个executor进程内RDD的分片数据是用Iterator流式访问的</p>
</li>
<li><p>RDD lineage上各个transformation携带的闭包函数复合而成的Iterator，<br>  每访问一个元素，就对该元素应用相应的复合函数，得到的结果再流式地落地<br>  对于shuffle stage是落地到本地文件系统留待后续stage访问，<br>  对于result stage是落地到HDFS或送回driver端等等，视选用的action而定</p>
</li>
<li><p>用户要求Spark cache该RDD，且storage level要求在内存中cache时，<br>Iterator计算出的结果才会被保留，通过cache manager放入内存池</p>
</li>
<li><p><a href="https://www.zhihu.com/question/23079001/answer/23569986">内存有限的情况下 Spark 如何处理 T 级别的数据</a></p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow</title>
    <url>/2021/04/05/tensorflow/</url>
    <content><![CDATA[<br>

<p>基础、遇到的问题、源码相关汇总</p>
<span id="more"></span>

<ul>
<li>tensorflow的调试<ul>
<li>nan灾难<ul>
<li>Debugger V2</li>
<li>tensorboard Debugger V2 GUI</li>
<li>tf.print()</li>
<li>tfdbg</li>
</ul>
</li>
<li>显存不断增长<ul>
<li>释放临时节点  auto reuse\是不是需要train的变量</li>
<li>避免在run里面进行运算产生临时节点，即出了结果再计算</li>
</ul>
</li>
<li>emb lookup耗时</li>
<li>多个id统一hash </li>
<li>gpu cpu copy</li>
</ul>
</li>
</ul>
<ul>
<li><p>神经网络八股<br>就不搞了。太多人做过了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">checkpoint_save_path &#x3D; &quot;.&#x2F;checkpoint&#x2F;fashion.ckpt&quot;</span><br><span class="line">if os.path.exists(checkpoint_save_path + &#39;.index&#39;):</span><br><span class="line">    print(&#39;-------------load the model-----------------&#39;)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback &#x3D; tf.keras.callbacks.ModelCheckpoint(filepath&#x3D;checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only&#x3D;True,</span><br><span class="line">                                                 save_best_only&#x3D;True)</span><br><span class="line"></span><br><span class="line">history &#x3D; model.fit(x_train, y_train, batch_size&#x3D;32, epochs&#x3D;5, validation_data&#x3D;(x_test, y_test), validation_freq&#x3D;1,</span><br><span class="line">                    callbacks&#x3D;[cp_callback])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">print(model.trainable_variables)</span><br><span class="line">file &#x3D; open(&#39;.&#x2F;weights.txt&#39;, &#39;w&#39;)</span><br><span class="line">for v in model.trainable_variables:</span><br><span class="line">    file.write(str(v.name) + &#39;\n&#39;)</span><br><span class="line">    file.write(str(v.shape) + &#39;\n&#39;)</span><br><span class="line">    file.write(str(v.numpy()) + &#39;\n&#39;)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"># 显示训练集和验证集的acc和loss曲线</span><br><span class="line">acc &#x3D; history.history[&#39;sparse_categorical_accuracy&#39;]</span><br><span class="line">val_acc &#x3D; history.history[&#39;val_sparse_categorical_accuracy&#39;]</span><br><span class="line">loss &#x3D; history.history[&#39;loss&#39;]</span><br><span class="line">val_loss &#x3D; history.history[&#39;val_loss&#39;]</span><br><span class="line"></span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line">plt.plot(acc, label&#x3D;&#39;Training Accuracy&#39;)</span><br><span class="line">plt.plot(val_acc, label&#x3D;&#39;Validation Accuracy&#39;)</span><br><span class="line">plt.title(&#39;Training and Validation Accuracy&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line">plt.plot(loss, label&#x3D;&#39;Training Loss&#39;)</span><br><span class="line">plt.plot(val_loss, label&#x3D;&#39;Validation Loss&#39;)</span><br><span class="line">plt.title(&#39;Training and Validation Loss&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>todo</title>
    <url>/2021/06/10/todo/</url>
    <content><![CDATA[<br>

<br>

<h3 id="always-to-do"><a href="#always-to-do" class="headerlink" title="always to do"></a>always to do</h3><p>专利 办理居住证 </p>
<span id="more"></span>

<p>transformer、命令整理、配置个虚拟账户，开始开发自己有思考的代码</p>
<br>

<br>



<h3 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h3><ul>
<li>获得历史每一笔买卖，然后想办法做一些分析。每天的分时图、每一定比例的涨幅中累积的量是多少、统计出所有的筹码分布 – (所有买卖出来了 持仓自然出来了)、每一个价位上的成交笔数、做一些阈值过滤后的成交图形</li>
</ul>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>transformer</title>
    <url>/2021/06/07/transformer/</url>
    <content><![CDATA[<br>

<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>lstm的门机制对长依赖的解决不够彻底。更长的依赖关系无法学习到。并且门机制并行度提不上去。</p>
<span id="more"></span>

<br>

<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>encoder-decoder架构，本质上是seqtoseq加上attention。</p>
<p>整体思想是，encoder接收初始序列，六层的最终输出，再输入到decoder的每一层模块。decoder能输出翻译后的句子。</p>
<p>encoder：输入序列进行embedding，然后加上position的encoding，进入self attention。self attention后面加入一个前馈网络层。（selfattention + 前馈FNN）为encoder的一个模块。</p>
<p>decoder：接收encoder的输入以及上一模块的输入。（self attention +encoderdecoder 的atten+ 前馈FNN）为decoder的一个模块。</p>
<br>

<h3 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h3><h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self attention"></a>self attention</h4><p>假设输入词序列A B，对应的embedding为 a b。三个矩阵，Q K V。</p>
<p>a b分别和Q K V相乘，各得到三个向量，aq ak av，bq bk bv。</p>
<p>aq * ak = aq_ak_score ;aq * bk = aq_bk_score。两个➗8。再将这些分数softmax。得到0-1之间。</p>
<p>这里，得到了a对所有其他词的分数向量。</p>
<p>再将av  bv分别和这些分数相乘后加和，得到了第一词A的输出。</p>
<p>类似得到B的输出。</p>
<br> 

<h4 id="矩阵形式"><a href="#矩阵形式" class="headerlink" title="矩阵形式"></a>矩阵形式</h4><p>inputX * WQ = Q;inputX * WK = K;inputX * WV = V</p>
<p>softmax( Q*K / 8 ) * V = attentionOut</p>
<br> 



<h4 id="多头"><a href="#多头" class="headerlink" title="多头"></a>多头</h4><p>多个attentionOut进行concat然后再*W0 = 新的out</p>
<p>W0进入模型一起训练</p>
<br> 



<h4 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h4><p><br> <br> </p>
<h4 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h4><p><a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a><br>        <a href="https://zhuanlan.zhihu.com/p/127774251">https://zhuanlan.zhihu.com/p/127774251</a></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>《做生意的艺术》读后感</title>
    <url>/2021/05/23/%E3%80%8A%E5%81%9A%E7%94%9F%E6%84%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<p>1、夸张大胆，想象力</p>
<span id="more"></span>

<p>2、专注而游戏，能够悬空</p>
<p>3、重视宣传</p>
<p>4、对拒绝和批评，不太过放在心上</p>
<p>5、直觉判断。非常强悍的判断力</p>
<p>有了一个眼光后，认识到宝地。就开始找人商议，筹划合作。肯定是顶着压力的。但是大方向没有错。</p>
<p>6、手段</p>
<p>7、道德 ？ 生意？</p>
<p>很大程度是演戏和忽悠。但是有一定的真实度。演戏和演说的说服力，是一种能力。和道德无关。</p>
]]></content>
      <categories>
        <category>读后感</category>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>读后感</tag>
      </tags>
  </entry>
  <entry>
    <title>xgboost</title>
    <url>/2021/03/11/xgboost/</url>
    <content><![CDATA[<br>

<h1 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul>
<li>xgboost是Gradient Boosting的一种高效系统实现</li>
</ul>
<span id="more"></span>
<h2 id="基学习器"><a href="#基学习器" class="headerlink" title="基学习器"></a>基学习器</h2><ul>
<li>tree(gbtree)，也可用线性分类器(gblinear)。GBDT则特指梯度提升决策树算法</li>
</ul>
<h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><ul>
<li><p>xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数</p>
</li>
<li><p>xgboost工具支持自定义代价函数</p>
</li>
<li><p>代价函数里加入了正则项</p>
<ul>
<li>树的叶子节点个数</li>
<li>每个叶子节点上输出的score的L2模的平方和</li>
<li>正则项降低了模型的variance</li>
</ul>
</li>
<li><p>Shrinkage（xgboost中的eta）</p>
<ul>
<li>xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。</li>
<li>一般把eta设置得小一点，然后迭代次数设置得大一点</li>
</ul>
</li>
<li><p>gamma </p>
<ul>
<li>当增益大于阈值时才让节点分裂，gamma即阈值</li>
<li>它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。</li>
</ul>
</li>
<li><p>lambda</p>
<ul>
<li>正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。</li>
</ul>
</li>
</ul>
<h2 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h2><ul>
<li>缺失值<ul>
<li>对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向</li>
</ul>
</li>
<li>列抽样<ul>
<li>xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算</li>
</ul>
</li>
</ul>
<h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><ul>
<li><p>在特征粒度上</p>
</li>
<li><p>最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）</p>
</li>
<li><p>预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构</p>
</li>
<li><p>在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
</li>
<li><p>可并行的近似直方图算法</p>
<ul>
<li>用贪心法枚举所有可能的分割点，当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低</li>
</ul>
</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li><a href="https://www.zhihu.com/question/41354392">GBDT 和 XGBOOST 的区别有哪些</a></li>
<li>这个总结的也不错 <a href="https://zhuanlan.zhihu.com/p/38946959">https://zhuanlan.zhihu.com/p/38946959</a></li>
</ul>
<hr>
<p>mind:</p>
<p><img src="/2021/03/11/xgboost/xgboost.png" alt="xgboost"></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>书list</title>
    <url>/2021/06/29/%E4%B9%A6list/</url>
    <content><![CDATA[<br>

<br>

<ol>
<li>Balanced Asset Allocation</li>
</ol>
<span id="more"></span>

<p>大幅提及的金属在RPAR中被弱化（被黄金和金属股票代替），如果拉出数据跑一跑，就知道虽然金属在2008年前表现不错，但2008年危机和之后一直在亏，如果RPAR用了20%的金属就无法宣传自己是equity-like return了。如果对历史再有了解，就知道金属之前的表现部分是因为其本身原是小众市场，而当其变为主流后是否能够复制之前的表现是值得商榷的（而这是Milken垃圾债券的逻辑悖论）。</p>
<p>自己其实可以复制、甚至是改良策略，同时把VOO、VBINX、RPAR当做标杆，看看backtesting是否能够照进现实。</p>
<ol start="2">
<li>BrianW.Kernighan ,Dennjs M Ritchie的c语言</li>
<li>Principles of Compiler Design</li>
</ol>
<ol>
<li>《三一恨别长沙–梁稳根 的内心独白》和《恨别长沙谎言曝光》。中联重科和三一的斗争，有国资、非法手段。</li>
<li>到了后来，爷爷只有背影和沉默。只有零星几张吃饭的照片，很少娱乐，很少活动。人离世之时都是落寞的。万物带不走，唯一能缓解死亡降临恐怖的，只有爱和陪伴。相视无言。却留不住。只能想，都是要走的，我也是要走的，才能通透些继续这生活。和你面对死亡的，能有谁？父母来不及，孩子已出门，唯有伴侣。走时若是充实无遗憾的，心里也坦然。不枉岁月。</li>
<li>让子弹飞： 站着还把钱赚了。</li>
<li>中国知识分子招聘会</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2021/02/21/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<br>
让我联想到博弈论中的决策树。根据多种可能地情况路径，利用概率和已知知识，进行决策判断。
<br>
<br>

<h4 id="基本问题"><a href="#基本问题" class="headerlink" title="基本问题"></a>基本问题</h4><p>Q1：如果建立一颗树，使得经过路径判断，走到叶子时候，能够得到最终样本归属的label(类别)或者y(连续值)？</p>
<ul>
<li>分析这棵树需要满足的基本性质<br>互斥、完备。即通过各条到叶子的不同路径，能够最终覆盖绝多数的样本。</li>
</ul>
<span id="more"></span>

<ul>
<li><p>生成这棵树的过程<br>1、要得到中间节点(特征)。分类能力越强的特征，越靠近根。<br>2、要找到当前特征的切分点，而进行分叉。<br>即涉及到特征选择(特征间重要性比较以及当前特征的切分点)。<br>而构建的过程，则是递归地选择特征的过程。<br>直到，没有特征可选、或者样本点已经完全覆盖(特征选择的标准达到阈值，不再分叉)。</p>
</li>
<li><p>分析回归树<br>1、回归树对特征选择和切分点选择的标准，肯定是区别于分类树的。<br>2、回归树的y如何生成？<br>分类树的label来自于多数投票。回归树则来自于，落入叶子的样本的均值。</p>
</li>
</ul>
<p>综上，<br>1、特征选择只在当前考虑选择最优，属于贪心策略，为局部最优。<br>2、本质上，是将样本空间进行直线(线性棱的空间体)切割。是概率中的条件概率。多个特征规则组合的条件下，样本所属类别的判定。<br>3、由于1，生成的树易产生过拟合。<br>4、回归树的回归结果，相比于lr等回归预测模型，结果数量是少的。</p>
<p>Q2：如何进行特征选择？</p>
<ul>
<li><p>对于分类树：<br>1、例如性别、年龄，如果已知性别能够比已知年龄，是更好的信息–即可以更大概率地判断出所属类别，那么就是更重要的特征。<br>2、其中，更大概率地判断出所属类别 – 即降低经验条件熵。<br>3、而经验条件熵的缺点是，对于取值比较多的特征，具有偏向性。其公式导致，取值多则经验条件熵值会小。可以考虑极端情况，当每个样本的该特征都取值不同，则经验条件熵为0。就有了C4.5，通过信息增益比来选择特征。<br>4、CART的gini系数本质和熵类似。越小，熵越小，确定性越高，特征越重要。</p>
</li>
<li><p>对于回归树：<br>1、cart回归树是二叉，要么是要么否。<br>2、回归树的切分选择不以熵入手考虑，而以均方误差作为判断标准。<br>3、对于当前特征，找到这样的切分点 – 使得此切分下的两个叶子样本集中，均方误差和最小。误差为：y-所有样本y的均值。<br>4、生成树过程和分类树一样。</p>
</li>
<li><p>为什么不用相关性？<br>余弦相似度是特征向量在label向量方向上的投影长度。一定程度也是特征重要的体现。<br>但是，在树模型中，由于本质是条件概率模型，熵就更能反映出概率模型的好坏。</p>
</li>
</ul>
<p>Q3：过拟合处理？</p>
<ul>
<li><p>剪枝<br>1、ID3,C4.5的剪枝是一个动态规划算法。对于某个非叶节点，计算剪去子树后的树的loss和不剪的loss。根据loss小的选择相应动作。而loss大小的比较，可以进行局部计算。因此可用动规实现。<br>2、loss为，每棵树的所有叶子上样本的熵之和，加上惩罚项(叶子数量或者节点数量)。<br>3、本质为，正则化的极大似然函数，来选择概率模型。</p>
</li>
<li><p>CART剪枝<br>1、区别于ID3&amp;C4.5，利用了递归思想，对所生成的树进行剪枝。<br>2、对于某个中间节点，找到进行剪枝动作的阈值–a的大小，a大于阈值则剪枝。即对于每一个中间节点，都有这样的阈值thre所决定的区间[thre,正无穷），剪枝后的树优于生成树。<br>3、对于所有的最优字数，交叉验证得到最终的最优树，并得到响应的a的thre。</p>
</li>
</ul>
<br>


<h4 id="扩展问题"><a href="#扩展问题" class="headerlink" title="扩展问题"></a>扩展问题</h4><p>Q4：信息增益比的缺点？<br>偏好取值少的特征。<br>C4.5不是直接选择增益比最大的特征，而是之前先把信息增益低于均值的属性剔除，然后在剩下的特征中选择而信息增益比最大的。得到兼顾。</p>
<p>Q5：预剪枝？<br>1、对当前节点在划分时，进行估计，如果不能够提升泛化能力则不进行划分。<br>2、本质是基于贪心，对当前节点进行判断是否划分。<br>3、坏处：当前虽然不能提升泛化能力，但可能划分后子树能够提升泛化能力。导致模型欠拟合。</p>
<p>Q6：对比LR和决策树？<br>1、y的结果上，决策树是固定的几个值。<br>2、LR比决策树慢，时间复杂度高。<br>3、LR无法处理缺失值，需要赋值。并且对极端值更敏感<br>4、LR对线性关系、全局拟合较好。决策树则是局部最优、局部数据探查更细致，不能更好地对多个特征同时考量。</p>
<p>Q7：CART做了哪些简化？<br>1、log函数的计算量大，而gini由图形可知，是对熵模型的很好的近似。<br>2、CART是二叉树，对每个特征进行二分而非多分，减少了特征选择时的计算。</p>
<p>Q8：测试集上缺失值的处理？<br>1、赋平均值或出现频次最高的值<br>2、走特征的常用分支<br>3、特征值专门处理的分支<br>C4.5的方式是，探查所有的分支，然后得到每个类别的概率，取最大的赋给该样本。</p>
<p>Q9：CART对于离散特征处理、连续特征处理的不同之处？<br>1、ID3&amp;C4.5都是多叉，特征只出现在一个中间节点；CART是对离散特征不断地二分。<br>2、连续特征：遍历每个特征，对某特征找到最小化均方误差(loss)的thre点，该点作为该特征的切分点。在所有特征中找到最小的loss，得到最优的(特征，特征切分点)作为中间节点。二分后，对每个孩子继续进行最优的(特征，特征切分点)的查找。同离散特征一样，连续特征可以重复出现，作为中间节点。直到达到停止条件。<br>3、CART连续特征：换种表述：对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。<br>4、ID3不支持连续特征划分。C4.5的做法是，将连续值进行排序，取两个值的中间值作为划分点，然后算信息增益比，(连续和离散的)最大的信息增益比为该中间节点。当连续值较多时，计算量的大。并且和CART一样，该连续特征同样可以继续作为中间节点，而非和离线一样一次性地生成多个孩子分叉。</p>
<p>Q10：缺点或者优化？<br>1、分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1。<br>2、如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。</p>
<p>其他–都可以在上述问题中找到答案：<br>如何找到切分点？</p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式mindroad</title>
    <url>/2021/04/05/%E5%88%86%E5%B8%83%E5%BC%8Fmindroad/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>汇总</strong></p>
<span id="more"></span>

<h2 id="分布式系统"><a href="#分布式系统" class="headerlink" title="分布式系统"></a>分布式系统</h2><p>1、层次结构</p>
<p>2、通信协议<br>默认dobbo<br>还有rmi hessian http webservice </p>
<p>3、序列化协议<br>默认hessian<br>java二进制序列化、json、soap</p>
<p>4、hessian数据结构<br>8种原始、3种递归、一种特殊类型</p>
<p>5、pb为什么最高性能<br>1编译器 2数据压缩</p>
<p>6、负载均衡策略<br>随机–根据权重，大则流量高<br>自动感知<br>一致性hash</p>
<p>7、集群容错<br>Failover<br>Failfast<br>Failsafe<br>Failback<br>Forking<br>Broadcast</p>
<p>8、动态代理<br>动态字节码生成</p>
<p>9、spi思想及扩展dubbo组件</p>
<p>10、服务治理<br> 1接口的调用次数和耗时 TP50/90/99、调用成功率、失败的监控<br> 2全链路的次数和耗时<br> 3timeout和retry<br> 4 Mock中实现降级逻辑<br> 5 分布式服务接口的幂等：redis+插入unique key<br> 6 接口调用的顺序性：会提升系统复杂度，降低效率-热点压力。dubbo进行hash负载均衡，然后打到一台机器，将order_id相同的请求放到该机器的一个线程内存队列中。更好的策略是合并请求。</p>
<hr>
<p>11、rpc框架设计</p>
<ul>
<li>zookeeper注册中心，本地动态代理发出请求，hessian序列化、长连接协议，负载均衡，服务端动态代理监听并代理实现</li>
<li>再加上 监控、配置化</li>
</ul>
<hr>
<p>12、zookeeper使用场景<br>分布式协调、分布式锁、注册中心(元数据管理)、HA高可用</p>
<ul>
<li>zk<ul>
<li>分布式协调<ul>
<li>A系统在zk建立一个监听，当系统B从MQ消费完订单消息，将zk状态改掉，zk则通知监听的系统A</li>
</ul>
</li>
<li>分布式锁<ul>
<li>a获取锁（尝试创建临时节点），b没得到就注册监听器（zk会将变化情况反向推送b），释放则获取到</li>
<li>临时节点保证，a宕机能够把临时节点自动删除，避免死锁</li>
</ul>
</li>
<li>配置中心/元数据<ul>
<li>dubbo的服务端地址都放到zk，消费从zk获取最新的地址，服务地址改动，zk及时地变化</li>
</ul>
</li>
<li>高可用<ul>
<li>a宕机了，删除zk的临时节点，监听的b及时发现则插入新的临时节点，a恢复发现有了b临时节点则注册监听</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>13、分布式锁及其效率<br>三点： 互斥、无死锁(业务意外情况下仍然可以获取锁)、高可用(锁的机子宕了，能够继续获取锁)</p>
<ul>
<li><p>zk</p>
<ul>
<li>while +  countdownlatch–await卡着 + 临时节点</li>
<li>临时顺序节点更好</li>
<li>创建临时的znode，别的客户端注册监听器，释放锁就是删除znode，释放了就会通知客户端</li>
</ul>
</li>
<li><p>redis<br>1如何保证原子性：lua<br>2业务时间&gt;定期删除时间：守护线程延长定期时间并设置超时放弃<br>3锁的误删除：uuid+threadid<br>4防止死锁：key加上过期时间 setnx expire</p>
</li>
<li><p>set orderic:lock 随机值 nx 3000 （nx代表无锁才能set返回ok），其他nil的节点每隔1s看是否还能set成功</p>
</li>
<li><p>释放：lua获取value对比是否是自己的，如果是则删除，否则不处理</p>
</li>
<li><p>redis主从架构时，由于异步更新到从，可能导致set的分布式锁失效</p>
</li>
</ul>
<p>5redis集群(三种:主从、哨兵、cluster)无法满足高可用：主从是为了读写分离，锁还是在master上面。哨兵机制的集群，当master上的锁没有同步到slave时，此时有加锁请求则仍能够获得锁，不满足互斥性。三主三从的cluster模式(hash槽分配)，超过了10w+的并发，则一个主的槽不可用，则该redis集群全不可用。</p>
<p>6时间漂移：硬件解决</p>
<p>7 redlock：多个实例，但是不是集群关系。锁的有效时间= ttl-时间漂移(所有实例加锁花费的时间)。</p>
<p>获取锁只需要超过一半的实例获取成功。避免死锁：重试机制-加锁不成功则撤回请求(避免死锁)，并且在重试时加上随机时间，避免同时加锁的请求都重试结果都无法获得。避免不互斥：延迟重启-如果master挂了，延迟ttl时间对slave重启替换为master。<br>TTL&gt; 业务执行时间+redis加锁时间+时钟漂移<br>删除锁则所有实例都执行删除。</p>
<ul>
<li>redlock 不健壮、无效请求<ul>
<li>至少一半以上节点set成功且在超时时间内，才算创建成功。否则说明失败，通过lua删除已经set的节点。</li>
<li>没成功就每隔1s去看是否被释放</li>
</ul>
</li>
</ul>
<hr>
<p>14、分布式接口幂等性</p>
<ul>
<li>数据库unique key</li>
<li>redis ：set orderid payed</li>
</ul>
<hr>
<p>15、分布式接口顺序性</p>
<ul>
<li>接入服务进行分发到(hash分发)不同机子，然后机子分发到一个线程的内存队列<br>  或者接入服务分发到MQ，然后其他机子从MQ拉消费</li>
<li>但如果接入服收到的顺序不是你要的，那还是无法完全保证顺序</li>
<li>完全保证顺序<ul>
<li>分布式锁<ul>
<li>不仅以orderid标识，还需要有标识请求序号的seq。<br>  请求先获取zookeeper锁，查库，如果seq不对则释放锁。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>16、分布式会话</p>
<ul>
<li>spring session + Redis，解耦web容器</li>
</ul>
<hr>
<p>17、分布式事务</p>
<ul>
<li><p>事务ACID </p>
<ul>
<li>全部结束或者回滚、和5000转账不影响和、并发事务相互不影响(脏读(看到另一个事务的中间状态)、commit读、重复读、串行化)、宕机之后恢复数据和事务成功结束后一致</li>
</ul>
</li>
<li><p>CAP </p>
<ul>
<li>获得准确数据、及时响应返回、单节点挂了不影响其他</li>
<li>一般选择AP – 柔性事务 base理论，达到最终一致。CA是一个必须有分布式锁，一个可以没有</li>
</ul>
</li>
<li><p>2pc</p>
<ul>
<li>依赖数据库、不适合高并发</li>
<li>第一阶段，每个节点记录log，并且返回给总协调是否成功，第二阶段，有一个不成功则协调让所有的人回滚。</li>
<li>SEATA <ul>
<li>第一阶段各个节点就提交然后释放锁，第二阶段如果成功则删除log，否则按照log回滚。（全局事务id和分支事务id，定位到log）</li>
</ul>
</li>
</ul>
</li>
<li><p>tcc</p>
<ul>
<li>转账的例子</li>
<li>严格、业务代码繁琐</li>
<li>try 系统a预留系统bc资源，锁</li>
<li>confirm  rpc调用系统b扣减，调用系统c转账，本地记下log</li>
<li>cancel 回滚</li>
</ul>
</li>
<li><p>可靠消息最终一致性</p>
<ul>
<li>本地消息表<ul>
<li>todo</li>
</ul>
</li>
<li>rocketmq事务消息</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式事务</title>
    <url>/2021/02/04/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-%E4%B8%8A/</url>
    <content><![CDATA[<p><strong>摘要</strong></p>
<p>1、什么是事务、分布式事务，分布式事务的场景<br>2、CAP理论，及BASE理论，柔性事务</p>
<span id="more"></span>
<p>3、分布式事务解决模型:2PC TCC （概念、区别）<br>4、2PC解决方案：XA、AT（角色、流程、实战、缺点、区别）<br>5、TCC解决方案：三种异常处理、Hmily实现</p>
<h2 id="1、事务、分布式事务及场景"><a href="#1、事务、分布式事务及场景" class="headerlink" title="1、事务、分布式事务及场景"></a>1、事务、分布式事务及场景</h2><ul>
<li><p>一般利用数据可事务特性，为数据库事务。数据库和应用在一个服务器，则为本地事务。</p>
</li>
<li><p>事务则需要有ACID性质。</p>
<ul>
<li>原子性、一致性、隔离性(一个事务不能看到其他事务的中间状态)、持久性</li>
</ul>
</li>
<li><p>而分布式事务则和分布式架构相关，当服务被拆分，并通过网络进行协作，则一个事务就涉及到多个服务以及远程调用。此时为分布式事务。</p>
</li>
<li><p>场景：</p>
<ul>
<li>微服务架构中，即需要跨JVM进程。无论是多个服务访问一个实例，还是多个服务多个实例，本地事务都无法解决。</li>
<li>单个系统访问多个数据库实例，就需要进行不同的数据库连接</li>
</ul>
</li>
</ul>
<br>

<h2 id="2、CAP"><a href="#2、CAP" class="headerlink" title="2、CAP"></a>2、CAP</h2><h3 id="分布式事务控制目标"><a href="#分布式事务控制目标" class="headerlink" title="分布式事务控制目标"></a>分布式事务控制目标</h3><p>场景：mysql一主一从，商品写主读从。</p>
<p>一致性、可用性、分区容错性</p>
<ul>
<li><p>consistency：写操作后的读(任意节点读)可以读到最近状态。</p>
<ul>
<li>在从同步数据的过程中，将从锁住，同步完再允许查询。 需要1写响应有延迟；2资源锁定与释放；3同步失败则返回错误信息而不能返回旧数据</li>
</ul>
</li>
<li><p>availability：任何事务操作都可以得到响应，且不会响应错误或超时。</p>
<ul>
<li>需要能够立即响应并非错误、非超时，可以允许旧数据。需要1同步时不能锁定；2返回旧数据或者默认值</li>
</ul>
</li>
<li><p>partitio tolerance：分布式各个节点在不同的子网，就形成了网络分区，彼此需要通过网络进行交互，当网络通信失败时，仍能够提供服务。</p>
<ul>
<li>单个节点挂了不影响其他节点，同步时不影响读写操作。需要1添加主备从备节点、避免主或从挂了；2异步进行数据从主到从的同步</li>
</ul>
</li>
</ul>
<blockquote>
<p>三个特性不能共存。一般选择AP，达到最终一致性即可。</p>
</blockquote>
<ul>
<li>AP，通常实现AP都会保证最终一致性</li>
<li>CP，zookeeper追求的就是强一致</li>
<li>CA，不进行分区，本地事务隔离级别即可。</li>
</ul>
<br>

<h2 id="BASE理论与柔性事务"><a href="#BASE理论与柔性事务" class="headerlink" title="BASE理论与柔性事务"></a>BASE理论与柔性事务</h2><p>在AP中，满足1 基本可用 2 软状态 3 最终一致。即满足base，为柔性事务。</p>
<ul>
<li>软状态: 中间状态– 当同步过程中来了查询，给出“支付中”状态，一致后再返回“成功”。</li>
</ul>
<br>

<h2 id="3、2PC"><a href="#3、2PC" class="headerlink" title="3、2PC"></a>3、2PC</h2><h3 id="3-1-2PC概念"><a href="#3-1-2PC概念" class="headerlink" title="3.1 2PC概念"></a>3.1 2PC概念</h3><ul>
<li><p>两阶段提交协议，prepare准备阶段和commit提交阶段。</p>
</li>
<li><p>包含 事务管理器和事务参与者(数据库实例)。管理器决定整个事务的提交和回滚，参与者负责本地事务的提交和回滚。</p>
</li>
<li><p>过程：</p>
<ul>
<li>prepare： 管理器向每个实例发送prepare消息，每个实例写本地的undo(修改前的数据)和redo(修改后的数据)日志。此时没有提交。</li>
<li>commit： 管理器收到参与者的失败或超时，则向每个参与者发送rollback消息。否则向每个实例发送commit。每个参与者进行执行指令并释放锁。</li>
</ul>
</li>
</ul>
<br>

<h3 id="3-2-2PC解决方案"><a href="#3-2-2PC解决方案" class="headerlink" title="3.2 2PC解决方案"></a>3.2 2PC解决方案</h3><h4 id="3-2-1-XA方案"><a href="#3-2-1-XA方案" class="headerlink" title="3.2.1 XA方案"></a>3.2.1 XA方案</h4><p>规范数据库实现2pc协议的分布式事务处理模型DTP。<br>定义了角色：AP RM TM,TM 和 RM 通讯接口为XA。即数据库提供的2pc接口协议，基于该协议的2pc实现为xa方案。</p>
<p>缺点：<br>1、资源锁需要事务的两个阶段结束才能释放<br>2、本地数据库需要支持XA协议<br><br></p>
<h4 id="3-2-2-Seata方案"><a href="#3-2-2-Seata方案" class="headerlink" title="3.2.2 Seata方案"></a>3.2.2 Seata方案</h4><p>Seata是提供AT和TCC模式的分布式事务解决方案。</p>
<ul>
<li><p>与XA区别：</p>
<ul>
<li>1、XA是两阶段后释放锁，而AT模式(2pc)第一阶段则提交释放锁</li>
<li>2、AT是应用层的中间件，对业务0侵入。</li>
</ul>
</li>
<li><p>实现：</p>
<ul>
<li>三个角色，TC TM RM。</li>
<li>TC负责，接收TM的全局事务提交或回滚指令，和RM通信协调分支事务。</li>
<li>TM，开启全局事务，向TC发出提交或回滚指令。</li>
<li>RM，分支注册、状态汇报、接收TC指令、驱动本地事务提交或回滚的执行。</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>具体的执行流程如下:</li>
</ul>
<ol>
<li>用户服务的 TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的XID。</li>
<li>用户服务的 RM 向 TC 注册 分支事务，该分支事务在用户服务执行新增用户逻辑，并将其纳入 XID 对应全局<br>事务的管辖。</li>
<li>用户服务执行分支事务，向用户表插入一条记录。</li>
<li>逻辑执行到远程调用积分服务时(XID 在微服务调用链路的上下文中传播)。积分服务的RM 向 TC 注册分支事<br>务，该分支事务执行增加积分的逻辑，并将其纳入 XID 对应全局事务的管辖。</li>
<li>积分服务执行分支事务，向积分记录表插入一条记录，执行完毕后，返回用户服务。</li>
<li>用户服务分支事务执行完毕。</li>
<li>TM 向 TC 发起针对 XID 的全局提交或回滚决议。</li>
<li>TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。</li>
</ol>
<hr>
<ul>
<li>详解流程：<ul>
<li>每个RM使用DataSourceProxy连接数据库，其目的是使用ConnectionProxy，使用数据源和数据连接代理的目 的就是在第一阶段将undo_log和业务数据放在一个本地事务提交，这样就保存了只要有业务操作就一定有 undo_log。</li>
<li>在第一阶段undo_log中存放了数据修改前和修改后的值，为事务回滚作好准备，所以第一阶段完成就已经将分 支事务提交，也就释放了锁资源。</li>
<li>TM开启全局事务开始，将XID全局事务id放在事务上下文中，通过feign调用也将XID传入下游分支事务，每个 分支事务将自己的Branch ID分支事务ID与XID关联。</li>
<li>第二阶段全局事务提交，TC会通知各各分支参与者提交分支事务，在第一阶段就已经提交了分支事务，这里各各参与者只需要删除undo_log即可，并且可以异步执行，第二阶段很快可以完成。</li>
<li>第二阶段全局事务回滚，TC会通知各各分支参与者回滚分支事务，通过 XID 和 Branch ID 找到相应的回滚日志，通过回滚日志生成反向的 SQL 并执行，以完成分支事务回滚到之前的状态，如果回滚失败则会重试回滚操作。</li>
</ul>
</li>
</ul>
<ul>
<li><p>实战step：</p>
<ul>
<li>下载，解压并启动seata服务器 /bin/seata-server.bat -p 8888 -m file 。端口和文件方式存储信息。(TC)</li>
<li>添加discover-server子模块，discover-server基于Eureka实现。</li>
<li>添加微服务子模块，子模块pom中引入spring-cloud-alibaba-seata（包含了RM TM），并配置TC的地址：registry.conf、file.conf中：</li>
</ul>
<blockquote>
<p>在file.conf中更改service.vgroup_mapping.[springcloud服务名]-fescar-service-group = “default”，并修改 service.default.grouplist =[seata服务端地址]</p>
</blockquote>
<ul>
<li>创建代理数据源（配置mysql连接）</li>
</ul>
<blockquote>
<p>Seata的RM通过DataSourceProxy才能在业务代码的事务提交时，通过这个切<br>  入点，与TC进行通信交互、记录undo_log等。每个RM使用DataSourceProxy连接数据库，其目的是使用ConnectionProxy，使用数据源和数据连接代理的目 的就是在第一阶段将undo_log和业务数据放在一个本地事务提交，这样就保存了只要有业务操作就一定有 undo_log。</p>
</blockquote>
<ul>
<li>代码部分细节：<code>FeignClient</code>、<code>@GlobalTransactional</code>、<code>@Transactional</code></li>
</ul>
<blockquote>
<p>将@GlobalTransactional注解标注在全局事务发起的Service实现方法上，开启全局事务: GlobalTransactionalInterceptor会拦截@GlobalTransactional注解的方法，生成全局事务ID(XID)，XID会在整个分布式事务中传递。 在远程调用时，spring-cloud-alibaba-seata会拦截Feign调用将XID传递到下游服务。</p>
</blockquote>
</li>
</ul>
<br>

<h3 id="4、TCC"><a href="#4、TCC" class="headerlink" title="4、TCC"></a>4、TCC</h3><h4 id="4-1-TCC相关解决方案"><a href="#4-1-TCC相关解决方案" class="headerlink" title="4.1 TCC相关解决方案"></a>4.1 TCC相关解决方案</h4><p>tcc-transaction、<br>Hmily、<br>ByteTcc、<br>EasyTransaction</p>
<br>

<h4 id="4-2-TCC的三种异常处理"><a href="#4-2-TCC的三种异常处理" class="headerlink" title="4.2 TCC的三种异常处理"></a>4.2 TCC的三种异常处理</h4><ol>
<li><strong>空回滚</strong><br> 没有执行try就执行了cancel</li>
<li><strong>幂等</strong><br> cancel和commit会重试</li>
<li><strong>悬挂</strong><br> RPC 调用分支事务try时，先注册分支事务，再执行RPC调用，如果此时 RPC 调用的网络发生拥堵， 通常 RPC 调用是有超时时间的，RPC 超时以后，TM就会通知RM回滚该分布式事务，可能回滚完成后，RPC 请求 才到达参与者真正执行，而一个 Try 方法预留的业务资源，只有该分布式事务才能使用，该分布式事务第一阶段预 留的业务资源就再也没有人能够处理了，对于这种情况，我们就称为悬挂，即业务资源预留后没法继续处理。</li>
</ol>
<p>转账最终方案：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;A</span><br><span class="line">try: </span><br><span class="line">	try幂等校验</span><br><span class="line">	try悬挂处理 </span><br><span class="line">	检查余额是否够30元 </span><br><span class="line">	扣减30元</span><br><span class="line">confirm: </span><br><span class="line">	空</span><br><span class="line">cancel: </span><br><span class="line">	cancel幂等校验</span><br><span class="line">	cancel空回滚处理 </span><br><span class="line">	增加可用余额30元</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;B</span><br><span class="line">try: </span><br><span class="line">	空</span><br><span class="line">confirm: </span><br><span class="line">	confirm幂等校验</span><br><span class="line">	正式增加30元 </span><br><span class="line">cancel:</span><br><span class="line">	空</span><br></pre></td></tr></table></figure>

<h4 id="4-3-Hmily实现TCC事务"><a href="#4-3-Hmily实现TCC事务" class="headerlink" title="4.3 Hmily实现TCC事务"></a>4.3 Hmily实现TCC事务</h4><ul>
<li><p>新增配置类接收application.yml中的Hmily配置信息，并创建HmilyTransactionBootstrap Bean</p>
</li>
<li><p>A账户</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;DAO 略</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;service </span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">    @Transactional</span><br><span class="line">    @Hmily(confirmMethod &#x3D; &quot;commit&quot;, cancelMethod &#x3D; &quot;rollback&quot;)</span><br><span class="line">	public  void updateAccountBalance(String accountNo, Double amount) &#123;</span><br><span class="line">		&#x2F;&#x2F;事务id</span><br><span class="line">		&#x2F;&#x2F;try幂等校验</span><br><span class="line">		&#x2F;&#x2F;try悬挂处理</span><br><span class="line">		&#x2F;&#x2F;从账户扣减及扣减失败</span><br><span class="line">		&#x2F;&#x2F;增加本地事务try成功记录，用于幂等性控制标识 accountInfoDao.addTry(transId);</span><br><span class="line">		&#x2F;&#x2F;远程调用bank2</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;feign</span><br><span class="line"></span><br><span class="line">	@FeignClient(value &#x3D; &quot;seata‐demo‐bank2&quot;, fallback &#x3D; Bank2Fallback.class)</span><br><span class="line">	public interface Bank2Client &#123;</span><br><span class="line">		@GetMapping(&quot;&#x2F;bank2&#x2F;transfer&quot;)</span><br><span class="line">		@Hmily</span><br><span class="line">		Boolean transfer(@RequestParam(&quot;amount&quot;) Double amount);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;controller 略</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>B账户 略</li>
</ul>
<h4 id="4-4-TCC与2PC对比"><a href="#4-4-TCC与2PC对比" class="headerlink" title="4.4 TCC与2PC对比"></a>4.4 TCC与2PC对比</h4><ol>
<li>2PC通常在DB层面，TCC在应用层面</li>
<li>2PC无侵入性，TCC自定义数据操作粒度，锁冲突降低，提高吞吐，但业务逻辑每个分支都需要实现TCC三个方法，且需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式事务-下</title>
    <url>/2021/02/04/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-%E4%B8%8B/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>摘要</strong></p>
<p>1、可靠消息最终一致性，概念特点、问题、针对问题的方案(本地消息表、Rocketmq)<br>2、本地消息表流程图<br>3、Rocketmq流程、实战<br>4、最大努力通知，流程、</p>
<span id="more"></span>

<h2 id="可靠消息最终一致性"><a href="#可靠消息最终一致性" class="headerlink" title="可靠消息最终一致性"></a>可靠消息最终一致性</h2><p>当事务发起方执行完成本地事务后并发出一条消息，事务参与方(消息消费者)一定能 够接收消息并处理事务成功，此方案强调的是只要消息发给事务参与方最终事务要达到一致。</p>
<ul>
<li><p>特点</p>
<ul>
<li>适合执行周期长且实时性要求不高的场景</li>
<li>引入消息机制后，同步的事务操作变为基于消 息执行的异步操作, 避免了分布式事务中的同步阻塞操作的影响，并实现了两个服务的解耦。</li>
</ul>
</li>
<li><p>网络通信的不确定性会导致分布式事务问题：</p>
</li>
</ul>
<ol>
<li>事务发起方(消息生产方)将消息发给消息中间件</li>
<li>事务参与方从消息中间件接收消息</li>
</ol>
<h3 id="可靠消息最终一致性的问题"><a href="#可靠消息最终一致性的问题" class="headerlink" title="可靠消息最终一致性的问题"></a>可靠消息最终一致性的问题</h3><p>1、本地事务与消息发送的原子性问题</p>
<ul>
<li>发送消息成功，数据库操作失败  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">begin transaction; </span><br><span class="line">		&#x2F;&#x2F;1.发送MQ</span><br><span class="line">		&#x2F;&#x2F;2.数据库操作 </span><br><span class="line">commit transation;</span><br></pre></td></tr></table></figure></li>
<li>如果发送MQ消息失败，就会抛出异常，导致数据库事务回滚。但如果是超时异常，数 据库回滚，但MQ其实已经正常发送了，同样会导致不一致。  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">begin transaction; </span><br><span class="line">	&#x2F;&#x2F;1.数据库操作</span><br><span class="line">	&#x2F;&#x2F;2.发送MQ </span><br><span class="line">commit transation;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>2、事务参与方接收消息的可靠性，需要如果接收消息失败可以重复接收消息</p>
<p>3、消息重复消费的问题，要解决消息重复消费的问题就要实现事务参与方的方法幂等性。 </p>
<h2 id="本地消息表方案"><a href="#本地消息表方案" class="headerlink" title="本地消息表方案"></a>本地消息表方案</h2><p><img src="/2021/02/04/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-%E4%B8%8B/bendixiaoxibiao.png" alt="本地消息表方案"></p>
<ul>
<li>交互流程规范</li>
</ul>
<ol>
<li>用户服务在本地事务新增用户和增加 ”积分消息日志“。(用户表和消息表通过本地事务保证一致)</li>
<li>定时任务扫描日志–&gt;保证将消息发送给消息队列。启动独立的线程，定时对消息日志表中的消息进行扫描并发送至消息中间件，在消息中间件反馈发送成功后删除该消息日志，否则等待定时任务下一周期重试。</li>
<li>MQ的ack(即消息确认)机制–&gt;幂等，收到ack，MQ将不再向消费者推送消息，否则消费者会不断重 试向消费者来发送消息。</li>
</ol>
<h2 id="RocketMQ事务消息方案"><a href="#RocketMQ事务消息方案" class="headerlink" title="RocketMQ事务消息方案"></a>RocketMQ事务消息方案</h2><ul>
<li>交互流程实现</li>
</ul>
<ol>
<li>Producer (MQ发送方)发送事务消息至MQ Server，MQ Server将消息状态标记为Prepared(预备状态)，注意此时这条消息消费者(MQ订阅方)是无法消费到的。</li>
<li>MQ Server接收到Producer 发送给的消息则回应发送成功表示MQ已接收到消息。</li>
<li>Producer 端执行业务代码逻辑，通过本地数据库事务控制。</li>
<li>若Producer 本地事务执行成功则自动向MQServer发送commit消息，MQ Server接收到commit消息后将状态标记为可消费，此时MQ订阅方(积分服务)即正常消费消息。若Producer 本地事务执行失败则自动向MQServer发送rollback消息，MQ Server接收到rollback消息后 将删除消息。</li>
<li>如果执行Producer端本地事务过程中，执行端挂掉或者超时，MQ Server将会不停的询问同组的其他 Producer来获取事务执行状态，这个过程叫事务回查。MQ Server会根据事务回查结果来决定是否投递消息。 </li>
</ol>
<br>

<ul>
<li>以上主干流程已由RocketMQ实现，对用户侧来说，用户需要：</li>
</ul>
<ol>
<li>实现本地事务执行</li>
<li>本地事务回查方法，因此关注本地事务的执行状态</li>
</ol>
<h3 id="RocketMq事务"><a href="#RocketMq事务" class="headerlink" title="RocketMq事务"></a>RocketMq事务</h3><p>RocketMQ主要解决了两个功能:<br>1、本地事务与消息发送的原子性问题。<br>2、事务参与方接收消息的可靠性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;回调</span><br><span class="line"></span><br><span class="line"> public interface RocketMQLocalTransactionListener &#123;</span><br><span class="line">      &#x2F;**</span><br><span class="line">		‐ 发送prepare消息成功此方法被回调，该方法用于执行本地事务</span><br><span class="line">		‐ @param msg 回传的消息，利用transactionId即可获取到该消息的唯一Id</span><br><span class="line">		‐ @param arg 调用send方法时传递的参数，当send时候若有额外的参数可以传递到send方法中，这里能获取到</span><br><span class="line">		‐ @return 返回事务状态，COMMIT:提交 ROLLBACK:回滚 UNKNOW:回调</span><br><span class="line">		*&#x2F;</span><br><span class="line">          RocketMQLocalTransactionState executeLocalTransaction(Message msg, Object arg);</span><br><span class="line"></span><br><span class="line">      	&#x2F;**</span><br><span class="line">		‐ @param msg 通过获取transactionId来判断这条消息的本地事务执行状态</span><br><span class="line">		‐ @return 返回事务状态，COMMIT:提交 ROLLBACK:回滚 UNKNOW:回调</span><br><span class="line">		*&#x2F;</span><br><span class="line">          RocketMQLocalTransactionState checkLocalTransaction(Message msg);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> &#x2F;&#x2F; 发送事务消息API  </span><br><span class="line">TransactionMQProducer producer &#x3D; new TransactionMQProducer(&quot;ProducerGroup&quot;); producer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;);</span><br><span class="line">producer.start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;设置TransactionListener实现 producer.setTransactionListener(transactionListener);</span><br><span class="line">&#x2F;&#x2F;发送事务消息</span><br><span class="line">SendResult sendResult &#x3D; producer.sendMessageInTransaction(msg, null);  </span><br></pre></td></tr></table></figure>


<ul>
<li><p>实践：</p>
<ul>
<li>在application-local.propertis中配置rocketMQ nameServer地址及生产组</li>
<li>service</li>
</ul>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;从event构建消息体</span><br><span class="line">	public void sendUpdateAccountBalance(AccountChangeEvent accountChangeEvent) </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;发送消息并收到回应后执行本地事务</span><br><span class="line">public void doUpdateAccountBalance(AccountChangeEvent accountChangeEvent)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;实现执行本地事务和事务回查两个方法</span><br><span class="line"> 	@RocketMQTransactionListener(txProducerGroup &#x3D; &quot;producer_group_txmsg_bank1&quot;)</span><br><span class="line"> 	public class ProducerTxmsgListener implements RocketMQLocalTransactionListener </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>controller 略</li>
<li>B账户的MQ 监听类</li>
</ul>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;需要实现幂等</span><br><span class="line">@RocketMQMessageListener(topic &#x3D; &quot;topic_txmsg&quot;,consumerGroup &#x3D; &quot;consumer_txmsg_group_bank2&quot;)</span><br><span class="line">public class TxmsgConsumer implements RocketMQListener&lt;String&gt; </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="最大努力通知"><a href="#最大努力通知" class="headerlink" title="最大努力通知"></a>最大努力通知</h2><ul>
<li>流程</li>
</ul>
<ol>
<li></li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>利率</title>
    <url>/2021/05/09/%E5%88%A9%E7%8E%87/</url>
    <content><![CDATA[<br>



<h3 id="利率"><a href="#利率" class="headerlink" title="利率"></a>利率</h3><h5 id="定义与分类"><a href="#定义与分类" class="headerlink" title="定义与分类"></a>定义与分类</h5><span id="more"></span>

<p>定义：金融资产的到期收益率</p>
<p>别名：贴现率、要求回报率</p>
<br>

<p>构成： 无风险收益率+风险溢价(系统性风险的回报) </p>
<p>风险构成： <code>期限、违约(信用)、流动性、通胀</code></p>
<p>结构分析：利率期限结构、利率风险结构(控制期限变量分析)</p>
<br>

<p>分类：官定/市场，基准利率/一般利率，名义利率/实际利率、固定利率/浮动利率</p>
<br>

<p>其他：</p>
<p>​    违约评级机构：标普、穆迪、惠誉</p>
<p>​    国家信用：税收、债务货币化(内债)、政局稳定性</p>
<p>​    经济衰退-&gt;违约风险溢价上升</p>
<p>​    流动性风险：交易佣金、买卖价差(做市商 <code>市场交易的活跃度</code> 影响来bid 和 ask)、市场冲击(<code>市场规模</code>大则冲击压力越小) …</p>
<p>​    税后收益率 = 税前收益率 * (1- 税率) ，税收优惠的债券，可以接受更低的税前收益率</p>
<br>

<h5 id="利率决定论"><a href="#利率决定论" class="headerlink" title="利率决定论"></a>利率决定论</h5><br>

<p>1、古典利率决定理论(实际利率决定理论)</p>
<p>储蓄和利率正相关  投资和利率负相关 投资和储蓄决定均衡利率(产品市场出清)</p>
<p>货币只是交易媒介，不影响经济行为。货币和商品的二元无关论。</p>
<br>



<p>2、凯恩斯利率决定理论(流动性偏好理论)</p>
<p>货币供给与利率无关 货币需求与利率成负相关 货币需求和货币供给决定均衡利率(货币市场出清)</p>
<p>预防、交易、投机都影响货币需求。</p>
<p>古典的不成立：均衡利率取决于S位置，S取决于收入，收入取决于I，I取决于均衡利率。</p>
<p>延期消费带来利息不成立：等待或者延期不能带来利息，而是 流动性，也即放弃周转的灵活换来利息。</p>
<br>



<p>3、可贷资金理论</p>
<p>可贷资金的需求包括投资和新增货币需求，与利率负相关</p>
<p>可贷资金的供给包括储蓄和新增货币供给，与利率正相关</p>
<p>可贷资金供求决定均衡利率(可贷资金市场出清)</p>
<p>从微观出发，怎样保护自己的资产。</p>
<br>

<p>和流动性偏好，相似之处：</p>
<br>



<p>4、IS-LM 模型</p>
<br>

<h5 id="一般影响因素"><a href="#一般影响因素" class="headerlink" title="一般影响因素"></a>一般影响因素</h5><br>

<p>长期因素、短期因素；行为主体；国内国际；模型中 经济或金融变量(P I S C π e Ms Md …)；供需两面；</p>
<br>

<p>财政支出(预算赤字↑) - 总投资↑ - 货币需求↑ </p>
<p>国债↑ - 国债价格↓ 利率↑</p>
<p>放水(准备金率、回购) - 货币供给↑ - 名义利率↓</p>
<p>疫情 - 分情况看</p>
<p>价格↑ - 货币需求↑ -（货币供给不变时）名义利率↑</p>
<p>收入↑ - 货币需求↑ -（货币供给不变时）名义利率↑</p>
<p>通胀预期↑ - 现金持有↓ - 购买国债或其他权益证券↑ - 利率↓ ； 通胀预期↑ - (实际利率不变时)名义利率↓</p>
<p>经济上行(边际消费倾向、投资) ↑- (收入水平给定时)储蓄(货币供给)↓ - 资金成本↑ ，实际利率↑ - (通胀不变)名义利率↑</p>
<br>

<p>结论：利率是顺周期变量</p>
<p>…</p>
<h5 id="利率市场化改革"><a href="#利率市场化改革" class="headerlink" title="利率市场化改革"></a>利率市场化改革</h5><p>why</p>
<br>

<p>利率传导效率、货币政策价格型转变(对基准利率的可控性、可测性；货币供应量为中介目标影响越来越弱)、信贷配给的腐败(银行国企盆满锅满 – LPR倒逼)、资源配置效率低下 … </p>
<p>金融与互联网的结合，更是利率市场化和金融脱媒的推手。人们越来越难以忍受这种双轨制的损失。</p>
<p>利率是市场经济中最核心也是最主要的<code>生产要素的价格</code>。<code>提高生产力的办法之一就是提高生产要素的供给，降低生产要素的价格。</code>这就是利率市场化的本质原因。</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>前端</title>
    <url>/2021/07/20/%E5%89%8D%E7%AB%AF/</url>
    <content><![CDATA[<br>

<br>

<h4 id="路线"><a href="#路线" class="headerlink" title="路线"></a>路线</h4><p>HTML CSS3 JS</p>
<span id="more"></span>

<p>三个基础后面开框架：JQuery 、 Vue、 React、Angular</p>
<p>UI库：bootstrap 、 element…</p>
<p>构建平台：Node.js(服务器上运行的js环境)及其优化Expressjs、npm(下载各种依赖)</p>
<p>打包工具：webpack、grunt、gulp</p>
<p>高阶：css预编译SCSS、 LESS；js的超集TypeScript；Vuex；Ajax优化异步处理… HTTP协议…uni-app… </p>
<p>Flutter的ui…</p>
<br>

]]></content>
  </entry>
  <entry>
    <title>协同过滤</title>
    <url>/2021/04/05/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/</url>
    <content><![CDATA[<ul>
<li><p>user based</p>
<span id="more"></span>
<ul>
<li>有一个行为user，列为item的打分矩阵，且得到每个用户的平均分</li>
<li>对于矩阵中的空值：<br>  通过计算用户a和用户b\c\d等(对itemb有分数的)相似度(皮尔逊相关系数)，然后<br>  根据均分，计算出用户a对itemb的分数</li>
</ul>
</li>
<li><p>item based</p>
<ul>
<li>由一条条访问记录得到一个个user*item的矩阵</li>
<li>上述所有矩阵加和，得到item*item的共现矩阵</li>
<li>根据共现矩阵，计算item和item的相似度，得到相似度的item*item矩阵</li>
<li>item*item的相似度共现矩阵和某用户的浏览序列相乘，得到该用户的打分向量</li>
</ul>
</li>
</ul>
<ul>
<li><a href="https://www.jianshu.com/p/5463ab162a58">协同过滤</a></li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>ml</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>协程</title>
    <url>/2021/04/05/%E5%8D%8F%E7%A8%8B/</url>
    <content><![CDATA[<p>协程、多核与cpu</p>
<span id="more"></span>

<ul>
<li>协程<ul>
<li>从隔离、通信、调度、切换开销角度分析</li>
<li>进程<ul>
<li>资源完全隔离、进程挂了不影响其他进程，通信需要Inter-Process Communication，有七中状态，os进行调度</li>
</ul>
</li>
<li>线程<ul>
<li>每个进程可以有多个线程，共享进程资源、共享全局变量，但会引起多线程并发问题，需要锁机制，通信需要wait signal</li>
<li>切换<ul>
<li>进程切换与线程切换的一个最主要区别就在于进程切换涉及到虚拟地址空间的切换而线程切换则不会。线程共享虚拟地址空间。</li>
<li>和进程一样涉及到内核状态、硬件上下文的切换</li>
</ul>
</li>
<li>线程池里一个线程挂了<ul>
<li>当执行方式是execute时,可以看到堆栈异常的输出。</li>
<li>当执行方式是submit时,堆栈异常没有输出。但是调用Future.get()方法时，可以捕获到异常。</li>
<li>不会影响线程池里面其他线程的正常执行。</li>
<li>线程池会把这个线程移除掉，并创建一个新的线程放到线程池中。</li>
</ul>
</li>
</ul>
</li>
<li>协程<ul>
<li>协程是属于线程的。协程程序是在线程里面跑的，因此协程又称微线程和纤程等.协没有线程的上下文切换消耗</li>
<li>不涉及内核，完全在用户态进行，不用锁</li>
<li>如python中的yield</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>多核的数据传递、总线</p>
<ul>
<li>不管别的CPU私有Cache是否缓存相同的数据，都需要发出一次广播事件。这在一定程度上加重了总线负载，也增加了读写延迟。<br>针对该问题，提出了一种状态机机制降低带宽压力。这就是MESI protocol（协议）。</li>
<li>cache line具有4中状态，分别是Modified、Exclusive、Shared和Invalid。<br>当cache line状态是Modified或者Exclusive状态时，修改其数据不需要发送消息给其他CPU</li>
<li>多核Cache一致性由硬件保证，对软件来说是透明</li>
<li>MOESI Protocol。多一种Owned状态。多出来的状态也是为了更好的优化性能。</li>
</ul>
</li>
<li><p>多cpu和单cpu多核</p>
<ul>
<li><a href="https://www.zhihu.com/question/20998226">cpu</a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>回溯</title>
    <url>/2021/04/13/%E5%9B%9E%E6%BA%AF/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>学习模型的模式</title>
    <url>/2021/02/21/%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<p>伪代码的算法流程 + 数学逻辑 –&gt;才能真的掌握住这个模型的本质。<br>而其他都是表象。所以数学肯定是要啃的。矩阵论和概率统计，微积分，必然是需要的。<br>更多的应用和实践，会深化对于模型的认知。</p>
<span id="more"></span>

<p>基本要求：<br>1、历史背景及演进：解决什么问题而生<br>2、伪代码表述流程<br>3、前提、局限、适用场景<br>4、演化和可优化之处、优化思路<br>5、和其他模型对比</p>
<p>进阶要求：<br>1、根据伪代码思路，看具体实现<br>2、为什么会有这样的前提和局限–&gt;数学证明：唯一性证明、误差上界证明、loss函数证明、模型参数的迭代公式证明</p>
<p>炉火纯青：<br>1、当我问你某个公式，你能够给出，并给出推导过程<br>2、和其他模型相似问题下的解决方案(公式)对比</p>
]]></content>
      <categories>
        <category>模型方法论</category>
      </categories>
      <tags>
        <tag>方法论</tag>
      </tags>
  </entry>
  <entry>
    <title>对偶</title>
    <url>/2021/03/11/%E5%AF%B9%E5%81%B6/</url>
    <content><![CDATA[<br>


<ul>
<li>对于凸形<br>结论1：凸形可行域只有1个峰，只要达到那个峰，我们就达到了最优，是全局最优。<br>结论2：至少有一个顶点是峰。</li>
</ul>
<span id="more"></span>
<ul>
<li><p>单纯形法：</p>
<ol>
<li>单纯形是什么？数学上可以写成一堆线性不等式限制出来的区域。</li>
<li>单纯形法，仅适用于求解线性规划，线性规划又是凸优化的一种。因为线性规划的定义域是单纯形，单纯形是凸的，即线性规划是定义域为凸、目标函数为线性的问题。</li>
<li>先找到一个顶点，然后从这个顶点，沿着某条边线，走到下一个顶点，直到最优。方向的选择可以有很多种，最多使用的是比较短视的方法：沿着最陡峭的那一条，追求当前步上升最快。</li>
<li>单纯形法寻找路线优化目标函数，直至达到一个峰，而该峰就是全局最优。</li>
</ol>
</li>
<li><p>Slater’s condition 根据wiki， </p>
<blockquote>
<p>Slater’s condition (or Slater condition) is a sufficient condition for strong duality to hold for a convex optimization problem, named after Morton L. Slater. Informally, Slater’s condition states that the <strong>feasible region must have an interior point</strong>.<br>即在对偶问题中gap为0要满足的条件，即可行域中必须有内点的条件。</p>
</blockquote>
</li>
<li><p>线性规划的对偶理论没出现的时候，线性规划是不知道能不能解的。也就是说，对偶理论能够证明一个线性规划问题<strong>不</strong>存在解。思路是找到一个跟原问题的对偶问题密切相关的问题，如果这个问题有解，原问题就没解。</p>
</li>
<li><p><strong>那么证明便归为两个主要部分，1 如何转化为对偶问题 2 为什么两个问题的解相关？</strong></p>
</li>
<li><p>首先[问题要满足是凸优化]。对于凸优化来说，在满足constraint qualifications(如上文的slater condition为其中一种，满足该条件，这里涉及到仿射函数即可表示为f=A*w+b 。仿射函数其实就是线性变换liner。)情况下，gap=0，为强对偶。</p>
</li>
<li><p>其次[在求解凸优化时引入乘子以及最优解需要满足的条件]。在凸优化中的非线性优化问题下，该问题满足一些constraint qualifications，当存在不等式约束时(只有等式约束时，即拉格朗日function求偏导(也就是KKT turns into the Lagrange conditions))，我们用KKT引进 <a href="https://en.wikipedia.org/wiki/Lagrange_multipliers" title="Lagrange multipliers">Lagrange multipliers</a>，这些乘子需要满足KKT 的四个条件。注意到这里的目标函数与约束函数一定是<a href="https://en.wikipedia.org/wiki/Smooth_function" title="Smooth function">continuously differentiable</a> at a point x(local optimum点)，如果functions are non-differentiable,<a href="https://en.wikipedia.org/wiki/Subderivative" title="Subderivative">subdifferential</a><br>versions of Karush–Kuhn–Tucker (KKT) conditions are available.</p>
</li>
<li><p>对偶问题， 这里代表Lagrangian dual problem，还有其他对偶问题 <a href="https://en.wikipedia.org/wiki/Wolfe_dual_problem" title="Wolfe dual problem">Wolfe dual problem</a> and the <a href="https://en.wikipedia.org/wiki/Fenchel%27s_duality_theorem" title="Fenchel&#39;s duality theorem">Fenchel dual problem</a>。</p>
</li>
<li><p>Lagrangian dual problem 需要先形成一个L函数，这个原问题的解是，</p>
</li>
</ul>
<ul>
<li>todo</li>
</ul>
<ul>
<li> 用数学表达：</li>
</ul>
<p>凸优化问题：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-0c145b1194ca5146.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="standard form for convex minimization problem.jpg"></p>
<p>reference:<br>[1]<a href="https://cowles.yale.edu/sites/default/files/files/pub/d00/d0080.pdf">https://cowles.yale.edu/sites/default/files/files/pub/d00/d0080.pdf</a><br>[2]<a href="https://en.wikipedia.org/wiki/Convex_optimization">https://en.wikipedia.org/wiki/Convex_optimization</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>工作问题repo</title>
    <url>/2021/06/11/%E5%B7%A5%E4%BD%9C%E9%97%AE%E9%A2%98repo/</url>
    <content><![CDATA[<p><br> <br></p>
<h4 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h4><span id="more"></span>

<p>1、AUC与点击率的关系，解释为什么rerank会使AUC上升但是点击率下降的问题</p>
<p>模型a的线上效果auc降低还是提高于离线</p>
<p>模型a对模型b的线上auc高于模型b自身的线上auc吗</p>
<p>gauc呢</p>
<p>ctr和auc gauc关系</p>
<p>gauc直接用click次数作为权重是否存在偏差和问题</p>
<br>

<p>实验流量的auc和基线流量的auc。孰高孰低</p>
<p><br> <br></p>
<p>2、id的emb含义，包含了所有特征？比属性特征的本质区别？</p>
<p><br> <br></p>
<p>3、为什么统计上的一些头部推荐 auc高于模型。或者 ，模型没有显著高</p>
<p><br> <br></p>
<p>4、特征一点点地加进去，auc变化的解释</p>
<p><br> <br></p>
<p>5、矩阵分解的好处和弊端，对比于deep。</p>
<p>什么情况下用矩阵分解是有效的，效果是否和先验的加入以及数据的分布问题相关。</p>
<p>根据id共现来推荐的思路，有什么问题。</p>
<p>个性化推荐里，用户会购买的商品到底是不是估出来分数高的</p>
<p><br> <br></p>
<p>6、加入先验约束后的影响</p>
<p><br> <br></p>
<p>7、deep侧和wide侧，加入什么去强化，什么样的特征放在哪里，为什么wide起了效果</p>
<p><br> <br></p>
<p>8、bert</p>
<p>1、bert层数、参数、如何微调</p>
<p>2、bert base还是…</p>
<p>3、两两计算的简化</p>
<p>4、关键词检索…？知识图谱（问题是属性还是label）</p>
<p><br> <br></p>
<p>9、itemid的编码来加和成用户向量，存在的问题</p>
<p><br> <br></p>
<p>10、漏斗越到下面，样本、模型、特征都有怎样的适配更好</p>
<p><br> <br></p>
<p>11、负采样里通过哈夫曼树，弊端。</p>
<p>以频次进行负采样的好处和优化点。</p>
<p><br> <br></p>
<p>12、loss变化的解释</p>
<p><br> <br></p>
<p>13、batch size的选择</p>
<p><br> <br></p>
<p>14、为什么 parallel_interleave来并发from_generator并不比最先版本单进程单队列性能高。</p>
<p><br> <br></p>
<p>15、</p>
<p><br> <br></p>
<p>key tips：多种badcase</p>
]]></content>
      <categories>
        <category>破站</category>
      </categories>
      <tags>
        <tag>破站</tag>
      </tags>
  </entry>
  <entry>
    <title>常见算法复杂度</title>
    <url>/2021/04/05/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/</url>
    <content><![CDATA[<br>

<ul>
<li><p>复杂度</p>
<span id="more"></span>
<ul>
<li><p>排序</p>
<ul>
<li>冒泡、直选、直插 n^2 – 对每个元素找位置、两个for循环嵌套</li>
<li>快排、堆、希尔、归并 nlogn – 二分思想(树)</li>
<li>计数 n+max – 先遍历放到max长度数组，再遍历数组取数</li>
<li>基数 n*位数 – 每位都来来一遍分配</li>
<li>不稳定算法： “快希选堆”（快牺牲稳定性） </li>
</ul>
</li>
<li><p>查找</p>
<ul>
<li>分块、二分、二叉排序查找  logn</li>
<li>顺序查找 n</li>
<li>哈希  O(1)</li>
</ul>
</li>
<li><p>遍历</p>
<ul>
<li>树遍历：时间复杂度为O(n)，同样空间复杂度也为O(n)，n为结点数。</li>
<li>图遍历： 邻接表O(V+E) 邻接矩阵O(V^2)</li>
</ul>
</li>
<li><p>红黑</p>
<ul>
<li>插入、删除、查找的最坏时间复杂度都为 O(logn)。</li>
</ul>
</li>
<li><p>b树</p>
<ul>
<li>平衡二叉树没能充分利用磁盘预读功能<br>B树是为了充分利用磁盘预读功能来而创建的一种数据结构<br>专门做索引而发明</li>
</ul>
</li>
<li><p>b+</p>
<ul>
<li>查找，(m/2) * log(m)n</li>
</ul>
</li>
<li><p>b b+区别</p>
<ul>
<li>叶子才有数据-&gt;减少磁盘io</li>
<li>利于范围查询</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>庄家</title>
    <url>/2021/02/22/%E5%BA%84%E5%AE%B6/</url>
    <content><![CDATA[<br>
<br>

<p>Q1：庄家坐庄步骤？<br>1吸货 2拉升 3洗盘 4出货✅<br>1吸货 2洗盘 3拉升 4出货❎</p>
<span id="more"></span>
<p>需要先拉升，在于底部吸货不足，卖家少。<br>拉升2倍左右，开始洗盘，洗盘过程中获利盘出货，给庄家吸货机会。这同时也抬高了吸货成本。<br>洗盘时间视情况而定，有时一个月左右。</p>
<p>Q2：庄家吸货方法？<br>跌停板吸货 （亿安科技）<br>熊市顺势低吸（底部但是爆量）<br>抬升吸货 （洗盘但是无量）</p>
<p>Q3：洗盘？<br>洗盘的目的：洗掉大户和集中的持股人。避免在高位卖出砸盘。<br>大户：几万、几十万都不是大户，几百万、上千万股的”人“。这样的人，属于对手。（主力和老庄家(被套的)–对手）</p>
<p>Q4：主力和庄家？</p>
<p>Q5：出货？<br>拉高出货，以变现锁定的获利。<br>成本区在之前的拉升和吸货之间的位置。<br>填权再拉。<br>然后高位震荡也能出货。<br>之前的拉升，净吃入。拉高出货不能吃，要卖。拉两三天，做个平台，拉两三天再做个平台，别人就会认为每次到平台就突破，就会买入。如果大盘面差，要跑的时候，向下钓鱼出货法–开始砸、跌停板打下来，然后巨大买盘，买盘巨大，不停再往上推，就会有买盘跟进，卖盘都是庄的，择机再砸掉买盘，又往上推，再砸，再推。1天可以卖掉好几千万资金。利用追涨杀跌。</p>
<p>Q6：大波段操作<br>胆大心细。有胆量入场，指定止损–买入后预计涨结果跌了、或者预计强势结果未强势–&gt;改错择机出来。<br>买错即卖。<br>国家政策–&gt;板块和强势股(股票强度、涨停板)–&gt;龙头、二三龙头、补涨股等<br>–&gt;F10基本面符合–&gt;看技术面，平台突破点找买点</p>
<p>Q7：二龙头补涨<br>二龙头的突破(某一天放量、十几分钟拉了3%-5%、分时线k线高于原来的平台)时加仓。补涨等不到回调机会。<br>卖出点只和技术面有关。<br>跌破五日线，先卖1/3。五日线十日线高位死叉卖1/3。死叉后又跌则全出。</p>
<p><img src="/2021/02/22/%E5%BA%84%E5%AE%B6/yakj.png" alt="yakj"></p>
]]></content>
      <categories>
        <category>股市理论</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>股市理论</tag>
      </tags>
  </entry>
  <entry>
    <title>彩妆</title>
    <url>/2021/07/28/%E5%BD%A9%E5%A6%86/</url>
    <content><![CDATA[<br>

<h4 id="彩妆"><a href="#彩妆" class="headerlink" title="彩妆"></a>彩妆</h4><ol>
<li>Kevyn Aucoin的修容<span id="more"></span></li>
<li>硅E乳 北京协和</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>微信的GraphTR模型</title>
    <url>/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h4 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h4><p>“迁移学习+多任务”的场景<br>通过将不同领域的节点、关系都建模在一幅图中，通过图卷积，完成知识从数据丰富的领域向数据稀疏领域的迁移，并兼顾两个领域的指标。</p>
<span id="more"></span>

<p>多 域信息的异构图上完成图卷积，每个节点要聚合来自多个领域的异构消息。之前传统的聚合方式，如mean/max pooling，矩阵相乘，可能带来异构消息相互抵销而引入信息损失。</p>
<p>为此微信团队采用了GraphSAGE+FM+Transformer多种手段，从不同粒度来交叉、聚合消息，极大提升了模型的表示能力，这种新的消息聚合方式值得借鉴。</p>
<h4 id="场景及难点"><a href="#场景及难点" class="headerlink" title="场景及难点"></a>场景及难点</h4><p>1、微信团队面临的场景是：</p>
<p>每个视频都打有若干tag（人工标注或由内容理解算法打上的）<br>用户观看视频时，需要有算法从这个视频自带的tag中挑选出与当前用户最相关的若干个tag，展示在视频的下方。<br>用户点击某个tag，会进入一个沉浸式频道，其中展现的全部是与该tag相关的视频</p>
<p>2、难点在于：用户点击视频的行为比较丰富，但是用户点击tag的行为比较稀疏，训练数据不足。</p>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>方案一：<br>训练一个模型，输入视频的多模态信息(标题、封面图、关键帧)，输出是与这个视频最match的tag。训练时，拿人工打标的结果作为label。线上serving时，将预测出来的top-K个标签，展示在视频的下方。</p>
<p>这个方案可行，但是其只利用了视频的静态属性，没有用户的信息，所以推荐出来的tag只有与视频在语义上的相关性，完全没有针对当前用户的个性化，不满足业务需求。</p>
<p>方案二：<br>1、tag embedding用tag的word embedding。<br>2、用户的embedding是其过去有过”正交互”的tag embedding的pooling<br>所谓“正交互”，可以是用户过去一段时间内点击过的tag<br>但是考虑到user-tag的交互太稀疏，因此可以选用户过去点击过视频所携带的tag<br>pooling时，也可以考虑进播放完成度、时间衰减等因素，进行加权平均。<br>3、线上serving时，拿user embedding在当前视频所携带的tag embedding中寻找Top-K近邻，展示在视频下方。</p>
<p>怎么评价这一方案：</p>
<p>1、该方案，考虑了用户的历史，有更强的个性化。<br>2、但是拿word embedding做tag embedding，仍然只考虑了tag的语义信息。用户行为蕴含的信息，要比语义信息更加重要。<br>3、用户与tag的交互行为太少了，很难在“用户点击tag的序列”上套用word2vec来学习到tag embedding</p>
<p>方案三：微信的GraphTR模型</p>
<hr>
<h4 id="优化点"><a href="#优化点" class="headerlink" title="优化点"></a>优化点</h4><p>GraphTR是为了要<em>学习优质tag embedding</em>，为此要注重利用用户的行为信息<br>但是由于user-tag的行为太稀疏，因此GraphTR需要<em>通过user-video的行为学习到tag embedding</em><br>要达成以上目标，也有多种作法。而GraphTR的做法是：</p>
<p>1、将user, video, tag（还加上video的来源media）都放入一个大的异构图<br>通过图卷积，学习到video embedding</p>
<p>2、再建模video与video之间的相关性（比如在同一个session中播放过）</p>
<p>3、因为video embedding融合了tag embedding，因此在优化目标达成之后，一个优质的副产品就是得到tag embedding</p>
<h4 id="GraphTR是如何构建这个异构图的？"><a href="#GraphTR是如何构建这个异构图的？" class="headerlink" title="GraphTR是如何构建这个异构图的？"></a>GraphTR是如何构建这个异构图的？</h4><p>1、node：</p>
<p>图上要包括：user, video, tag, media (视频来源)这 4类节点。<br>因为用户数目太多，而每个用户的行为相对稀疏，GraphTR将用户按照gender-age-location分成84000组，用user group替代user，在图中建模。</p>
<p>2、edge：<br>video-video：同属一个观看session中的两video之间有边<br>user-video：某视频被某user group一周观看超过3次<br>video-tag：video和其携带的tag<br>video-media：video和其来源<br>tag-tag：两个 tag属于同一个视频</p>
<h4 id="如何传递、融合图上异构节点的信息？"><a href="#如何传递、融合图上异构节点的信息？" class="headerlink" title="如何传递、融合图上异构节点的信息？"></a>如何传递、融合图上异构节点的信息？</h4><p>1、为了完成user, video, tag, media这四类节点的信息融合，GraphTR设计了3层卷积结构，称为Heterogeneous field interaction network (HFIN)。</p>
<p>2、最底层Heterogeneous Feature Layer：<br>3-hop的embdding是lookup获得的，分别有四个域(user/video/tag/media域的特征)，相加得到2-hop邻居的embedding。</p>
<p>3、中间层：Multi-field Interaction Layer：<br>这一层的任务是由2-hop邻居的embedding，聚合生成1-hop邻居的embedding。<br>而HFIN采用了GraphSAGE+FM+Transformer三种方式，粒度上从由粗到细，完成聚合。</p>
<h4 id="三种聚合方式"><a href="#三种聚合方式" class="headerlink" title="三种聚合方式"></a>三种聚合方式</h4><p>1、GraphSAGE聚合<br>graphsage聚合<br><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/graphsage%E8%81%9A%E5%90%88.png" alt="graphsage聚合"></p>
<p>这里的hGraph就是1-hop的最终embding。</p>
<p>2、FM 聚合<br>FM聚合，区分各域，因此粒度更细一些。<br><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/fm%E8%81%9A%E5%90%88.png" alt="fm聚合"></p>
<p>hFM2就是1-hop的最终embding。</p>
<p>3、Multi-field transformer aggregator</p>
<p>GraphTR觉得FM聚合时，各域节点（即各域特征）交叉得还不够：1:FM聚合，只有在第2步才做域与域之间的交叉。2:在一个域内部，这n+1个特征之间，只有简单pooling，不存在交叉。3:FM聚合的第1步，每个域average pooling的是，这1+n个节点的原始特征。</p>
<p>Transformer聚合，希望增强各域节点（即各域特征）的交叉。步骤如下：</p>
<p>S1:Transformer决定在第1步引入交叉。具体方式就是，在一个域的1+n个节点之间进行Transformer变换，重新生成1+n个向量，每个新向量是老向量的加权平均，权重是当前老向量相对于其他老向量的attention score。(一套attention恐怕没有代表性，还引入多头机制)</p>
<p><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/transformer%E8%81%9A%E5%90%88.png" alt="transformer聚合"></p>
<p>S2:再拿生成的1+n个新向量，做average pooling。</p>
<p>S3:最后将“域间交叉结果”与”域内交叉结果”拼接在一起返回，作为由Transformer聚合得到的1-hop邻居的embedding。论文的实验结果证明，这个最复杂、最细粒度的聚合，对于模型性能的提升也最大。</p>
<p>4、通过三种聚合方式，我们就可以得到1-hop邻居的最终embedding，是这三种聚合结果的concat。</p>
<p>5、最上层：The Second Aggregation Layer<br>这一层负责由1-hop邻居节点（1个target node自身，m个邻居节点，一共1+m个）的embedding(下边公式中的矩阵H)，生成target node上的embedding。聚合方式也是基于Transformer的。</p>
<p>根据1+m个原向量，生成1+m个新向量，每个新向量是所有老向量的加权平均，权重是当前原向量与其他原向量的attention score<br>再拿这1+m个新向量，取平均，得到target node上的最终向量表示。</p>
<h4 id="如何定义loss"><a href="#如何定义loss" class="headerlink" title="如何定义loss?"></a>如何定义loss?</h4><p>通过以上三层卷积，就能够给图上所有类型的所有节点，都产生一个embedding。接下来的问题就是，如何定义优化目标，使这些节点的embedding得到优化？</p>
<p>这一部分的解决方案比较常规，无非就是建模节点之间的相关性，可以有选择是:</p>
<p>建模user-tag之间的相关性，user与点击过的tag之间的距离要尽可能小。但是user-tag之间交互的数据太少；建模user-video之间的相关性，user与点击过的视频之间，距离应该较近。但是图上建模的不是单个user而是user group，一个user group包含的用户兴趣太复杂，拿user-goup与video训练，可能噪声比较大；建模video-video之间的相关性，在同一个session被观看的视频之间，距离要尽可能小。因为video的点击行为比较多，这方面的数据比较丰富，文中采用的是这种方案。</p>
<p><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/loss.png" alt="loss"></p>
<h4 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h4><p>将这些tag emedding代入上文的”第二个方案”，即拿用户观看过视频携带的tag的embedding加权平均得到user embedding，再拿这个user embedding在当前视频所携带的tag的embedding中寻找出距离最近的top-k个tag，作为推荐结果显示在视频的下方。因为这些tag embedding蕴含了丰富的user-video行为信息，不仅有助于提升用户对tag的点击率，也有助于提升进入沉浸式tag频道后的观看时长。</p>
<h4 id="借鉴"><a href="#借鉴" class="headerlink" title="借鉴"></a>借鉴</h4><p>1、数据少的领域如何借力于数据多的领域，同时要兼顾两个领域的优化目标：<br>通过将不同领域的不同节点、关系建立在一张异构图上，通过图卷积，使得每个节点的embedding都浓缩了多个领域的知识，达成了“知识迁移+目标兼顾”。</p>
<p>2、GraphTR采用了GraphSAGE+FM+Transformer多种手段，粒度上从粗到细，交叉、聚合来自不同领域的异构消息，相比于mean/max pooling、浅层FC等传统聚合方式，极大提升了模型的表达能力。</p>
]]></content>
      <categories>
        <category>模型框架</category>
      </categories>
      <tags>
        <tag>dl</tag>
        <tag>模型框架</tag>
      </tags>
  </entry>
  <entry>
    <title>思想深度</title>
    <url>/2021/02/22/%E6%80%9D%E6%83%B3%E6%B7%B1%E5%BA%A6/</url>
    <content><![CDATA[<br>
<br>

<h4 id="观点"><a href="#观点" class="headerlink" title="观点"></a>观点</h4><ul>
<li><p>计划经济是狂妄、愚蠢。</p>
<span id="more"></span></li>
<li><p>小波动基本是没意义的，一个现象进入分析的视野，已经承认这现象不属于小的波动，这点是必须明确的。</p>
</li>
<li><p>有领涨的并不一定都能最终走出行情，即使是上涨的趋势，行情还可以分大、中、小。</p>
</li>
<li><p>一个股票中的常见现象，就是当前一段领涨个股和大盘走势相背离的时候，往往意味着一个结构性震荡的来临，这一点至少在股票中是通用的，而76年的改变其实也是很符合这一点的。 </p>
</li>
<li><p>汉奸，当然可以有很高的的艺术造诣。所谓坏人就不能有优点了？无所谓好坏吧。</p>
</li>
<li><p>真正的你又何曾生死，生死又与你何干？。而你的心究竟又是哪个心？</p>
</li>
<li><p>所有的现代战争，从根本意义上就是货币战争。</p>
</li>
<li><p>和庄股一样，目前的关键是不能让资本大量逃离，否则就会连续崩盘跳水。</p>
</li>
</ul>
<br>

<h4 id="民族复兴周期-与-世界经济周期-历史性共振下的-国家地缘与货币战略"><a href="#民族复兴周期-与-世界经济周期-历史性共振下的-国家地缘与货币战略" class="headerlink" title="民族复兴周期 与 世界经济周期 历史性共振下的 国家地缘与货币战略"></a>民族复兴周期 与 世界经济周期 历史性共振下的 国家地缘与货币战略</h4><p>框架：</p>
<p>1、从历史大现象规律出发，得到–&gt;当前为强盛时期。</p>
<p>2、由马克思的”五阶段论”，得到–&gt;列宁的社会主义本质是公有制，斯大林的社会主义是资本主义–因此苏东突变后权力或权力资本迅速转为资本主义(本来就同源)，毛泽东的文革，重点放在了人与人的关系而非人与自然，而不能真的实现反资。邓小平的中国特色社会主义和社会主义初级阶段策略同样也应该是一个民族主义的策略。</p>
<blockquote>
<p>社会中一部分人对另一部分人不再存在依附关系，而是全社会的人都毫不例外地依附于一个非自然的身外之物：资本，就叫做资本主义社会。<br>因此人与自然的关系被打破，不再依附自然。</p>
</blockquote>
<p>3、文革的必然失败–&gt;使得面对资本全球化成了无可逃避的现实</p>
<p>4、1得到中国处于强盛期 + 23得到世界资本全球化–&gt;机遇<br>为什么是力量在中国？：从霸业的人口上得到的。大不列颠王国以5000万，美国和苏联在2亿5千万，下一个12亿5千万。<br>时间推定：1929英德老的5千万级别主导循环结束；美苏90年的循环在一半1974年形成了石油危机的中型调整，美苏这两个不同类型的资本主义之间的同级别竞争以美国的胜利结束；然后到2000年美国出现高点，开始调整；2019年中国开始。</p>
<p>5、地缘战略：以环渤海湾地区、珠江三角洲地区、秦川地区建构大的战略三角，成为亚洲之王，其领土（或附庸性质的影响）应该从乌拉尔山往东直到大海与美洲对望，从北冰洋直到太平洋俯视澳洲，形成世界的中轴，让欧洲和美洲成为其两翼</p>
<p>6、货币：<br>最有竞争力的货币将是美元、欧元、卢比和人民币。<br>在目前阶段一定要坚持对美元采取一种不挂钩的挂钩政策，坚决长期地维持人民币对美元的币值稳定。<br>逐步扩大对亚洲区的影响，取代日元的地位，逐步成为实质亚洲货币。<br>利用第一个阶段形成的对美元的极大落差，配合世界经济大循环周期，选择时机释放，将美元在一次精心策划的战役中一次性击毁。最后在一个长期反复、拉锯的过程中，利用新的12亿5千万级别世界经济大循环周期确立中国对美国的领先地位。</p>
<p>7、分析美国：<br>美国操纵汇率–&gt;为了其总体利益服务的。<br>美国危机–&gt;泡沫化，0以下的储蓄率<br>2000年的下跌速度极快–&gt;大规模的资本逃离还没有出现<br>目前的大级别反弹–&gt;构成资本逃离的机会，一旦反弹到位，出现大资本逃离。<br>为了避免大资本逃离–&gt; 大反弹到位前把货币贬值到一个相应的地位，这样才使得美圆资本套现后不能以一个较高的汇率出逃</p>
<p>但，如果有一个容量极大的货币紧贴美圆，则美圆贬值的所有如意算盘将打不响，而人民币正好就是这种货币。人民币与美圆的挂钩使得美圆资产变现以后有了一个顺畅的逃跑渠道。</p>
<p>这也是以前帖子里面预测2019年90年大周期世界经济大危机的现实基础，正确的人民币战略将加快、加深这个进程。</p>
<hr>
<p>综上，</p>
<p>1、够有想象力，三角洲和攻击美元的策略也能想得出，而且还自圆其说<br>2、对社与资(列林斯大林)的理解，我闻所未闻<br>3、历史的宏观视角，让我有大历史之感，得到一些规律，证明论点<br>4、美国现象到本质的精辟概括</p>
<h4 id="我的观点"><a href="#我的观点" class="headerlink" title="我的观点"></a>我的观点</h4><p>1、出得了方案<br>2、给得了完备解释<br>3、理解深刻性或者说独到处，一语成谶<br>4、精辟</p>
<p>=&gt;想象力、知识沉淀、思考本质 =&gt;精辟且自洽</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>投资大佬</title>
    <url>/2021/07/28/%E6%8A%95%E8%B5%84%E5%A4%A7%E4%BD%AC/</url>
    <content><![CDATA[<br>

<br>

<h3 id="人物和组织"><a href="#人物和组织" class="headerlink" title="人物和组织"></a>人物和组织</h3><br>

<span id="more"></span>

<ul>
<li>television business news program</li>
<li><ol>
<li>asia-squawk-box</li>
<li></li>
</ol>
</li>
</ul>
<ul>
<li>Senior officials</li>
<li><ol>
<li>State Department spokesman Ned Price</li>
<li>Secretary of State Antony Blinken</li>
<li>China’s Vice Premier Liu He</li>
<li>U.S. Treasury Secretary Janet Yellen</li>
<li> Chinese Foreign Minister Wang Yi, Vice Foreign Minister Xie Feng</li>
<li>U.S. Defense Secretary Lloyd Austin </li>
<li> Secretary of State Antony Blinken</li>
</ol>
</li>
</ul>
<ul>
<li>investment group</li>
<li><ol>
<li>Bespoke Investment Group </li>
<li></li>
</ol>
</li>
</ul>
<ul>
<li>famous investors</li>
<li><ol>
<li>Kelvin Tay of UBS Global Wealth Management</li>
<li></li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统</title>
    <url>/2021/04/05/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<br>

<p>推荐系统</p>
<span id="more"></span>

<ul>
<li>冷启动<ul>
<li>用户冷启动<ul>
<li>热门</li>
<li>内容品类 多样性探索</li>
<li>用户画像</li>
<li>快速探索(各个筛选的选择tab)</li>
<li>主产品的用户迁移</li>
</ul>
</li>
<li>作品冷启动<ul>
<li>标的物跟用户行为的相似性</li>
<li>利用标的物跟标的物的相似性</li>
</ul>
</li>
<li>用户自己搜索、自己筛选、选择主题等</li>
</ul>
</li>
</ul>
<ul>
<li><p>es向量召回</p>
<ul>
<li><p>订单规则太多，导致召回排序后没多少曝光    </p>
<ul>
<li>走es召回。先过滤再取topN</li>
</ul>
</li>
<li><p>订单主页的日活100w 直播间dau30w 聊天室10w</p>
</li>
<li><p>冷门无机会曝光</p>
<ul>
<li>具体哪一个阶段导致，然后加入策略</li>
</ul>
</li>
<li><p>item侧的embding隔天变化。user的emb实时更新<br>  请求来了去es的item里找top</p>
</li>
<li><p>es比fassi的好处</p>
</li>
<li><p>性能</p>
<ul>
<li>qps100左右，128的emb，召回压力不大</li>
</ul>
</li>
</ul>
</li>
<li><p>向量距离方法</p>
<ul>
<li>fassi</li>
<li>kdtree</li>
</ul>
</li>
</ul>
<ul>
<li><p>推荐系统召回层打压热门item</p>
<ol>
<li>首先，大方向分为统计和算法两类模型。</li>
<li>召回阶段区别于排序，在于<ol>
<li>数量级千万级，一般为双塔。以便于**单独生成user embedding和item embedding，喂入的特征禁止含有user/item之间的交叉特征 **user特征喂入user tower得到user embedding，item特征喂入item tower得到item embedding。离线时，先将item embedding喂入FAISS建立索引，线上召回时，拿user embedding去FAISS里进行top-k近邻搜索，找到与其最接受的item embedding。</li>
<li>样本选择上， 正样本没有太多的争议，以内容推荐为例，选“用户点击”的item为正样本。最多考虑一下用户停留时长，将“用户误点击”排除在外。负样本的选择比较有讲究。如果说排序是特征的艺术，那么召回就是（负）样本的艺术。 </li>
<li>原则之一就是不能（只）拿“曝光未点击”做负样本，负样本的绝大部分应该由“<strong>随机负采样</strong>”生成。具体原因见我的另一篇文章《**<a href="https://zhuanlan.zhihu.com/p/165064102">负样本为王</a>**》。原则之二就是要打压热门item</li>
<li>因为绝大多数负样本是通过随机采样生成的，含有一定的噪声，因此召回不适合采用CTR预估常用的pointwise cross-entropy loss，而经常采用pairwise loss，比如margin-based bpr loss或hinge loss。（具体原因见我的另一篇文章《**<a href="https://www.zhihu.com/question/341529083/answer/1616964921">CTR和推荐算法有什么本质区别？</a>**》） </li>
<li>因此喂入模型的样本，区别于排序中常见的&lt;user, item, label&gt;，而是三元组&lt;user, item+, item-&gt;，预测的目标是MatchScore(user, item正)要远高于MatchScore(user, item负)</li>
</ol>
</li>
<li>20%的热门item占据了80%的曝光量或点击量。</li>
<li>为什么要打压<ol>
<li>训练时，为了降低loss，<strong>算法会使每个user embedding尽可能接近少数热门item embedding</strong></li>
<li>预测时，每个user embedding从FAISS检索出来的邻居都是那少数几个热门item embedding，<strong>完全失去了个性化</strong></li>
</ol>
</li>
<li>如何打压：<ol>
<li>item越热门，其成为item+的概率就应该越低。公式略。图形<img src="https://pic3.zhimg.com/80/v2-556679fa3e11b0cb0ba11e533bfc3793_1440w.jpg?source=1940ef5c" alt="img"></li>
<li><strong>提升热门item成为item-的概率</strong>。在随机采样负样本时，一方面需要采集到的item-能够尽可能广泛地覆盖所有候选item，另一方面又需要使采集到的item-尽量集中于高热item。</li>
</ol>
</li>
</ol>
  <br>

  <br>

<hr>
  <br>

<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ol>
<li>随机负采样时，ctr的loss和推荐召回的loss各自适合什么？</li>
</ol>
<p>  召回不适合采用CTR预估常用的pointwise cross-entropy loss，而经常采用pairwise loss，比如margin-based bpr loss或hinge loss</p>
<ol start="2">
<li>ctr和推荐算法本质区别</li>
<li>负样本为什么用随机采样更好</li>
<li></li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>提升方法</title>
    <url>/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<br>

<h4 id="bagging和boosting"><a href="#bagging和boosting" class="headerlink" title="bagging和boosting"></a>bagging和boosting</h4><p>1、区别？<br>采样：随机采样(Bootstrap sampling–有放回)、加大错误样本权重<br>特征的采样</p>
<span id="more"></span>
<p>并行计算上<br>弱学习器的权重</p>
<p>2、从偏差和方差的角度解释bagging和boosting的原理？<br>bagging：<br>重采样、权重也相同–模型的区别性不大，bias小。<br>但如果假设各个子模型独立，则显著降低variance。如果完全相同的子模型，则var和单个模型一样。bagging属于两者之间，一定程度降低了var。RF特征上随机选择，进一步降低了模型的相关性，从而进一步降低了var。</p>
<p>boosting:<br>前向分步学习算法，是sequencial地减少损失函数，loss是逐步地下降的，bias也随之逐步下降。但由于是这种sequence、adaptive地，模型相关性较高，不能显著减少var。</p>
<h4 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h4><p>1平均法；2投票法：多数投票、绝对多数投票、加权投票；3学习法。<br>学习法，代表方法是stacking。stacking是再加上一层学习器，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。</p>
<h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><p>1、Adaboost是模型为加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题。</p>
<p><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E5%8A%A0%E6%B3%95%E5%89%8D%E5%90%91.png" alt="加法前向"></p>
<p><strong>为什么loss是指数函数？证明如下：</strong></p>
<p>————————————————————————————————————————————————————————————<br><br><br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/exp%E8%AF%81%E6%98%8E1.png" alt="exp证明1"><br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/exp%E8%AF%81%E6%98%8E2.png" alt="exp证明2"><br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/exp%E8%AF%81%E6%98%8E3.png" alt="exp证明3"></p>
<!-- <img src=exp证明1.png width=80% height=450>
<img src=exp证明2.png width=80% height=700>
<img src=exp证明3.png width=80% height=300> -->
<br>
——————————————————————————————————————————————————————————--

<p>2、分类器结合时的权重？<br>由于Adaboost中若干个分类器的关系是第N个分类器更可能分对第N-1个分类器没分对的数据，而不能保证以前分对的数据也能同时分对。所以在Adaboost中，每个弱分类器都有各自最关注的点，每个弱分类器都只关注整个数据集的中一部分数据，所以它们必然是共同组合在一起才能发挥出作用。所以最终投票表决时，需要根据弱分类器的权重来进行加权投票，权重大小是根据弱分类器的分类错误率计算得出的，总的规律就是弱分类器错误率越低，其权重就越高。</p>
<p>计算公式：</p>
<p>1 误差<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E8%AF%AF%E5%B7%AE.png" alt="误差"></p>
<!-- <img src=误差.png width=60% height=150>
 -->
<p>2 权重系数<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E5%BC%B1%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%83%E9%87%8D%E7%B3%BB%E6%95%B0.png" alt="弱分类器权重系数"></p>
<!-- <img src=弱分类器权重系数.png width=60% height=100> -->

<p>3 样本权重<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E6%A0%B7%E6%9C%AC%E6%9D%83%E9%87%8D.png" alt="样本权重"></p>
<!-- <img src=样本权重.png width=80% height=300> -->

<p>4 分类器结合及最终分类器<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E5%88%86%E7%B1%BB%E5%99%A8%E7%BB%93%E5%90%88.png" alt="分类器结合"></p>
<!-- <img src=分类器结合.png width=80% height=230> -->


<p>由上面的公式，可得到：<br>每次新增加一个弱分类器的时候，前面的弱分类器分错的样本的权重占总样本权重的0.5，前面弱分类器分对的样本等权重也占总样本权重的0.5。</p>
<p>3、正则化</p>
<p>fn = fn-1 + θ * a * G<br>θ 为正则化项</p>
<p>4、评价</p>
<p>可解释性<br>参数个数<br>performance<br>异常点敏感<br>弱分类器选择<br>可用于特征选择</p>
<h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p>1、区别于adaboost</p>
<p>Adaboost是通过提高错分样本的权重来定位模型的不足，GBDT是通过负梯度来定位模型的不足，因此GBDT可以使用更多种类的损失函数。由于loss可以选择更鲁棒的，对于adaboost存在异常点敏感的问题,gbdt更健壮。</p>
<p>可以灵活处理离散和连续值。</p>
<p>分类的GBDT：是用指数损失函数，此时GBDT退化为Adaboost算法。<br>另一种方法是用类似于逻辑回归的对数似然损失函数的方法。</p>
<p>使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。有很多人对GBDT算法进行了开源代码的开发，比较火的是陈天奇的XGBoost和微软的LightGBM。</p>
<p>2、为什么只能分类树</p>
<p>GBDT的核心在于累加所有树的结果作为最终结果，而分类树的结果显然是没办法累加的，所以GBDT中的树都是回归树，不是分类树。</p>
<p>3、损失函数有哪些？</p>
<p>1指数损失；2对数损失；3均方差(如果我们选择平方损失函数，那么这个差值其实就是我们平常所说的残差。)；4绝对损失；5Huber损失(它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。)；6分位数损失。56主要用于健壮回归，也就是减少异常点对损失函数的影响。</p>
<p>4、例子</p>
<p><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/gbdt%E4%BE%8B%E5%AD%90.png" alt="gbdt例子"></p>
<p>5、SGBDT</p>
<p>子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。</p>
<p>6、构造特征输入LR<br>如果我们想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力。Facebook 在2014年 发表的一篇论文便是这种尝试下的产物，利用gbdt去产生有效的特征组合，以便用于逻辑回归的训练，提升模型最终的效果。如我们 使用 GBDT 生成了两棵树，两颗树一共有五个叶子节点。我们将样本 X 输入到两颗树当中去，样本X 落在了第一棵树的第二个叶子节点，第二颗树的第一个叶子节点，于是我们便可以依次构建一个五纬的特征向量，每一个纬度代表了一个叶子节点，样本落在这个叶子节点上面的话那么值为1，没有落在该叶子节点的话，那么值为 0。于是对于该样本，我们可以得到一个向量[0,1,0,1,0] 作为该样本的组合特征，和原来的特征一起输入到逻辑回归当中进行训练。实验证明这样会得到比较显著的效果提升。</p>
<p>7、CART分类树过程<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/cart%E5%88%86%E7%B1%BB%E8%BF%87%E7%A8%8B.png" alt="cart分类过程"></p>
<p>8、相比于传统的LR，SVM效果为什么好一些</p>
<ul>
<li><p>GBDT基于树模型，继承了树模型的优点 [对异常点鲁棒、不相关的特征干扰性低（LR需要加正则）、可以很好地处理缺失值、受噪音的干扰小]</p>
</li>
<li><p>处理 missing feature</p>
</li>
<li><p>数据规模影响不大，因为我们对弱分类器的要求不高，作为弱分类器的决策树的深 度一般设的比较小，即使是大数据量，也可以方便处理。像 SVM 这种数据规模大的时候训练会比较麻烦。</p>
</li>
<li><p> 通常在给定的不带噪音的问题上，他能达到的最佳分类效果还是不如 SVM，逻辑回归之类的。<br>实际问题中，往往有很大的噪音，使得 Decision Tree 这个弱势就不那么明显了。</p>
</li>
</ul>
<p>9、加速训练？<br>是否预排序,预排序可以加速查找最佳分裂点（不确定）.在样本规模上的并行计算。</p>
<p>10、参数</p>
<ul>
<li><p>第一类Miscellaneous Parameters </p>
</li>
<li><p>第二类：Boosting Parameters:<br>n_estimators 最大弱学习器的个数，太小欠拟合，太大过拟合<br>learning_rate 学习率，太大过拟合，一般很小0.1，和n_estimators一起调<br>subsample 子采样，防止过拟合，太小欠拟合。GBDT中是不放回采样</p>
</li>
<li><p>第三类：Tree-Specific Parameters<br>max_features 最大特征数<br>max_depth 最大树深，太大过拟合<br>min_samples_split 内部节点再划分所需最小样本数，越大越防过拟合<br>min_weight_fraction_leaf 叶子节点最小的样本权重和。如果存在较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。越大越防过拟合<br>max_leaf_nodes:最大叶子节点数 ，太大过拟合<br>min_impurity_split:节点划分最小不纯度<br>presort:是否对数据进行预分类，以加快拟合中最佳分裂点的发现。默认False。非稀疏数据则预排序，若稀疏数据则不预排序。小规模数据预排序。</p>
</li>
</ul>
<p>11、调参思路</p>
<p>1、首先使用默认的参数，进行数据拟合；<br>2、从步长(learning rate)和迭代次数(n_estimators)入手；一般来说,开始选择一个较小的步长来网格搜索最好的迭代次数。这里，可以将步长初始值设置为0.1。对于迭代次数进行网格搜索；<br>3、接下来对决策树的参数进行寻优<br>4、首先我们对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索。【min_samples_split暂时不能一起定下来，因为这个还和决策树其他的参数存在关联】<br>5、接着再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参；做到这里，min_samples_split要做两次网格寻优，一次是树的最大深度max_depth，一次是叶子节点最少样本数min_samples_leaf。<br>【具体观察min_samples_split的值是否落在边界上，如果是可以进一步寻优】<br>6、继续对最大特征数max_features进行网格搜索。做完这一步可以看看寻找出的最优参数组合给出的分类器的效果。<br>7、可以进一步考虑对子采样的比例进行网格搜索，得到subsample的寻优参数<br>8、回归到第2步调整设定的步长(learning rate)和迭代次数(n_estimators)，注意两者的乘积保持不变，这里可以分析得到：通过减小步长可以提高泛化能力，但是步长设定过小，也会导致拟合效果反而变差，也就是说，步长不能设置的过小。</p>
<hr>
<p>mind:</p>
<p><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E6%8F%90%E5%8D%87.png" alt="提升"></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>操作系统</title>
    <url>/2021/07/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<br>

<br>

<h4 id="要刷的几个题目"><a href="#要刷的几个题目" class="headerlink" title="要刷的几个题目"></a>要刷的几个题目</h4><span id="more"></span>

<br>

<br>



<h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><ol>
<li><p>进程实体：PCB 程序段 数据段。进程：进程实体运行的过程，动态的，系统资源分配和调度的单位。</p>
</li>
<li><p>PCB：进程描述信息：pid(进程标识符)，uid；进程管理控制信息：进程优先级，当前状态；资源分配清单：程序段、数据段指针，键盘、鼠标等资源；处理机cpu相关：寄存器值。</p>
</li>
<li><p>进程的组织：1链接方式–PCB分多个队列、os有队列的指针；2索引方式–建立几个索引表(索引表的每一项是属于该表的各个PCB的指针)、os有索引表指针</p>
</li>
<li><p>进程特性：动态性、并发性、独立性–独立获得资源、异步性、结构性</p>
</li>
<li><p>三种基本状态：运行、就绪、阻塞。还有两种：创建态、终止态(撤销pcb、回收系统资源等)。阻塞原语：等待资源；等待其他进程。唤醒原语：等待的事件发生。挂起状态，比如就绪挂起，那么<strong>创建态、阻塞挂起、运行态、就绪态</strong>的进程都有可能被调入到就绪挂起里。挂起的进程，进程除了pcb会被调入到外存中。</p>
</li>
<li><p>进程控制：对进程各个状态中切换进行控制。os提供了几种原语：功能–更新PCB、插入PCB到合适的队列、分配回收资源</p>
</li>
<li><p>原语：采用关中断和开中断实现，过程中不能被中断，这样的操作也是 原子操作。在 核心态执行。</p>
</li>
<li><p>进程通信：由于进程有自己的内存空间，所以之间的通信需要一定的共享机制。1共享存储，分为数据结构的共享(queue)和存储区域的共享(更快、高级)。2管道，即内存的一块缓冲区，连接着读写，半双工的通信。读写之间是阻塞的，没有写满不允许读，没有读空不允许写。3消息传递，分为直接和间接两种方式。直接就是每个进程有自己的消息队列，其他进程往这个队列里去发消息，间接则有个中间邮箱暂存消息，大家都往一个地方写，系统管理着信箱。</p>
</li>
<li><p>多线程及属性：来提高系统并发度。1进程就只作为资源分配的单元，而调度的基本单位是线程。2切换线程的系统开销很低，线程几乎没有系统资源，而是共享一个进程的资源，线程间的通信只要通过进程而不用系统干预。3多cpu ，各个线程可以占用不同的cpu。4线程有threadid，以及TCB。</p>
</li>
<li><p>线程实现方式：</p>
<ol>
<li><p>用户级线程：用户线程由应用程序通过线程库(pool)实现，切换也是用户态完成。os意识不到线程存在，对os是透明的，是多对一。(多cpu 的，我开了多个线程，如果是用户级的，能不能充分利用多核？–是不能的。) –&gt;切换效率高，但并发度就很低，一个线程被阻塞，整个进程都会被阻塞，多线程不能在多核上并行。</p>
</li>
<li><p>内核级线程：线程管理是os内核负责的，内核线程切换要核心态完成。用户的线程和核心线程是一一对应的。而内核级线程才是os分配cpu的单位。–&gt;并发能力强，管理成本高。</p>
</li>
<li><p>1和2的组合，n个用户的线程映射到m个内核线程上。n&gt;=m。</p>
</li>
<li><p>上面的总结，可以理解为是多线程模型。多对一。一对一。多对多。</p>
</li>
</ol>
</li>
<li><p>cpu调度：分为高级调度、中级和低级。高级(面向作业)：后备的作业队列中选择一个或多个作业，调入内存并分配资源，建立PCB，然后就可以去竞争cpu。中级：引入虚拟存储技术后，os会将空闲的进程调出到外存，即为挂起状态(有就绪挂起和阻塞挂起)，(PCB常驻内存)被os放在挂起队列里；中级调度是决定哪个挂起状态的进程重新调入内存。低级：即进程调度，从就绪队列里选择进程进入运行(是内存和cpu之间的，其他为内存和外存之间)。三种名称又是，作业调度、内存调度、进程调度。</p>
</li>
<li><p>进程调度：时机、方式、与进程切换。</p>
<ol>
<li><p>时机</p>
<ol>
<li>主动放弃cpu以及被动。主动：完成、异常、等待io。</li>
<li>被动：时间片到了、优先级高的抢了、有其他需要先处理如io中断。</li>
<li>不能调度：处理中断过程、原子操作过程、os内核程序临界区(临界区是指访问临界资源的那段代码，临界资源是必须要互斥地被访问的资源)中(某个进程正在访问内核的数据结构如PCB队列，此时会将此结构上锁，那么如果在此时进行调度，调度必然要访问这个PCB队列，而已被锁住，这就死锁了，此时要等进程对内核临界区访问结束然后再调度。如果是一般的非内核临界区，则可以直接剥夺进行调度。不会有系统隐患。)。</li>
</ol>
</li>
<li><p>方式</p>
<ol>
<li><p>抢占式，分时操作和实时操作系统</p>
</li>
<li><p>非抢占，早起的批处理</p>
</li>
</ol>
</li>
<li><p>进程切换：</p>
<ol>
<li>调度选中的进程可能是刚刚暂停的进程，切换是进程让出cpu而另一个进程占用cpu。切换需要保存，pc、程序状态字、寄存器现场等</li>
<li>广义的进程调度包括，选择一个进程以及进程切换</li>
</ol>
</li>
</ol>
</li>
<li><p>调度算法评价指标</p>
<ol>
<li>cpu利用率 =  忙碌时间/总时间</li>
<li>系统吞吐量 = 总共完成的作业/总共的时间</li>
<li>周转时间 = 完成时间 - 作业提交时间 ，os更加关注平均周转时间</li>
<li>带权周转时间 = 周转时间/作业实际运行时间</li>
<li>等待时间 = 所有等待时间之和</li>
<li>响应时间 = 首次响应时间 - 提出请求时间</li>
</ol>
</li>
<li><p>调度算法</p>
<ol>
<li><p>FCFS</p>
<ol>
<li>等待时间越长越先，可以用于作业调度也可以用于进程调度，非抢占式，公平、简单</li>
<li>缺点：带权周转时间比较大，不利于短作业</li>
<li>不会饥饿</li>
</ol>
</li>
<li><p>SJF</p>
<ol>
<li>进程调度时候是，短进程优先。有抢占版本：最短剩余时间优先算法。默认做题是非抢占</li>
<li>在所有的进程同时可运行时/几乎同时到达，SJF的平均等待和平均周转时间是最少的。但相比于FCFS，是要少的平均等待和平均周转</li>
<li>不公平，利于短作业，会饥饿，且作业时间是用户可设置的</li>
</ol>
</li>
<li><p>HRRN高响应比算法</p>
<ol>
<li>响应比= (等待时间+作业时间)/作业时间，非抢占</li>
<li>不会饥饿</li>
</ol>
</li>
<li><p>总结</p>
<ol>
<li><p>FCFS考虑了等待时间但不考虑作业时间，所以短作业不友好，SJF考虑了作业时间但是没考虑等待，所以不公平。=&gt;来了HRRN。</p>
</li>
<li><p>这三种，考虑了公平、系统性能，但是没有考虑响应时间及优先级，适用于批处理系统，但是不适用交互式。</p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>调度算法二</p>
<ol>
<li>时间片轮转RR<br>1. </li>
<li>优先级调度</li>
<li>多级反馈队列</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>整体思维</title>
    <url>/2021/04/15/%E6%95%B4%E4%BD%93%E6%80%9D%E7%BB%B4/</url>
    <content><![CDATA[<br>

<p>人生怎么走，成为什么人</p>
<span id="more"></span>



<p>时间，大多被浪费。各种事情打散了力量。</p>
<p>能量如果不凝聚起来，和热量有什么大区别？</p>
<p>深度。体系地思考决定深度。而小聪明，片段化地推理，没有温故和反复的逻辑，无法体现深度。</p>
<p>体系，需要书籍、积累、重复构建。需要阅历、经验和磨炼。有质量地思考。</p>
<p>力求成为什么样的人，就去照着那样行动。不要太过顾忌，在意别人的想法。甚至有时候你觉得，这样是不是太故作姿态了。但是也要做。不去模仿，没有可能性成为。</p>
<p>理性和思考，让你自信。大家都能想到的思路和方案，不是你的特殊性。要在你自己的特殊性上发力，要进一步去思考和抽象。而不是停留在表象，或者前滩。其实这样的随笔也不过是警戒。不必要写太多，起到作用就行。更多还是要自己去创造。走到最前方。关注着有思想的人。</p>
<p>去解决问题。</p>
<p>研究生博士生上不了，你能够有创造出产品的能力吗。研究生博士生最终还是为了创造和能力的提升。</p>
<p>你能到达的优秀必须有厚重的基底。否则还是走不远。</p>
<p>我需要去读。因为我需要时间再去沉淀。我看得到自己的智慧，我希望能够发光。而不是在局部里挣扎。</p>
<p>不过始终还是为了站得更高。</p>
<p>语言、倾听、拒绝、让人信赖……</p>
<p>都是要你去反思和提升深度广度。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>机器</title>
    <url>/2021/04/05/%E6%9C%BA%E5%99%A8/</url>
    <content><![CDATA[<p>物理cpu、逻辑cpu、内存、硬盘、gpu</p>
<span id="more"></span>

<p>命令：<br>cat /proc/cpuinfo | grep “physical id” | uniq | wc -l<br>cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c<br>cat /proc/cpuinfo | grep physical | uniq -c<br>grep MemTotal /proc/meminfo<br>df -h<br>nvidia-smi<br>lspci | grep -i nvidia</p>
<p>1    prodbigdata-aitm-distributed-job001 10.111.20.180    [DATACOMMAND]           1<em>4c  8g 200g                                                                                 <br>2    prodbigdata-aitm-distributed-job002 10.111.20.181    [DATACOMMAND]              1</em>4c  8g 200g                                                                                 <br>3    prodbigdata-algo001                 10.111.4.208     [DATACOMMAND]                      1<em>16c  16g 200g                                                                         <br>4    prodbigdata-algo002                 10.111.51.113    [DATACOMMAND]                       1</em>8c  16g 256g                                                                      <br>5    prodbigdata-algo004                 10.111.4.204     [DATACOMMAND]                          1<em>16c  16g 200g                                                                      <br>6    prodbigdata-algo005                 10.111.4.205     [DATACOMMAND]                          1</em>16c  16g 200g                                                                      <br>7    prodbigdata-rec-algo-service001     10.111.106.61    [DATACOMMAND]                                                                                            <br>8    prodbigdata-rec-algo-train001       10.111.106.62    [DATACOMMAND]                       1<em>54c 500g   PB级别     8gpu 每个显存16g                                                              <br>9    prodbigdata-youtube-dnn001          10.111.22.113    [DATACOMMAND]                      1</em>16c 120g  1T             2gpu 每个显存16g                                                   <br>10   prodbigdata-ypp-keras-frcnn         10.111.3.220     [DATACOMMAND]                        1*16c 120g  1T             2gpu 每个显存16g                                                                          <br>11   testbigdata-kafka-streaming-job001  10.111.51.50     [DATACOMMAND]            </p>
<p>Mon Mar 22 12:53:07 2021       <br>+—————————————————————————–+<br>| NVIDIA-SMI 430.40       Driver Version: 430.40       CUDA Version: 10.1     |<br>|——————————-+———————-+———————-+<br>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>|===============================+======================+======================|<br>|   0  Tesla P100-PCIE…  Off  | 00000000:00:09.0 Off |                    0 |<br>| N/A   29C    P0    32W / 250W |   1419MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   1  Tesla P100-PCIE…  Off  | 00000000:00:0A.0 Off |                    0 |<br>| N/A   28C    P0    25W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   2  Tesla P100-PCIE…  Off  | 00000000:00:0B.0 Off |                    0 |<br>| N/A   29C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   3  Tesla P100-PCIE…  Off  | 00000000:00:0C.0 Off |                    0 |<br>| N/A   34C    P0    27W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   4  Tesla P100-PCIE…  Off  | 00000000:00:0D.0 Off |                    0 |<br>| N/A   32C    P0    31W / 250W |   2384MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   5  Tesla P100-PCIE…  Off  | 00000000:00:0E.0 Off |                    0 |<br>| N/A   35C    P0    34W / 250W |    263MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   6  Tesla P100-PCIE…  Off  | 00000000:00:0F.0 Off |                    0 |<br>| N/A   27C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   7  Tesla P100-PCIE…  Off  | 00000000:00:10.0 Off |                    0 |<br>| N/A   29C    P0    31W / 250W |   1301MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+</p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>样本</title>
    <url>/2021/04/05/%E6%A0%B7%E6%9C%AC/</url>
    <content><![CDATA[<ul>
<li><p>正样本稀疏</p>
<span id="more"></span>
<ul>
<li>欠采样、过采样</li>
<li>集成<ul>
<li>负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据<br>  然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。</li>
<li>这种解决问题的思路类似于随机森林</li>
</ul>
</li>
<li>权重<ul>
<li>不同样本数量的类别分别赋予不同的权重</li>
</ul>
</li>
<li>一分类<ul>
<li>把它看做一分类（one class learning） 或异常检测问题，这类方法的重点不在于捕捉类间的差别，而是为其中一类进行建模，比较有代表性的是 one-class-SVM。</li>
<li>符合这些图像特征的就属于人脸，反之则不是。对比二分类，显著的区别就是，二分类不但能的出来这个图片不是人脸，他还能告诉你这个图片是猪脸。</li>
</ul>
</li>
</ul>
</li>
<li><p>推荐之样本</p>
<ul>
<li><p>避免高度活跃用户对loss的影响</p>
<ul>
<li>训练集中对每个用户提取相同数量的训练样本</li>
</ul>
</li>
<li><p>根据用户最后一次点击行为的位置，过滤掉最后一次点击之后的展示，可以人为认为用户没有看到。</p>
</li>
<li><p>一个用户对同一个内容点击与不点击并存的情况，如果多次曝光的间隙非常短，考虑只使用其中的一次曝光数据。</p>
</li>
<li><p>考虑去除只有曝光但没有点击操作的用户的样本（也就是说有的用户只有负样本，没有正样本），不过去除的话，那模型就只能够学习到活跃用户或者有意向用户的行为习惯</p>
</li>
<li><p>要求当线上模型在预测时，需要将喂给模型的特征做一次落地，比如传到kafka，后续再由相应程序进行解析生成之后的的训练样本。</p>
</li>
<li><p>同一个request中，如果收到某样本后面样本的展示或者点击事件，5min后还没有收到该样本的点击事件，则作为负样本进行训练；如果在作为负样本训练之后，在一段时间之后又收到该样本的正例行为，则需要作出补偿。</p>
</li>
<li><p>专家样本</p>
</li>
<li><p>坏样本</p>
<ul>
<li>样本偏差、模型敏感、无法代表全体、</li>
</ul>
</li>
<li><p>在信用卡欺诈模型中，对于召回率的要求比较高（不希望漏掉一个欺诈用户），并且预测出来的数据还会经过人工审核，相对的对于准确率要求就低一些</p>
</li>
<li><p>但是在我们的原始数据中，正样本的比例本身就占比非常小了，或者正样本本身就是正太分布部分，但是在预测的时候，连长尾分布的部分也不能放过，（尽量的把所有欺诈用户召回），比如信用卡欺诈里有的超级用户虽然数量小，但是一次违约就是几十万，比几百个普通用户还严重，这种时候是否要用权值设置或者复制正样本的方式，来做识别增强。</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度</title>
    <url>/2021/02/04/%E6%A2%AF%E5%BA%A6/</url>
    <content><![CDATA[<h2 id="梯度消失和爆炸"><a href="#梯度消失和爆炸" class="headerlink" title="梯度消失和爆炸"></a>梯度消失和爆炸</h2><ul>
<li><p>deep后带来的信息传递/梯度传递问题</p>
<ul>
<li><p>层数过多导致？sigmoid和tanh为什么会导致梯度消失？</p>
<ol>
<li>直观解释：从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。</li>
<li>反向传播角度解释：由于反向传播过程中，前面网络权重的偏导数的计算是逐渐从后往前累乘的，如果使用激活函数如sigmoid，导数小于一，因此累乘会逐渐变小，导致梯度消失，前面的网络层权重更新变慢；如果权重 本身比较大，累乘会导致前面网络的参数偏导数变大，产生数值上溢。<br>    </li>
</ol>
</li>
<li><p>梯度消失</p>
<ol>
<li>原因：层数过多，学习率的大小，网络参数的初始化，激活函数的边缘效应</li>
<li>在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。<br>
<span id="more"></span></li>
</ol>
</li>
<li><p>梯度爆炸</p>
<ol>
<li>原因：1）隐藏层的层数过多；2）<strong>权重的初始化值过大</strong></li>
<li>在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为$NaN$值，再也无法更新。</li>
<li>解决：1）用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。（2）用Batch Normalization。（3）<strong>LSTM的结构设计也可以改善RNN中的梯度消失问题。</strong>（4）进行梯度裁剪(clip), 如果梯度值大于某个阈值，我们就进行梯度裁剪，限制在一个范围内.（5）使用正则化，这样会限制参数 的大小，从而防止梯度爆炸。（6）设计网络层数更少的网络进行模型训练</li>
</ol>
</li>
</ul>
</li>
</ul>
<h4 id="LSTM为什么有助于解决梯度消失和爆炸问题？"><a href="#LSTM为什么有助于解决梯度消失和爆炸问题？" class="headerlink" title="LSTM为什么有助于解决梯度消失和爆炸问题？"></a>LSTM为什么有助于解决梯度消失和爆炸问题？</h4><p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
<p>RNN 中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。</p>
<p>其一是遗忘门接近 1（例如模型初始化时会把 forget bias 设置成较大的正数，让遗忘门饱和），这时候远距离梯度不消失；其二是遗忘门接近 0，但这时模型是故意阻断梯度流的，这不是 bug 而是 feature（例如情感分析任务中有一条样本 “A，但是 B”，模型读到“但是”后选择把遗忘门设置成 0，遗忘掉内容 A，这是合理的）。当然，常常也存在 f 介于 [0, 1] 之间的情况，在这种情况下只能说 LSTM 改善（而非解决）了梯度消失的状况。</p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>1、优秀激活函数的性质<br>非线性–多层后能够逼近所有函数<br>可导–优化器大多用梯度下降更新参数<br>单调–能够保证单层网络的损失函数是凸函数<br>近似恒等性– f(x)近似=x，参数初始化为随机小值时，神经网络更稳定<br>输出均值为0 – 能够加速收敛</p>
<p>2、激活函数输出范围<br>有限 – 基于梯度更新参数更稳定<br>无限 – 调小学习率</p>
<p>3、sigmoid<br>导数范围在0-1/4，梯度消失问题<br>输出均值不为0，收敛慢<br>幂运算耗时高</p>
<p>4、tanh<br>梯度消失<br>幂运算耗时高</p>
<p>5、 relu<br>缺点：<br>输出均值非0<br>dead relu – 某些神经元永远不被激活，导致相应的参数无法更新</p>
<p>6、dead relu改进<br>负数输入过多导致，改变参数初始化避免过多的负数特征送入relu，设置更小的学习率，避免参数分布巨大变化<br>leaky – 虽然能解决dead，但实际效果中没有证明出比relu更好</p>
<h4 id="初始化建议"><a href="#初始化建议" class="headerlink" title="初始化建议"></a>初始化建议</h4><p>均值为0<br>标准差为 sqrt(2/当前层输入特征个数)的正太分布</p>
<h4 id="指数加权平均（Exponentially-weighted-average）"><a href="#指数加权平均（Exponentially-weighted-average）" class="headerlink" title="指数加权平均（Exponentially weighted average）"></a>指数加权平均（Exponentially weighted average）</h4><p>V_t=β*V_t−1+(1−β)*θ_t</p>
<p>1、当 β 较大时（β = 0.98 相当于每一点前50天的平均气温)。曲线波动相对较小更加平滑，因为对很多天的气温做了平均处理，正因为如此，曲线还会右移。</p>
<p>较小，0.5时，曲线波动相对激烈，但是它可以更快的适应温度的变化。</p>
<p>2、当 β = 0.9时，我们可以近似的认为当前的数值是过去10天的平均值，但是显然如果我们直接计算过去10天的平均值，要比用指数加权平均来的更加准确。但是如果直接计算过去10天的平均值，我们要存储过去10天的数值，而加权平均只要存储V_t−1</p>
<p>3、指数加权平均 不能很好地拟合前几天的数据，因此需要 偏差修正<br>在机器学习中，多数的指数加权平均运算并不会使用偏差修正。因为大多数人更愿意在初始阶段，用一个捎带偏差的值进行运算。不过，如果在初试阶段就开始考虑偏差，指数加权移动均值仍处于预热阶段，偏差修正可以做出更好的估计。<br>​<br>V_t = V_t / (1 - β_t)</p>
<h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>GD 到 BGD 到 SGD–即minibatch的SGD(现在说SGD一般都指MBGD)，<br>BGD即每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型参数。这样就会大大加快训练速度。<br>SGD即每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。</p>
<p>步骤：<br>1、计算 t时刻损失函数关于当前参数的梯度 g_t = ▽loss<br>2、计算 t时刻的一阶动量m_t和二阶动量 v_t<br>3、计算 t时刻下降梯度 n_t = lr * m_t/sqrt(v_t)<br>4、计算t+1时刻的梯度即 w_t+1 =  w_t - n_t</p>
<p>其中一阶动量是，梯度相关的函数<br>二阶动量是，梯度平方相关的函数</p>
<p>1、SGD<br>m_t = g_t<br>v_t = 1</p>
<p>SGD每次都会在当前位置上沿着负梯度方向更新（下降，沿着正梯度则为上升），并不考虑之前的方向梯度大小等。<br>动量（moment）通过引入新的变量去积累之前的梯度（通过指数衰减平均得到），得到加速学习过程的目的。</p>
<p>若当前的梯度方向与累积的历史梯度方向一致，则当前的梯度会被加强，从而这一步下降的幅度更大。若当前的梯度方向与累积的梯度方向不一致，则会减弱当前下降的梯度幅度。如下图</p>
<p>2、SGDM 含有momentum的SGD</p>
<p>m_t = θ * m_t-1 + (1 - θ) * g_t   – 指数加权平均<br>v_t = 1 </p>
<p>初始化，m_t = 0</p>
<p><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/momentum.png" alt="momentum"></p>
<p>3、Ada 引入了二阶动量<br>m_t = g_t<br>v_t = ∑ g_t^2</p>
<p>优点：对于梯度较大的参数，意味着学习率会变得较小。而对于梯度较小的参数，则效果相反。这样就可以使得参数在平缓的地方下降的稍微快些，不至于徘徊不前。<br>缺点：由于是累积梯度的平方，到后面累积的比较大，会导致梯度消失。</p>
<p>在凸优化中，AdaGrad算法具有一些令人满意的理论性质。但是，在实际使用中已经发现，对于训练深度神经网络模型而言，从训练开始时累积梯度平方会导致学习率过早过量的减少。AdaGrad算法在某些深度学习模型上效果不错，但不是全部。</p>
<p>Adadelta<br>Adadelta是对Adagrad的改进，主要是为了克服Adagrad的两个缺点（摘自Adadelta论文《AdaDelta: An Adaptive Learning Rate Method》）：<br>the continual decay of learning rates throughout training<br>the need for a manually selected global learning rate</p>
<p>4、RMSProp<br>m_t = g_t<br>v_t = θ * v_t-1 + (1 - θ) * g_t^2</p>
<p>RMSprop也是对Adagrad的扩展，以在非凸的情况下效果更好。和Adadelta一样，RMSprop使用指数加权平均（指数衰减平均）只保留过去给定窗口大小的梯度，使其能够在找到凸碗状结构后快速收敛。</p>
<p>在实际使用过程中，RMSprop已被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习人员经常采用的优化算法之一。keras文档中关于RMSprop写到：This optimizer is usually a good choice for recurrent neural networks.</p>
<p><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/rmsprop.png" alt="rmsprop"></p>
<p>5、Adam</p>
<p>Adam实际上是把momentum和RMSprop结合起来的一种算法<br><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/adam.png" alt="adam"><br><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/adamformula.png" alt="adamformula"></p>
<p>reference:</p>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html">https://ruder.io/optimizing-gradient-descent/index.html</a></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>母仪天下</title>
    <url>/2021/05/25/%E6%AF%8D%E4%BB%AA%E5%A4%A9%E4%B8%8B/</url>
    <content><![CDATA[<br>

<br>

<p>1.</p>
<p>看了5个小时左右，总要写写感想。</p>
<p>女人对感情多多少少的不满足。萧育那样的男人，大多女人都会受诱惑。才华，浪漫，个性，有所思想。</p>
<span id="more"></span>

<p>他和王政君的单独相处，都还会让我心悸。而我以为自己，早已不会陷入这样的”调情“气氛了。</p>
<p>讲到底，调情，讲究魅惑，讲究女人的技术，也讲究两人之间感情的厚度。没有厚度，不过也是一场无聊的戏。</p>
<p>女人还是渴望一个骑士样的男人吧。一生钟情。即使不可得。但是萧育开始却是个风花雪月的男人。而此时，是没有厚度可言的，却是厚度的种子。因为从开始，王政君就是有原则的。哪怕为他所动。</p>
<p>富二代、公子哥、英俊风流，追逐大哥的女人。不能得却一生守护的剧情。比一般的网剧略有质量的，就是加了些历史作料，对白略文雅。但也是白话。但表达至少是好些的。敢于表达，有真有假。但不会感觉太虚伪。</p>
<p>加上演员还是够撑得住的。才能让观众入得了情。</p>
<p>历史上到底有没有这一说，实在不重要。这段情感塑造，满足了女人对爱情的想象。不过也终是想象，现实中追求这样的爱情无可厚非，只是要清醒。遇到了万幸。大部分情况，只是一厢情愿，伤害了自己后认清现实。本就没有真相，只有想象和想象的破灭。</p>
<p>而空想剥离了当下，是不如想象来得有约束感的。想象之所以有魅力，里面裹挟着人们的判断、信念、偏执、努力、期待… </p>
<p>女人太过专注于感情，就浪费了很多人间乐趣。女人亦可风花雪月，只要她喜欢。到了一定阶段，也就认为这个没什么意思了，因为也激发不了她感官的刺激、满足。所有外在形式虽然能过把瘾，他也不再追逐此风流。人是摇摆的。有时候觉得小三美轮美奂。有时候觉得正妻与你共柴米油盐才难得。</p>
<p>两者不可兼得。兼得了也不过一种过渡形式而已。以此为乐则受其所困。不以为乐，也会百无聊赖，也会有所坚持。但是坚持是反感官的。</p>
<br>

<p>2.</p>
<p>王政君让我看到一种不精明，却远比精明讨人喜。或许只是编剧的感性，有先验的倾向性。把她塑造在”母仪天下“的荣耀里。为了此，做出很多让步。</p>
<p>她很慢。事情来了，没有清楚之前，是无言耐性的状态。她可以为了爱的人，有条件地忍下一些原则底线的试探突破。</p>
<p>灾难面前的无畏和冷静。心中有原则而沉稳。有希望和信念就不畏。并且能够吃苦而承受住生理的疼痛折磨。</p>
<p>寂寞于她，也是苦楚。但她不诉。她有泪水，而节制。因为坚强。忍住了寂寞，让那份感情更加满足想象。</p>
<p>过于坚强，秉持”母仪天下“的权力信念，使得王政君更加让人无法共情。</p>
<p>和飞燕合德而比，是无风情、无趣之人。她亲近的男人都无法给予她爱。能认识到这种女人的美，是很少的，因为反感官。女人更多被认可的美德，还是在于魅惑和漂亮。端庄大气，智慧坚毅也是要的，但是最好不要摆在软弱而沉溺情感、耽于享乐的男人面前。武则天就更能变通了。</p>
<p>人是很难做到灵活的。原则最大的坏处就是僵硬。让渡一些获得灵活性。</p>
<p>有趣的事情，大多掺杂着情愫和欲望。理性如果被标榜起来，大多是启人心智，而感性被抬高，就不免都希望能有意思、满足才行。理性会制约你的满足，让你忧道不忧贫。而感性的倾斜，让你更能被理解，使人共情。</p>
<p>嫉妒的人，不讨人喜，加上恶毒，简直就是所谓坏人了。现实中，坏人要比傅瑶更加高明。不精明反而更有手段。这个没有被揭示出来，降低了剧的质量。太简单粗暴。细腻的就少了。虽然，说不定受众会更热捧。看编剧境界吧。</p>
<br>

<p>3.</p>
<p>班婕妤向汉成帝证明自己没有参与许皇后巫蛊事件：</p>
<p>妾闻生死有命，富贵在天，修正尚未得福，为邪还有何望？若使鬼神有知，岂肯听信谗说？万一无知，咒诅何益，妾非但不敢为，也是不屑为！</p>
<br>

<p>4.</p>
<p>无法挖掘什么更体系、深刻的智慧或者逻辑性。文字尚且可以忽悠，更别说这种影视作品。细节里隐藏所有可能的深刻性。本质也展露在微妙之中。体悟之中。那些沉默和忍耐的画面，比一句句的对白，更能看到背后的舍弃，和对一个人物、对人性的考量。</p>
<p>如果记不住一些精妙的细节，过程中不带着审视的眼光，只是享受了被动的感情带入而已。</p>
<p>不过我们成为很难不被带入的动物。不被带入的剥离感，悬空感，需要破除身在此山的迷惑。</p>
]]></content>
  </entry>
  <entry>
    <title>汇率</title>
    <url>/2021/05/23/%E6%B1%87%E7%8E%87/</url>
    <content><![CDATA[<br>



<h3 id="汇率决定理论"><a href="#汇率决定理论" class="headerlink" title="汇率决定理论"></a>汇率决定理论</h3><p>1、流量</p>
<p>1】购买力平价</p>
<span id="more"></span>



<br>

<p>2】利率平价</p>
<br>

<p>3】国际收支说</p>
<br>

<br>

<p>2、存量</p>
<p>1】弹性</p>
<br>

<p>2】粘性</p>
<br>

<p>3】资产组合分析法</p>
<br>



<h3 id="影响汇率的一般因素"><a href="#影响汇率的一般因素" class="headerlink" title="影响汇率的一般因素"></a>影响汇率的一般因素</h3><p>两个角度，经济基本面 以及 政策因素。</p>
<br>

<br>

<p>基本面：</p>
<p>1、GDP</p>
<p>国际收支说，GDP ↑ - 国内进口↑ - 逆差 ↑ CA利空 - 贬值 e↑ (短期)</p>
<p><strong>弹性分析法，GDP ↑ - Md ↑ KA利好 - Ms → - 升值 e↓ (短期)</strong></p>
<p>实践： <strong>购买力平价，GDP↑ - BS效应 e↓ (长期)</strong></p>
<br>

<p>2、通胀</p>
<p>购买力平价，通胀↑ - 价格↑ - e ↑</p>
<p>国际收支说，通胀↑ - 出口↓ - CA 利空 逆差 - e ↑</p>
<p>实践：高通胀带来贬值压力</p>
<br>

<p>3、国际收支</p>
<p>顺差 - 升值压力</p>
<p>逆差 - 贬值</p>
<br>

<p>4、利率</p>
<p>利率平价，i ↑ - e ↓</p>
<p>国际收支，i ↑ - 利好KA - OB↑ - e↓</p>
<p>**弹性货币分析，i ↑ - Md↑ - Ms↑ - 价格↑ - e↑ **</p>
<p>实践：i ↑ - 利好KA - OB↑ - e↓</p>
<br>

<br>

<p>政策：</p>
<p>1、央行的直接、间接干预</p>
<p>2、预期的自我实现机制</p>
<br>

<br>

<h3 id="受汇率影响的经济变量"><a href="#受汇率影响的经济变量" class="headerlink" title="受汇率影响的经济变量"></a>受汇率影响的经济变量</h3><p>1、GDP</p>
<p>短期，</p>
<p>e ↑ - NX↑ -Y↑ </p>
<p><strong>e ↑ - 贬值税效应↑ - 消费↓ -Y↓</strong></p>
<p><strong>e ↑ - 外债负担↑ - 投资↓ -Y↓</strong></p>
<p>长期，看，供给侧 劳动生产率和产业结构、民族工业等</p>
<br>

<p>2、价格</p>
<p>生产成本 – 进口成本推动型通胀，<strong>e ↑ - 进口原材料成本↑ - 进口产品价格↑ -价格↑</strong></p>
<p>货币工资 – ，e ↑  - 进口消费品成本↑ - 工资购买力↓ wage↓ - 价格↑</p>
<br>

<p>3、国际收支</p>
<p>CA:</p>
<p>弹性论，满足ML条件时 e ↑ - CA↑ - TB↑</p>
<p><strong>吸收论，e ↑ - Y↑ and A(国内吸收)↓ 出口↑  - TB↑</strong>  ，这里Y和e相关体现在NX上</p>
<br>

<p>KA:</p>
<p><strong>回归性</strong> ：  e ↑ - 升值预期、Md↑ - KA↑</p>
<p><strong>外推性</strong> ： e ↑ - 贬值、Md↓ - KA↓ </p>
<br>

<br>

<h3 id="人民币汇率形成机制"><a href="#人民币汇率形成机制" class="headerlink" title="人民币汇率形成机制"></a>人民币汇率形成机制</h3><p>市场供求为基础，参考一篮子货币进行调节，有管理的浮动汇率制</p>
<p>中间价 = 上个交易日收盘价 + 保持人民币对一篮子货币夜间汇率稳定所需的汇率变化 + 逆周期调节因子(核心：抑制因预期导致的汇率顺周期波动，让汇率更多反映基本面，少反应预期)</p>
<br>

<br>

<h4 id="汇率市场化"><a href="#汇率市场化" class="headerlink" title="汇率市场化"></a>汇率市场化</h4><p>1、汇率管制工具</p>
<p>中间价、波动幅度、外汇市场常态式干预</p>
<br>

<p>2、好处</p>
<p>1 减少外部失衡对内部的冲击，浮动汇率起着国际收支自动稳定器的作用</p>
<p>2 固定汇率下跨进资金危害金融体系稳定性。浮动汇率灵活调整市场预期，平衡资本流动</p>
<p>3 三元悖论。KA的开放程度越高，为了保证货币政策独立性，越需要放开汇率</p>
<br>

<p>3、政策建议</p>
<p>1 外汇市场交易品种多样化</p>
<p>2 增强企业对冲汇率波动风险能力，进行汇率风险对冲保值</p>
<p>3 由于 羊群效应 汇率超调的存在，央行需要对外汇市场保留干预能力</p>
<br>

<br>



<h4 id="人民币贬值"><a href="#人民币贬值" class="headerlink" title="人民币贬值"></a>人民币贬值</h4><p>1、原因</p>
<p>1 报价机制中，一篮子货币里其他非美元货币对美元贬值 ： 1欧元区 英国等经济体经济弱于美国 2不确定性上升使得美元成为避险资产</p>
<p>2 市场供求导致 – 具体结合当下政策，展开分析经济变量近况，以及分析对e的影响</p>
<p>3 央行退出外汇市场的常态式干预导致人民币近期贬值</p>
<br>

<p>2、影响</p>
<p>结合模型分析对其他变量的影响</p>
<p>对贸易摩擦 影响 : 企业外债负担、结构性关税(总量型贬值税)、打击进口、激化摩擦… </p>
<p>对人民币国际化</p>
<p>金融市场危机防范等… </p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>统计角度看ml</title>
    <url>/2021/03/11/%E7%BB%9F%E8%AE%A1%E8%A7%92%E5%BA%A6%E7%9C%8Bml/</url>
    <content><![CDATA[<br>

<p>一些思考，从整体上看模型的思路，进行比较。</p>
<span id="more"></span>
<p>极大似然？就是后验、大量样本的整体出现概率值最大。样本之间独立。可应用乘法原理。</p>
<p>条件概率，即某(些)条件下某(些)事件出现的概率。</p>
<p>决策树则是求其极大值，局部选择当前条件概率最大。条件概率越大，不确定性越低，条件熵越小。整体熵未必减小。考虑整体熵值的是最大熵模型和logistics模型。logistics是可以通过假设先验概率，转化为这样的问题：推导出满足极大似然极值而得到的w参数。都可以通过拉格朗日将问题转化为无约束求极值，偏导为0，算法里有迭代尺度、梯度下降、牛顿插值。牛顿插值迭代收敛快些。</p>
<p>特征选择无论是熵、增益(比)还是基尼系数都体现出不确定性的思想。增益大则说明特征对结果的影响力大，因为说明条件熵越小，即条件概率越大，该条件下不确定性越低。增益或比越大，而基尼越小越好。因为前者都有-号，类似相反数的关系。本质还条件概率的问题。</p>
<p>决策生成树可以用动归提高算法效率。</p>
<p>感知器从loss func出发，求最小。距离如何定义还要看具体应用场景。主要算法里有<a href="https://blog.csdn.net/greenyang5277/article/details/104270803">Gram矩阵</a>，对偶算法。</p>
<p>k邻很简单，经验风险最小，就是多数为胜，即最简单的频率派概率最大思路，k不一样结果可能瞬间不同了，参数也少。</p>
<p>朴素贝叶斯主要是独立性假设，在类确定下特征条件独立，才能将公式分子简化，否则不那么容易求最大值，后验概率最大，还是极大似然思路，后验概率这是可用简单的乘法原理表达。如果有0，这里提出可以平滑的思想。</p>
<p>无论是熵、条件熵、基尼系数，还是贝叶斯、条件概率，还是感知器的loss func，还是c4.5里的loss(在剪枝时通过熵建立的loss，加入了模型复杂度因子，通过比较剪枝前后大小来判断是否剪枝)，还是logistics(初始分布进行求对数几率，求极大似然最大的参数)、最大熵(公式也化为求极大似然最大)两个对数线性模型的经验分布推导出来的无约束最优化公式，都是对初始概率进行包装，要么转为极大似然问题或者说转为条件概率问题，要么转为loss最小问题。概率问题则为监督学习中的两种，生成和判别。非概率则自定义的一些代数loss。</p>
<p>对给定输入判别输出，判别要么是f(x),要么是P(Y|X)。前者会出现loss func，定义距离如感知器、LDA，后者则决策树、k邻、贝叶斯、GMM。对数线性模型也是P(Y|X)概率分布公式表达的分类模型。都可以说是在对这两种函数求最优的问题。但如决策树、k邻，没有什么公式，也就没有什么参数需要调整，大多重点在算法，如决策树里主要是三个算法里剪枝过程应用到熵之类问题，而kmeans主要kd树解决高维搜索问题。理论框架虽然可以和统计通过加条件等方式相关联，但是更多是另一种思路。</p>
<p>生成模型，由学习数据得到原始分布，再来求P(Y|X)，如贝叶斯。判别模型，学习数据不从分布入手，而直接对条件概率进行假设，如LR。</p>
<p>具体细节，如收敛性证明，拉格朗日转化，最大熵的约束公式，包括泛化误差利用切比雪夫求上界的前提条件，感知器中正负，logistics里对数几率特征空间是n+1维，都需落在数学上去一步步转化推证。</p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>编译tensorflow</title>
    <url>/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/</url>
    <content><![CDATA[<br> 

<p>记录编译过程</p>
<span id="more"></span>

<br> 

<h4 id="前序安装"><a href="#前序安装" class="headerlink" title="前序安装"></a>前序安装</h4><p>1、bazel 2.0.0 通过sh安装，需要chmod，然后export环境变量。我的路径在/var/root/bin，添加到source ~/.bash_profile。把这句source添加到 vi .zshrc ，然后source ~/.zshrc ，重启也有bazel了。</p>
<p>2、tensorflow v2.2.0 </p>
<p>3、conda环境，选择了python2.7，然后安装requirements (tensorflow-2.2.0/tensorflow/tools/pip_package/setup.py )如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">future</span><br><span class="line">absl-py &gt;&#x3D; 0.7.0</span><br><span class="line">astunparse &#x3D;&#x3D; 1.6.3</span><br><span class="line">backports.weakref &gt;&#x3D; 1.0rc1</span><br><span class="line">enum34 &gt;&#x3D; 1.1.6</span><br><span class="line">gast &#x3D;&#x3D; 0.3.3</span><br><span class="line">google_pasta &gt;&#x3D; 0.1.8</span><br><span class="line">h5py &#x3D;&#x3D; 2.10.0</span><br><span class="line">keras_preprocessing &gt;&#x3D; 1.1.0</span><br><span class="line">numpy &#x3D;&#x3D; 1.16.0</span><br><span class="line">opt_einsum &gt;&#x3D; 2.3.2</span><br><span class="line">protobuf &gt;&#x3D; 3.8.0</span><br><span class="line">tensorboard &#x3D;&#x3D; 2.1.0</span><br><span class="line">tensorflow_estimator &#x3D;&#x3D; 2.2.0</span><br><span class="line">termcolor &gt;&#x3D; 1.1.0</span><br><span class="line">wrapt &gt;&#x3D; 1.11.1</span><br><span class="line"></span><br><span class="line"># python3 requires wheel 0.26</span><br><span class="line">wheel &gt;&#x3D; 0.26</span><br><span class="line"></span><br><span class="line"># mock comes with unittest.mock for python3 need to install for python2</span><br><span class="line">mock &gt;&#x3D; 2.0.0</span><br><span class="line"></span><br><span class="line"># functools comes with python3 need to install the backport for python2</span><br><span class="line">functools32 &gt;&#x3D; 3.2.3</span><br><span class="line">six &gt;&#x3D; 1.12.0</span><br><span class="line"></span><br><span class="line"># scipy &lt; 1.4.1 causes segfaults due to pybind11</span><br><span class="line"># Latest scipy pip for py2 is scipy&#x3D;&#x3D;1.2.2</span><br><span class="line">scipy &#x3D;&#x3D; 1.2.2</span><br></pre></td></tr></table></figure>



<p>4、如果需要gpu，则还要安装cuda 10.1 和 cudnn 7.6.5</p>
<br> 

<h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><p>1、配置基本选择N，需要gpu则在CUDAsupport选择y</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd tensorflow-2.2.0</span><br><span class="line">.&#x2F;configure</span><br></pre></td></tr></table></figure>

<p>2、bazel</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bazel build --noincompatible_do_not_split_linking_cmdline --local_ram_resources&#x3D;2048  &#x2F;&#x2F;tensorflow&#x2F;tools&#x2F;pip_package:build_pip_package</span><br><span class="line"></span><br><span class="line">bazel build --noincompatible_do_not_split_linking_cmdline --local_ram_resources&#x3D;2048 &#x2F;&#x2F;tensorflow:libtensorflow_cc.so</span><br></pre></td></tr></table></figure>



<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/inprogress.jpg" alt="过程"></p>
<br>



<p>编译成功，花了12个小时多。</p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/success.jpg" alt="success"></p>
<br>

<h4 id="生成whl"><a href="#生成whl" class="headerlink" title="生成whl"></a>生成whl</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo .&#x2F;tensorflow&#x2F;tools&#x2F;pip_package&#x2F;build_pip_package.sh &#x2F;tmp&#x2F;tensorflow_pkg</span><br><span class="line">#接着我切了conda环境</span><br><span class="line">pip install --upgrade pip</span><br><span class="line">pip uninstall tensorflow</span><br><span class="line">pip uninstall tensorboard</span><br><span class="line">pip uninstall tensorflow-tensorboard</span><br><span class="line">#pip3 install tensorboard&#x3D;&#x3D;2.2.1  #有个巨坑。pip3才能安装2.2.1，但是我们环境用的都是python2.7</span><br><span class="line">#要么把之前的切到3，要么下载下来site-package 装到目录里。我是选择粘贴到目录 #&#x2F;Users&#x2F;wyq&#x2F;anaconda2&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;tensorboard</span><br><span class="line">pip install -U --ignore-installed wrapt  </span><br><span class="line">#又是坑。wrapt：ERROR: Cannot uninstall &#39;wrapt&#39;. It is a distutils installed project and thus #we cannot accurately determine which files belong to it which would lead to only a partial #uninstall.</span><br><span class="line">pip3 install &#x2F;tmp&#x2F;tensorflow_pkg&#x2F;tensorflow-2.2.0-cp27-cp27m-macosx_10_14_x86_64.whl</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/whl0.jpg" alt="whl0"></p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/whl.jpg" alt="whl"></p>
<h4 id="python验证"><a href="#python验证" class="headerlink" title="python验证"></a><br>python验证</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#打开ipython </span><br><span class="line">#如果遇到这个问题 ImportError: cannot import name pywrap_tensorflow</span><br><span class="line">#看这个issue </span><br><span class="line"># https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;35953210&#x2F;error-running-basic-tensorflow-example</span><br><span class="line">import tensorflow as tf</span><br><span class="line">tf.__version__</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></table></figure>

<br>

<p>终于成功了</p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/whlsuccess.jpg" alt="whlsuccess"></p>
<br>

<h4 id="c-验证"><a href="#c-验证" class="headerlink" title="c++验证"></a>c++验证</h4><p>选择了vscode，下次clion可以也试试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#在tf根目录下运行</span><br><span class="line">bazel build &#x2F;&#x2F;tensorflow&#x2F;cc:tutorials_example_trainer</span><br><span class="line">bazel build &#x2F;&#x2F;tensorflow&#x2F;cc:client_client_session_test￼</span><br><span class="line"></span><br><span class="line">#然后运行程序</span><br><span class="line">.&#x2F;bazel-bin&#x2F;tensorflow&#x2F;cc&#x2F;client_client_session_test￼</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/csuccess.jpg" alt="csuccess"></p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/clienttest.jpg" alt="demo"></p>
<br>

<h4 id><a href="#" class="headerlink" title></a></h4><h4 id="c-demo"><a href="#c-demo" class="headerlink" title="c++demo"></a>c++demo</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;main.cpp</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &quot;tensorflow&#x2F;cc&#x2F;client&#x2F;client_session.h&quot;</span><br><span class="line">#include &quot;tensorflow&#x2F;cc&#x2F;ops&#x2F;standard_ops.h&quot;</span><br><span class="line">#include &quot;tensorflow&#x2F;core&#x2F;framework&#x2F;tensor.h&quot;</span><br><span class="line"></span><br><span class="line">using namespace tensorflow;</span><br><span class="line">using namespace tensorflow::ops;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    Scope root &#x3D; Scope::NewRootScope();</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Matrix A &#x3D; [3 2; -1 0]</span><br><span class="line">    auto A &#x3D; Const(root, &#123; &#123;3.f, 2.f&#125;, &#123;-1.f, 0.f&#125; &#125;);</span><br><span class="line">    &#x2F;&#x2F; Vector b &#x3D; [3 5]</span><br><span class="line">    auto b &#x3D; Const(root, &#123; &#123;3.f, 5.f&#125; &#125;);</span><br><span class="line">    &#x2F;&#x2F; v &#x3D; Ab^T</span><br><span class="line">    auto v &#x3D; MatMul(root.WithOpName(&quot;v&quot;), A, b, MatMul::TransposeB(true));</span><br><span class="line"></span><br><span class="line">    std::vector&lt;Tensor&gt; outputs;</span><br><span class="line">    ClientSession session(root);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Run and fetch v</span><br><span class="line">    TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs));</span><br><span class="line">    std::cout &lt;&lt; &quot;tensorflow session run ok&quot; &lt;&lt; std::endl;</span><br><span class="line">    &#x2F;&#x2F; Expect outputs[0] &#x3D;&#x3D; [19; -3]</span><br><span class="line">    std::cout &lt;&lt; outputs[0].matrix&lt;float&gt;();</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;BUILD file</span><br><span class="line"></span><br><span class="line">load(&quot;&#x2F;&#x2F;tensorflow:tensorflow.bzl&quot;, &quot;tf_cc_binary&quot;)</span><br><span class="line">tf_cc_binary(</span><br><span class="line">     name &#x3D; &quot;main&quot;, #目标文件名</span><br><span class="line">     srcs &#x3D; [&quot;main.cpp&quot;], #源代码文件名</span><br><span class="line">     deps &#x3D; [</span><br><span class="line">        &quot;&#x2F;&#x2F;tensorflow&#x2F;cc:cc_ops&quot;,</span><br><span class="line">         &quot;&#x2F;&#x2F;tensorflow&#x2F;cc:client_session&quot;,</span><br><span class="line">         &quot;&#x2F;&#x2F;tensorflow&#x2F;core:tensorflow&quot;</span><br><span class="line">         ],</span><br><span class="line"> )</span><br><span class="line"> </span><br><span class="line"> &#x2F;&#x2F;执行</span><br><span class="line"> bazel build &#x2F;&#x2F;tensorflow&#x2F;demo:main</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/demo.jpg" alt="demo"></p>
<br>

<p>来bili 看跳转~</p>
<p><a href="https://www.bilibili.com/video/BV1Q54y1b74n">https://www.bilibili.com/video/BV1Q54y1b74n</a></p>
<h4 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h4><p>相关编译、过程中error issue、cmake、bazel以及用c++ api构建线上服务</p>
<p><a href="https://github.com/tensorflow/tensorflow/issues/43654">https://github.com/tensorflow/tensorflow/issues/43654</a></p>
<p><a href="https://xugaoxiang.com/2020/05/22/compile-tensorflow2-with-gpu/">https://xugaoxiang.com/2020/05/22/compile-tensorflow2-with-gpu/</a></p>
<p><a href="https://www.tensorflow.org/install/errors">https://www.tensorflow.org/install/errors</a></p>
<p><a href="https://www.javatt.com/p/43568">https://www.javatt.com/p/43568</a></p>
<p><a href="https://www.jianshu.com/p/72b228223804">https://www.jianshu.com/p/72b228223804</a></p>
<p><a href="http://sixerwang.github.io/2018/12/24/cpp-call-tf/">http://sixerwang.github.io/2018/12/24/cpp-call-tf/</a></p>
<p><a href="https://github.com/hemajun815/tutorial/blob/master/tensorflow/compilling-tensorflow-source-code-into-C%2B%2B-library-file.md">https://github.com/hemajun815/tutorial/blob/master/tensorflow/compilling-tensorflow-source-code-into-C%2B%2B-library-file.md</a></p>
<p><a href="https://cmake.org/cmake/help/latest/guide/tutorial/index.html">https://cmake.org/cmake/help/latest/guide/tutorial/index.html</a></p>
]]></content>
      <categories>
        <category>tensorflow源码</category>
      </categories>
      <tags>
        <tag>tensorflow源码</tag>
      </tags>
  </entry>
  <entry>
    <title>编辑距离</title>
    <url>/2021/03/11/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/</url>
    <content><![CDATA[<br>
<br>


<h2 id="编辑距离"><a href="#编辑距离" class="headerlink" title="编辑距离"></a>编辑距离</h2><span id="more"></span>

<br>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">static int editDist(String str1 , String str2 , int m ,int n)&#123;</span><br><span class="line"></span><br><span class="line">    if (str1.charAt(m-1) &#x3D;&#x3D; str2.charAt(n-1))</span><br><span class="line">        return editDist(str1, str2, m-1, n-1);</span><br><span class="line"></span><br><span class="line">    return 1 + min ( editDist(str1,  str2, m, n-1),    &#x2F;&#x2F; Insert</span><br><span class="line">                     editDist(str1,  str2, m-1, n),   &#x2F;&#x2F; Remove</span><br><span class="line">                     editDist(str1,  str2, m-1, n-1) &#x2F;&#x2F; Replace                     </span><br><span class="line">                   ); &#x2F;&#x2F;三个子问题</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;s: beforeediting,length&#x3D;n  t: afterediting,length&#x3D;m</span><br><span class="line">&#x2F;&#x2F;d: recording the distance</span><br><span class="line">        d &#x3D; new int[n + 1][m + 1];  </span><br><span class="line">              </span><br><span class="line">        for (i &#x3D; 0; i &lt;&#x3D; n; i++) &#123;  </span><br><span class="line">            d[i][0] &#x3D; i;  </span><br><span class="line">        &#125;  </span><br><span class="line">        for (j &#x3D; 0; j &lt;&#x3D; m; j++) &#123;  </span><br><span class="line">            d[0][j] &#x3D; j;  </span><br><span class="line">        &#125;  </span><br><span class="line">          </span><br><span class="line">         </span><br><span class="line">        for (i &#x3D; 1; i &lt;&#x3D; n; i++) &#123;  </span><br><span class="line">            s_i &#x3D; s.charAt(i - 1);  </span><br><span class="line">              </span><br><span class="line">            for (j &#x3D; 1; j &lt;&#x3D; m; j++) &#123;  </span><br><span class="line">                t_j &#x3D; t.charAt(j - 1);  </span><br><span class="line">                 </span><br><span class="line">                cost &#x3D; (s_i &#x3D;&#x3D; t_j) ? 0 : 1;  </span><br><span class="line">                 </span><br><span class="line">                d[i][j] &#x3D; min(d[i - 1][j] + 1, d[i][j - 1] + 1,  </span><br><span class="line">                        d[i - 1][j - 1] + cost);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if x &#x3D;&#x3D; y, then d[i][j] &#x3D;&#x3D; d[i-1][j-1]</span><br><span class="line">if x !&#x3D; y, 插入y则 d[i][j] &#x3D; d[i][j-1] + 1</span><br><span class="line">if x !&#x3D; y, 删除x则 d[i][j] &#x3D; d[i-1][j] + 1</span><br><span class="line">if x !&#x3D; y, x改变为y则 d[i][j] &#x3D; d[i-1][j-1] + 1</span><br><span class="line">When x!&#x3D;y, d[i][j] 取三中编辑方式最小代价。</span><br><span class="line">初始化条件 ： d[i][0] &#x3D; i, d[0][j] &#x3D; j</span><br></pre></td></tr></table></figure>
<ul>
<li><p>为什么不同操作就是对应与d的左移上移或左上移？<br>这个问题递归角度较易理解。<br>DP角度，d记录的是目前最小编辑距离。左、上、左上为子问题，即儿子。若为插入，则j需+1得到t的下一个字符，而x继续与此字符比较，i不变。</p>
</li>
<li><p>由递归到DP，实质就是找到递归中子问题。<br>找到后，将子问题结果记录在数组即可。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>缠解读后感</title>
    <url>/2021/06/01/%E7%BC%A0%E8%A7%A3%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<br>

<br>

<h4 id="练脑子"><a href="#练脑子" class="headerlink" title="练脑子"></a>练脑子</h4><span id="more"></span>

<p>还是得读个自圆其说。如能完满不矛盾地解释，并且释义能够高于一般理解。已然是高一层的解读了。没有思考，便自相矛盾。过段时间再看，又会更透彻些。其中缺陷也会自然显露。于是乎，记下当即的想法。</p>
<hr>
<br>

<p>整体理解是，”当下“、”现实“为最鲜活有生命之物，所有”君子“皆于当下成就。那么，当下又何以把握？即”行“、”一以贯之“。”一“是现实，”之“是”圣人之道“。再白话，就是在当下、现实之中，能够时刻地闻见学行君子之道，能够躬身力行。重点落在行上。如”性相与习相“。孔子要求是改变世界。</p>
<p>比如，按照对颜回的赞赏，能够”食不求饱“，”居不求安“，能”安贫乐道“，陋巷不陋。此为贤也。但是颜回所为，改变世界了么？当然，因其能不受相所困，已然是种作为。相还有就是受自己好恶和知识所困，而不察。而回，”不迁怒，不贰过“，回的理解和言行，都没有偏差，没有那些个小人儒，假儒，也没有乌托邦。</p>
<p>这和”道德“到底有什么相干。孔子不尊崇道德，不尊崇忠孝。尊崇”当下“、”行“、”乐“和”说“、”不相“，”好学“。尊崇富而好礼，贫而得乐。不是勤奋，不是说要遵守纪律，要守“礼”。而是说，要“不相” “当下” 之相。不受当下所困，而能超拔，有自己所乐。总体是自由的，而不是伦理条条框框。小人不过就是选择了小的视野，汲汲于利，忘乎自己是“人”，而天地人之心的人，岂能为利字所困？而不得得乐呢。岂不是“小”了太多。</p>
<p>进一步，也不能将现实作为本位、先验。因为现实是可改变的。改变现实，与天其时的君子，才是闻见学行对照、调整的对象。”见机而作、明时通变。“</p>
<p>因为当下去实践，才能够保持道的鲜活和生命力。</p>
<hr>
<br>

<p>文中，勾连着马克思、海德格尔、康德，易经。大体结论是，孔子和马克思遥相呼应，不同于康德（列宁是康德的延伸，因其都是必然如何走向自由，而与孔子无关，孔子并不假设从必然到自由有一条路，孔子认为人本是自由，他更强调自由意志在现实中的实践，孔子认为实践才是皈依 ），高明于海德格尔(因为海纠结于无为什么不存在，而孔子直接认为，有因无才成为有，”不患，无位”。无位次不意味”不存在“。也即存在与否不重要，重要的是无位次与位次)，符合易经(如艮卦)。</p>
<p>无位次与位次：就像无味、甜、咸。有了无味，才能区别出甜。有了震荡，才能区别出来趋势。</p>
<p>人“原罪”，要上帝or理性来“救赎”。但是，没有上帝，也不是理智的光辉给你救赎，也不是科学唯上。关键在于”人“，若握住理性，则为我所用，展现我的言行，若”无知”，也未尝不可，但也要”无愠“才得道。也不是”人本“，主义若是本源、本位，则又丧失了同样的 – 人。人本不过是种秉持的主义。有了人本主义为本位，人就不在本位。</p>
<p>而人，即君子。为天地之心。天地为何要有心？</p>
<p>你只有像“人”一样地承担，承担你的当下。并且固守如此，而不是朝三暮四。把承担，看成成就之事，若能“乐”于此承担，岂不妙过其他？</p>
<p>人无非两种，”不知“与”闻见学行“。”不知“者，要么小人，要么闻见不彻底、而得过且过，随波逐流，亦为一种不知。更恰当 的是，每个人都有这两种成分。</p>
<p>人容易相，富有富者相，贫有贫者相。善人为邦，要”胜残去杀“。不是杀灭，而是慢慢地不相、不愠、消除。</p>
<p>”庶之、富之、教之“，多样性和差异性，到全面发展、层次性，再到不相、不愠之知。”必世而后仁“也是这个意思，先后有序，先”富庶“再谈”仁“。”仁“”教“即不相，即有教无类。为什么先”世“？</p>
<p>还有一些告诫，如”放于利则多怨“、”使民战，既往不咎“等，要”即戎“。</p>
<p>”患与不患“，”人不知与不己知“，”不在其位不谋其政“等，都是在说无位次与位次的问题，无和有的问题。无是没有位次的，算是先验吧，有因为无的无位次而有位次。就像，有些必然的、其他因其必然而有关联，而产生程度和范围，而有了自己一定的概率，虽然不至于必然的1，但有了概率，有了位次。永远无法改造人，无法达到己知，那只能从不知不愠出发，而达到君子世界。因不在其位，无法达到，有现实的局限和鲜活性，即”知新“，所以应当不谋其政。也是顺应天时。与天其时。</p>
<br>



<hr>
<p>道家要无为，因其立论在有天道，有一，而人因返璞归真，回到自然状态，撇除人为达到无为、无人为。但孔子是有为的，”认识从实践中来，再指导实践，实践是认识的根本来源，从来达到螺旋上升“。和马克思的理论接近。</p>
<br>



<hr>
<p>关于学，”多闻“而”识“。左媒和党媒的天下，不是多闻，难以多闻，而不多闻，如何择善而从呢，如何能识呢。不能识，如何能知。当下入世磨炼，才能多识而有知啊。”犹恐失之“，陷于一些无用的情绪中，就会失去承担。差异多样的学、多闻，要和同一的思相结合，多样性要有、理论性高度要有。</p>
<p>不效仿人，而效仿道，见解，识辨。</p>
<br>



<hr>
<p>关于小人。不能依附也不能违背，”小人难养“。要受其历练，才能成就。”学而优则仕“也是”小人儒”。君子是”周而不比“、”喻于义“的，小人就….不要以”相“，看君子小人。</p>
<br>



<hr>
<p>很多是一意多词：”位次 先验“、”不相“、”同、和“；“承担”、“固守 不舍昼夜”；”闻 识 思 学”…</p>
<p>之间的关系：</p>
<p>以”不知不愠“为目的。要在每一个”当下“”固守“”不相、与天其时天与其时、无有位次、和而不同“，在当下”习相“以保持道的鲜活。</p>
<p>步骤：</p>
<p>按照”学、立、不惑、知天命、耳顺、从心所欲不逾矩“，先多闻多见，学而思，并在现实中多识，明时通变，积累一段时间则能知道事物的位次，客观关系、万事万相等，即看山是山，再然后，由位次上升到无位次，无位次即无相，即看山不是山，在后来，能明白天机天时，能明时，六十，闻见的道理，明白的天时，能落实实践。七十，君子之道的规矩方圆，大体已经融入习惯，游刃有余，能够顺人心同时不违背道。</p>
<hr>
<h4 id="辟谣"><a href="#辟谣" class="headerlink" title="辟谣"></a>辟谣</h4><br>

<p>我一直认为自己的理性和客观规律是最高权威了。然而规律是要”利用“的。人要练。脑子要练。心要练。观念要刷洗。</p>
<p>一直认为思之”邪“要去除，要“破”，然而”不知“如果是以”攻乎异端“的方式来解决，则”斯害也已“，破除的方式本身，就是不符合君子之道的。那如何煮米成饭呢，就要坚持闻见学行。即使无法做到，也当”不愠“。让它放在那里，不愠，不能扰乱主成分。</p>
<p>认为”民主“至少比”不民主“或者”社会主义民主“要好，”市场“要比”政府“要好。然而这样的比较，就是在判断客观性上哪一个主义更优越。然而这样比较意义何在呢？不和当下结合起来衡量，不和现实与人，结合起来，又是把本位给设定前提了而已。</p>
<p>一直以为要掌握规律，通过此来分析问题解决问题。但是，没有任何先验逻辑存在，具体问题具体分析，是从现实问题出发，而不是有一个通用原则。</p>
<p>慎言慎行之类的位次是后的，多闻见，有识有知的慎，才不是人云亦云的。</p>
<p>乱说话多人太多。</p>
<br>



<hr>
<h4 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h4><p>整体也算融汇起来。也是有中心，然后顺着去注解。</p>
<p>大体是怎样的世界，怎样去做的第一原则。然后顺着这个第一原则，展开注解。</p>
<p>一意多词太常见了。</p>
<p>主要观点还是， 位次与不相、明时、现实实践而一贯。目的为不知不愠，皈依为现实实践，标准为不相、明时，要求为固守。</p>
]]></content>
      <categories>
        <category>读后感</category>
      </categories>
      <tags>
        <tag>读后感</tag>
      </tags>
  </entry>
  <entry>
    <title>股市基础</title>
    <url>/2021/06/20/%E8%82%A1%E5%B8%82%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<br>
<br>

<p><strong>委比&amp;委差</strong></p>
<span id="more"></span>

<p>就是委托买入卖出的一个差值，但是很多软件只是显示了5档，所以只是这5挡的一个差异。</p>
<p><strong>量比</strong></p>
<p>当天成交量和最近5日平均成交量的比值</p>
<p><strong>换手率</strong></p>
<p>成交数量和流通股的一个比率，换手率高就代表成交活跃，换手率低代表成交不活跃。</p>
<p><strong>外盘&amp;内盘</strong></p>
<p>外盘是以卖出下单成交的交易，内盘是以买入下单成交的交易</p>
<p>外盘+内盘=成交量</p>
<br>



<p>左侧交易</p>
<p>最大回撤</p>
<p>挂单</p>
<p>老鼠仓</p>
<p>封板</p>
<p>扫货</p>
<p>敲单员</p>
<p>集合竞价</p>
<p>摆单、吃单、补单</p>
<p>2013光大证券 乌龙指</p>
<p>18线家电太次了</p>
<p>某个价位突然地补单</p>
<p>落袋为安和降低持仓成本</p>
<p>资金动向和主力手法</p>
<p>杀某行的多头头寸把原油价格打到负数</p>
<p>盘口 市场深度</p>
<p>资金流入：</p>
<p>主动性买盘 是资金流入；现价买入，计入资金流入</p>
<p>买一到买四的都被击穿。股价下跌</p>
<p>并购换股为什么不稀释股权</p>
<p>预增</p>
<p>中概股</p>
<p>KDJ、MACD、WMS、布林线、RSV、RSI、PSY、OBV、OBOS、乖离率、资金流入流出、ADL……</p>
<hr>
<h3 id="游资逻辑"><a href="#游资逻辑" class="headerlink" title="游资逻辑"></a>游资逻辑</h3><p>1、</p>
<p>开盘价不高。因为开盘价不高，有些人会纪律止盈，然后早上分时就有个下探。下探后是正常的，能不能被拉起来，就得继续观察了。这还得看资金活跃度，还有分时是否稳定。</p>
<p>盘子相对比较大的票，而且关注度很高，成交量很大，分时也相对稳定，这种情况可信度就会高一些。所以就可以追高。但<strong>这个手法的特点是，位置越低越安全</strong>。做一进二的时候成功率会高点。但是下午就被砸了。这个原因就是市场有记忆效应，可能认为小康会重复昨天的走法。然后就会有些赌徒因为错过了昨天的小康，开始手痒，于是就介入了。</p>
<br>

<p>2、</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>能源</title>
    <url>/2021/06/08/%E8%83%BD%E6%BA%90/</url>
    <content><![CDATA[<br>

<p>1、提升非化石能源在一次性能源消费比重 、电能在终端能源消费比重</p>
<span id="more"></span>

<p>2、</p>
<table>
<thead>
<tr>
<th>目标</th>
</tr>
</thead>
<tbody><tr>
<td>60亿吨标准煤</td>
</tr>
<tr>
<td>化石能源20%</td>
</tr>
<tr>
<td>非化石发电50%</td>
</tr>
<tr>
<td>全社会用电7万亿千瓦时</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>英文</title>
    <url>/2021/07/18/%E8%8B%B1%E6%96%87/</url>
    <content><![CDATA[<br>

<br>



<h5 id="0718"><a href="#0718" class="headerlink" title="0718"></a>0718</h5><span id="more"></span>

<p>I slept for the whole day yesterday,felt exhausted. I got no interest in doing anything inculding sleeping. Time flys ,but i still don’t have any resolution to start anything.When i find others so exsited and vigorous,i just cannot figure out why.I have to do list which already accumulated a lot, still cannot impel myself to complete any of those things.</p>
<p>I spent almost three hours searching the internet for this pdf called 《Fiasco:the inside of a Wall Street trader》which is recommeded by Charlie Munger .  I googled for the pdf version and alsosearch it on the apps like  PDD ,Amazon, DangDang and TaoBao. I asked so many client services  of the stores in  PDD, and they all only had the Chinese ebook which i barely had interest in. This was  a complete waste of time that they both assured me that they had the English version but after i placed the order they told that they found what they got is Chinese. WTF. </p>
<p>My writing only reached primary level…</p>
<p>I conclude that when i need to search for some resources  the first and foremost i should list who would have the answer or where have i met with the problem before. In line with this idea, i would get the collection of websites used for downloading free pdf ,also the websites once had been favorited by myself should come out since they might be valuable tools i once had found  to navigate to resources.In one word, when  confronted with problem related to locating resources ,instead of searching right away, extract information from your memory or brain’s database to  pick up the train of thought and then carry out the ideas to test and verify.</p>
<br>





<h5 id="0719"><a href="#0719" class="headerlink" title="0719"></a>0719</h5><p>My mood cannot be predicted by any signal.I changed as if i forgot what happened before.I just talked  with Gao about the future with expectation,but yesterday i still had a gloomy  perspective.</p>
<p>I talked a lot and then i ate a substantial lunch and had a break.I felt different from last week  for the less i thought about how  unpleasant the trivialities were and what a meaningless life i was living for the more relaxed i became.So i thought to myself that maybe i was too stressed and it was no good at all .And it occured to me that the only pressure I’m under is the pressure I’ve put on myself.</p>
<p>I was more focus on the solution of problems rather than how hard to figure it out.The more difficulty i put in the progress of finding the answer the less effort i would to try to stimulate my enthusiasm to be more aggresive and adventurous.</p>
<p>I was awake until the 5am playing around staff i even didn’t care. I genuinely had no will to move forward and everything marched very much depended on my mood. Moods are never predicable  even they belongs to yourself.</p>
<p>I  made plans and made a list including what i expected myself to accomplish like daily English news reading and writing ,also daily preparations for the entrance exam. </p>
<br>



<h5 id="0720"><a href="#0720" class="headerlink" title="0720"></a>0720</h5>]]></content>
      <categories>
        <category>英文写作</category>
      </categories>
      <tags>
        <tag>英文写作</tag>
      </tags>
  </entry>
  <entry>
    <title>药</title>
    <url>/2021/07/28/%E8%8D%AF/</url>
    <content><![CDATA[<br>





<h4 id="药"><a href="#药" class="headerlink" title="药"></a>药</h4><ol>
<li>曲安奈德益康挫乳膏<span id="more"></span>，哈西奈德溶液</li>
<li>莫匹罗星软膏</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>行为分析</title>
    <url>/2021/06/01/%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<br>

<br>

<h4 id="言行之蠢"><a href="#言行之蠢" class="headerlink" title="言行之蠢"></a>言行之蠢</h4><span id="more"></span>

<p>1、”只此一次，以后不再。不用担心***“  –&gt; 自欺欺人，借口</p>
<p>2、”我先去**，就很好了。“ – 环境不行。”再去那里吧，万一被占了呢，嗯不会的。“ –被占了。”太烧钱，不去了，不想回去“ –来回走。 –&gt;“钱” 和 犹豫</p>
<p>3、“我再看会。看这个有什么用呢，做这个没用啊。”  –&gt;消极</p>
<p>4、“唉，不过是总结，又会忘，有什么意义呢。”  –&gt;消极</p>
<p>5、“算了，退掉吧。我担心*** ”  –&gt;犹豫</p>
<p>6、“我就是想舒服下。看看书，想想事，享受美… 能大量消化知识”  –&gt;自欺欺人 ，不计后果，不考虑代价</p>
<p>7、”我可以控制，这样才合理…“ </p>
<p>8、”没意义，一直在浪费时间做没意义的事情“</p>
<p>9、”又耽误了… “</p>
<p>10、“嗯，是的… 嗯嗯”</p>
<p>11、小表情，反驳</p>
<p>12、“明天少点…现在不想控制” –&gt;接口，重复，没有真的要做事，就像”施“</p>
<p>13、“这个是你的代码啊，我不知道recall逻辑，我怎么测。” “那你那个…给我吧，行吧，这个可以吧” “好” –&gt; 安静。 “第一，我是会造数据。现在的问题是，比如，我把出的商品 mock到活动队列，逻辑是什么。“</p>
<p>14、</p>
<br>

<p>会重复一遍又一遍。太阳升起又是一天，就又来一遍。</p>
<hr>
<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>1、找到“疏漏”，精准，而不是东榔头西棒槌地尝试。精准需要背后有思量。</p>
<p>2、找到症药，“ trick”，解决问题。</p>
<p>3、“缺乏信心，才导致去寻求其他欲望满足，而不选择固守。” </p>
<p>4、更多情况，只是想“拥有”外物，而并非真的“需求”，而当你真的想在解决问题时，其他是没有什么多余欲望的。</p>
<p>5、只有不断地突破，反抗自己已认知的。才能慢慢再次形成新的认知。</p>
<br>



<hr>
<h4 id="大嫂"><a href="#大嫂" class="headerlink" title="大嫂"></a>大嫂</h4><br>

<br>

<p>和大嫂聊天的时候，没有勇气反驳，没能马上反应出来问题。</p>
<p>一个是谈的具体东西体味不深刻，二是，自己不够放松。总是限制在她的话语里，而不能足够自信去拆解。</p>
<p>这个可以解决。反驳是不好的，克制反驳是对的，但是不同观点的理性表达是应该的。不要觉得自己的提问愚蠢，也不要认为沉默是糟糕的。即使问题比较愚蠢，但是不同的表达方式会产生不同的效果。</p>
<p>so，表达和提问都是必要的。大可随意些，但是用词需要考虑。考虑到用词的效果后表达，并且尽量让大脑放松，即可。问题的关键是什么，他们陷入什么思维里，边角的问题是什么。</p>
<p>1随意自信些 2表达不同意见，注意用词效果 3提问和思考  4顺着直接感受的表露就是小表情，等待下、放下</p>
<p>其实都是各种经验的交流，以及想法的表达。各自对某一方面的理解。对“打比方”的攻克是什么。我是不是只是想在语言上战胜？</p>
<p>“打比方”的方式，为了抽象，不谈具体而谈模式和思路。概括全貌和思路。</p>
<br>

<p>讨论的话题：大哥这个人、他们的历史-唐伟和他老婆、管理人的问题-小五-(人和思路、模式、技巧)、创业的策划思路、她见过的“人”以及“生意”以及“模式”、物流的问题和她的见解、房子的贷款问题、婚姻(平衡、空间)等等。</p>
<p>我思考过生意？她见过的那些我是否有自己的交叉部分？她的思路是否可行和落地？我对这个是否有自己的想法？大哥对她的影响、她目前的局限和需要？</p>
<p>对这些我没有更进一步的思考。也没有意识到能够如何给我带来什么。没有利用起来。我觉得这个是需要思考和讨论的。并且是真的用心去思考。因为这个是我的现实和我的当下。</p>
<p>可做的事情非常多。只是是否选择去真的在当下里去取舍和判断。怎样的当下。如何能更好利用当下。</p>
<br>



<hr>
<br>

<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>1、行为考虑因素太多，也意味着受制于太多，显得没有生机。比如，去干啥干啥，又会犹豫，觉得耽误，没有意义。然后手里正在做的事情，不过更缺乏意义。并且还是减少能量消耗的一种方式。人应该勇敢果决地选择多去消耗能量。并且努力地去想办法补充能量，从而形成正向的循环。</p>
<p>2、lazy，用时加载。按点满足。欲望，总是延迟也是问题。需要安排适当。才是正道。</p>
<p>3、效率低。没有计划。软绵绵的。周日只看了四课。我不知道这个理论是否有用。以我的智商，完全无法判断。但是我知道，他们都看过。比如谷。那么他们都看过的，我必须看过。但是我效率低很多。是我自己还是没有全身心地放在这里。</p>
<p>4、中午又40min买吃的。大部分时间，不是买东西，就是在聊天。当然我觉得聊天是好的。买东西需要减少。我想减少。包括各方面的需求。从什么时候开始，我越来越多的需求呢。并且还想着要花费更少。如果我突破不了，静不下来，没有办法真的从其中去悟，那么我始终被自己打败。还是我的脑里在逃避问题。逃避去看。我每天给自己的计划，反馈力度不够。</p>
<p>– 受制于太多而选择节约热量而不是去补充和消耗，生命在于消耗、快速反应</p>
<p>–逃避计划中的事情，总是在意于效果和变化，而不把脑子放在问题本身上。缺少反馈和调节，比较僵硬。也没有调动好积极–</p>
<br>

]]></content>
      <categories>
        <category>读后感</category>
      </categories>
      <tags>
        <tag>读后感</tag>
      </tags>
  </entry>
  <entry>
    <title>说服的技巧</title>
    <url>/2021/05/25/%E8%AF%B4%E6%9C%8D%E7%9A%84%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>贝叶斯网络</title>
    <url>/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<br>

<h2 id="概率图"><a href="#概率图" class="headerlink" title="概率图"></a>概率图</h2><ul>
<li><p>概率图模型分为贝叶斯网络（Bayesian Network）和马尔可夫网络（Markov Network）两大类。</p>
<span id="more"></span></li>
<li><p>贝叶斯网络可以用一个有向图结构表示，马尔可夫网络可以表示成一个无向图的网络结构。若随机变量Y构成一个无向图 G=(V,E)表示的马尔科夫随机场（MRF），则条件概率分布P(Y|X)称为条件随机场（Conditional Random Field, 简称CRF。</p>
</li>
<li><p>更详细地说，概率图模型包括了朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型等，在机器学习的诸多场景中都有着广泛的应用。</p>
</li>
</ul>
<p><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E6%A6%82%E7%8E%87%E5%9B%BE.png" alt="概率图"></p>
<h2 id="频率派与贝叶斯派"><a href="#频率派与贝叶斯派" class="headerlink" title="频率派与贝叶斯派"></a>频率派与贝叶斯派</h2><p>频率派与贝叶斯派各自不同的思考方式：</p>
<p>频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值，同时，样本X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X 的分布；</p>
<p>而贝叶斯派的观点则截然相反，他们认为参数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布。</p>
<p>贝叶斯派既然把概率看做是一个随机变量，所以要计算概率的分布，便得事先知道的无条件分布，即在有样本之前（或观察到X之前），有着怎样的分布呢？这种在实验之前定下的属于基本前提性质的分布称为先验分布，或着无条件分布。</p>
<p>其中，先验信息一般来源于经验跟历史资料。而后验分布π（θ|X）一般也认为是在给定样本X的情况下的θ条件分布，而使π（θ|X）达到最大的值，称为最大后验估计，类似于经典统计学中的极大似然估计。</p>
<h2 id="判别和生成"><a href="#判别和生成" class="headerlink" title="判别和生成"></a>判别和生成</h2><p>常见的概率图模型有朴素贝叶斯、最大熵模型、贝叶斯网络、隐马尔可夫模<br>型、条件随机场、pLSA、LDA等。</p>
<p>朴素贝叶斯、贝叶斯网络、pLSA、LDA等模型都是先对联合概率分布进行建模，然后再通过计算边缘分布得到对变量的预测，所以它们都属于生成式模型；而最大熵模型是直接对条件概率分布进行建模，因此属于判别式模型。隐马尔可夫模型和条件随机场模型是对序列数据进行建模的方法，将在后面的章节中详细介绍，其中隐马尔可夫模型属于生成式模型，条件随机场属于判别式模型。</p>
<p><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E5%88%A4%E5%88%AB%E4%B8%8E%E7%94%9F%E6%88%90.png" alt="判别与生成"></p>
<p>我的视频讲解(<a href="https://www.bilibili.com/video/BV16y4y187pE">https://www.bilibili.com/video/BV16y4y187pE</a>)</p>
<h2 id="解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？"><a href="#解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？" class="headerlink" title="解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？"></a>解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？</h2><p>先验概率：就是因变量（二分法）在数据集中的比例。这是在你没有任何进一步的信息的时候，是对分类能做出的最接近的猜测。<br>似然估计：似然估计是在其他一些变量的给定的情况下，一个观测值被分类为1的概率。例如，“FREE”这个词在以前的垃圾邮件使用的概率就是似然估计。<br>边际似然估计：边际似然估计就是，“FREE”这个词在任何消息中使用的概率。</p>
<h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><p>贝叶斯网络(Bayesian network)，又称信念网络(Belief Network)，或有向无环图模型(directed acyclic graphical model)</p>
<p>例子：<br><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C.png" alt="贝叶斯网络"></p>
<h3 id="结构形式"><a href="#结构形式" class="headerlink" title="结构形式"></a>结构形式</h3><p>1、 a-&gt;c b-&gt;c</p>
<p>P(a,b,c) = P(a)P(b)P(c|a,b)成立，即在c未知的条件下，a、b被阻断(blocked)，是独立的，称之为head-to-head条件独立。</p>
<p>2、c-&gt;a c-&gt;b</p>
<p>考虑c未知，跟c已知这两种情况：</p>
<p>在c未知的时候，有：P(a,b,c)=P(c)P(a|c)P(b|c)，此时，没法得出P(a,b) = P(a)P(b)，即c未知时，a、b不独立。</p>
<p>在c已知的时候，有：P(a,b|c)=P(a,b,c)/P(c)，然后将P(a,b,c)=P(c)P(a|c)P(b|c)带入式子中，得到：P(a,b|c)= P(a|c)*P(b|c)，即c已知时，a、b独立。</p>
<p>3、a-&gt;c-&gt;b</p>
<p>还是分c未知跟c已知这两种情况：</p>
<p>c未知时，有：P(a,b,c)=P(a)P(c|a)P(b|c)，但无法推出P(a,b) = P(a)P(b)，即c未知时，a、b不独立。</p>
<p>c已知时，有：P(a,b|c)=P(a,b,c)/P(c)，且根据P(a,c) = P(a)P(c|a) = P(c)P(a|c)，可化简得到：<br><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/headtail.png" alt="headtail"><br>所以，在c给定的条件下，a，b被阻断(blocked)，是独立的，称之为head-to-tail条件独立。<br>这个head-to-tail其实就是一个链式网络。</p>
<h3 id="因子图"><a href="#因子图" class="headerlink" title="因子图"></a>因子图</h3><p>wikipedia上是这样定义因子图的：将一个具有多变量的全局函数因子分解，得到几个局部函数的乘积，以此为基础得到的一个双向图叫做因子图（Factor Graph）。</p>
<p>通俗来讲，所谓因子图就是对函数进行因子分解得到的一种概率图。一般内含两种节点：变量节点和函数节点。我们知道，一个全局函数通过因式分解能够分解为多个局部函数的乘积，这些局部函数和对应的变量关系就体现在因子图上。</p>
<p>根据贝叶斯网络的例子，</p>
<p float="left">
  <img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/factor.png" width="48%">
  <img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/推导.png" width="48%"> 
</p>


<h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><p>1、求某个变量的边缘分布是常见的问题：这问题有很多求解方法，其中之一就是把贝叶斯网络或马尔科夫随机场 转换成 因子图，然后用sum-product算法求解。换言之，基于因子图可以用sum-product 算法高效的求各个变量的边缘分布。</p>
<p>2、</p>
<p>reference：<br><a href="https://blog.csdn.net/v_july_v/article/details/40984699">https://blog.csdn.net/v_july_v/article/details/40984699</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>货币</title>
    <url>/2021/05/22/%E8%B4%A7%E5%B8%81/</url>
    <content><![CDATA[<br>



<h5 id="货币需求"><a href="#货币需求" class="headerlink" title="货币需求"></a>货币需求</h5><span id="more"></span>

<ul>
<li>1马克思货币量</li>
</ul>
<p>​    M = P*T/V</p>
<p>​    货币需要和商品流通相适应。</p>
<br>

<ul>
<li>2费雪方程式</li>
</ul>
<p>​    MV = PT</p>
<p>​    M是外生变量，V是制度型因素、短期不变、视为常数，T对产出水平保持一定比例 =&gt;P取决于M</p>
<p>​    只有M和T保持1/V的比例，才能保持既定的价格水平。</p>
<br>

<ul>
<li>3剑桥方程式</li>
</ul>
<p>​    Md = kPY</p>
<p>​    资产的角度而非交易；存量而非流量；微观而非宏观</p>
<br>            

<ul>
<li><p>4凯恩斯货币需求</p>
<p>交易、预防、投机</p>
<p>预期利率下降，则人们选择多持有债券，Md减少。</p>
<br></li>
<li><p>5弗里德曼货币需求</p>
<br></li>
<li><p>6</p>
</li>
</ul>
<br>]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>通货膨胀</title>
    <url>/2021/02/17/%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>摘要</strong></p>
<p>通胀的基础知识、历史</p>
<p><br> <br></p>
<h1 id="谨记"><a href="#谨记" class="headerlink" title="谨记"></a>谨记</h1><p><strong>只是看了一本书，必然受作者局限。作者有误，不要认为天经地义。勿从众。勿从威。</strong></p>
<p><br> <br></p>
<h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><h4 id="古罗马的铸币成色下降"><a href="#古罗马的铸币成色下降" class="headerlink" title="古罗马的铸币成色下降"></a>古罗马的铸币成色下降</h4><p>当铸币被减少成色时，首先感知的是发行者，最终才是到物价上。最终感知的人的财富被转移到发行者手里。在刚减少、甚至未减少前感知到时，购入资产–窖藏黄金。此时流通的多为劣币。随着铸币数量增多，价值越跌。而执政者采取的冻结物价的措施，结果却是，规则越来越细、维护规则而设立的官员数量越来越多，触犯规则而死亡的人越来越多，市场越发萧条、充斥恐惧。</p>
<span id="more"></span>
<p>=&gt;<br>无法用立法(价格控制)取代经济规律。通胀是更深问题的表象。</p>
<h4 id="西班牙的白银涌入"><a href="#西班牙的白银涌入" class="headerlink" title="西班牙的白银涌入"></a>西班牙的白银涌入</h4><p>特殊的是，大量涌入没有带来恶性通胀。</p>
<p>1白银涌入<br>2西班牙物资缺乏，多进口<br>3白银官价低于真实价值<br>4人们没有认识到价格上涨的原因在于白银本身供给</p>
<p>=&gt;<br>物价上涨，人们窖藏白银，市场中白银少，流通的为劣币。拥有白银的人以为富有、白银价值不变，不进行生产投资。在舶来品和军事冒险上消耗白银，却未能将白银投入促进生产。流通货币成色降低导致物价进一步上涨。但因为滞后的工资上涨、军事冒险、皇室奢靡、文艺的黄金时代，西班牙一度凭借美洲白银输入和通胀，而使得其王室贵族强极一时。富有者的财富得到巩固，农奴制进一步加强。北欧的革命、宗教革命都未能影响西班牙。</p>
<p>而当美洲白银输入回落，西班牙落后。</p>
<p>=&gt;<br>货币更多不等于财富更多；西班牙最终并未能将货币转为资产、留住涌入的白银的价值；白银收入减缓了制度变革的压力，但并非一件好事；通胀具有传导性，通过贸易传导到其他国家；温和的通胀有助于短期繁荣；虽然金属货币铸造需要成本和受采矿影响，但同样会产生通胀，而纸币受发行者–中央政府官员的良心影响，与通胀的结合就更为天然。</p>
<p>=&gt;<br>工资上浮，并不意味这购买力增加，需要警惕货币幻觉；<br>财富如果不投资、自己不进行变革成长，则长期自然落后，受货币幻觉影响而躺在舒适里，并非好事。<br>大水漫灌的虚假繁荣。</p>
<h4 id="约翰劳的纸币"><a href="#约翰劳的纸币" class="headerlink" title="约翰劳的纸币"></a>约翰劳的纸币</h4><p>法国政府财政赤字重,政府用纸币支付、削减债务<br>1716/05，通用银行成立<br>10月，银行券可支付税收</p>
<p>1717/8，西印度公司成立</p>
<p>开始疯狂:</p>
<blockquote>
<p>1718/12，承诺12%-40%股息回报，垄断烟草业，获得铸币权，接手包税公司(交税给国家、从民众手里征税)，银行大量发行纸币，将国债成功转为股票。<br>出售更多的股票来支付股息。高于市场价33%购入自己公司股票期货。向原始股东限售股票。继续发行大量纸币。</p>
</blockquote>
<blockquote>
<p>1720/1，投放一年前9倍的纸币，新股发行价为40倍，保证金交易出现。</p>
</blockquote>
<p>开始破灭:</p>
<blockquote>
<p>1720上半年，亲王要求兑成硬通货。和东印度合并、高股价回购都不能挽回局势。<br>1720/05，发布规定6个月内纸币贬值，股票降价各50%。</p>
</blockquote>
<p>工薪和小店主受打击小–物价和工资同步地涨跌、且幅度不算太高，投机者有的丧失了所有财富。<br>疯狂到破灭，也就一年时间。</p>
<h4 id="大陆币"><a href="#大陆币" class="headerlink" title="大陆币"></a>大陆币</h4><p>1、战争时期，货币发行来获得物资，导致事后的物价抬升、货币贬值。通过通胀转移私人财富资源用以战备。多种手段，通胀是其一。<br>2、战后的纸币贬值，此时纸币退出流通，或者限制物价，导致通货紧缩。则债务人负担被动加重、物价骤降、经济萧条，价格秩序又被流动性减少而打破。因此，更好的策略是维持战时的物价水平，并保证流动性，促进经济复苏。<br>3、工薪阶层希望价格恢复到战前水平，但是价格本身就是相对的，之前价格的绝对值并不具有意义。战后的价格本应是绝对值高于战前的，而和货币发行相适应。</p>
<p>=&gt;<br>如果流动性和通胀不断地上升或者继续，房价水涨船高，并不会跌回十年前。除非极大萧条，利率极高，央行官员不热爱财富、国家不追求GDP。价格的绝对值，无意义。</p>
<h4 id="法国大革命和分配券"><a href="#法国大革命和分配券" class="headerlink" title="法国大革命和分配券"></a>法国大革命和分配券</h4><p>1788，旱灾导致物价上涨<br>1789，法国政府严重赤字<br>1789/07，攻占巴士底监狱<br>1790/04，革命者没收教会财产，发行分配券，首次利率3%<br>1792，反革命战争爆发<br>1793，路易十六上断头台，9月价格管制出台，开始了恐怖统治；没收有钱人的不动产<br>1794，罗伯斯庇尔遇害，开始了白色恐怖，直到1799拿破仑掌权<br>1795，限价法被废，通胀、经济复苏，粮食价格飞涨，开始了反革命行动 – 烧纸币，恢复铸币，经济秩序(价格)迅速恢复稳定<br>1796，距离首次发行分配券6年不到，分配券面值已经为最为保证的教会财产的20倍。</p>
<p>1770-1787通缩；89-1796通胀；97-1870通缩。</p>
<blockquote>
<p>发行分配券后很快开始贬值，金银开始窖藏，劣币再次驱逐良币。<br>当政府宣布废弃分配券时，大量的分配券沉淀在普通公民手中，他们没有将财富及时转换为永久价值的物品。<br>而工资滞后于物价上涨，工薪和固定收入者丧失了购买力。<br>社会里都是暴发户、投机者、穷人。<br>民众抢掠商人。商业活动萧条，易货贸易、违法交易层出不穷。</p>
</blockquote>
<blockquote>
<p>起初，企业主和大众即布尔乔亚和工人，联合起来对付王室。王室倒台后，开始内斗。最高限价法标志这个马克思阶级斗争的开始。企业主获得了胜利，管制被解除。</p>
</blockquote>
<p>=&gt;<br>历史再度重演。和当初古罗马的通胀没什么区别，都是价格管制来控制，结果无效。和大陆币也区别不大，通胀都是在战争时的手段(通过抬升物价，降低人民生活水平而获得资源，人物力投入到战争)，并以管制解决，然后无效。战争和革命，虽然让社会进步，但不止以士兵生命为代价，还有穷人和富人们。投机者却大发横财。</p>
<p>而人们却没有察觉。</p>
<h4 id="美国内战时期北方"><a href="#美国内战时期北方" class="headerlink" title="美国内战时期北方"></a>美国内战时期北方</h4><p>1836，各州私立银行涌现<br>1861的联邦开支占GDP2%，1865则为26%。联邦政府债务高达28亿美元，33倍于战前，占GDP一半<br>1861年底，纸币不再兑付黄金，18年后才恢复兑换<br>1862，纽交所开设黄金交易</p>
<p>1860-1864，<br>教师工资上涨20-30%，物价上涨到两倍多。则实际工资下降了40%。</p>
<p>1864-1896，<br>由于铸币拥护者的政策导致的绿币升值(与黄金的比对由61%上升到战前的100%)+贸易里其他国家物价下跌，导致物价下跌。最大跌幅65%。30年物价回到世界平均水平。<br>其他国家物价下跌，由于黄金增长低于商品增长。</p>
<blockquote>
<p>白银黄金比价：1867，16：1；1896，31：1；1989，71：1。<br>如果在1867年实行白银本位，则劣币驱良。黄金被窖藏。而流通的白银(因其贬值)，反而可能会促使价格回升。</p>
</blockquote>
<p>=&gt;<br>温和通胀有效，而通缩不利于经济。<br>一国货币贬值，则物价相对于其他货币稳定的国家是上涨的。<br>硬通货的拥护者占了上风，要求按照战前水平兑付黄金，使得货币升值，通缩产生。</p>
<h4 id="德国马克"><a href="#德国马克" class="headerlink" title="德国马克"></a>德国马克</h4><p><strong>1、事实：</strong></p>
<p>每月价格上涨50%–&gt;恶性通胀；马克的恶通胀认为导致了希特勒。</p>
<ul>
<li><p>通胀从小跑进入狂奔<br>一战后的德国，物价是战前的2.5倍，一年后上涨3倍，一年后又上涨2倍多，后来的几个月停顿了下。<br>1921年11月，为战前的40倍。<br>1922年11月开始进一步加速。23年年中开始恐怖狂奔。年末停止。<br>算下爬行阶段耗时2年多，小跑1年半多，狂奔半年多。最高能一个月涨二十多倍。小跑中间也有几个月算温和。<br>23年1月，法国进入鲁尔盆地，8月政府宣布征税，11月与法国战败。此种种不满终于带来了希特勒发动的啤酒馆暴动。</p>
</li>
<li><p>现象<br>民间的借贷利息是政府的120倍。<br>政府和企业主精英重建了工厂，巩固了权力，而对各个家庭生活或者不同经济部门，何等不平等不公正。<br>低利息时大肆借贷购入不动产和商品，马克贬值再用马克归还，产生了新富。<br>政府注销了所有内债。<br>农民享受了高的粮食卖出价格，抵押贷款偿债压力减弱。<br>实际工资下降，真实工资已经低于维持生活的需要。<br>工会要求工资同生活费用指数挂钩，但是消费在一周后进行，当前的指数仍然无法避免手里的钱一周后失去价值。<br>死亡、移民、犯罪上升。<br>耐用品的价格不再取决于需求，而是取决于一周后获得的成本，一周后可能需要双倍的当前价格。<br>汇率贬值-&gt; 国内物价上升-&gt; 增发纸币-&gt; 进一步贬值-&gt; 物价进一步上升  的恶性循环。<br>但，<br>20-23年间国GDP表现不错。钢铁产量稳定。</p>
</li>
<li><p>1923年<br>通胀狂奔后，马克失去信用。11月，失业23%，煤产量44%等经济毁灭。<br>对策：财政部领导人变更，确定结束通胀的新方案，严格削减开支、建立新的税种，德国回复了预算平衡。引入新马克作为临时货币。</p>
</li>
</ul>
<p>=&gt;<br>通胀到了极限，就停顿了。</p>
<p><strong>2、原因探究：</strong></p>
<ul>
<li><p>赔款<br>巨额赔款需要金马克支付，国际市场美元升值高于国内，外国银行购入马克，而使得德国免费获得大量的食品和原材料。但22年7月停止以外汇支付赔款，通胀却加速了。</p>
</li>
<li><p>国际收支<br>出口商品以支付赔款的压力(需要大量出口来获得用以赔偿的外汇，主动性地贬值来刺激出口) 导致马克贬值，国内物价上涨，为维持国内商贸不得不继续印发纸币。</p>
</li>
<li><p>马克投机<br>卖空马克者</p>
</li>
<li><p>避免革命<br>失业和商品短缺会带来俄国一样革命。通胀至少有表面的繁荣。</p>
</li>
<li><p>通胀获利团体的助力<br>马克贬值，外国人希望用马克购买德国艺术品(马克需求增多、而增发的马克进一步大于需求？)；担心受损，资产纷纷被转移到国外。这些都进一步导致货币进一步超发。</p>
</li>
<li><p>政府赤字</p>
</li>
</ul>
<p>=&gt;美元升值和出口压力，为超发找到了借口。本质是巨额赔款成为超发货币的借口。而且政府是通胀获利的最大团体，政府不想看到革命，而马克投机者里，更多是否是政府中人呢。经济现象的解释里，很多结果会反作用于现象，造成现象的强化，而这样的结果被认为成”原因”。这并非根因。</p>
<p><strong>3、问题</strong></p>
<ul>
<li>如果是超发，那么当时真的减少发行，又会出现什么样子呢，会不会紧缩–失业和短缺呢。会不会赔款付不了呢。受害的会否比这样的通胀好？为什么这样的恶性通胀，竟然伴随着就业和GDP的稳定甚至增长呢？</li>
</ul>
<p>(减少发行，如果控制得好能够不会恶性通胀，但是亦不能因此通缩。需要把握节奏。并且，大量的赔款如果不发行足够的货币，除非经济的增长速度快，能够产生相应的流动性需要，否则赔款就是突发的流动性需求。会付不了，或者时间很长。就业和GDP稳定因为，也是算温和的。23年狂奔后导致了大量失业和降产，失业大幅增多现象比通胀发生地慢一些。也可能是失业统计的数据，发生在通胀之后。)</p>
<ul>
<li>新马克和分配券有什么不同呢，都是以土地作为抵押保证。新马克的背景：赔款和政治经济都还是老样子，而新马克却使得恶性通胀恢复地惊人。劣币无法流通后良币取而代之。–海温斯坦为啥能取得稳定货币的成就？</li>
</ul>
<p>(本质是，新马克保证了其价值、控制了发行；而分配券还是超发。似乎根本不在于表面的这些相似，而在于对欲望的态度。)</p>
<ul>
<li>为什么通胀是旧价格的延续而通缩不是？</li>
</ul>
<p>(如果货币价值✖️价格 = 商品劳务价值，那么通胀不改变商品劳务价值，则旧的价格是由于货币价值而反向变动。通缩则是流动性不足，当人们不愿意拿钱买、不愿意出钱投资，认为商品都不值得买，并不是因为觉得货币更值钱，更多可能出于谨慎因素，回报低或者对未来悲观。商品也会慢慢退出市场。由于悲观，影响到投资和产出，变化来自等式右边。)</p>
<ul>
<li>为什么会小跑到狂奔？</li>
</ul>
<p>=&gt;<br>惊人的数字让人记忆犹新。德国现在仍会谨慎。</p>
<h4 id="俄国计划经济"><a href="#俄国计划经济" class="headerlink" title="俄国计划经济"></a>俄国计划经济</h4><p>todo                    </p>
<br>]]></content>
      <categories>
        <category>货银</category>
        <category>货币政策</category>
      </categories>
      <tags>
        <tag>货银</tag>
        <tag>货币政策</tag>
      </tags>
  </entry>
  <entry>
    <title>通货膨胀-下</title>
    <url>/2021/02/21/%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80-%E4%B8%8B/</url>
    <content><![CDATA[<p><strong>摘要</strong></p>
<p>定义、相关概念及现象、分类、成因、影响、相应政策。</p>
<p><br> <br></p>
<h1 id="谨记"><a href="#谨记" class="headerlink" title="谨记"></a>谨记</h1><p><strong>只是看了一本书，必然受作者局限。作者有误，不要认为天经地义。勿从众。勿从威。</strong></p>
<p><br> <br></p>
<h2 id="货币幻觉"><a href="#货币幻觉" class="headerlink" title="货币幻觉"></a>货币幻觉</h2><span id="more"></span>
<ul>
<li><p>货币幻觉是新凯恩斯主义的代表人物之一阿克洛夫再次提出的。<br>简单说是100元的钱，被认为有100元的购买力。但实际只有50，另一半则为“铸币税”被征收。<br>而幻觉的存在，因为我们不能够完全理性，价格的传导存在时滞，我们私心喜欢虚幻的“富裕”。</p>
</li>
<li><p>货币本质<br>一般等价物？资产？负债？债券？税票？<br>一般等价物是在交换中起的作用。并不能表现出货币的真实所值，或者说购买力。<br>劳动或资产所得的个人财富，通过货币形式，用于购置资产则为资产。而借来的货币，用以购置资产则为债务。<br>对国家而言，可以将美元看做债券，人民币看做税票，这样的视角更能看出货币所值的变化。</p>
</li>
<li><p>成本<br>借贷的利息，意味着货币资源的机会成本。</p>
</li>
<li><p>货币是一种符号。意味着涨涨跌跌的可能性。货币的乘数效应既能放大资产，也能放大负债。能熬得住，承受得了时间成本，则货币将沉淀为资产。</p>
</li>
</ul>
<h2 id="影响"><a href="#影响" class="headerlink" title="影响"></a>影响</h2><ul>
<li>微观</li>
</ul>
<p>1、货币幻觉导致财富再分配：银行存款名义利率往往不能随通胀充分调整以保证实际利率不变，短期内会出现挤兑， 长期内会造成负储蓄、负投资、负就业、负产出。<br>2、实体企业投资:由长期投资转向短期投资，由生产性投资转向非生产性投资<br>3、累进税制下税收扭曲:通胀税<br>4、商品之间相对价格的变动，价格信号配置资源的效率下降– 模糊了工作效果和报酬的关系、使得投机赌博盛行。</p>
<ul>
<li>宏观<br>  1、对于产出的影响<ul>
<li>促进论:凯恩斯名义工资刚性理论、新凯恩斯粘性工资价格模型;弗里德曼和卢卡斯的预 期错误模型;</li>
<li>促退论:通胀造成微观效率损失;</li>
<li>中性论:“二分法”</li>
</ul>
</li>
</ul>
<p>2、对于就业的影响:菲利普斯曲线，附加预期菲利普斯曲线的运动 &amp; 长期垂直的菲利普斯曲线<br>货币政策的短期有效性及长期无效性、中央银行通胀预期管理的重要性;</p>
<p>3、对利率、汇率等重要经济变量的影响</p>
<h2 id="成因"><a href="#成因" class="headerlink" title="成因"></a>成因</h2><ul>
<li>需求拉上：大量的货币发行，导致过多的货币和商品供给的增长不平衡，供给弹性不高不能及时地跟上货币发行。</li>
<li>成本推动：劳动力市场的不完全或产品市场的不完全等造成的，主要包括工资推动型、利润推动型、进口成本推动型。如70s两次石油价格上涨和次贷危机后的大宗商品价格上涨。</li>
<li>预期：当期高通胀率带来市场主体的高通胀预期，进而导致下期高通胀。</li>
<li>结构：在供求总量基本相同的情况下，由于某些结构性因素，如本国产业结构老化，资源流动效率较低等造成的通胀–本质是新部门的供给未能及时跟上。</li>
</ul>
<br>
而各种政策环境，如金本位下的铸币成色、白银涌入、转型为纸币制、战争等历史聚变、计划经济、金融秩序等都通过间接影响到以上四个成因因素而导致通胀。



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>1、货币在流通过程中的乘数效应导致的通胀是几何级数的，与通常讲的CPI所标示的通胀不是一个概念。CPI是有欺骗性的，最真实的通胀就是M2与GDP的比值。</p>
<p>2、对偏离正常现象的分析，会带来更深刻的认知。</p>
<p>3、价格的绝对值无意义，房价回不到过去。</p>
<p>4、一般来说，批发价上涨幅度高于零售。</p>
]]></content>
      <categories>
        <category>货银</category>
        <category>货币政策</category>
      </categories>
      <tags>
        <tag>货银</tag>
        <tag>货币政策</tag>
      </tags>
  </entry>
  <entry>
    <title>银行拨备</title>
    <url>/2021/02/17/%E9%93%B6%E8%A1%8C%E6%8B%A8%E5%A4%87/</url>
    <content><![CDATA[<p><br> <br></p>
<p>拨备有一个重要的功能，就是“以丰补歉”，以平滑利润。</p>
<span id="more"></span>

<blockquote>
<p>为什么要平滑利润？<br><br>如果银行全部股东一直没有变动，那么其实要不要以丰补歉都是一样的，反正赚和亏都是这些股东的。但是，因为股东每个交易日有进有出，所以得考虑利润跨期平滑问题。<br><br>假设有一家银行，过去每年盈利100元，非常稳定地持续了5年以上。突然有一年，发生了一笔不良资产，当年会亏损200元。如果这家银行没有以丰补歉，而是前5年把利润全分红了，然后有股票换手，有新股东进来，然后到了第6年亏损200元，那么新股东怎么都不会开心的。他们会觉得，前面的股东赚走了贷款的利息，而这笔贷款发生违约时，亏损让自己承担，这太不公平了。<br><br>这跟信用债交易有点类似。有些信用债在违约发生前发生交易，新买入的投资者成了接盘侠……</p>
</blockquote>
<p>如果按照要求计提拨备，也很做到平滑利润（不良太多，把超额拨备消耗完毕之后还不够）。那么，还有一个方法，就是在不良贷款的确认上动手脚。因为不良资产和非不良资产之间，并没有清晰、客观的边界，边界划分在哪，是有一定的主观性的。</p>
<p>基于拨备覆盖率指标，我们可以有这样一个假设（确实存在例外的情况）：<code>银行不太可能一边宽松认定不良（甚至隐藏不良），一边又计提大量超额拨备，保持很高的拨备覆盖率。</code></p>
<blockquote>
<p>（1）如果一家银行拨备覆盖率远超监管标准（比如150%），并且还在持续提升，那么很明显是处于丰的阶段。很显然，市场上，这样的银行股，估值一般不低，而且主升浪是从它们拨备覆盖率开始显著上升开始的。<br><br>（2）如果一家银行拨备覆盖率在较长时期内仅维持在监管标准附近，那么有可能是：每年的营业收入用于消化存量不良之后，无能力留存额外的拨备。甚至，它是每年收入能消化多少不良就确认多少不良，并且还有存量不良还未消化完毕（存量不良有可能还未确认到报表中），还在补欠。<br><br>（3）如果一家银行拨备覆盖率高位回落，可能也是处于补欠的阶段，即每年收入已经不足以消化新发生不良，而是需要拿过去的超额拨备去消化新发生的不良。<br><br>（4）一家银行的拨备覆盖率从监管标准开始起飞，则有可能是存量不良处置完毕了，开始进入丰的阶段，积累超额拨备。</p>
</blockquote>
]]></content>
      <categories>
        <category>货银</category>
        <category>商业银行</category>
      </categories>
      <tags>
        <tag>货银</tag>
        <tag>商业银行</tag>
      </tags>
  </entry>
  <entry>
    <title>长期价值</title>
    <url>/2021/06/25/%E9%95%BF%E6%9C%9F%E4%BB%B7%E5%80%BC/</url>
    <content><![CDATA[<br>

<br>

<p>如果以供需平衡来看待现象，很多时候更容易看到本质。</p>
<span id="more"></span>

<p>比如，为什么越贪婪，在当今这个市场中更受追捧。即所谓强势文化。本质是，贪婪是用来抵抗非理性的。因为恐惧的人多，大多人在追随，所以贪婪更稀缺。</p>
<p>这里隐藏的逻辑是，理性是最终目的。这是当然。</p>
<p>理性的本质是避免受干扰。我们每个人都是有限理性，并且由于惯性，很容易受他人、他事干扰，甚至于会受”情商“干扰。有些人的惯性就是不受干扰。他们属于幸运的。</p>
<br>

<p>避免干扰的有效途径，即信念。对自己认知的高度清醒地自信和坚持。遵循自己的认知，继而调整。做到有序，而非一口一个胖子，各种角度都要兼得，只会显得为人冗长。</p>
<p>信念塑造行为模式。而行为模式最终导向各种生态，自我的生态和自我与他者形成的生态。人的心理组成，无非贪婪和恐惧的平衡。越受恐惧支配，则越发减少能量消耗，信念度也随之降低。</p>
<p>那些硬刚的人，失败也刚的人，只要不是愚蠢，那便是有大智慧的人。</p>
<p>理性和信念，这便构成一个良性循环。一般有了这个循环，就可以持续。</p>
<br>

<p>基本概念和初心一样，是需要不断被重复，才能形成惯性的。这就启迪在实践上，程序性上，要做的则是重复和提醒。结合判断。这种理论到实干的沟壑不给打通，永远只是酒囊饭袋。人就是在干。</p>
<p>不要以为批判看待事物就是最佳的。既然你是批判的，那你就应该批判地看待批判。达到一个新的自我认知，并且认可自由。认可自由的人，也是有责任意识的人。否则就是在要求兼得。批判不是要求兼得。</p>
<br>

<p>理性、信念或者说初心，都是自我表达。有的人的表达就是感性。</p>
<p>每个人都在学习自我表达。大多数人凭着惯性展示自己。戒掉不好的惯性，第一要就是，慢。然后再快。不慢，永远快不起来。</p>
<p>追求价值的第一要，就是放弃眼前利得，因为价值的属性本来就是长期。价值就是要经得起沉淀。</p>
<br>

<p>着眼于长期价值，即找到不患而无位次之处。考虑长远的事情，更容易得到合理的、可预期的结果。确定性远远比更大更快重要。</p>
<br>

<p>整体的逻辑是：</p>
<p>首先要有责任感，对认知有自信，这样才会坚定，然后宁慢勿急，保证理性的环境，在理性的状态下，考虑长期，根据知识，做出相对合理的判断，每一个小判断构成一各个上上下下的波动轨迹，日久，就拉开了趋势性的倍数差距。细节里都需要对此初心进行秉持。</p>
<p>这个是大范围的。</p>
<br>

<p>小范围的，则是，具体的长期价值，如何着眼的？</p>
<p>搞清楚你背后的逻辑、看好你自己的那个篮子，严格要求逻辑清楚、并且能够确定边界和异常。</p>
<br>

<p>和几十年前所不同的在于，之前拉开差距的只是有无，而现在，信息便捷，摩擦成本小，套利机会少，更多拉开差距的是深刻和广泛性。以及性格。</p>
<br>

<p>长期思维是很大的区别之处。包括对基本概念的理解。</p>
<p>抓住了长远，就能立足，而获得长久的不败，而非短期的冲高。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>阶段性总结</title>
    <url>/2021/06/29/%E9%98%B6%E6%AE%B5%E6%80%A7%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<br>

<br>

<h4 id="看到的"><a href="#看到的" class="headerlink" title="看到的"></a>看到的</h4><span id="more"></span>

<ol>
<li>《三一恨别长沙–梁稳根 的内心独白》和《恨别长沙谎言曝光》。中联重科和三一的斗争，有国资、非法手段。</li>
<li>到了后来，爷爷只有背影和沉默。只有零星几张吃饭的照片，很少娱乐，很少活动。人离世之时都是落寞的。万物带不走，唯一能缓解死亡降临恐怖的，只有爱和陪伴。相视无言。却留不住。只能想，都是要走的，我也是要走的，才能通透些继续这生活。和你面对死亡的，能有谁？父母来不及，孩子已出门，唯有伴侣。走时若是充实无遗憾的，心里也坦然。不枉岁月。</li>
<li>让子弹飞： 站着还把钱赚了。</li>
<li>中国知识分子招聘会</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>笛卡尔积</title>
    <url>/2021/03/11/%E9%9B%86%E5%90%88%E7%9A%84%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF/</url>
    <content><![CDATA[<br>
<br>


<h2 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h2><span id="more"></span>

<br>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static String cartesianProduct(final String[][] inputs) &#123;</span><br><span class="line">        if (inputs &#x3D;&#x3D; null) &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        final StringBuilder sb &#x3D; new StringBuilder();</span><br><span class="line">        </span><br><span class="line">        product(&quot;&quot;, 0, inputs, sb);</span><br><span class="line">        </span><br><span class="line">        return sb.toString();</span><br><span class="line">    &#125;</span><br><span class="line">public void product(String prefix,int index, String[][] input,StringBuilder sb)&#123;</span><br><span class="line">        for (int i &#x3D; 0; i &lt; input[index].length; i++) &#123;</span><br><span class="line">            if (index &gt;&#x3D; input.length - 1) &#123;</span><br><span class="line">                sb.append(prefix + input[index][i]);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                product(prefix + input[index][i], index + 1, input, sb);</span><br><span class="line">            &#125;</span><br><span class="line">            if (i &lt; input[index].length - 1) &#123;</span><br><span class="line">                sb.append(&quot;, &quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>两个变量关键prefix和index，作为递归方法的参数时进行变化<code>prefix + input[index][i]</code>和<code>index+1</code>。</p>
<p>从数列角度看，sb(j) = sb(j-1) + charAt(i) ,charAt(i)需要一个for循环即<code>for (int i = 0; i &lt; input[index].length; i++)</code>。sb(j-1)为<code>prefix</code>，下一步需要<code>prefix + input[index][i]</code>；其中<code>j-1</code>为<code>index</code>，进入下一步需要<code>index+1</code>。</p>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>零散问题repo</title>
    <url>/2021/04/05/%E9%9B%B6%E6%95%A3%E9%97%AE%E9%A2%98repo/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>无中心、碎片知识点</strong></p>
<span id="more"></span>


<ul>
<li><p>get post</p>
<ul>
<li>GET请求是通过URL直接请求数据，而POST请求是放在请求头中</li>
<li>GET提交有数据大小的限制，POST请求在HTTP协议中也没有做说明，一般来说是没有设置限制的，但是实际上浏览器也有默认值</li>
<li>登录操作的时候，尽量使用HTTPS请求，安全性更好</li>
</ul>
</li>
<li><p>gmv gtv</p>
<ul>
<li>总销售额，总交易额</li>
</ul>
</li>
</ul>
<ul>
<li>感受野<br>如果两个3<em>3和一个5</em>5对原始输入后卷积都得到了1<em>1的输出<br>那么哪个filter好呢？<br>从计算量和参数个数来看，选择多层卷积，而每个卷积为3</em>3的更好。</li>
<li></li>
</ul>
<p>更倾向于给你一个场景看你的思路，场景里你怎么分析<br>怎么挑出来好的样本、选择什么样子的模型<br>特征工程的做法–<br>ab背后的正交<br>特征管理的方案<br>知识图谱好的应用方式<br>模型选型，为啥用图模型<br>一维的卷积的加速<br>模型 的可解释性、业务上解释<br>长尾item<br>采样方式–正负样本的做法<br>实时模型<br>实时特征落地<br>adam优化器背后的优化，解决了什么问题<br>sgd有什么问题<br>你是如何调参的<br>图模型有什么弊端<br>xgb的特征重要性怎么算出来的<br>ab正交机制<br>gnn的输入 gnn的思路是？<br>文本的情感类别，比如 美食领域的正向还是负向 怎么判断<br>Self attention和普通attention的区别<br>多种transformer的架构、对比<br>cnn输入的如果是长文本 怎么处理<br>各种loss<br>图模型还有哪些<br>同构图和异构图的区别<br>vae的loss<br>分裂加速<br>Mmoe<br>Highway network<br>Bidaf<br>Elmo<br>esmm<br>逻辑回归的时候问一下odds的概念，<br>贝叶斯线性回归要是需要强制系数非负该怎么办，<br>核函数，relu函数是什么意思。<br>半监督学习或者时间序列<br>lasso，逻辑回归，非负矩阵分解，svm的目标函数<br>什么叫显著，什么叫p值，<br>rerank阶段的强化学习、多目标任务<br>tf-serving的搭建<br>推荐中多样性、头部问题<br>其他的排序模型、召回框架<br>召回更好的做法，优化思路<br>计算量（样本量、分类效果or打分效果、epoch、时间，在线学习设计）<br>各个场景推荐策略的区别<br>目的是什么。<br>CET<br>概率图的条件独立？<br>spark解决数据倾斜<br>widedeep，为什么需要wide+deep好处的解释<br>如何避免落入局部最优<br>约束方程怎么解 – y&gt;=0的条件下……<br>hmm具体内容<br>attention bert transformer<br>pytorch的常用函数整理<br>全连接层有什么作用？做一个图像识别的网络，可以不要全连接层吗？<br>机器学习训练误差由哪些构成；<br>BN层加在激活函数前与后效果有何不同；<br>如何判断异常点；<br>逻辑回归的分布函数；<br>逻辑回归的参数求解在优化方面属于什么类型；<br>seq-seq有哪些结构形式；<br>soft-attention与hard-attention的区别；<br>数据分类不均匀的话，要做哪些处理；<br>手写逻辑回归极大似然函数的数学推导；<br>Glove与word2vec的比较；<br>PageRank是怎么回事；<br>在resnet中，什么是残差，有何意义？<br>在GEMM中，如何优化缓存？<br>在ARM平台上，SIMD(单指令多数据)介绍一下大概？<br>是否了解其他平台的SIMD指令？intel的AVX和ARM NEON有何不同？<br>在实现一个SIMD程序时，应该注意哪些方面？如何判断一个算法适不适合SIMD加速？<br>如何证明，SIMD已经达到了最优化性能？<br>在大量的SIMD指令中，如何选择性能最佳的指令？<br>实现memcpy.<br>如何在main函数之外之行一个函数。<br>这样声明变量有没有问题：int a[10000000].<br>static修饰符有什么用？如果不加会出现什么后果？<br>常见的语言模型；<br>文本表达方法；<br>新词如何发现；<br>句子中关键词如何提取（tfidf，textrank）；<br>如何计算两个句子相似度；<br>讲一下Bert；讲一下fastText；<br>文本处理常用步骤；<br>分词分得不准确的话，该如何处理。<br>DenseNet的网络结构？与全连接有何不同？<br>简历项目中网络用了多少层FeatureMap，每层面积？<br>FeatureMap提取了哪些特征？<br>词向量怎么训练的？<br>word2vec两种常用模式？<br>词向量后面的Softmax如何优化？<br>CBOW和Skipgram哪个更适合采用？（大规模训练的话Skipgram要更好）<br>Glove的原理？（简历上有体现）<br>讲一下fastText（简历上有体现），说一下与word2vec的联系；<br>Huffman树；<br>如果用seq-seq进行embedding，做相似度计算，会如何（替代Glove的话）；<br>Glove如何训练的，用的多少维；<br>解释下TextRank（简历有体现）；<br>为什么数据量越大SVM训练越慢；<br>分词是如何处理的；<br>语料中遇到新词如何处理；<br>决策树和SVM在数据预处理上有何不同（缺失值）；SVM对文本要先做什么处理；<br>fastText最初先对文本如何处理（固定格式）；<br>词性标注比较好的方法是哪些，有没有最新的模型。<br>手推SVM；LSTM的结构，优势在哪里。<br>生成模型主流就那几个<br>VAE系列<br>Glow系列<br>GAN系列<br>思考，VAE的优缺点有哪些，为什么VAE的结果通常是比较模糊的？VQVAE的提出是希望解决什么问题，他存在VAE类似的问题吗？如果存在，那是为什么。<br>Glow的优缺点？为什么用Glow？Glow的分布会对结果产生什么影响？<br>GAN的稳定训练措施有哪些？各个稳定方法的优缺点比较？pair与非pair数据对GAN的训练影响？<br>从生成模型的角度来看，当我们希望对生成内容的属性进行控制的时候，你会选哪些方法，为什么？<br>进一步地，从模型训练的角度。参数初始化，激活函数选择，数据的均值方差，正则化，归一化，优化器（adam以及变种），自回归与非自回归的选择。<br>最后的最后，从训练数据而言。因为我接触信号比较多，那么，这段信号有什么特点，怎么提取feature输入网络？干净的数据怎么做，带噪的数据怎么做，数据的干净程度对结果的影响，一定要用深度学习吗，传统的信号分析能作为辅助loss吗？怎么衡量你的模型效果？<br>最最后的深坑，这个东西可以用强化学习做吗</p>
]]></content>
      <categories>
        <category>零散知识点</category>
      </categories>
      <tags>
        <tag>零散知识点</tag>
      </tags>
  </entry>
  <entry>
    <title>面试repo</title>
    <url>/2021/07/28/%E9%9D%A2%E8%AF%95repo/</url>
    <content><![CDATA[<br>



<h4 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h4><ol>
<li><p>微服务架构<span id="more"></span></p>
<ol>
<li><p>RPC原理：RMI ，原理，手写</p>
</li>
<li><p>Dubbo应用及源码</p>
<ol>
<li>原理</li>
<li>eureka和zk作为注册中心区别</li>
</ol>
</li>
<li><p>SpringBoot底层：分布式事务，性能优化，@SpringBootApplication</p>
<ol>
<li>启动原理</li>
</ol>
</li>
<li><p>SpringCloud进阶</p>
<ol>
<li>各个组件及集成</li>
</ol>
</li>
<li><p>Docker虚拟化技术：compose、service、redis分布式集群部署、file构建…</p>
<ol>
<li>基于Swam构建Docker集群实战</li>
</ol>
</li>
<li><p>Spring</p>
<ol>
<li>ipc aop 事务</li>
</ol>
</li>
<li><p>SOA和微服务区别和联系</p>
</li>
</ol>
</li>
<li><p>高并发分布式</p>
<ol>
<li>分布式锁</li>
<li>分布式场景及解决方案：单点登录、分布式任务调度</li>
<li>分布式协调和分流：ZK、Nginx</li>
<li>FastDFS分布式文件存储</li>
<li>RunnableFutrue、FutureTask、Thread、Runnable、ExecutorService…</li>
<li>lock、ReentrantLock、Condition、ReadWriteLock、LockSupport</li>
<li>atomic、ThreadLocal、ABA、JMM、cas算法、乐观锁</li>
<li>CountDownLatch、CyclicBarrier、Semaphore、Exchange</li>
<li>BlockingQueue、ConcurrentHashMap、HashTable…</li>
</ol>
</li>
<li><p>jvm</p>
<ol>
<li>垃圾回收算法</li>
<li>优化策略</li>
<li>类加载流程</li>
<li>进入老年代？</li>
</ol>
</li>
<li><p>mysql</p>
<ol>
<li>事务四大特性，隔离性，索引，索引优化，主从复制</li>
</ol>
</li>
<li><p>redis</p>
<ol>
<li>数据类型</li>
<li>list底层实现</li>
<li>分布式做法</li>
</ol>
</li>
<li><p>消息中间件</p>
<ol>
<li>怎么存储的</li>
<li>考虑哪些问题</li>
</ol>
</li>
<li><p>优化及debug</p>
<ol>
<li>接口响应慢的优化思路</li>
<li>缓存一致性问题</li>
<li>10个线程，让某一个最后执行，有几种方式</li>
</ol>
</li>
<li><p>netty</p>
<ol>
<li>Tomcat线程模型</li>
<li>Tomcat的NIO NIO2 ,MAX Threads、Max Connections…</li>
<li>Netty线程模型</li>
<li>Eventloop EventLoopGroup</li>
<li>Channl如何处理的，线程安全吗</li>
<li>Netty如何实现零拷贝</li>
<li></li>
</ol>
</li>
<li><p>linux</p>
<ol>
<li>内核IO 操作实现原理</li>
<li>内核如何零拷贝</li>
</ol>
</li>
<li><p>工程化：</p>
<ol>
<li><p>Maven</p>
<ol>
<li>scope</li>
<li>类冲突、包冲突的问题定位</li>
<li>lifecycle、phase、goal</li>
<li>Maven生成Archetype</li>
<li>手写插件</li>
<li>Nexus使用、上传</li>
<li>对比Gradle</li>
</ol>
</li>
<li><p>Jenkins</p>
<ol>
<li>集成maven和git</li>
<li>多环境配置、权限管理和插件使用</li>
</ol>
</li>
<li><p>git</p>
<ol>
<li>best practise</li>
<li>规范</li>
</ol>
</li>
<li><p>Sonar</p>
<ol>
<li><p>质量管理</p>
</li>
<li><p>FindBugs 、PMD运用</p>
</li>
</ol>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>2021.06</title>
    <url>/2021/07/28/2021-06/</url>
    <content><![CDATA[<br>



<h4 id="0610"><a href="#0610" class="headerlink" title="0610"></a>0610</h4><p>做了些总结<span id="more"></span>，讨论了几个问题(主要是deep wide各自的区别，每一侧特征的选择，以及负采样的一些问题)，脑子足够的，但是一些基本概念会混淆。真是太随性了。</p>
<p>中午又买东西，买了一个多小时。早上tm没车。时间在我这里不是金钱。反过来了。还是贫穷没脱圈。看着小羊的消费，一下子生了相心。想写写抖音。观察下。晚上被洗了几个小时的脑。</p>
<p>一楼就是，返水、采光、潮湿、蚊虫、烟头。可避免也不可。有法子也费心。看机缘吧。</p>
<br>

<blockquote>
<p>全分类和框架、悬空与抽象性、实践性–即程序、细节、oneshot</p>
</blockquote>
<br>



<p>这几天没有记录时间和写zhimap。总是分散和旁支地被干扰。一周又将凋谢。</p>
<br>

<p>王朔(两个节目一个锵锵)，看了杂七八的。做了几道题。看了些高数。看了些缠书。看了些c++。抖音洗脑性太强了。</p>
<p>chamath让我开始对英文的news重视起来。以及带着我那些讨论的问题看论文的方案。我还没有真的解决一个问题去查找文献。</p>
<p>看了很多阴谋论的东西。资本市场的血雨腥风。以及赤裸地直面程度。没有艺术面纱。</p>
<p>讨论了些问题，仍然纷乱。自己的思想还没有构建起来。</p>
<p>上周看了些顾颉刚的书。不行。周末看了房子。</p>
<p>我还干嘛了呢上周。对了花了一个晚上时间对tf来debug，还是内存不够。没能跑起来。整个框架光看代码还是头绪不多。没能坚持日结。</p>
<p>也有大部分时间在购物。</p>
<br>

<br>

<h4 id="0612-13-14"><a href="#0612-13-14" class="headerlink" title="0612 13 14"></a>0612 13 14</h4><p>和老公吃饭，看金水苑二手房，练车，写了篇不太价值的文–这类文章还是别写了、行为也别有了，去大嫂那边，和李聊，来公司吃喝加简单看了点–效率几乎无。</p>
<p>大部分时间在车上。瞎聊。</p>
<p>“不要从手机获取资讯。而只是必要时用。”</p>
<p>没用心想，也是没有精力。没有锻炼。没看论文。没写啥笔记。</p>
<br>

<h4 id="0615"><a href="#0615" class="headerlink" title="0615"></a>0615</h4><p>1、复习到20课。√</p>
<p>2、活动开发，哪里开发，怎么测试√</p>
<p>3、c++对象模型√</p>
<p>比如，const和return by reference，以及析构–基本的对象是否含有指针，以及inline，以及构造函数的默认初始化。这些细节，trick，影响着性能。</p>
<p>人生就是各个trick。有能力从理论上自动地自然地，捕捉到trick，使得人生越走越高。其他很多情况，是各种碰了。比如胆子大比知识多，大概率地碰对。就对准到了那个情境下的trick。</p>
<p>他会告诉你，从那个全分类的角度，为什么需要这样的trick。</p>
<p>比如析构函数，为什么需要析构，为什么编译器不能自动地销毁。那么就需要手动。</p>
<p>比如，为什么这样写更好…</p>
<p>比如，为什么要有class，struct哪里不够。</p>
<p>比如，这个语言根本的模式好在哪里。是多态、封装，那么这些有体现在哪些写法或者设计上。而这个特性又将产生哪些影响。</p>
<p>这样的思考，就会从根源知道区别。并且更加理解。</p>
<p>4、go反射√</p>
<br>

<h4 id="0616"><a href="#0616" class="headerlink" title="0616"></a>0616</h4><p>为什么资本泡沫破裂来沉淀市场里的优质公司？</p>
<p>院线为什么能保证万达商业地产的业态能够快速的扩张？</p>
<p>资本市场商誉减值的问题，什么引起的？</p>
<p>其实资本还有一个很大的作用，把这个公司作为一个基础，把我们的产业链可以高度的整合融合起来。</p>
<p>四家影视公司的市值目前分别是224亿元、350亿元、336.5亿元和101.4亿元，市值之和为1011.9亿元。</p>
<br>

<p>一天了，活动开发还没完成</p>
<p>这些代码都很简单，在清楚了大概框架逻辑之后去看。不能不看。其实很简单。从上到下和从下到上的思维都有了。</p>
<br>

<br>

<h4 id="0617"><a href="#0617" class="headerlink" title="0617"></a>0617</h4><p>极慢。事情零散地做。不集中快速消灭。今天至少把代码看完、该改的开发完。</p>
<br>

<br>

<h4 id="0621"><a href="#0621" class="headerlink" title="0621"></a>0621</h4><p>1、30课尽力吧</p>
<p>2、高数看完两ch吧 </p>
<p>3、想好自己的反馈机制。</p>
<br>

<br>

<h4 id="0622"><a href="#0622" class="headerlink" title="0622"></a>0622</h4><p>30课</p>
<br>

<p>巴菲特 – 真正的价值在于不患；搞清楚你背后的逻辑、看好你自己的那个篮子，严格要求逻辑清楚、有理据、想清楚前后，才能够坚定，要的就是坚定性；材料要公开的。</p>
<p>1、本质上来说，短期投资和长期投资的理念是一致的。一致性在于 长期的时间也是有限的。长期需要估值，那么是一年期的估值，还是永久现金流的贴现估值呢。是一两年的增长，还是十年呢。长期短期总是有分界和阈值的。</p>
<p>2、长期的逻辑：进行估值，和价格成本相比较。</p>
<p>背后是因为，市场是不完美的。市场的波动性是随机、疯子一样的。而根本的价值在于，管理层以及企业未来的前景。投资的能力：是能够对企业内在价值有判断，并且能和市场的噪音想隔离。</p>
<p>市场不完美：资金力量、各种因素都在用影响着短期的供需的均衡。没有办法找到里面的不患。</p>
<p>对于内幕：知道是无意义的。市场节奏难以踏准。因为这样的短差带来了因小失大。主要因为，其中没有规律可循。</p>
<p>对于持有期：他们甚至想一直持有，只要增长是符合他们的预期。</p>
<p>但是缠论，理论基础在于说，他找了不患。</p>
<p>3、“捡烟头”的思路在当下还可行吗，他那时候是57年之后，每年的收益高则近50%，低也在8%，我觉得这些数字就算一个上限或者阈值了。并且波动性是小于指数的。他的资料是一手的研报和财报，他从中获得对于价值的分析(财报上作假？比对…等等，还是对于材料有很大的运用能力)。</p>
<p>虽然那些公司的安全边际也不是很大空间，但是足以能够开始，有了第一桶金。</p>
<br>

<p>4、材料是一致的，优质的。然后就是理解力的问题。</p>
<p>5、生活，真实的生活和想象的、憧憬的。现实和虚幻的。能够识辨已经很少人了。</p>
<p>6、强势文化和弱势文化的问题，是信仰的坚定性问题。以及信仰自身和行动，还是信仰别人–消息、见解等。做长期的，要信仰长期价值，不轻易所动。但是都是自己去决断，而不受别人左右。要严格。对思路、逻辑的严格。</p>
<p>7、贫穷的本质，健康、教育质量、以及经济上的做法。经济上的问题，是本质。”电视机比食物更重要“：代表娱乐、希望，而越缺乏则越渴望。渴望的层级也只是到电视机的层次了。那么也就不可能像富人一样去思考向富裕进步。</p>
<p>富人不用太过节制欲望，而是将时间放在思考进步上。而穷人在一天劳作之后，根本只想享受。这样看，大多数的工薪阶层，都是穷人。因为他们只能通过放纵一些”消费“性欲望来排解。而不能像富人一样，相对自由，不那么报复性地”满足“，因为他们的消费欲望是不需要节制的。他们能够完全满足。</p>
<p>只有当你没有被限制，而一直是满足的状态，你才能够不去陷入。只要缺乏，必会陷入。越低层次的缺乏，则越低层次的渴望，激发出来报复性地满足，而不是正向地去思考脱离困境的方法。只有你相信能够脱离，才会慢慢地采取行动，哪怕是推迟那种强烈的渴望。否则，最多，也只是拥有一个电视机而已。</p>
<br>

<p>反馈机制：</p>
<p>没有按照原来的想法做。思考更重要。哪怕做了一点，是否有自己的思考。</p>
<p>对问题的总结就是反馈。思考和复盘都是有效的反馈。</p>
<br>





<p>业务指标：</p>
<p>大多的badcase，类型就几种：1 相关的（实时行为或者是）却没有及时地推出来；2 该排上来的没上来 – 低效；3 ee的部分做的不够，太过头部、指向指标，不利于长期 ； 4 gmv和其他指标的此消彼长，指标变动的解释</p>
<p>解决方案：1 没召回出来，要么是召回路数需要增加、在粗排排分时候考虑更多行为和交叉特征。多队列 ； 2 精排优化，或者精排后人工规则的重排 ； 3 指标之间的权衡，需要考虑如何在尽量不变成本的情况下，增加新策略的好处。即一个 f ；</p>
<p>总结，目前的一贯方式，要么是找到“公式”，也是一个分解目标的过程，拆解出来因子后，分析关系。构成f，即建模和进行假设。其次的方式，就是从头部角度去解释。比如，为什么某个没有上来，因为这个的整体转化低或者这个的整体ctr低，那么我不推其他的是更利于全局指标的。–即个例不能保证。因为有全局的目标。这就是整体上指标和个例的差异性。</p>
<p>大多数的人，基本对数据是不够了解的。最基本的，基础信息在哪里、埋点数据在哪里、有没有问题、有没有检验。以及指标的大概值。大部分的工作，也只是在追查而已。创造性不多。</p>
<h4 id="0623"><a href="#0623" class="headerlink" title="0623"></a>0623</h4><p>t检验中用户群pair</p>
<p>模拟交易和行情数据</p>
<p>35课</p>
<br>

<h4 id="0625"><a href="#0625" class="headerlink" title="0625"></a>0625</h4><p>1、我没有去看数据，再进一步思考和分析。包括一些正在出现的问题。没有去发现。</p>
<p>2、我的思考会漫游出去，但是很多相关是否真的有价值，还没来得及深入。这也显得自己特别不清晰。初心要坚持，一些自己总结的也应当在行为里避免。</p>
<p>3、下午来罗森和星巴克了。如果我工程上能够cover，策略上能够专注于数据找到问题。并且比他们更加敏感和迅速。为什么不被认可。承认自己的不足。对代码不够熟悉。没能一下子get。</p>
<p>3：50出来，15min在罗森，4：20到星巴克。</p>
<p>1h看了几页，和同事聊。5：20-6：20，买饭，菜。-7：10，看了企微。 3h，效率非常低。如果我不以15min</p>
<p>4、活动总结：</p>
<p>被推动的时候，我都是能够找到原因的。但是没有推动力，我总是在拖。我不想主动地投入。总觉得小事，不想去付出。</p>
<p>只是捡一些边边角角无关紧要的东西，扣一扣。太过因小失大。</p>
<br>

<br>

<p>初心：</p>
<p>1、和别人沟通，考虑这几点：逻辑是否清晰、表达是否扼要、是否本质。</p>
<p>2、开始一件事情：短期目的是、长期目的是</p>
<p>3、别人无法帮你决断。相信自己。并且看明白别人的优劣。</p>
<p>我能做的，更深刻提升我的处境的，有哪些。1搞清楚几个项目的结构和代码、努力去重构；2考研的事情；3投资回报–书</p>
<p>而我更多的时间花费在消费、琐事。我没有找到自己的重心。受”恐惧“桎梏。本质上，对金钱和地位的渴望，都来自于恐惧和贪婪。而对问题的解决、业务的精进、真理的探求、原因本质的追思，却都是过程里，需要去享受的。</p>
<br>

<p>核心在我如何找到自己更惬意轻松的思考状态。书写是的。但是与人沟通无法进行书写。</p>
<br>

<br>

<h4 id="0628"><a href="#0628" class="headerlink" title="0628"></a>0628</h4><p>1、独立和干货。要负责。</p>
<p>论文，整理，学习代码。考研。复习。</p>
<p>2、周末晚上睡不太着。青春不再。积累不够。飘摇。但是飘摇也应乐观，有坚持才能自信。</p>
<p>35课</p>
<p>3、发现还是没有应用。比如去看万达的时候，盘整、背离、趋势，这些概念还是没法说清楚。买点卖点如果不是对照着结局看，很难看到迹象似的。</p>
<p>重复的遍数少了。</p>
<p>4、发现一个道理</p>
<p>如果你能发现价值，无论是长期还是短期，只要你比别人看到真实的价值，基于你的经验、知识或者睿智，只要你能够评估和判断出来真正的价值或者看到不同、差值。就是你的价值。因为你可以利用。</p>
<p>而这才是核心。是关于理解、评判的核心。大的方向的把握。</p>
<br>

<h4 id="0629"><a href="#0629" class="headerlink" title="0629"></a>0629</h4><p>1、网络ch 1和2 ，ch3</p>
<p>2、35课</p>
<p>3、无穷级数和常微分方程  多元微分多元积分</p>
<br>

<h4 id="0630"><a href="#0630" class="headerlink" title="0630"></a>0630</h4><p>1、价值的识别</p>
<p>2、对记忆的提取，才能强化</p>
<p>3、我能感受到爷爷还是有感觉和思维的，只是开始对身体没有了控制能力。即使生活在一个屋檐下，也难以产生强烈的感情。爱。因爱而生的动力，才会让你一直想去床边陪伴。而我只是逃避。谁真的爱谁。是爱恐惧的抚慰还是爱那个人。</p>
<p>4、对言辞激动的最大化解，就是不要陷入，高一个阶级的胸怀和礼貌来对待，客观地描述和解释问题。不要遮掩。同样，最坏的情况，就是自己被带入。而情绪和思绪是别人供给的情况下，就陷入了很大程度的被动。</p>
<p>5、你不懂电动车，买什么byd</p>
<p>6、风口、创新和品味、开放、表达、长期主义、品牌</p>
<p>雷军(人情味 文化 低调 顺势而为 风口)、乔布斯(品味 抓住机会 坚持做 产品的文化 品牌)、段永平(有想法讨论的时候 他很直接，并且很接地气，没有空的 考虑价值的部分 和雷军不太一样的风格 )、曹德旺(很实在 ，出轨也坦然地讲，毕竟的确是人生感受，没有什么可忌讳的)、巴菲特(沟通 – 把你的想法都表达出来，不必要留着 ； 对身体和大脑 物尽其用；选择优秀的人结婚 – 都很现实的价值观，爱优秀的人总是更好的； 从小公司入手、没人能够告诉你)、张磊(重仓中国 长期主义理念–初心的坚持(王兴)即足够专注、持续学习)</p>
<p>7、发现巴菲特非常能够了解死人的思维。</p>
<p>8、穷人，输的都是心态</p>
<p>9、每年10%，持续10年，不容易。但是一次2倍，持续2次，就 远 高于10年的。勤奋固然重要，但是功夫都在之外。风口和顺势的眼光，更加重要。</p>
<p>10、李永乐的小知识课堂</p>
<ol>
<li><p>地球半径测量：关键在于引力常数 – 实验</p>
</li>
<li><p>基尼系数：对角线和曲线面积 比上 洛伦兹曲线下的面积。香港&gt;大陆&gt;台湾，台湾最低，相对公平 。这个可以和ck的1% 的 1%联系。100w美金的已经是世界的1%。要知道1%并不是尖子生，是近7000w人。其中，高于500w美金的，就是70w人。</p>
</li>
<li><p>北京摇号：数学上算政策给的几率。</p>
</li>
<li><p>买股票的为什么总是输：</p>
<ol>
<li><p>可以出正面和反面，各自采取策略。概率上看正+正，反+反和正+反的概率是一样的。但是给的效果不一样，即正正3，反反1，正反-2，这样的话，就可以控制期望了。所以期望比概率更重要。</p>
</li>
<li><p>这个和赔率的概念是相近的，即高盈亏比、低胜率，收益会高的，可以接收盈亏各半，但是只要控制回撤即可。这是有数学逻辑的：信仰价格投机+截断亏损+让利润奔跑。</p>
</li>
<li><p>那么庄家是可以拉升和压低的，即正或者反，而散户可以买多也可以卖空(相当于空仓吧)，即正和反。拉升和做多，则散户收益为3，拉低和做空则收益为1，反之则输-2。立论是，庄家可以控制期望。</p>
</li>
<li><p>这个是博弈论。首先散户是不知道具体数值的，就很难猜测到庄家的策略。</p>
</li>
<li><p>没有期望大于0的理论上证明么？ – 这个分析技巧还是应该具体分析的。有漏洞…</p>
</li>
</ol>
</li>
<li><p>贫穷的本质：”下班了就想着满足下疲惫的自己“，就是穷人思维了。毕竟富人都是享受工作的。</p>
</li>
<li><p>虎门大桥的抖动：物理上解释。</p>
</li>
<li><p>退税：数学上算政策给的几率。</p>
</li>
<li><p>最有钱的公司 以及泡沫：最高都是7w亿级别的。全民财富都在这里吧。当时人们不懂。多了几次，到现代互联网普及就懂了。但是这些概念 比如国债转股、股价上升到泡沫破裂，又被包装起来，重新售卖。都是从被收割到收割。</p>
</li>
<li><p>切尔诺贝利:</p>
</li>
<li><p>宇称不守恒：实验挺有意思的。但是没深究。</p>
</li>
<li><p>墨菲定律:</p>
</li>
<li><p>粒子对撞机:</p>
</li>
</ol>
<p>11、youtube</p>
<p>基本锁定在 文昭、破空、ck go、缠变、chemath？、天狗、温相…也是头部。</p>
<p>一些历史的、投资的、政治的。</p>
<p>12、b的季度会</p>
<p>13、不同博客，不同文章，不同的人的断片，或者只言片语的，或者十几秒的视频，或者几个问答的采访。都可以给以灵感和启发。生活处处是启发。无字之书，时刻在无言地谆谆教诲你。这些散落的发光点和思路，难以被收纳，难以形成记忆和直觉，没有能够及时地被提取。无字之书需要梳理，但是如果大部分都沉浸在对此的分析之中，难免纷杂。因为没有框架。并且各自不够有逻辑地被融合。</p>
<p>如果能将这些点点滴滴融入到已有的框架里，更能够被提取。</p>
<p>14、林园</p>
<p>15、张磊、段永平</p>
<p>16、马男</p>
<p>​    影视行业：PE PB ，增长预期、周期性、技术壁垒、优质公司韧性、类刚需产品的长期复苏逻辑、</p>
<p>17、招财大牛猫</p>
<p>18、清华游资女</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM</title>
    <url>/2021/02/04/SVM/</url>
    <content><![CDATA[<br>


<ul>
<li><p>reference：[非常好的两本书。再加上libsvm的源码与调参的论文。]<br>[1]<a href="http://files2.syncfusion.com/Downloads/Ebooks/support_vector_machines_succinctly.pdf">http://files2.syncfusion.com/Downloads/Ebooks/support_vector_machines_succinctly.pdf</a><br>[2]An Introduction to Support Vector Machines and Other Kernel-based Learning Methods<br>[3]<a href="https://pan.baidu.com/share/link?shareid=262520779&amp;uk=1378138793">https://pan.baidu.com/share/link?shareid=262520779&amp;uk=1378138793</a></p>
</li>
<li><p>干货</p>
</li>
<li><p>首先，SVM是解决supervised learning 中classification问题。有两种情况，看是否linearly separable。线性不可分则引入kernel，想法为先做transformation到其他空间进而转为可分问题。</p>
</li>
<li><p>对于线性可分的监督分类问题，SVM的目标是什么呢? find  the optimal separating hyperplane which maximizes the margin of the training data</p>
</li>
<li><p>为什么以最大化间隔为目标？因为it correctly classifies the training data and because it is the one which will generalize better with unseen data</p>
</li>
</ul>
<span id="more"></span>
<ul>
<li><p>这里的 间隔 指？关于间隔涉及到两种分类，一种分类为几何间隔与函数间隔；一种为软、硬间隔。几何间隔在二维则为点线距离，三维空间就是我们学习的点面距离。函数间隔二维中可以理解为<em>几个</em>点没有在线上，三维则为<em>几个</em>点没有在面上；或者结合几何间隔可理解为，是将几何间隔求解的分母去掉了，没有归一化(也因此SVM中不能选择以函数间隔衡量，否则maximizes是没有意义的)。关于软硬，是看噪声，There will never be any data point inside the margin.  If data is noisy, we need soft margin classifier.</p>
</li>
<li><p>继上面的目标，假设该超平面的公式为W*X=0，这里会有疑惑：</p>
<ul>
<li>Why do we use the hyperplane equation W<em>X instead of   Y=a</em>x+b? –&gt; the vector W will always be normal to the hyperplane</li>
</ul>
</li>
<li><p>澄清下要做的步骤：<br>  1 数据集<br>  2 选择两个超平面能够分类数据并在两平面间没有其他点<br>  3 最大化超平面间隔</p>
</li>
<li><p>将步骤整理成数学过程</p>
<ul>
<li><p>设两个超平面， W<em>X+b = -θ  和  W</em>X+b = +θ。这里，为什么我们需要两个超平面？我们设想的是，假定最佳的超平面在这两个超平面的中间。我们求得两个超平面即可求得最佳分类超平面。</p>
</li>
<li><p>θ取值无关，直接设为1。 即得W<em>X+b = -1  和  W</em>X+b = +1。这里要想明白W与b到底是什么关系？b依赖还是独立于W？显然，是独立的，可以想象为，我们需要求得W与b两个变量，能够最大化间隔。</p>
</li>
<li><p>需要满足两个约束: 1. 任何&gt;=1的为class 1 2.任何&lt;=-1的为class -1 –&gt;这个限制使在两平面间没有其他点</p>
</li>
<li><p>将两个约束写为一个式子即： y*(w*x+b)&gt;=1</p>
</li>
<li><p>最大化间隔 ？对于这个问题，目前我们已知条件是两个。一个是两个平面 W<em>X+b = -1  和  W</em>X+b = +1。一个是有一个点x在平面  W<em>X+b = -1 上</em>。得：<br><code>w*(x + m*w/||w||)+b=1</code><br>化简得 <code> m = 2/||w||</code></p>
</li>
<li><p>得到的公式意味着：如果||w||没有限制，那么m我们可以取得任意大的值。</p>
</li>
<li><p>现在自然就面临optimization problem。所有的点subject to  y*(w*x+b)&gt;=1, 在此条件下如何minimize ||w||?先引入<strong>理论1</strong>，该理论为两个条件，在两个条件满足的情况下，可以说我们得到了一个scalar function的local minimum。</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2021/02/04/SVM/theorem1.png" alt="theorem1"></p>
<ul>
<li>f为从集合σ(其元素为vector)到实数集(其元素为值)的映射，且在x处 连续、可二阶导。这里涉及到两个的概念：<ol>
<li>**gradient ：a generalization of the usual concept of derivative of  a function in one dimension to a function in several dimensions  ( the gradient of a function is a vector containing each of its partial derivatives.)**注意符号为 nabla,图中倒三角。 </li>
<li><strong>scalar function：A scalar valued function is a function that takes one or more values but returns a single value. In our case f is a scalar valued function.</strong></li>
</ol>
</li>
<li>positive definite：<br>A symmetric matrix A  is called positive definite if x.T<em>A</em>x&gt;0 , for all n维的实数向量x。</li>
<li><strong>theorem 2</strong>中的四个条件是等价的。因此可以通过其他三种情况来判断是否为正定。这里选择主子式来判断Hessian正定，涉及到三个概念：</li>
</ul>
<p><img src="/2021/02/04/SVM/theorem2.png" alt="theorem2"></p>
<ol>
<li>Minors： 删除某行和某列的所有值再计算行列式。remove the ith line and the jth column</li>
<li>Principal minors ：删除的行、列号一致。remove the ith line and the jth column and i=j</li>
<li>Leading principal minor ：The leading principal minor of A of order k is the minor of order k obtained by <strong>deleting the last n−k rows and columns</strong>.（这里包含一个正三角符号，标注删除哪些行列）栗子看图Leading principal minor</li>
</ol>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-42fb025afc8f45d5.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="leading principal minor.jpg"></p>
<ul>
<li>得到了local minimum，How can we find a global minimum?两步走， 1. Find all the local minima 2. Take the smallest one; it is the global minimum. 另一个思路是看我们的f是否是<strong>convex</strong> ,是then we are sure its local minimum is a global minimum.</li>
</ul>
<p><strong>Theorem: A local minimum of a convex function is a global minimum</strong> 这里又涉及到convex function, convex set的定义。</p>
<ul>
<li>What is a <a href="http://mathworld.wolfram.com/ConvexFunction.html">convex function</a>? A function is convex if you can trace a line between two of its points without crossing the function line.<br><img src="http://upload-images.jianshu.io/upload_images/8716089-fd9cfea722d77a1d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="convex  0&lt;λ&lt;1.jpg"><blockquote>
<p>A function is convex if its epigraph (the set of points on or above the graph of the function) is a convex set. In Euclidean space, a convex set is the region such that, for every pair of points within the region, every point on the straight line segment that joins the pair of points is also within the region. </p>
</blockquote>
</li>
</ul>
<p>栗子看图convex set</p>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-b484ebf0cfdf222d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="convex set.jpg"></p>
<ul>
<li>同样根据Hessian判断是否convex，这里需要看是否是<strong>positive semidefinite</strong>,而semi有对应三个条件是与之等价。看 theorem 3<blockquote>
<p>**More generally, a continuous, twice differentiable function of several variables is convex on a convex set if and only if its Hessian matrix is positive semidefinite on the interior of the convex set.**The difference here is that we need to check all the principal minors, not only the leading principal minors. </p>
</blockquote>
</li>
</ul>
<p>  <img src="http://upload-images.jianshu.io/upload_images/8716089-744d4e22dac7d5aa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="theorem 3.jpg"></p>
<ul>
<li>其中on the interior of the convex set是什么意思呢？定义：the domain of a convex function is a convex set，那么 a function is convex on a convex set意思就是在domain上是convex function，而interior只是意味着为两边开区间。</li>
<li><a href="http://www.math.cmu.edu/~ploh/docs/math/mop2013/convexity-soln.pdf">其他证明是convex function的方法</a></li>
<li>这里就谈convex function的optimization问题求解。涉及对偶概念，根据wiki，<blockquote>
<p>Duality :duality means that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem (the duality principle). The solution to the dual problem <strong>provides a lower bound</strong> to the solution of the primal (minimization) problem. </p>
</blockquote>
</li>
</ul>
<p>  给最小值以下限。lower bound中有一个值为<strong>infimum</strong> (即 greatest lower bound)。补充，相对而言</p>
<blockquote>
<p>The same logic apply with the relation “greater than or equal” and we have the concept of upper-bound and supremum.</p>
</blockquote>
<ul>
<li>对偶，在求最小值时求对应的最大值，求出的最大值将是=&lt;最小值，两者之差即为<strong>duality gap**。对应来说，在求最大值时求对应最小值，求出的最小值将是&gt;=最大值即upper bound。</strong>duality gap<strong>为正，我们称之</strong>weak duality holds<strong>，为0则为</strong>strong duality holds**。</li>
<li>拉格朗日乘子 ： <blockquote>
<p>In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to <strong>equality</strong> constraints. </p>
</blockquote>
</li>
</ul>
<p>  <img src="http://upload-images.jianshu.io/upload_images/8716089-59b1f2660a4869b6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Lagrange_portrait.jpg"></p>
<ul>
<li><p>如何将3D图在2D平面表示：Contour lines  两个要点：1. 线上的点z值不变，for each point on a line, the function returns the same value 2. 颜色扮演标识，the darker the area is, the smallest the value of the function is<br><img src="http://upload-images.jianshu.io/upload_images/8716089-271a7e44b55a0424.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="contours.png"></p>
</li>
<li><p>那么梯度就可以用<strong>向量场</strong>进行可视化。箭头指向函数增长的方向。与Contour lines 图有什么关系呢？在Contour lines 图中，gradient vectors非常容易画出，1 垂直于Contour lines 2.指向增加的方向。<img src="http://upload-images.jianshu.io/upload_images/8716089-8ee3efb4b3646c71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="gradient_and_contour.png"></p>
</li>
<li><p>将约束函数和优化目标函数以contour lines 画在一幅图中，并画出两者的gradient vector。可得到最优点。图中的优化目标函数为x^2+y^2, 约束函数为 y=1-x。 <img src="http://upload-images.jianshu.io/upload_images/8716089-fc1f3051bbf27c8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="function_and_constraint.png"></p>
</li>
<li><p>▽f(x,y) = λ ▽g(x,y)<br>λ 这就是拉格朗日乘子。根据图中，当两个gradient vector平行时，我们得到最优解。无论是否同向。更无论是否等长。乘以λ 即意味着不必等长。即求▽L(x,y,λ )=0时的x，y。现在我们需要列出L并求解。</p>
</li>
<li><p>由于我们需要求f(w)=1/2*||w||^2的最小值，将每个约束函数乘以的 λ需要取正数。<img src="http://upload-images.jianshu.io/upload_images/8716089-76c3fc3fc413375d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="L.jpg"></p>
</li>
<li><p>又面临一个问题，求L(x,y,λ )=0， the problem can only be solved <strong>analytically when the number of examples is small</strong> (Tyson Smith, 2004 即只有当约束函数数量比较小的时候，λ 个数不多，我们才能用分析的方法求解). So we will once again rewrite the problem using the duality principle–&gt;we need to minimize with respect to w and b, and to maximize with respect to a at the same time.我们在上一步需要最小化<br><img src="http://upload-images.jianshu.io/upload_images/8716089-4ca65da333340058.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="duality_after_L.jpg"></p>
</li>
<li><p>这里需要讲清楚三个问题。1. 拉格朗日是对约束函数是等式的情况，那么我们在这里是不等式约束的问题，也用拉格朗日乘子解决，需要满足什么条件吗？(KKT)2.之前说了，对偶问题有强与弱，只有当强时，gap才为0，我们才能将对偶问题的最大值作为原问题的最小值。那么，这里是否满足是strong duality holds? （强对偶 即下文Slater’s condition）3.或许你对为什么能够是对w b求min，对a求max还是留有疑问。(拉格朗日到对偶问题这两个之间的转化过程)</p>
</li>
<li><p>仍需要引入两个理论。1.  duality principle 2.Slater’s condition 3.KKT<br>首先，L对w与b求偏导，令为0(这里两个等式)，再将这两个等式带入到L中，消去了w、b，只剩下变量a，即得L(a)。于是将问题转化为 Wolfe dual Problem<br><img src="http://upload-images.jianshu.io/upload_images/8716089-67cdfbf09b820630.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="wolfe dual.jpg"></p>
</li>
<li><p>慢着，这里又出现一个问题。</p>
<blockquote>
<p>Traditionally the Wolfe dual Lagrangian problem is constrained by the gradients being equal to zero. In theory, we should add the constraints θL/θw=0  and  θL/θb=0 . However, we only added the latter, because it is necessary for removing   b from the function. However, we can solve the problem without the constraint   θL/θw=0.</p>
</blockquote>
<p>这里就会不明白为什么不需要加上θL/θw=0约束仍能够solve the problem？暂且保留疑问。</p>
</li>
<li><p>Karush-Kuhn-Tucker conditions :<strong>first-order necessary conditions</strong> for a solution of an optimization problem to be optimal<br>除了KKT还需要满足一些regularity conditions，其中之一为Slater’s condition。</p>
<blockquote>
<p>Because the primal problem we are trying to solve is a convex problem, the KKT conditions are also sufficient for the point to be primal and dual optimal, and there is zero duality gap.</p>
</blockquote>
<p>这里说的，即只要为convex问题，KKT也满足，即可说得出的结果是原问题或对偶问题的最优解，因为Slater’s condition是一定满足了的，gap=0。对于SVM，如果结果满足KKT，那么即可说是最优解。(详细证明过程[2] ch5)</p>
<p>   <img src="http://upload-images.jianshu.io/upload_images/8716089-62f19539ba8a88bc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="KKT.jpg"></p>
</li>
<li><p>可见，1. stationary 即为偏导为0即为驻点,若无约束函数则直接gradient是0，有了约束则gradient of the Lagrangian为0。2. primal feasibility为原问题约束函数  3. dual feasibility 为我们对L求解时使用对偶理论时的约束函数  4.complementary slackness含义是，要么a=0，要么y*(w*x+b)-1=0.这里与<strong>Support vectors</strong>相关，</p>
<blockquote>
<p>Support vectors are examples having a positive Lagrange multiplier. They are the ones for which the constraint y*(w<em>x+b)-1&gt;=0  is active. (We say the constraint is active when y</em>(w*x+b)-1=0 )</p>
</blockquote>
<p>这里，是否会疑惑为什么不能同时为0？为什么multiplier一定是正数？在KKT中，我们只选取支持向量，即将不等号约束改为等号约束，其他的点不考虑。</p>
<blockquote>
<p>Solving the SVM problem is equivalent to finding a solution to the KKT conditions.” (Burges, 1988)</p>
</blockquote>
</li>
<li><p>现在有了L(a),求导即可。得到了a。再根据偏导为0的公式回代得到w 。再根据prime problem中的约束函数y*(w*x+b)-1&gt;=0，计算b</p>
</li>
<li><p>用QP solver来解对偶问题。用python CVXOPT包。将wolfe dual.jpg中我们需要求解的公式转化到下面CVXOPT支持的形式。这里引入了一个Gram matrix - The matrix of all possible inner products of X.</p>
</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-907dcfe22f62860e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="CVXOPT.jpg"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-ac41318c2c79efb1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="转化过程.jpg"></p>
<p>这个图里有问题，minimize部分最后一项需要<code>q.T*a</code>, 详见代码部分，需要q = cvxopt.matrix(-1 * np.ones(m))。</p>
<ul>
<li>code部分：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cvxopt.solvers</span><br><span class="line">X, y &#x3D; 这里获取到数据</span><br><span class="line">m &#x3D; X.shape[0]  #data有多少</span><br><span class="line"># Gram </span><br><span class="line">K &#x3D; np.array([np.dot(X[i], X[j]) for j in range(m) for i in range(m)]).reshape((m, m)) </span><br><span class="line">P &#x3D; cvxopt.matrix(np.outer(y, y) * K)</span><br><span class="line">q &#x3D; cvxopt.matrix(-1 * np.ones(m))</span><br><span class="line"># 等式约束</span><br><span class="line">A &#x3D; cvxopt.matrix(y, (1, m))</span><br><span class="line">b &#x3D; cvxopt.matrix(0.0)</span><br><span class="line"># 不等式约束</span><br><span class="line">G &#x3D; cvxopt.matrix(np.diag(-1 * np.ones(m))) h &#x3D; cvxopt.matrix(np.zeros(m))</span><br><span class="line"># 求解</span><br><span class="line">solution &#x3D; cvxopt.solvers.qp(P, q, G, h, A, b)</span><br><span class="line"># 拉格朗日乘子</span><br><span class="line">multipliers &#x3D; np.ravel(solution[&#39;x&#39;])</span><br><span class="line"># 支持向量</span><br><span class="line">has_positive_multiplier &#x3D; multipliers &gt; 1e-7 </span><br><span class="line">sv_multipliers &#x3D; multipliers[has_positive_multiplier]</span><br><span class="line">support_vectors &#x3D; X[has_positive_multiplier] </span><br><span class="line">support_vectors_y &#x3D; y[has_positive_multiplier]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#计算w，b</span><br><span class="line">def compute_w(multipliers, X, y):</span><br><span class="line">    return np.sum(multipliers[i] * y[i] * X[i]  for i in range(len(y)))</span><br><span class="line">def compute_b(w, X, y):</span><br><span class="line">    return np.sum([y[i] - np.dot(w, X[i]) for i in range(len(X))])&#x2F;len(X)</span><br><span class="line"></span><br><span class="line">w &#x3D; compute_w(multipliers, X, y)</span><br><span class="line">w_from_sv &#x3D; compute_w(sv_multipliers, support_vectors, support_vect</span><br><span class="line">b &#x3D; compute_b(w, support_vectors, support_vectors_y)</span><br><span class="line"> </span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<p>We saw that the original optimization problem can be rewritten using a Lagrangian function. Then, thanks to duality theory, we transformed the Lagrangian problem into the Wolfe dual problem. We eventually used the package CVXOPT to solve the Wolfe dual.</p>
</blockquote>
<ul>
<li>为什么需要将拉格朗日函数转化为对偶问题到wolfe dual？<br>这里还差对偶原则及Slater’s condition 概念。</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>股市整体价值理论</title>
    <url>/2021/02/22/%E8%82%A1%E5%B8%82%E6%95%B4%E4%BD%93%E4%BB%B7%E5%80%BC%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[<br>
<br>

<h4 id="大牛股"><a href="#大牛股" class="headerlink" title="大牛股"></a>大牛股</h4><ul>
<li>现在没涨，一年内会涨；<code>现在涨了，预计继续上涨（至少在未来3个月内）</code>；曾经大涨，但现在已经停止上涨或者开始下跌或者已经大跌，且在短期内股价不存在大涨（创新高）的可能性。</li>
<li>“买大牛股，抓主升浪”，这两句话是不可分的。</li>
<li>高确定性的操作与盈利模式</li>
<li>只关注那9～10类强势股</li>
<li>股票投资要记住两句话：第一句话是不要赔钱，第二句话是记住第一句。<span id="more"></span></li>
<li>绝大多数赔钱者，其赔钱的主要原因是缘于<code>选股问题——选错了股票，或者选中了非牛股，甚至是大熊股</code>，而操作技巧方面的不当只占赔钱的次要因素。</li>
<li>短期暴涨型主升浪：<code>在短期内股价涨幅达到80%～100%的上涨波段，或者是股价涨幅能够在一年内至少翻倍的股票</code>。<br>
<br></li>
</ul>
<p><img src="/2021/02/22/%E8%82%A1%E5%B8%82%E6%95%B4%E4%BD%93%E4%BB%B7%E5%80%BC%E7%90%86%E8%AE%BA/tqly.png" alt="63.28/19.42=3.26倍"><br>63.28/19.42=3.26倍，11/06-01/09，<br>10年50.22-&gt;488.57，年收益0.8729<br>PEG=[44.7 79]/87=[0.506 0.908]</p>
<br>

<ul>
<li>上涨动力：一是内在价值低估–&gt;估值修复力，二是内在价值成长–&gt;内在增长力，三是投机价值增长–&gt;投机价值增长力。</li>
<li><code>从低估值股票挖掘，再看其成长性，最后看其有无热门概念，这就是选股的逻辑。</code></li>
<li>一是信息系统，包括信息搜集、整理、归类与处理；二是分析系统，分析信息对于股价的作用，特别是对于主升浪的作用，这需要在事前建立相关的评价标准；三是操作系统，在“五面整体顺势”的情况下加仓，在“五面整体逆市”的情况下减仓，即，在判断正确的情况下持仓或者加仓，在判断错误的情况下不增仓或者减仓止损。</li>
<li>一个运动的物体受到的力可以分为四种：一是原动力，二是持续推动力，三是阻力，四是运动过程中新加入的作用力。</li>
<li>炮射导弹可以利用大炮而获得极高的初速度与相当远的运动距离，可以大量节省火箭发动机的燃料，这对于降低导弹成本、提高导弹射程是很重要的。</li>
<li>洲际导弹的变轨技术</li>
<li>主升浪是十分罕见的，主升浪所占的时间一般不到该股票的整体交易时间的30%。也就是说，大多数股票在**70%**以上的时间里是处于非主升浪状态，其中的多数又是处于振荡状态，或者说无明显的趋势状态。无趋势状态，是很难预测的，故可以归为随机波动范畴。</li>
<li>小市值低价股现在被公告注入热门资产后，这是一个巨大的利好题材，该利好题材带来“一字涨停板”方式暴涨，我们经常看到这类股票会以<strong>连续5～6个甚至是十几个“一字涨停板”暴涨</strong>。这样的暴涨过程，就是一个惯性运动，即，以巨大的初速去克服股价上涨的所有阻力，当股价运动速度无法抵消阻力的反作用时，股价的向上运动就会停止，甚至会因前期的过度上涨而发生走势逆转，导致短期大跌，最终形成暴涨暴跌之态。</li>
<li>股价原动力的爆发力度：估值法，就是对股票进行估值，看看股价是否被低估。绝对估值法，就是评估股票的绝对投资价值。相对估值法。</li>
</ul>
<br>

<p>综上，<br>1、不能选错股票、不能赔钱，核心：大牛股的主升浪，核心：低估值股票挖掘，再看其成长性，最后看其有无热门概念。<br>2、短期暴涨型主升浪<br>3、连续5～6个甚至是十几个“一字涨停板”暴涨<br>4、一是原动力，二是持续推动力，三是阻力，四是运动过程中新加入的作用力。<br>5、股价原动力的爆发力度：估值法</p>
<br>
<br>

<h4 id="绝对估值法"><a href="#绝对估值法" class="headerlink" title="绝对估值法"></a>绝对估值法</h4><p>评估股票的内在价值与股价的差值，当内在价值与股价的比值越大，则股价的上涨潜力越大，这种潜力一旦遇到合适的市场时机，就有可能爆发出来，转变为引发股价暴涨的原动力。</p>
<ul>
<li><p>投资价值估值法<br>1、目标股票的长期投资收益率与长期国债收益率进行比较，两者差值越大，则长期投资价值越高，这里的长期应该是指5～10年以上。巴菲特的标准更高一些，他的长期价值投资收益率基本定在10年内的年均复合收益率达到10%以上。<br>2、在考虑到风险溢价的情况下，择股标准应该趋于保守，所选择的长期价值股应该是低市盈率的成长股。按照价值投资的基本标准，对于长期成长股来说，其合理的股价范围应该满足PEG＜1（PEG=市盈率/收益增长率），且越低越好。当PEG＜0.5时，股票就存在足够的安全边际。<br>3、若一只股票未来的5～10年的年均复合收益增长率能够达到20%，则当该股的市盈率跌到10倍时，就具有极好的投资价值。应该注意的是，此时的股价虽然不高估，但也不是<strong>明显低估</strong>，还难以产生爆发性很强的估值修复力。假若此时，股市暴跌，该股股价随着股市暴跌而意外地跌到5倍市盈率，那么，此时该股股价就被明显低估了。</p>
</li>
<li><p>资产价值估值法<br>1、比较股票净资产值与总市值的比值，比值越大，则股价低估越多，股价上涨空间越大。<br>2、这类股票主要体现在隐蔽资产股上面。自2006年来，隐蔽资产股经常走出大牛股。<br>3、一隐蔽资产的大幅升值不被知晓，二隐蔽资产的增值过程是公开的、渐进式的，但隐蔽资产股的股价涨幅在起初滞后于隐蔽资产的增值幅度，最终股价会出现报复性补涨，使得隐蔽资产增值的这一事实成为了股价暴涨的原动力。</p>
<blockquote>
<p>2012年7月的罗顿发展：当《二十一世纪经济报道》披露该股在10年前曾花了2亿元在博鳌的黄金地段买下了1800亩土地，这些土地现值100多亿元，以罗顿发展总股本4.4亿股计算，仅这些土地价值就足以支撑该股股价到达15～20元。在该信息公告时，罗顿发展的股价只有3.9元。该信息公告后，罗顿发展的股价就出现了连续涨停板，不到一个月股价涨幅达到170%！在此期间，上证指数还是下跌的。<br><br>2013年1月28日至2月6日的西水股份就属此例。西水股份持有1.3亿股兴业银行，当兴业银行股价暴涨后，西水股份持有的兴业银行价值大幅增值，几乎等于西水股份的总市值，由于西水股份还持有未上市的天安保险11亿股，以及其它资产，这等于说除去兴业银行外，西水股份持有的其它资产被忽略不计了，这是明显不合理的，于是，在2013年1月28日至2月6日，该股就出现报复性的补涨，股价从6.6元涨到了11.8元，几乎翻倍。</p>
</blockquote>
</li>
<li><p>绝对套利估值法<br>1、上市公司被以高出市场价一大截全额要约收购时，其要约收购价格就是股票的真实内在价值，这个内在价值与现价的差值就是股价的无风险套利空间。假若市场是有效的，那么，股价就会几乎一步到位地涨到那个要约收购价格，于是，这个要约收购公告就是造成股价暴涨的原动力。<br>2、前几年被要约收购退市的石油大明、辽河股份等，就属此例。<br>3、当然，在有些时候，要约收购并非是全额的，而是部分的，如去年的重庆啤酒，那么，这类股票的上涨就可能不是一步到位的，而是渐进式的，二级市场就存在套利空间。在渐进式的情况下，要约收购公告虽然还是股价上涨的原动力，但它却会受到当时的市场运行状态、该股同板块股票运行状态等因素的影响，因此，股价运行就不仅受到原动力的作用，还受到其它新的作用力的作用。<br>3、这种情况还出现在某些上市公司大股东回购公司部分股票时，回购部分股票的股价并非是股票的真实内在价值，该价格只是大股东认可的价格而已，不能与完全要约收购价格相提并论，这是需要搞清楚的。</p>
</li>
</ul>
<p>综上，<br>1、按照逻辑，估值存在潜力时，股价未能反应的原因为，存在其他阻力，如市场运行状态、同版股票运行状态等。<br>2、概括，三个绝对估值方法为，PEG&lt;0.5的长期投资、隐蔽资产(所持股票或投资的公司或土地等)、全额要约收购与部分要约收购。</p>
<br>

<h4 id="相对估值法"><a href="#相对估值法" class="headerlink" title="相对估值法"></a>相对估值法</h4><p>一是基本面预期类，二是市场面预期类，三是技术面预期类，四是大盘面预期类。</p>
<p>1、就是领涨的市场、板块或者个股，它们是那些拥有比价效应、等待补涨的市场、板块或者个股的追赶的目标。<br>2、在补涨者开始补涨时，领涨者可以暂时停止上涨；在补涨者开始补涨时，领涨者还可以继续上涨，这对补涨者的领涨作用更大。<br>3、随着价格等自变量的变化，心理预期这个因变量也会随之变化。可见，若用一句话表述心理预期类投机价值，那就是–&gt;一切处于变化之中。<br>4、戴维斯双击，是指因股票收益持续成长，投资者会对股票未来的收益增长产生更高的预期，在未来收益还未实现的情况下，就以未来的高收益定位其市盈率，从而提升其市盈率定位水平，最终导致股价过度上涨。<br>5、<code>提升了股票的市盈率，也就是提升了股票的估值，这相当于产生了新的投机价值</code><br>6、戴维斯双击原理： 由于 <code>股价=每股收益*市盈率</code> ，当每股收益持续增长时，即使市盈率保持不变，股价也会同比上涨，这属于戴维斯双击中的第一击；当每股收益持续增长时，若还提升市盈率，则股价会更上一层楼，涨幅会更大，这属于戴维斯双击中的第二击。戴维斯双击能够导致乘数效应，使得股价涨幅加倍。由于戴维斯双击提升了股票的市盈率，也就是提升了股票的估值，这相当于产生了新的投机价值。戴维斯双击效应产生的根源，就是投资者依据历史数据的惯性，顺势推导、过度预期的结果。<br>7、更重要的是<code>因比价关系引发的股价跟涨而造成的股价虚高</code>，但这并不妨碍我们得出因基本面过度预期而产生新的投机价值的结论。<br>8、主流热点是市场热钱追逐的对象，处于热点中的板块与股票一定是短期内最牛的，基本上属于短线暴涨型品种。<br>9、市场面预期，就是对于市场未来炒作热点的预判。一旦判断某个板块或者个股刚刚成为或者在未来会成为市场炒作热点，那么，该板块或者个股就具有新的投机价值，股价就具有投机价值所赋予的上涨空间。<br>10、比价关系是领涨者已经给出了方向，给出了大致的投机价值空间，但市场面预期是市场才刚刚开始启动热点，或者还未开始热点，但投机者只是预见到了炒作热点将出现，但投机者一般还难以确定投机价值到底有多大，上涨空间有多大，这要走一步看一步。<br>11、在2012年12月中旬，当媒体披露北斗系统将投入商业应用时，超级主力立即抢进了北斗星通等龙头股，造成北斗概念股暴涨。我认为，这次对于北斗概念股的炒作，属于市场面预期主导下的“自我增强”型炒作模式。其基本逻辑是：超级主力判断北斗概念也许会成为市场热点，但到底能形成多大的热点，他们也许并没有很大把握，因为北斗概念是一个全新的概念，人们很陌生，超级主力担心投资者不认同；但超级主力知道，要让投资者认识且认同北斗概念，最佳的方法就是让北斗概念股暴涨，因为股票暴涨一定会吸引全市场的眼光，所有人都会好好研究的，这就是一种“胡干胡有理，越干越有理”的江湖思维。当然，平心而论，毕竟北斗导航属于国家级的概念，超级主力知道，即使胡干的风险也不大，最终，他们成功了。<br>12、赚钱的境界是这样的：最低的境界是打工，用自己的身体赚钱；次低的境界是做实业，让别人的身体为自己赚钱；较高的境界是玩钱（做金融），让钱生钱；而最高的境界是玩规则，让所有的人与所有的钱为自己赚钱。这就是赚钱的金字塔模型。<br>13、以二级市场来论，也存在一个投资者的金字塔模型：<br>最低的境界是交易，次低的境界是跟庄，较高的境界是坐庄（单只股票），而最高的境界是引领市场、发动行情（玩板块甚至玩整个市场）。北斗概念主力就属于最高境界的，不仅是北斗概念，去年市场的几乎所有的市场热点——3D打印、手游、传媒等，都是不同的超级主力们发动的行情。这些主力们“敢为天下先”的底气何在？有人可能认为是他们的资金实力雄厚，此言谬矣，你若有钱，去发动钢铁板块、水泥板块试一试？我认为，这些超级主力的过人之处，还是<strong>对于未来市场主流热点的预期、研判的功力深厚</strong>，任何人要是有这个本事，就一定会走在市场曲线的前面。<br>14、价量时空。<br>15、基本面分析公司未来价值，关注的是现在的股价，以及未来的股价定位，不太关心过去股价。<br>16、基本面分析的核心就是定价理论，即，对于现在股价是否合理进行评判——若低估了就可以买进。公司未来价值的预测，以期判断股价的上涨空间。巴菲特的办公室里是没有电脑的，但他非常关心现在的股票报价与其内在价值的关系，至于股票过去是什么价格或者什么走势，他也从来不看股价走势图。<br>17、市场面分析是通过比价关系来推测股价定位。所以，市场面分析主要是看现在股价与未来股价，对于过去股价也不是太关心。这很容易理解，若某个板块因热门概念而出现暴涨时，在依据比价关系挖掘该板块内的补涨股时，看绝对价格要比看历史走势重要得多，只要股价被低估，任何形态的股价走势都可以出现补涨。<br>18、技术分析的目的是为了预测股价未来走势，侧重于研究过去价格。它将股价走势图从基本面、市场面中抽提出来，让股价走势图完全独立了。走势图一旦独立，技术分析也就自成一统。技术分析将股价走势图看成一个活物留下的足迹，通过研究这些历史足迹，去预测这个活物下一步或者未来将走向何处。技术分析就是分析与预测<strong>这三种趋势的产生、持续与转换关系</strong>。<br>19、首先，技术分析要研究<strong>趋势的产生</strong>。<strong>一个新趋势的产生，一定是原有的其它两类趋势中的某一个终结的结果。</strong>如，一个新的上涨趋势的产生，就一定意味着一个原有的下跌趋势，或者横盘趋势的结束。所以，研究新趋势的产生，意味着要同时研究旧趋势的结束，两个相关趋势要同时研究。<br>20、其次，技术分析要研究趋势的持续。<strong>一个趋势一旦产生后，就会持续</strong>，这种持续就相当于趋势产生了顺势发展的惯性。很显然，假若能够认识到一个上涨趋势是处于持续发展中，或者惯性上涨中，那么，投资或者投机就变成了很简单的事情——买入持有，随着趋势惯性上涨就可以了；假若…<br>21、再次，技术分析还要研究趋势的结束。与研究趋势产生，本质上是一回事，只是研究的对象不同罢了<br>22、所有的技术分析方法，万变不离其宗，就是<strong>为了揭示趋势的产生、持续与结束</strong>，或者简言之，就是为了揭示趋势惯性。投机价值就是预期价值，而趋势惯性本身就具有预期性质，所以，趋势惯性能够提升或者降低投机价值。<br>23、波浪理论有某一浪的预期上涨高度、形态理论有形态突破后的预期量度涨幅、量价理论有放量突破后的惯性上涨高度，等等。224、股市有谚：横有多长，竖有多长。说的是一只股票若长期做底或者横盘，股价一旦启动向上突破，那么，涨幅机会十分惊人。这个惊人的涨幅，就是技术面预期类赋予的投机价值，因为这个投机价值的存在，使得股价最终具有了预期中的上涨空间。这是一个自我实现的预言——因为技术面给出了预期涨幅，那么，买方就会在涨幅到顶之前持续买入，以获取预期中的那个投机价值收益。<br>25、技术面预期类投机价值对于短线交易，特别是对于挖掘主升浪启动点或者启动阶段、主跌浪的启动点与启动阶段都是至关重要的。<br>26、对于短线交易来说，趋势、形态、量价关系是最重要的三个因素，而均线、指标等，就属于次级重要的因素了。<br>27、“济安金信价值分析系统”，该系统的主开发人杨健教授，说<strong>均线系统属于滞后指标</strong>，对于操作意义不大，而该系统运用了许多自创的新指标。<br>28、巴菲特说：“投资只需要学习两门课程就可以了，一门是如何评估企业价值，一门是如何看待股市波动。” </p>
<p>综上，<br>1、基本面预期<br>2、热点等市场面预期研判<br>3、戴维斯双击<br>4、领涨、补涨、跟涨虚高</p>
<br>

<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>大体说的，也是已知的，但是给了个分析框架。<br>基本面向好，为什么不能反映在股价。为什么8个月涨了4倍多。涨停的逻辑在哪。等问题在框架内还是有一定程度解释力。</p>
<p>框架基本操作思路是牛股的大波段，选股思路为低估值+成长+题材。<br>并由力学中四种力，四种力的不同合力模式构成物体运动。<br>股市中不同合力模式构成股价上涨，主要三方面估值修复+价值成长+投机价值。<br>转而对原动力进行估值法定量衡量。<br>绝对估值法包含：PEG、隐蔽资产、要约收购。<br>比价效应带来领涨、补涨、跟涨虚高。而预期无参照物，不同于比价，即一切处于变化之中。<br>相对估值法包含：基本面预期、市场面预期(热点)、技术面预期、大盘面预期。</p>
<p>再统一，即为投资价值和投机价值。</p>
<p>补充：<br>基本面的信息主要包括政策性信息、行业性信息、经营信息（产品价格信息、重大合同信息、新项目与新产品信息）、财务信息（营收信息、净利润增减信息）等。<br>技术面信息主要包括价格图表、技术指标与交易信息，其中，价格图表是技术分析的核心要件，不同的交易者关注不同的图表；技术指标更是五花八门，多达数百种，只能是各取所需了；而交易信息主要是各种交易数据的排行榜，包括涨幅榜、换手率榜、成交量榜、量比榜、成交金额榜等。</p>
]]></content>
      <categories>
        <category>股市理论</category>
        <category>数女-谷</category>
      </categories>
      <tags>
        <tag>股市理论</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络</title>
    <url>/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<br>

<br>

<h4 id="要刷的几个题目"><a href="#要刷的几个题目" class="headerlink" title="要刷的几个题目"></a>要刷的几个题目</h4><span id="more"></span>

<ol>
<li>海明码、crc</li>
<li>信道利用率=（帧长/发送速率） / [（帧长/发送速率） +  rtt]   – 衡量发送方是不是在等待信道而空闲</li>
<li> 信道吞吐率 = 信道利用率 *  发送速率</li>
<li>二进制指数退避重传算法，k=min{k,10} [0,2^k-1]</li>
<li>PPP/HDLC的帧格式、区别</li>
<li>以太网相关记忆</li>
<li>停等、GBN、SR、ALOHA、CSMA、CSMACA、CSMACD、令牌环 (随机访问介质访问控制才有冲突)</li>
<li>ip数据报的分片过程，及其中某些首部字段含义</li>
<li>子网掩码</li>
<li>CIDR、最长匹配前缀、CIDR和子网掩码</li>
<li>ipv4和ipv6的报文格式，以及区别</li>
</ol>
<br>

<br>





<h4 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h4><ol>
<li>计算机网络的功能：数据通信、资源共享、分布式处理、提高可靠性、负载均衡</li>
<li>组成：硬件软件 及协议。工作方式：边缘部分(主机 p2p c/s方式)及核心部分(路由器及大量网络)</li>
<li>功能组成：通信子网(实现数据通信 – 包括物理层、数链层以及网络层)和资源子网(实现资源共享和数据处理 – 包括会话、表示和应用层)，传输层属于两个子网的接口</li>
<li>分类：按照分布范围：WAN MAN LAN PAN ；公用网、专用网；按交换方式分，电路交换、报文交换、分组交换；按拓扑结构分，网状(广域网)；按传输技术分，广播式、点对点</li>
<li>RFC – 因特网标准形式</li>
<li>组织：ISO ITU IEEE IETF</li>
<li>性能指标：速率 带宽 吞吐量。带宽：带宽变大是发送变快，一定时间往链路注入的bit更多，而不是传播更快。吞吐量：单位时间通过某个网络(信道/接口)的数据量。</li>
<li></li>
<li></li>
</ol>
<br>

<br>

<h4 id="层次"><a href="#层次" class="headerlink" title="层次"></a>层次</h4><ol>
<li>TCP/IP协议栈：从下到上：物理层、数据链路层、[ARP] IP [ICMP/IGMP]、TCP/UDP、HTTP/FTP/DNS</li>
</ol>
<br>

<br>



<h4 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h4><br>

<br>



<h4 id="数据链路"><a href="#数据链路" class="headerlink" title="数据链路"></a>数据链路</h4><ol>
<li><p>功能</p>
<p>保证数据无差错地传输到相邻节点，实现透明传输。进行： 组帧、差错控制–校验码、流量控制、链路管理(有连接的链路进行连接维持和释放)。从而为网络层提供服务(有连接有确认，无连接无确认，无连接有确认)。</p>
</li>
<li><p>封装成帧</p>
<p>4种方法。一般用比特填充和违规编码。</p>
</li>
<li><p>差错类型，位错和帧错。帧错：重复、失序、丢失。纠错 - 海明、检错 - 奇偶 crc(填上n-1个0 除以生成多项式,加上余数 又叫冗余码 FCS帧检验序列。除法是异或)。</p>
</li>
<li><p>有线传输一般，无确认无连接，保证交给上层-网络层。无线传输一般是有确认的(不一定有连接)，尽量避免差错。</p>
</li>
<li><p>流量控制，收不下就不去确认。包括，停止等待协议、滑动窗口。</p>
<p>传输层的流量控制是公告窗口。</p>
</li>
<li><p>GBN：累积确认(捎带)；接收方只按序接收、否则丢弃，并确认帧催更；发送发确认序列号最大的、按序号达到的；超时重传；发送窗口2^n-1，接收窗口1(n为帧序号长度)；</p>
</li>
<li><p>SR：选择重传协议。只重传出错的；接收方有缓存的窗口；对数据逐一确认，收到一个确认一个；最大接收和最大发送窗口大小：2^(n-1)</p>
</li>
<li><p>点对点链路，广播链路。信道访问介质控制：静态、动态划分信道。动态分为 轮询和随机访问。随机访问介质控制：aloha、csma、csmacd、csmaca。</p>
</li>
<li><p>ALOHA ，纯aloha 和 时隙aloha。吞吐是一段时间的发送成功的帧数。一个想发就发，一个在时间片开始发送和重传。</p>
</li>
<li><p>CSMA：载波监听、多点接入(逻辑总)，分为1坚持，非坚持，p坚持。1坚持是忙了还在监听，空了就发。非坚持是忙了随机等会监听，空了就发。p坚持是忙了随机等会再监听，空了p概率发送。</p>
</li>
<li><p>轮询：轮流(3个缺点)，令牌传递协议。令牌：1过程 2缺陷 令牌开销；等待；单点故障 3应用 令牌环网-逻辑环，物理星；负载重和通信大的。节点在令牌后加上数据，修改令牌为忙，每个节点看到接收方不是自己则传给下一个，直到回到发送方，发送方发现数据正确，则说明发送成功。</p>
</li>
<li><p>CSMACD：总线型的以太网 有线。半双工网络。边发送边监听，电压检测。但是由于传播时延还是会冲突。争用期2*传播时延τ。重传：二进制指数退避重传算法(重传上限16次)。最小帧长：为了碰撞及时地被叫停–帧 的 传输时延至少2τ–以太网64B。</p>
</li>
<li><p>CSMACA：无线。先听后发，载波能量以及混合检测。无线局域网中无法全方位检测、隐蔽站问题，我发给b的时候不知道c发不发给b。– 检测空闲，a先发rts(包括接收方、发送方地址以及下一份数据持续发送的时间)，b接收端收到返回cts。c收不到b的cts，则不发了。a收到cts后，开始发送数据并预约信道(告诉其他人我要发送多久)，b收到数据后crc检验然后再发ack给a。a收到ack继续发下一帧，没有则重传(二进制指数退避)。</p>
</li>
<li><p>局域网：网络拓扑(星、总线、环、树) 以太网则是逻辑上总线型 ；传输介质：有线无线；根据介质访问控制，分为以太网(802.3 将数据链路分为了LLC和MAC两个子层，LLC主要为上面的网络层服务 比如建立确认连接的等，MAC主要帧同步之类)、令牌环网、FDDI、ATM、无线局域网(WLAN 802.11)。wifi是WLAN的一种应用。802.5令牌环、802.8光纤…</p>
</li>
<li><p>以太网：CSMACD，基带总线，无连接不确认(以太网提供的服务是不可靠的交付，即尽最大努力的交付。当目的站收到有差错的数据帧时就丢弃此帧，其他什么也不做。差错的纠正和重传由高层来决定。)，无差错接收，但不可靠，物理星逻辑总，</p>
</li>
<li><p>10baseT：基带、双绞线UTP、10Mb/s、物理星逻辑总、每段双绞线最长100m、csmacd有冲突、曼切斯特编码</p>
</li>
<li><p>适配器–网卡，每个网卡有mac地址，全球唯一，48位 – 前24位代表厂家 ieee确定 6个16进制数表示，在网卡的rom上有</p>
</li>
<li><p>以太网的MAC帧：前同步码7B + 帧定界符1B + <strong>以太网MAC帧 （源地址6B + 目的地址6B + 类型2B + 数据 + FCS4B）–这个是mac层</strong> = 物理层  数据46B-1500B</p>
</li>
<li><p>高速以太网：大于100M就是高速。全双工时候，用的是交换机，隔离了冲突域，无冲突。光纤上1g or  10g。</p>
</li>
<li><p>无线局域网：802.11、MAC帧头格式：分为四种(to ap–BSSID SA DA,from ap–DA BSSID SA,wds–RA TA DA SA,ibss–DA SA BSSID) 、每个ap站点相连的为一个基本服务集BSS,多个BSS构成ESS，需要一个分配系统DS–将所有的ap接入到有线的线缆中，将有线和无线结合。自组织的网络即把所有的主机安排在同一个网段….所有主机之间直接通信，无固定基础设施(DS或者有线)</p>
</li>
<li><p>广域网：分组交换技术，多个局域网组件而成，链路层设备交换机互联，(局域网只覆盖物理层和链路层，广域网还有网络层–路由器)，广域网强调资源共享</p>
</li>
<li><p>ppp：只支持全双工，封装帧、透明传输(字符填充法 转义字符)但是不需要序号纠错、不需要流量控制，简单差错检测，支持多种ip网络层协议，数据部分最大长度MTU，压缩，IP地址协商… 三个部分：1LCP身份验证2 NCP建立逻辑连接以支持各种网络层协议 3将ip数据报封装到串行链路。面向字节</p>
</li>
<li><p>HDLC：只支持全双工，不属于tcpip，面相比特，0比特插入法(51插入0)的透明传输，crc，有编号有确认 可靠，不纠错，主站、从站、复合站，三种数据操作方式：正常响应(主站同意从才能发)、异步平衡(每个复合站都可以发 地位平等)、异步响应(从站无需同意)，三种帧类型：无监息 帧</p>
</li>
<li><p>链路层设备：物理层：集线器(集线器相连的任何两个设备之间有通信、其他都不能通信)、主干集线器(冲突域)、光纤调制器(距离)，链路层：网桥or交换机(将以太网连接并根据mac地址进行过滤 分隔冲突域)。物理层虽然解决了距离问题，但是导致冲突域扩大。</p>
</li>
<li><p>网桥：透明网桥和源路由网桥。透明网桥通过自学习算法来填网桥的转发表，转发表即为主机和网桥接口之间的对应。源路由则是，发送方在帧首部加入 路由最少/时间最少 的 详细路由信息。交换机分为：直通式(查完6B的mac目的地址就直接转发)和存储转发式交换机(高速缓存中存放帧并检查是否正确，错误则丢弃)。交换机就是多接口的网桥。</p>
</li>
<li><p>冲突域和广播域：冲突域中同一时间只有一台设备才能发送数据。广播域是，网络中能接收到广播帧的所有设备集合。网络层设备都可以隔离，链路只能隔离冲突。</p>
</li>
</ol>
<br>

<br>

<h4 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h4><ol>
<li><p>功能 ： 把分组从源传到目的，单位是数据报。1路由和分组转发2异构网络互联3拥塞控制(闭环和开环两种控制方式)</p>
</li>
<li><p>数据交换方式：首先，为什么需要数据交换？(为了各个节点之间的通信，建立的链路不能过于复杂也需要远距离，就需要交换网络里的交换设备完成交换)。类别：电路交换、报文交换、数据报交换(分组交换)。电路：建立连接后通信。报文：先存储，等链路空闲后转发。分组交换：将报文分组。传送数据大且传送时间远远大于呼叫则选择电路交换。后两者都有存储转发技术。</p>
</li>
<li><p>数据报和虚电路：虚电路号，路由器记录虚电路号的转发接口，过程：建立连接、发数据、释放，保证有序到达。</p>
</li>
<li><p>路由算法：路由表(目的网络ip地址、子网掩码、下一跳ip地址、接口)。为了找到最佳路由而使用的算法，有两种分类：静态路由算法(非自适应，手动配置路由信息)、动态算法(路由器间彼此交换信息)。动态分为：全局性(链路状态路由算法–掌握完整的网络拓扑和链路费用信息)和分散性(距离向量算法–只掌握物理相邻的邻居以及链路费用)。OSPF 、RIP</p>
</li>
<li><p>分层次的路由选择协议：AS自治系统内的路由协议在其他AS是透明的。只要AS内的路由器都用本AS内的路由协议，并确定好和其他AS之间的路由方式。分为两类：内部网关协议IGP(AS内，OSPF / RIP)以及外部网关协议EGP(AS间，BGP-4)。</p>
</li>
<li><p>ip数据报格式</p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ip%E6%95%B0%E6%8D%AE%E6%8A%A5.png" alt="ip数据报"></p>
<p>ip数据报分为首部和数据部分(TCP/UDP段)。首部又分为固定部分和可变部分。固定<strong>20B</strong>。首部长度最小为0101(5)，单位是4B。总长度是包含<strong>数据部分的总长，单位1B，即总长度最大2^16-1</strong>，一般不会这么大，一般满足MTU。用于<strong>分组的”标识、标志、片偏移“</strong>。生存时间是ip分组在网络中的寿命，防止无限制地兜圈子。协议指数据部分所使用的协议，(即上层的协议)，如<strong>TCP – 6 ,UDP – 17</strong>。首部检验和–质检验首部。源地址、目的地址都是32bit。可变部分为：可选字段、填充字段。</p>
</li>
<li><p>ip数据报分片：以太网中MTU是1500B，其中至少20B为首部。首部：同一个报文的分片都有同样的标识；标志DF有0或者1，0是允许分片；最低位MF，0是最后一片/无分片；片偏移能够确定分组在整个数据报的哪个位置开始的，单位为8B。</p>
</li>
<li><p>一种八片首饰，总长度是1B，片偏移是8B，首部长度单位是4B。</p>
</li>
<li><p>ipv4地址</p>
<ol>
<li><p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%88%86%E7%B1%BBip%E5%9C%B0%E5%9D%80.png" alt="分类ip地址"></p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%89%B9%E6%AE%8Aip%E5%9C%B0%E5%9D%80.png" alt="特殊ip地址">    </p>
</li>
</ol>
</li>
<li><p>网络地址转换NAT：私有ip地址(A：10.0.0.0-10.255.255.255;B：172.16.0.0-172.31.255.255;C：192.168.0.0-192.168.255.255)网段个数分别为1 16 256个。如何实现私有ip地址和外部互联网主机的通信？安装NAT软件，NAT路由器会为本地网各个主机生成一个全球的地址代表，然后再分发给具体主机，转换表里表明外部的主机以及端口（这里的外部主机就是那个生成的代表地址），LAN表示本地网段里的主机ip和端口号。也就是在外部，各个内部网段的主机只是一个地址的不同端口。</p>
</li>
<li><p>子网划分，子网掩码</p>
</li>
<li><p>ARP：数据经过传输层分组以及网络层分片、加ip首部后，在数链层加上mac地址首部。每个主机有自己的ARP缓存，缓存本局域网中的ip对应的mac。如果ip不在本局域网中，则填入目的网关的mac地址。如果此时不知道目的ip的对应mac地址，则需要发一个广播ARP请求(包括源ip、目的ip、源mac地址)，目的ip主机收到后则 返回(本ip、本mac地址)。数链层加上mac后，加上FCS等，传给物理层，进行传输。ARP协议是自动进行的</p>
</li>
<li><p>DHCP：1主机广播DHCP发现报文(找到网络中的DHCP) 2DHCP服务器广播提供报文 3主机广播DHCP请求报文 4DHCP服务器返回广播DHCP确认报文。</p>
</li>
<li><p>ICMP：差错报告报文，几种类型：终点不可达、源点抑制、ttl=0、首部字段错误(参数问题)、重定向(让源主机知道下次更好的路由)。ICMP的前8个字节+IP数据报首部+IP数据报数据部分前8B = ICMP差错报告报文。再加上IP数据报的首部，构成一个ICMP报文的IP数据报。不发送ICMP：ICMP出错不发ICMP；第一个分片的数据报后的所有数据报片都不发ICMP；组播(1对多)地址的不发；特殊地址(环回等)不发。应用：比如ping（用了ICMP询问报文），用于回送请求和回答报文；时间戳请求和回答报文等。Traceroute(用了ICMP时间超过差错报文)，来跟踪一个分组从源到终点的路径(方法是将ttl设为不同的值，那么到了中间的路由器，就会返回ttl=0的差错报告报文，就能知道路由过程)。</p>
</li>
<li><p>ipv6数据报格式</p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ipv6%E6%95%B0%E6%8D%AE%E6%8A%A5.png" alt="ipv6数据报"></p>
<p>流是用于确定一些数据报是同一个流的。下一个首部：下一个扩展首部或者上层协议首部。有效载荷：扩展首部+数据。跳数限制：ttl。源地址和目的地址都是128bit。v6的首部是固定的40B。可变的放在扩展里。</p>
</li>
<li><p>ipv4和ipv6的区别：地址位数的扩展；没有了首部校验；将可选移到扩展首部里；ipv6首部长度是8B倍数，而v4是4B的整数倍；v6即插即用，不需要DHCP协议；v6只能在主机分片，如果在后面的链路层有MTU的限制，路由器只能将此v6报文丢弃，并且发送一个ICMPv6的分组过大报文；v6有资源的预分配；没有了协议和总长度字段，多了下一个首部和有效载荷长度字段；v6没有了服务类型字段。</p>
</li>
<li><p>ipv6表示：一般冒号十六进制，压缩：删除0，每一组至少有一个数字表示；或者一连串的0用冒号取代 – 只能出现一次两个冒号。</p>
</li>
<li><p>ipv6的类型： 单播–目的or源、多播–目的地址、任播 – 目的(对多台主机里最近的一台主机通信)。</p>
</li>
<li><p>如何从ipv4迁移到ipv6：双栈协议(同时启用)、隧道协议(重新封装)。</p>
</li>
<li><p>RIP协议：适用于小互联网，16表示网络不可达，最多包含15个路由器。每30s，只和相邻 路由器交换自己的路由表。180s，没有信息来则更新自己路由表：这个邻居死了。在拥有所有信息后，用距离向量算法得到 距离和下一跳。</p>
</li>
<li><p>RIP报文</p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/RIP%E6%8A%A5%E6%96%87.png" alt="RIP报文"></p>
<p>是应用层协议，使用UDP传输数据。将RIP的首部和路由部分作为UDP的数据，加上UDP的首部，再加上IP首部，构成ip数据报。其中，路由部分最多包含25个路由信息，所以更多路由需要多发几个报文。</p>
</li>
<li><p>RIP好消息传得快，坏消息传的慢：不断更新，直到距离更新到16，才直到这个是不可达的。</p>
</li>
<li><p>OSPF协议：适用于大型互联网，通过划分区域的方式，区别区域内部路由器和区域边界路由器以及主干路由器，以及自治系统边界路由器。开放最短路径优先协议。使用了分布式的链路状态协议。只有变化时候，洪泛向所有路由器发送 <strong>本路由器相邻的所有路由器的链路状态</strong>，如何哪些路由器相邻以及费用。=&gt;所有路由器都会有一个全网的拓扑图。OSPF分组：数据加首部最为ip数据报的数据部分。暂时理解为是网络层协议。</p>
<p>TIPS:发送的信息是与本路由器相邻的所有路由器的链路状态 ,只涉及与相邻路由器的连通状态,与整个<em>互联网</em>的<em>规模无关</em></p>
</li>
<li><p>链路状态路由算法：1每个路由器发HELLO分组，发现邻居，并更新和邻居之间费用metric 2构造DD描述分组给邻站自己的数据库中链路状态信息的摘要 3DD信息我都有了，就不处理，否则发LSR链路状态请求分组来请求自己没有的信息 4收到LSR，发送LSU链路状态更新分组 5更新后返回LSAck链路状态确认分组   – 最后 用Dijkstra来计算最短路径。 (简单说，就是先hello找邻居，然后发摘要，然后看看别人的摘要再发请求，最后整理所有的信息)</p>
</li>
<li><p>BGP：网络较大，只要求到达其他AS的较好的路由。各个AS的BGP发言人，交换到达某个网络所经过的一系列AS，即交换的是一系列的路径(距离向量)。BGP的报文作为TCP的数据部分，为应用层协议，需要TCP传送，即首先要建立TCP连接，然后在此连接至上建立BGP会话，利用BGP会话交换BGP报文。支持CIDR，需要网络前缀。</p>
</li>
<li><p>最常用版本BGP-4的四种报文：OPEN报文(认证及发现邻居)，UPDATE(路径变更)，KEEPALIVE(邻居之间周期性连接，也可以作为OPEN确认)，NOTIFICATION（报告差错、关闭连接。）</p>
</li>
<li><p>RIP OSPF BGP区别：</p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E5%85%B3%E5%8D%8F%E8%AE%AE.png" alt="网关协议"></p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E5%85%B3%E5%8D%8F%E8%AE%AE2.png" alt="网关协议2"></p>
</li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>投资逻辑</title>
    <url>/2021/06/10/%E6%8A%95%E8%B5%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<br>
<br>



<h3 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h3><p>1、2005年6月最暴跌时，一旦人民币升值、国有股流通，股市将大涨。</p>
<span id="more"></span>

<h3 id="108"><a href="#108" class="headerlink" title="108"></a>108</h3><p>1、12课：</p>
<p>由5日均线与10日均线构成的买卖系统，首先，两者的体位构成一个完全分类，女上位是牛，男上位是熊，还有一种是互相缠绕的情况，这种情况最终都要演化成女上位或男上位，只有两种性质：中继或转折。对多头来说，值得介入的只有两种情况：<strong>男上位转折，女上位中继</strong>，空头反之。</p>
<p> 例如，<strong>女上位趋势出现的第一次缠绕是中继的可能性极大</strong>，如果是<strong>第三、四次出现，这个缠绕是转折的可能性就会加大</strong>；还有，出现<strong>第一次缠绕前，5日线的走势必须是十分有力的，这样缠绕极大可能是中继，其后至少会有一次上升的过程出现</strong>；第三，缠绕出现前的<strong>成交量不能放得过大，一旦过大，骗线出现的几率就会大大增加</strong>，如果量突然放太大而又萎缩过快，<strong>一般即使没有骗线，缠绕的时间也会增加</strong>，而且成交量也会现在两次收缩的情况。</p>
<p>  <strong>女上位选择第一次出现缠绕的中继情况，而男上位的就相反，要寻找最后一次缠绕的转折情况</strong>，其后如果出现急跌却背弛，那是最佳的买入时机。抄底不是不可以，但只能选择这种情况。</p>
<p>没有人百分百确认那是最后一次缠绕，一般，<strong>男上位后的第一次缠绕肯定不是，从第二次开始都有可能</strong>，如何判断，最有力的就是利用好<strong>背弛制造的空头陷阱。</strong></p>
<p><br><br></p>
<p><strong>利用均线构成的买卖系统，首先要利用男上位最后一次缠绕后背弛构成的空头陷阱抄底进入，这是第一个值得买入的位置，而第二个值得买入或加码的位置，就是女上位后第一次缠绕形成的低位。</strong></p>
<p>即，男上转折、女上中继。即男上最后一缠后背驰、女上第一缠后低点。</p>
<p>卖点，即女上转折–女上位缠绕后出现背弛、男上中继–男上后第一次缠绕后的高点。</p>
<p><br><br></p>
<p>2、从来不觉得自己手里的股票有什么好，只知道他们能搞。但其实他是看基本面的。比如考虑人民币升值和股权分置。所以并不是无用，只是不能信“消息”。</p>
<p><br><br></p>
<p>3、</p>
<p><br><br></p>
<p>4、均线系统：</p>
<p>飞吻：短期均线略略走平后继续按原来趋势进行下去。</p>
<p>唇吻：短期均线靠近长期均线但不跌破或升破，然后按原来趋势继续下去。</p>
<p>湿吻：短期均线跌破或升破长期均线甚至出现反复缠绕，如胶似漆。 </p>
<blockquote>
<p>短期向长期靠拢，但是不破，进入缠绕或者按照原来的趋势继续(空头市场中说明反弹结束，多头则调整结束)。短期稍微平了继续原来的短期趋势(几率少、火爆后震荡)。短期向长期但是跌破或者升破。短期和长期反复缠绕(趋势转折，空转多)。</p>
<p>任何行情的转折大概率是突破引起。可能先突破然后拉升，然后陷阱诱再转折。还可能是转折箱型，拉高，然后转折。</p>
<p>第一次突破可能不一定真的转折。但突破之后必然有空多势能的转换。即使当时没有拉升很猛，但是势能变化了。</p>
<p>理解： 5上10下 女上位，10上5下，5 10缠绕。但最终都会是第一或者第二种。缠绕时等待转折。因此在缠绕末端介入，并且需要判断“转折为5上”或者“5上继续”的风险。</p>
<p>1、如果5上，且强势，且第一次产生缠绕，且缠绕前的成交不能过大(防止骗线)，则极可能是延续原来的趋势，还有一波。如果是第三四次则极可能是转折。</p>
<p>2、如果成交在缠绕前，突然放大又快速萎缩，则即使没有骗线，缠绕时间也会增加。成交可能再次收缩。</p>
<p>3、如果10上，且不是第一次缠绕，第二次开始往后、当然最好是最后一次缠绕转折，如果转折后还出现急跌并背驰，则为空头陷阱，为最佳的抄底买入。</p>
<p>(万达，我觉得后面疫情复苏有更高的可能。又是一次追。大部分只是压在这个逻辑。结果，没有任何好消息。也没有任何青睐。也没有势头。继续等待看看。）</p>
<p>4、第一个卖点，也就是<strong>女上位缠绕后出现背弛</strong>以及第二个卖点也就是<strong>变成男上位的第一个缠绕高点</strong>把东西卖了，这样就完成一个完整的操作。</p>
<p>5、买尽量买男上的转折，卖尽量卖女上的缠绕背驰。  </p>
</blockquote>
<br>

<br>

<p>5、实践理性</p>
<p>任何技术指标、系统，本质上都是一个评价系统，也就是告诉你在这个系统的标准下，评价对象的强弱</p>
<p>设置一套分类评价系统，然后根据该系统，对所有可能的情况都设置一套相应的应对程序，这样，一切的风险都以一种可操作的方式被操作了。</p>
<p>股票，主要有从0开始建立起来的系统。然后把技术和基本勾连起来，进行填充。心理面的建设，只有在实践里面练习吧。把操作融化为子程序。设计筛查均线上下关系以及缠绕关系的股票，由程序进行预警。然后在每个里面去看交易的起伏和势能。这个能够有评估系统进行对势能的筛查等。最后进入到图像上的走势关系。</p>
<p>比如，强势女上的时候，第一个缠绕后无放量的诱空，为买点。但如果是转折呢，转折的话，那么一定在短缠绕或者缠绕前就已经出货了。能够在前面看出出货的迹象吗。大笔出货还是小单多笔呢 – 这个从哪里看出来？todo</p>
<p>大量信息需要去噪和保持新鲜的及时反馈。分类考虑清楚了，全局和框架大体就出来了。实践需要一个评估系统和反馈系统。评估指标的数据化比较难，实时地记录也比较困难，</p>
<br>

<p>均线程序系统：</p>
<p>1、一个时间段，拉出来所有的k图，以及均线图</p>
<p>2、一个时间段，成交图，横轴为单价，纵轴为手。</p>
<p>3、筛选：均线女上到第一次缠绕。男上到第一次缠绕的股票。（判断是否是最后一次缠绕、以及是否继续还是转折）</p>
<br>

<p>迹象系统：</p>
<p>1、出货迹象：</p>
<p>2、主力迹象：</p>
<br>

<p>概率系统：</p>
<br>



<p>6、15课：“吻“的分类是基于对原趋势的反抗程度：</p>
<p>男上(均线为长期在上短期在下)转为女上时，连短线均线都不能突破，那期间出现的高、低点，肯定只是低级别图表上的，在本级别图表上没有意义 – 看不出和均线的关系，只能降低图标级别再看关系。</p>
<p>当走势突破短期均线却不能突破长期均线，就会形成“飞吻”–突破短期则短期均线会显示走平但是没有继续突破，则不会改变短期的趋势；</p>
<p>当走势突破长期均线马上形成陷阱，就会形成“唇吻”–靠近而不破，还按照原来的趋势，即是“诱惑转折”；</p>
<p>当走势突破长期均线出现一定的反复，就会形成“湿吻” – 有可能转折。</p>
<br>

<br>



<p>7、15课：转折</p>
<p>一般只有两种：一、“湿吻”后继续原趋势形成陷阱后回头制造出转折 – 最大的标志就是所谓的“背驰”了；二、出现盘整，以时间换空间地形成转折。：：：：这里的所有判断都只关系到两条均线与走势，和任何技术指标都无关。</p>
<p>理解：一个是最后一个湿吻之后的背驰之后转折，一个是盘整后转折。</p>
<p>看图上的话，没有吻也转折了 – 看的级别高了。</p>
<br>

<br>



<p>8、15课：背驰todo</p>
<p>在盘整中是无所谓“背驰”的</p>
<p>背驰是一个趋势中，趋势的力度，对应的现象。</p>
<p>量化：前一“吻”的结束与后一“吻”开始由短线均线与长期均线相交所形成的面积。（两条均线）同向趋势，但是面积变小，力度变弱，则形成了背驰。：：：：必须等再次接吻后才能判断，这时候，走势离真正的转折点会已经有一点距离了。解决：看低一级别的图，从中按该种办法找出相应的转折点。这样和真正的低点基本没有太大的距离。</p>
<p> 趋势平均力度：当下与前一“吻”的结束时 短线均线 与 长期均线形成的面积 除以 时间。一旦这次比上次弱，就可以判断“背驰”即将形成，然后再根据短线均线与长期均线的距离，一旦延伸长度缩短，就意味着真正的底部马上形成。</p>
<p><br><br></p>
<p>9、15课：第一个买点买入的基础在于男上位最后一个缠绕后出现背驰(往下跌，短线与长线均线面积比之前的小 ？)，而现在又出现男上位的缠绕，意味着前面引导买入程序启动的缠绕并不是最后一个缠绕，也就是程序判断上出现问题，因此必须退出。</p>
<p>第二个买点，一旦该缠绕中出现跌破前面男上位的最低位，就意味着买入程序出现问题。</p>
<p><br><br></p>
<p>10、16课：第一类买点且之前走势是“下跌+盘整+下跌”类型</p>
<p>**陷阱式：上涨+下跌；下跌+上涨。反转式：上涨+盘整+下跌；下跌+盘整+上涨。中继式：上涨+盘整+上涨；下跌+盘整+下跌。 **</p>
<p>在下跌时买入，唯一需要躲避的风险有两个：<strong>一、该段跌势未尽；二、该段跌势虽尽，但盘整后出现下一轮跌势。</strong></p>
<p>下跌走势用背驰来找第一类买点，就是要避开上面的第一个风险。而当买入后，将面对的是第二个风险。解决：就是<strong>其后一旦出现盘整走势，必须先减仓退出。</strong></p>
<p><strong>盘整也会耗费时间，对于中小资金来说，完全没必要。</strong>特别对于不想坐庄的大资金来说，这是一个最重要的问题，因为<strong>不想坐庄的大资金的安全建仓在六种走势中只可能在下跌+盘整+上涨这一种</strong>，其他都不适用。至于<strong>坐庄的建仓方法，和这些都不同。</strong></p>
<p>买卖入方法：在<strong>第一类买点买入后，一旦出现盘整走势，无论后面如何，都马上退出</strong>。这种买卖方法的实质，即只参与唯一的一种：<strong>下跌+上涨</strong>。对于资金量不大的，这是最有效的一种买卖方法。因为：<strong>对于下跌+上涨来说，连接下跌前面的可能走势只会有两种：上涨和盘整</strong>。</p>
<p><strong>如果是上涨+下跌+上涨，那意味着这种走势在上一级别的图形中是一个盘整</strong>，因此这种走势可以归纳在盘整的操作中，这在以后对盘整的专门分析里研究。对于“下跌+上涨”买卖方法方法来说，必须是这样一种情况：就是<strong>一个前面是“盘整+下跌”型的走势后出现第一类买点</strong>。显然，这个下跌是<strong>跌破前面盘整</strong>的，否则就不会构成“盘整+下跌”型，只会仍是盘整。</p>
<p>那么在该盘整前的走势，也只有两种：上涨、下跌。<strong>对于“上涨+盘整+下跌”的，也实质上构成高一级别的盘整</strong></p>
<p>=&gt;<strong>出现第一类买点且之前走势是“下跌+盘整+下跌”类型。</strong></p>
<p>标准程序=&gt; 一、首先只选择出现“下跌+盘整+下跌”走势的。二、在该走势的<strong>第二段下跌出现第一类买点时</strong>介入。三、介入后，<strong>一旦出现盘整走势，坚决退出</strong>。注意，这个退出肯定不会亏钱的，因为可以利用<strong>低一级别的第一类卖点</strong>(第一个卖点是女上最后一缠的背驰，第二个卖点是男下第一缠的)退出，是肯定要赢利的</p>
<p><strong>前面出现“上涨+盘整+上涨”走势的，一旦第二段升势出现第一类卖点</strong>，一定要走，因为后面很可能就是“上涨+下跌”的典型走势。</p>
<p>这种方法，无论买卖，都极为适用于中小资金，如果把握得好，是十分高效的。</p>
<p><br><br></p>
<p>11、17课：</p>
<p>  把实践中总结出来的、很难实用的、静态的“所有级别的走势都能分解成趋势与盘整”，转化成动态的、可以实用的“走势类型终要完成”。<strong>任何走势，无论是趋势还是盘整，在图形上最终都要完成</strong>。另一方面，一旦某种类型的走势完成以后，<strong>就会转化为其他类型的走势</strong>，这就是“不患”而有其位次。</p>
<p><span style="color:red"><strong>某级别走势类型中，被至少三个连续次级别走势类型所重叠的部分，称为缠中说禅走势中枢</strong></span>一般来说，对实际操作，都把这最低的不可分解级别设定为<strong>1分钟或5分钟线</strong>，当然，也可以设定为1秒种线，但这都没有太大区别。</p>
<p><span style="color:red">盘整：在任何级别的任何走势中，<strong>某完成的走势类型只包含一个缠中说禅走势中枢</strong>，就称为该级别的缠中说禅盘整。</span></p>
<p><span style="color:red">趋势：在任何级别的任何走势中，<strong>某完成的走势类型至少包含两个以上依次同向的缠中说禅走势中枢</strong>，就称为该级别的缠中说禅趋势。该方向向上就称为上涨，向下就称为下跌。</span></p>
<p><strong><span style="color:red">任何级别任何完成的走势类型，必然包含一个以上的缠中说禅走势中枢。</span></strong></p>
<p>  由原理一、二以及缠中说禅走势中枢的定义，就可以严格证明：</p>
<p><span style="color:red">“缠中说禅走势分解定理一”：任何级别的任何走势，都<strong>可以分解成同级别“盘整”、“下跌”与“上涨”三种走势类型的连接</strong>。</span></p>
<p><span style="color:red">“缠中说禅走势分解定理二“：任何级别的任何走势类型，都<strong>至少由三段以上次级别走势类型构成</strong>。</span></p>
<p><br><br></p>
<p>12、17课：</p>
<p><strong>在第一类买点出现后第一次次级别回调制造的低点，是市场中第二有利的位置</strong>，为什么？因为上涨和盘整必然要在图形上完成，而上涨和盘整在图形上的要求，是必须包含三个以上的次级别运动，因此后面必须还至少有一个向上的次级别运动，这样的买点是绝对安全的。</p>
<p>“缠中说禅买卖点定律一”：<strong>任何级别的第二类买卖点都由次级别相应走势的第一类买点构成。</strong>这样，就像前面曾说过的，任何由第一、二类买卖点构成的缠中说禅买卖点，都可以归结到不同级别的第一类买卖点。由此得到“缠中说禅趋势转折定律”：<strong>任何级别的上涨转折都是由某级别的第一类卖点构成的；任何的下跌转折都是由某级别的第一类买点构成的。</strong></p>
<p>  <strong>注意，这某级别不一定是次级别，因为次级别里可以是第二类买卖点</strong>，而且还有这种情况，就是不同级别同时出现第一类买卖点，也就是出现不同级别的同步共振，所以这里只说是某级别。</p>
<p><br><br></p>
<p>13、17课：</p>
<p>1、 连接两相邻同级别缠中说禅走势中枢的一定是趋势吗？一定是次级别的趋势吗？    </p>
<p>解：也可以是中枢吧。</p>
<p>但是中枢或者趋势都一定是次级别的趋势。    </p>
<p>正答：也不一定是次级别的，<strong>只要是次级别以下，例如跳空缺口，就属于最低级别，如果图上是日线、周线，就不会是次级别了；最后，往往相连走势类型的级别越低，表示其力度越大，这也就是为什么缺口在分析中有比较强技术含义的理论依据所在。</strong></p>
<p>2、 背驰是两相邻同向趋势间，后者比前者的走势力度减弱所造成的，如果用均线或MACD等判断其力度，一定要在同级别的图上吗？同级别的MACD红绿柱子背驰一定反映某级别趋势间出现背驰吗？是相应级别的趋势出现背驰吗？</p>
<p>本质是，判断力度能不能用次级别的MACD或者均线来判断。当然可以。因为本级别的背驰趋势是由次级别的走势形成的。</p>
<p>同级别的MACD红绿柱子指什么？反映什么级别的趋势间背驰？本级别还是？还是不反应？</p>
<p>​        </p>
<p>3、 盘整的高低点是如何造成的。（这个问题有点难度，提示，用缠中说禅走势中枢以及级别等进行分析。）</p>
<p>次级别的三个趋势构成本级别的盘整。多个次级别的趋势高低点构成本级别盘整的高低点。</p>
<p><br><br></p>
<p>14、18课：</p>
<p><strong>次级别的前三个走势类型都是完成的</strong>才构成该级别的缠中说禅走势中枢，完成的走势类型，在次级别图上是很明显的，根本就不用着再看次级别下面级别的图了。</p>
<p>缠中说禅盘整：在任何级别的任何走势中，某完成的走势类型只包含一个缠中说禅走势中枢，就称为该级别的缠中说禅盘整。</p>
<p>缠中说禅趋势：在任何级别的任何走势中，某完成的走势类型至少包含两个以上依次同向的缠中说禅走势中枢，就称为该级别的缠中说禅趋势。该方向向上就称为上涨，向下就称为下跌。注意，趋势中的缠中说禅走势中枢之间必须绝对不存在重叠。</p>
<p>“缠中说禅技术分析基本原理一”：任何级别的任何走势类型终要完成。</p>
<p>“缠中说禅技术分析基本原理二”：任何级别任何完成的走势类型，必然包含一个以上的缠中说禅走势中枢。</p>
<p>“缠中说禅走势分解定理一”：任何级别的任何走势，都可以分解成同级别“盘整”、“下跌”与“上涨”三种走势类型的连接。</p>
<p>“缠中说禅走势分解定理二“：任何级别的任何走势类型，都至少由三段以上次级别走势类型构成。</p>
<p><br><br></p>
<p>15、18课：</p>
<ol>
<li><p>走势的完成是必然的，但是如何判断走势的完成。即什么时候盘整结束之类的问题。</p>
</li>
<li><p>一个盘整，三个重叠的连续次级别走势类型后，盘整就可以随时完成，也就是说，只要三个重叠的连续次级别走势类型走出来后，盘整随时结束都是完美的，但这可以不结束，可以不断延伸下去，不断围绕这缠中说禅中枢上上下下地延伸下去直到无穷都是可以的。</p>
</li>
<li><p>所以，核心在于，走势何时停止延伸？</p>
</li>
<li><p>判断不清，则会在趋势中中间的中枢时候被震出去，而无法坚持到卖点</p>
</li>
<li><p>对于趋势来说，其“延伸”就在于同级别的同向“缠中说禅走势中枢”不断产生；而对于盘整来说，其“延伸”就在于不能产生新的“缠中说禅走势中枢”。</p>
</li>
<li><p>“走势类型延伸”是否结束的判断关键就在于<strong>是否产生新的“缠中说禅走势中枢”。</strong></p>
</li>
<li><p>“缠中说禅走势中枢定理一”：在趋势中，连接两个同级别“缠中说禅走势中枢”的必然是次级别以下级别的走势类型。</p>
</li>
<li><p>用反证法，该定理的证明是很简单的，而这也回答了上一章中的作业一“<strong>连接两相邻同级别缠中说禅走势中枢的一定是趋势吗</strong>？一定是次级别的趋势吗？”</p>
</li>
<li><p>首先，这不必然是趋势，任何走势类型都可能，最极端的就是跳空缺口后形成新的“缠中说禅走势中枢”；其次，也不一定是次级别的，<strong>只要是次级别以下，例如跳空缺口，就属于最低级别，如果图上是日线、周线，就不会是次级别了；最后，往往相连走势类型的级别越低，表示其力度越大，这也就是为什么缺口在分析中有比较强技术含义的理论依据所在。</strong></p>
</li>
<li><p>维持“缠中说禅走势中枢”的一个充分必要条件就是任何一个离开该中枢的走势类型都必须是次级别以下的并以次级别以下的走势类型返回，该问题很容易证明，因为无论是离开还是返回，只要是同级别的走势类型，就意味着形成新的“缠中说禅走势中枢”，这与原中枢的维持前提矛盾。该命题表述成如下定理：</p>
</li>
<li><p>“缠中说禅走势中枢定理二”：在盘整中，无论是离开还是返回“缠中说禅走势中枢”的走势类型必然是次级别以下的。</p>
</li>
<li><p>把1分钟图当成最低级别，那么最后<strong>连接离开与返回走势类型连接处的最低级别</strong>图，只能有两种可能：<strong>三根以上1分钟K线的来回重叠震荡后回头</strong>–比较少见；二、<strong>1分钟K线无三根以上K线重叠的V型走势</strong>–十分常见。</p>
</li>
<li><p>定理三：某级别“缠中说禅走势中枢”的破坏，当且仅当<strong>一个次级别走势离开该“缠中说禅走势中枢”后</strong>，其后的次级别<strong>回抽走势不重新回到该“缠中说禅走势中枢”内</strong>。</p>
</li>
<li><p>两个次级别走势的组合只有三种：趋势+盘整，趋势+反趋势，盘整+反趋势。其中的趋势分为上涨与下跌，分别代表从上方突破与下方跌破两种情况。</p>
</li>
<li><p>最用力的破坏，就是：趋势+盘整。例如在上涨中，如果一个次级别走势向上突破后以一个盘整走势进行整理回抽，那其后的上涨往往比较有力，特别这种突破是在底部区间。</p>
</li>
</ol>
<p> <br><br></p>
<p>16、21课： </p>
<ol>
<li>所有买卖点都必然对应着与该级别最靠近的一个中枢的关系。</li>
<li>第三买点：即 在中枢向上突破中枢高点后，回抽不跌破此高点，形成第三买点。</li>
<li>中枢有三种情况：延续、扩张与新生。延续，则是高低点没有被突破，扩张或新生则突破高点，形成一个新的趋势，要么是新的中枢要么是上涨趋势。延续则买点只能在中枢下。第三类买点，在中枢上，并且，这个中枢是扩张的。</li>
<li>所以中枢上的买点，只要避免延续情况即可。</li>
<li>对于中枢下形成的买点，但如果该中枢是在上涨之中的，在中枢之下并不能必然形成买点，中枢下的买点，只可能存在于下跌与盘整的走势类型中。</li>
<li>对于盘整的情况，其中枢的扩张与新生，都不能必然保证该买点出现后能产生向上的转折，因为其扩张与新生完全可以是向下发展的，而对于中枢延续的情况，中枢形成后随时都可以打破而结束延续，也不必然有向上的转折，所以盘整的情况下，中枢下也不必然产生买点。因此，只有在下跌确立后的中枢下方才可能出现买点。这就是第一类买点。</li>
<li>一个上涨趋势确定后，不可能再有第一类与第二类买点，只可能有第三类买点。</li>
<li>第一买点出现后的第二段次级别走势低点就构成第二类买点。即次级别出现的第一类买点则为本级别的第二买点。</li>
<li>⭐️⭐️⭐️⭐️⭐️<strong>第二类买点，不必然出现在中枢的上或下，可以在任何位置出现，中枢下出现的，其后的力度就值得怀疑了，出现扩张性中枢的可能性极大，在中枢中出现的，出现中枢扩张与新生的机会对半，在中枢上出现，中枢新生的机会就很大了。但无论哪种情况，赢利是必然的。</strong></li>
<li>第一类与第三类买点，一个在中枢之下、一个在中枢之上，也不可能产生重合。</li>
<li>只有第二类买点与第三类买点是可能产生重合的，这种情况就是：但第一类买点出现后，一个次级别的走势凌厉地直接上破前面下跌的最后一个中枢，然后在其上产生一个次级别的回抽不触及该中枢，这时候，就会出现第二类买点与第三类买点重合的情况，也只有这种情况才会出现两者的重合。</li>
<li>当然，在理论上没有任何必然的理由确定第二、三类买点重合后一定不会只构成一个更大级别的中枢扩张，但实际上，一旦出现这种情况，一个大级别的上涨往往就会出现。</li>
<li>一个最典型的例子，就是大盘在94年7月底部跌到325点后，8月1日跳空高开，5分钟上形成单边上涨突破前面的30分钟中枢，第二天大幅上冲后突然大幅回洗形成5分钟的走势级别的回抽，那时候最高已经摸到快500点，一天半上涨50%，又半天回跌15%，这样的回抽，一般来说是很恐怖的，但如果明白第二类买点与第三类买点的重合道理，就知道这是最好的补进机会，结果第三天又开始单边上扬，第六天达到750点。这是指数上最典型的一个例子了。而且，<strong>325点留下的缺口至今未补</strong>，中国几十年的一个大牛市，从指数上看，这是一个最重要的缺口了，将支持中国股市几十年甚至上百年的大牛市。</li>
<li>补充一句，站在特大型牛市的角度，中国就从来没出现过熊市，大家打开上海的年线图就可以看到，从1992年到2005年，一个完美的年级别缠中说禅中枢的三段次级别走势完成，时间刚好是13年，一个完美的时间之窗。站在年线的角度，中国股市的真正大牛市才真正开始，因为该中枢是中国股市的第一个年中枢，区间在998到1558点。站在年线级别，在下一个年线级别中枢确立之前，中国股市的调整只可能出现一个季级别的调整，而第一个出现的季级别的调整，只要不重新跌回1558点，就将构成中国股市年线级别上的第三类买点，其后至少出现如去年类型幅度的上涨。即使出现调整，最多就是季级别的，其后反而构成第三类买点。而且更重要的是，站在年线的级别看，目前还在第一段的次级别上扬中，要出现第二段的季级别调整，首先要出现月线级别的中枢，目前连这个中枢都没出现，换言之，年线级别的第一段走势还没有任何完成的迹象，这第一段，完全可以走到6000点才结束。</li>
<li>对卖点的分析是一样的，归纳起来，就有缠中说禅买卖点的完备性定理：市场必然产生赢利的买卖点，只有第一、二、三类。</li>
<li>市场中的任何向上与下跌，都必然从三类缠中说禅买卖点中的某一类开始以及结束。换言之，市场走势完全由这样的线段构成，线段的端点是某级别三类缠中说禅买卖点中的某一类。</li>
<li>思考题：任何一个线段，其端点必然是一买点及一卖点，请完全列出各类买卖点之间可能的组合。如果一线段的端点是同级别的买卖点，有什么组合是绝对不可能出现的。</li>
</ol>
<p><br><br></p>
<p>16、22课：</p>
<p>本ID在下面放上一个9999的买单，本ID顺着着这身体的轨迹轻扫着，还真有点体液</p>
<p>第二天，继续往下扭动身体，本ID的扫动越来越快，江浙派大概突然发现，这样继续下去，他就有被吸干的危险，尾市几笔就拉起来</p>
<p>第三天开始，在不断的摩擦中，面首开始挺立，每天尾市的游戏继续。</p>
<p>突然有一天，他也玩起打压恐吓的游戏。前两天，本ID就看热闹，不管他，第三天突然发狠，严重警告他，在乱恐吓就把他给杀了</p>
<p>江浙派果然是胆小之人，要挺立。</p>
<p>一定要在适当的时候突然狠狠一下，他就会惊吓得往相应方向惯性下去，一般来说，这种面首都是反应有点迟钝的，注定这种面首画出来的面相，总是反反复复，缠绵不断。</p>
 <br>

<p>真理是干出来的。</p>
<p>第一天的体液就不少，浮码很多，10几个交易日前那两根大量暗示着，浮码多，水就混，藏点大米还不简单？</p>
<p>周线图上的中枢强烈地勾引着走势往上，一般在一个组合里，一定要放一个这样老实巴交的面首，万一其他股票出现什么特殊的情况，马上变现这个去增援是能随时办到的，这样就一定不会出大乱子了。</p>
<br> 

<p>一般来说，这种阻击，<strong>在一个低位的大级别第三类买点进行是比较安全的</strong>，首先，第一类买点不适合，你先进去，大家都看着你，找机会吃你，你还找哪里潜伏下来？第二类买点是可以的，但一般都采取比较温柔的办法，慢慢来。第三类买点介入，有点硬来的感觉，这要求有一定的功力，否则给吃了都不知道怎么死的。但这样的安全性在于，第一，时间利用率高，第三类买点等于箭在弦上了，你这样突然进去横插一刀，除非是实力特别强，而且所用资金又没有什么期限，所弄的题材也没到迫在眉睫的地步，这样，他会留下来和你折腾。从而变成持久战。</p>
<p>高手就是高在一定要对盘中庄家的脾性有充分的感觉，对症下药，而且对阻击的目标有充分的了解，这样就能避免陷入持久战，互相在那里干耗着。当然，干耗其实也不怕，就是不断弄短差，把成本降下来，熬都熬死对方。这样的前提是资金必须绝对自由、没有期限。一笔自有的，没有利息压力的资金，是阻击的一个最安全的保障。</p>
<p>阻击一定要控制好量，最失败的阻击就是阻击成了庄家。为什么要在低位的第三类买点出手，这个位置，庄家已经货不少了，而成本还在附近，如果大力打压，你有实力在低位顶住，除非那面首钱出问题了，否则不可能亏钱把所有货倒给你，如果真是这样，就成全他算了。对于第三类买点的阻击，资金实力是很重要的，<strong>关键就是要顶住突然变向的打压，所以也要求一定只能在低位</strong>，不能与庄家的成本相差太远。</p>
<p><br><br></p>
<p>17、24课：</p>
<ol>
<li>任一背驰都必然制造某级别的买卖点，任一级别的买卖点都必然源自某级别走势的背驰。</li>
<li>日线上向上的背驰制造一个卖点，回跌后，在5分钟或30分钟出现向下的背驰制造一个买点，然后由这买点开始，又可以重新上涨，甚至创新高，这是很正常的情况。</li>
<li>用MACD判断背驰的前提是，A、B、C段在一个大的趋势里，其中A之前已经有一个中枢，而B是这个大趋势的另一个中枢，这个中枢一般会把MACD的黄白线（也就是DIFF和DEA）回拉到0轴附近。而C段的走势类型完成时对应的MACD柱子面积（向上的看红柱子，向下看绿柱子）比A段对应的面积要小，这时候就构成标准的背弛。</li>
<li>注意，看MACD柱子的面积不需要全出来，一般柱子伸长的力度变慢时，把已经出现的面积乘2，就可以当成是该段的面积。所以，实际操作中根本不用回跌后才发现背驰，在上涨或下跌的最后阶段，判断就出来了，一般都可以抛到最高价位和买在最低价位附近。</li>
<li>盘整中往上的情况，如果C段不破中枢，一旦出现MACD柱子的C段面积小于A段面积，其后必定有回跌。</li>
<li>比较复杂的是如果C段上破中枢，但MACD柱子的面积小于A段的，这时候的原则是先出来，其后有两种情况，如果回跌不重新跌回，就在次级别的第一类买点回补，刚好这反而构成该级别的第三类买点，反之就继续该盘整。</li>
</ol>
<p><br><br></p>
<p><img src="/Users/wyq/Downloads/satyrsBlog/source/_posts/%E6%8A%95%E8%B5%84%E9%80%BB%E8%BE%91/wddy.png" alt="wddy"></p>
<p><br><br></p>
<p> 18、25 26课：</p>
<ol>
<li><p>MACD的灵敏度，和参数有关，一般都取用12、26、9为参数，这对付一般的走势就可以了</p>
</li>
<li><p>但一个太快速的走势，1分钟图的反应也太慢了，如果弄超短线，那就要看实际的走势，即盘面和柱子。</p>
</li>
<li><p><strong>MACD的柱子伸长，和乖离有关</strong>，大致就是走势和均线的偏离度。</p>
</li>
<li><p>打开一个MACD图，首先应该很敏感地去发现该股票MACD伸长的一般高度，<strong>在盘整中，一般伸长到某个高度，就一定回去了，而在趋势中，这个高度一定高点，那也是有极限的，一般来说，一旦触及这个乖离的极限，特别是两次或三次上冲该极限，就会引发因为乖离而产生的回调</strong>。这种回调因为变动太快，在1分钟上都不能表现其背驰，所以必须用单纯的MACD柱子伸长来判断。即，冲击乖离而不能突破(虽然其形成的面积大于前面的，但两次柱子伸长都不能突破该高度)，意味着强暴的走势要歇，特别是两三次上冲，则大概率要回调。</p>
</li>
<li><p>不断一字涨停，不能用背弛来看，最简单，就是用1分钟的中枢来看，只要中枢不断上移，就可以不管。直到中枢上移结束，就意味着进入一个较大的调整，然后再根据大一点级别的走势来判断这种调整是否值得参与。</p>
<p>​    如果用MACD配合判断，就<strong>用长一点时间的，例如看30分钟</strong>。一般来说，这种走势，其红柱子都会表现出这样一种情况，就是<strong>红柱子回跌的低点越来越低，最后触及0轴，甚至稍微跌破，然后再次放红伸长</strong>，这时候就是警告信号，如果这时候在大级别上<strong>刚好碰到阻力位，一但涨停封不住，出现大幅度的震荡就很自然了</strong>。</p>
</li>
<li><p>注意，如果这种连续涨停是出现在第一段的上涨中，即使打开涨停后，震荡结束，<strong>形成一定级别的中枢后，往往还有新一段的上涨</strong>，必须<strong>在大级别上形成背驰才会构成真正的调整</strong>，因此，站在中线的角度，上面所说的超短线，其实意义并不太大，有能力就玩，没能力就算了。关键是要抓住<strong>大级别的调整，不参与其中</strong>，这才是最关键的。即，一定要避免大级别的调整。能不能看出来是大级别上有调整呢，只要出现了调整开始的信号，并且没有结束的信号就是~</p>
</li>
<li><p>此外，一定要先分清楚趋势和盘整，然后再搞清楚背驰与盘整背驰。–todo 盘整背驰和趋势背驰的区别</p>
</li>
<li><p><strong>盘整背驰里的三种情况，特别是形成第三类买点的情况</strong>，一定要搞清楚。注意，<strong>盘整背驰出来，并不一定都要大幅下跌</strong>，否则怎么会有第三类买点构成的情况。而<strong>趋势中产生的背驰，一定至少回跌到B段中</strong>，这就可以预先知道至少的跌幅。–todo1盘整背驰的三种情况 2趋势背驰的跌幅判断</p>
</li>
<li><p>对<strong>背驰的回跌力度，和级别很有关系</strong>  ，如果日线上<strong>在上涨的中段刚开始的时候，MACD刚创新高，红柱子伸长力度强劲，这时候5分钟即使出现背驰，其下跌力度显然有限，所以只能打点短差，甚至可以不管</strong> 。而在<strong>日线走势的最后阶段，特别是上涨的延伸阶段，一个1分钟的背驰足以引发暴跌</strong>，所以这一点必须多级别地综合来考察，绝对不能一看背驰就抛等跌50%，世界上哪里有这样的事情。即：<strong>在大级别的走势的不同阶段，小级别的背驰力度是不同的。一般来说，越小的级别，背驰对大级别的影响力越小，但是在大级别的走势末期，即使是小级别的背驰，也会有大的影响力。</strong></p>
</li>
<li><p>一般来说，一个标准的两个中枢的上涨，在MACD上会表现出这样的形态，就是第一段，MACD的黄白线从0轴下面上穿上来(即近期12日均值上升高于26)，在0轴上方停留的同时，形成相应的第一个中枢(短期一直高于长期，并且有一段盘整时间)，同时形成第二类买点(有过上涨后的回抽)，<strong>其后突破该中枢，MACD的黄白线也快速拉起，这往往是最有力度的一段，一切的走势延伸等等</strong>，以及MACD绕来绕去的<strong>所谓指标钝化都经常出现在这一段，这段一般在一个次级别的背驰中结束</strong>，然后进入第二个中枢的形成过程中，同时MACD的黄白线会逐步回到0轴附近，最后，开始继续突破第二个中枢，MACD的黄白线以及柱子都再次重复前面的过程，但这次，<strong>黄白线不能创新高，或者柱子的面积或者伸长的高度能不能突破新高，出现背驰</strong>，这就结束了这一个两个中枢的上涨过程。明白这个道理，大多数股票的前生后世，一早就可以知道了。todo<strong>macd指标钝化、走势延伸</strong>即，次级别的背驰，带来更高级别的走势结束。</p>
</li>
<li><p>调整是日线级别还是周还是月？</p>
</li>
<li><p>⭐️⭐️⭐️⭐️⭐️MACD在0轴附近盘整以及回抽0轴所形成的中枢，不一定就是相应级别的中枢，而是至少是该级别的中枢。例如日线MACD的0轴盘整与回拉，至少构成日线的中枢，但也可以构成周线的中枢，这时候就意味着日线出现三段走势。</p>
</li>
<li><p>股票是无须选择的，唯一值得选择的，就是波动大的股票，而这个是不能完全预测的。对于资金量小的投资者，完全可以全仓进出，游走在不同的凭证之间。这样的效率当然是最高的，不过这不适用于大资金。</p>
</li>
<li><p>一般来说，本ID只在月线、最低是周线的买点位置进去，追高是不可能的，这样会让变负数的过程变得太长，而且都是在庄家吸得差不多时进去，一般都是二类或三类买点，这样可以骗庄家打压给点货，从散户手里买东西太累，一般不在月线的第一类买点进去，这样容易自己变庄家了。</p>
</li>
</ol>
<p><br><br></p>
<p>整理：</p>
<p><br><br></p>
<p>19、</p>
<p>：消息，必有人知道，且是大资金知道，他们有实力利用消息，因此消息早已在盘面中。走势是钱堆的，那么走势是被知道消息的人、为了赚钱走出来的。只有钱是唯一值得信任的，而钱在市场上运动的轨迹，就是走势。这是唯一公开的可观察的东西。</p>
<p>：市场先生是疯子。但最终会回归到内在价值。消息不过是会让你因小失大的无用信息。真正地影响价值的只有管理者以及公司业务和未来。</p>
<p>两者的共性是都认为消息是无用的。前者希望在走势中分析出资金的意图。后者希望在公开年报以及行情研究中，分析出公司的内在价值。长期的确是后者均衡回归。而短期，走势如果的确有一定的规律和必然性在，那么也是更加反应资金情绪的最佳被观察客体。</p>
<p>第一手的、有价值的资料永远不是消息或者内幕。内幕的真假不可靠。可靠的是现象，而现象到背后的意图，之间的差距需要全分类以及评价体系去给出反馈。两者没有矛盾。只是逻辑不一样。</p>
<p>不同的逻辑，只要经得住推理，即使不同，也会获得各自</p>
<p><br><br></p>
<p>20、</p>
<p>和算法的业务指标一样。数据纷杂。</p>
<p>但是需要厘清重点和方向。</p>
<p><br><br></p>
<p>21、</p>
<p>不要有依赖心理，只有自己在实践中成为自己一部分的，才是真实的。</p>
<p>一个坏习惯足以毁掉一切，每次操作后一定要不断总结，逐步提高。</p>
<p><br><br></p>
<p>22、27课：</p>
<ol>
<li><p> 第二个中枢后就产生背驰的情况，一般占了绝大多数的情况，特别在日线以上的级别，这种就几乎达到90%以上，因此，如果一个日线以上级别的第二个中枢，就要密切注意背驰的出现。而在小级别中，例如1分钟的情况下，这种比例要小一点，但也是占大多数。一般4、5个中枢以后才出现背驰的，都相当罕见了。</p>
</li>
<li><p> 如果在第一个中枢就出现背驰，那不会是真正意义上的背驰，只能算是盘整背驰，其真正的技术含义，其实就是一个企图脱离中枢的运动，由于力度有限，被阻止而出现回到中枢里。</p>
</li>
<li><p> 一般来说，小级别的盘整背驰，意义都不太大，而且必须结合其位置，如果是高位，那风险就更大了，往往是刀口舔血的活动。但如果是低位，那意义就不同了，因为多数的第二、三类买点，其实都是由盘整背驰构成的，而第一类买点，多数由趋势的背驰构成。即：高位的小级别背驰往往是前驱信号–预警风险。为什么盘整背驰多构成第二三类买点？(第一缠上涨后的回抽、中枢上)。第一类买点是转折形成的，男上转女上，因此是趋势背驰。跌过了。</p>
</li>
<li><p> 一般来说，<strong>第二、三类的买点，都有一个三段的走势，第三段往往都破点第一段的极限位置，从而形成盘整背驰</strong>，注意，这里是把第一、三段看成两个走势类型之间的比较，这和趋势背驰里的情况有点不同，这两个走势类型是否一定是趋势，都问题不大，两个盘整在盘整背驰中也是可以比较力度的。</p>
</li>
<li><p>在某级别的某类型走势，如果构成背驰或盘整背驰，就把这段走势类型称为某级别的背驰段。</p>
</li>
<li><p>盘整背驰最有用的，就是用在大级别上，特别是至少周线级别以上的，这种盘整背驰所发现的，往往就是历史性的大底部。配合MACD，这种背驰是很容易判断的。这种例子太多，</p>
</li>
<li><p>例如000002，谁都知道该股是大牛股，但这牛股的底部，如果学了本ID的理论，是谁都可以发现的。请看该股的季线图，也就是三个月当成一个K线的图。1993年第一季度的36。7元下跌到1996年的第一季度的3。2元，构成第一段，刚好前后13季度，一个神奇数字；1996年的第一季度然后到2001年第三季度的15。99元，构成第二段，一个典型的三角形，中枢的第二段出现三角形的情况很常见，前后23季度，和21的神气数字相差不大；2001年第三季度下跌到2005年的第三季度的3。12元，前后刚好17周，神奇数字34的一半，也是一个重要的数字。第一段跌幅是33.5元，第三段是12.87元，分别与神奇数字34和13极为接近。因为13的下一个神气数字是21，加上前面说过的17，都不可能是第三段的跌幅，因此，站在这种角度，万科的2.99元附近就是铁底了。不过这种数字分析意义不大，最简单的判断还可以用MACD来，第三段跌破第一段的3.2元，但MACD明显出现标准的背弛形态：回抽0轴的黄白线再次下跌不创新低，而且柱子的面积是明显小于第1段的，一般来说，只要其中一个符合就可以是一个背弛的信号，两个都满足就更标准了。从季度图就可以看出，万科跌破3.2元就发出背弛的信号。而实际操作中，光看季度线是不可能找到精确的买点的，但对大资金，这已经足够了，因为大资金的建仓本来就是可以越跌越买，只要知道其后是一个季度级别的行情就可以了。而对于小资金来说，这太浪费时间，因此精确的买点可以继续从月线、周线、日线、甚至30分钟一直找下去，如果你的技术过关，你甚至可以现场指出，就在这1分钟，万科见到历史性大底部。因为季度线跌破3.2元后，这个背驰的成立已经是确认了，而第三段的走势，从月线、周线、日线等，可以一直分析下去，找到最精确的背驰点。</p>
</li>
<li><p>学过数学分析的，都应该对区间套定理有印象。这种从大级别往下精确找大级别买点的方法，和区间套是一个道理。以万科为例子，季度图上的第三段，在月线上，可以找到针对月线最后中枢的背驰段，而这背驰段，一定在季度线的背驰段里，而且区间比之小，把这个过程从月线延伸到周线、日线、30分钟、5分钟、1分钟，甚至是每笔成交，这区间不断缩小，在理论上，甚至可以达到这样一种情况，就是明确指出，就这一笔是万科历史底部的最后一笔成交，这成交完成意味着万科一个历史性底部的形成与时代的开始。当然，这只是最理想的情况，因为这些级别不是无限下去的，因此，理论上并不能去证明就是一个如极限一样的点状情况的出现，但用这种方法去确认一个十分精确的历史底部区间，是不难的。</p>
</li>
<li><p>推而广之，可以证明缠中说禅精确大转折点寻找程序定理：某大级别的转折点，可以通过不同级别背驰段的逐级收缩范围而确定。换言之，某大级别的转折点，先找到其背驰段，然后在次级别图里，找出相应背驰段在次级别里的背驰段，将该过程反复进行下去，直到最低级别，相应的转折点就在该级别背驰段确定的范围内。如果这个最低级别是可以达到每笔成交的，理论上，大级别的转折点，可以精确到笔的背驰上，甚至就是唯一的一笔。（不过这些其实都意义不大，1分钟的背驰段，一般就是以分钟计算的事情，对于大级别的转折点，已经足够精确了，对大资金，基本没什么用处。）</p>
</li>
<li><p>各位有时间可以参考一下，600640、000001、000006、000009、000012、600643的季度图，看看历史底部是怎么形成的。当然，只有特别老的股票才可以用季度图。而月线图的，看600663、一个标准的例子。</p>
</li>
<li><p>上面说的是背驰构成的买点，注意，<strong>第一类买点肯定是趋势背驰构成的，而盘整背驰构成的买点，在小级别中是意义不大的</strong>，所以以前也没专门当成一种买点，但在大级别里，这也构成一种类似第一类买点的买点，因为在<strong>超大级别里，往往不会形成一个明显的趋势</strong>，这也就是以前回帖曾说过的，站在<strong>最大的级别看，所有股票都只有一个中枢</strong>，因此，站在大级别里，绝大多数的股票都其实是一个盘整，<strong>这时候就要用到这因为盘整背驰而形成的类第一类买点</strong>了。这个级别，至少应该是周线以上。</p>
</li>
<li><p>类似的，在<strong>大级别里，如果不出现新低，但可以构成类似第二类买点的买点，在MACD上，显示出类似背驰时的表现</strong>，黄白线回拉0轴上下，而后一柱子面积小于前一柱子的。一个最典型的例子，就是季度图上的600685，2005年的第三季度的2.21元构成一个典型的类第二类买点。在实际操作中，2.21元的相应区间的寻找，也是按上面级别逐步往下找背驰段的方法实现。</p>
</li>
<li><p>如果按照周线级别，那不用等30年了。不过，周线找出来的，不一定是历史性大底，可能就是一个比较长线的底部。如果把这种方法用在日线上，也是可以的，但相应的可靠性就不是那么绝对了。</p>
</li>
<li><p>精通找出<strong>各级别中枢</strong>的，是幼儿圆毕业；精通分别<strong>中枢的新生、延伸、扩展</strong>的，是学前班毕业；精通分辨<strong>盘整背驰与背驰</strong>，躲过盘整背驰转化为第三类买卖点的，是小学毕业</p>
</li>
</ol>
<p><br><br></p>
<p>23、28课：</p>
<ol>
<li> 任何一个空壳公司，理论上，只要能合法地发行基金，然后用这传销得到的钱部分地投在该空壳公司的资产上，就可以在股票上赚取10倍以上的增殖</li>
<li> 年线图就是最长线的图了，因为任何一个人大概也就能经历70、80根的年K线，一个年线的第一类买点加一个年线的第一类卖点，基本就没了。把握好这两点，比任何价值投资的人都要牛了，那些人，不过是在最多是年线的买点与卖点间上下享受了一番而已。</li>
<li> 站在中国股市的现实中，这轮牛市的一个大的调整，必然会出现基金的某种程度的崩溃，上一次的牛市，让证券公司毁了不少，这一次牛市，毁的就是基金。</li>
<li>投资的第一要点就是“你手中的钱，一定是能长期稳定地留在股市的，不能有任何的借贷之类的情况”。而基金，不过是所谓合法地借贷了很多钱而已，即使是没有利息的，性质一样。一旦行情严重走坏，基金必然面临巨大的风险，一次大的赎回潮就足以让很多基金永不超生。</li>
<li> 传销，通常只有一个后果：归零。基金，至少对大多数来说，一样。这是基金一个最大、严重违反投资要点的命门：他的钱都不是他的。对于开放式基金，这点更严重，因为这种赎回是可以随时发生的。而中国的开放式基金就更可怕，中国人的行为趋同性极为可怕，国人一窝蜂去干一件事的后果是什么，大概也见过不少了，无论政治、经济、学术上，无一例外。</li>
<li>基金经理必然要以净值为标准，一个基金拿某只股票是有一定比例限制的，也就是说，基金在这点上，连庄家都不如，一旦超配，唯一的办法就是找其他基金帮忙拉一把，几家基金一起持有，其实就是联合坐庄，万一都超配了，或者一时各基金都无暇他顾，那就构成了一个很好的阻击机会。</li>
<li>短差又弄不来，又不能随时护盘砸盘，他持有的实际效果，就是让股票的盘子变小了。</li>
<li>就算不用一些非市场的手段、一些在中国肯定效果一流的桌底游戏，一次设计合理的阻击足以让这基金，轻的，吃点哑巴亏，重的，让他清盘走人。</li>
<li>如果他能熬得住，大不了就弄了一次出色的短差，等于傻大个持有的筹码人间蒸发了一段时期，投资中，唯一重要的其实就是成本，成本比傻大个低，再起来时，傻大个就更危险了，一次搞不死，还不能搞两次、三次，总有搞死的时候。一旦往下搞，基金的净值熬不住，那基金经理就可以走人了，然后，那些筹码就可以信达、东方一番了。</li>
<li>如果在一个大级别的，例如月线中枢的调整中，一个集中的攻击，打破一个点，把一个基金公司集中搞跨，所有的基金公司都将面临严重的赎回潮，然后就整个市场都可以严重地信达、东方一番了。</li>
<li>吃散户有什么意思呀，基金，就是散户打包，让人一口吃，少麻烦。</li>
<li>最近，一个小的周线中枢震荡，就足以让本ID去试验一下。一个20%都不到的回调，一个就算跌停也就5%的股票，一个基本面面临严重好转的个股，已经让某些人坐不住了。某些傻大个超配了，找人护也没人有空了，看看上周基金的净值，这种局面再维持一周，估计就有人熬不住了。当然，现在的基金还有实力，一棍子肯定打不死的，这次只是闹着玩一次，感觉不错，最次就是权当洗了一次盘，弄了一个出色的短差。本ID可没在这次就把人击倒的想法，12元不行，难道不可以20元才搞死？只要短差出来了，死的一定是没弄短差的人！</li>
<li>这个命门如何化解，如何不让这成为外国游资的重大突破目标</li>
<li>市场打开，就必然要面对各种攻击，如果管理层的智力还达不到攻击者的千分之一，那只有瞎闹的份。下一个死的，一定是基金，在一个月线级别的调整中，这一幕必然上演，现在唯一有疑问的是，不会连一个周线级别的调整，都会有好戏提前上演吧？这个可能性是不大的，如果真出现，这基金也弱了。</li>
</ol>
<p><br><br></p>
<p>24、</p>
<p>“人口消费化”与“资产虚拟化”是资本主义社会经济发展的真正秘密，而在“人口消费化”与“资产虚拟化”的爆发期，这种流动性过剩就是最正常不过的事情，而中国目前正处在该爆发期的启始阶段。</p>
<p>大国，首要的是成为资本大国，而只有成为资本大国，才有真正的大国可言。而资本大国的首要前提，就是一个巨大的资金大池子，全世界的资金都汇聚其中。</p>
<p>而在这个池子逐步形成的过程里，流动性过剩就一定是常态。</p>
<p>问题不是水太多，而是为什么池子的扩张速度如此慢</p>
<p>资本市场，从来都是资金大池子一个最重要的部分，一个超常规的资本市场扩张就是这池子扩张中必然也是当然的事情，而且是最重要的事情。</p>
<p>人民币放开以来资本市场的扩张以一种超常的速度出现，是最正常不过了。就算目前是全流通，按全部的市值算，也就GDP的一半上下，而美国这比例是多少？且不说GDP的中国速度依然惊人，按目前的速度，7、8年后又翻一翻，那么现在资本市场的扩张速度，只不过是一种补课而已，而且补得还不够好，还应该更好一点。</p>
 <br>

<p>“资产虚拟化”的大国溢价是一个最常见的现象。而资产的大国溢价，最终都会反映在资本市场之上。更重要的是，资本市场对资产的收纳范围必然急剧扩大，一切中国<strong>最优秀的资产</strong>，必然通过各种途径汇聚到中国的资本市场上，这是支持资本市场成为资金大池子最重要组成部分的最坚实基础，而这也是资本市场今后发展的最大动力。不理解这个，是无法理解目前中国资本市场发展的历史意义的。这不单单是一个量上的改变，而是一个根本性的结构改变，成为中国“资产虚拟化”历史进程的最重要标志。</p>
<p>注： 什么是最优秀资产？todo</p>
<br>

<p>25、</p>
<ol>
<li><p>市场经济 = 资本主义 = 现代经济…</p>
</li>
<li><p><strong>原始社会模式的社会经济形态</strong>，如斯大林式的资本主义经济形态。欧美式的资本主义，其原始社会形态，是<strong>以封建到资本主义原始积累前的混沌过度</strong>为形态的。</p>
</li>
<li><p>市场经济原始社会破裂后，就进入人口消费化与资产虚拟化扩展的原始积累时期。人类开始资本主义以来，所有的经济大国崛起，都离不开这种形态。注意，大国与经济大国，有着一定的区别。像前苏联这种，站在经济的角度，从来算不了大国</p>
</li>
<li><p>18、19、20世纪，欧美的经济以及其后的军事扩张，都是以这种资本主义奴隶社会形态最强悍的扩张力为其根基。但，最终所有的军事殖民都几乎以失败告终，而经济、文化上，却是无比的成功，这也可以看出经济、文化的深刻腐蚀性。经济、文化上资本主义的军事奴隶制游牧民族般的强悍，是比纯粹的军事强悍更有力、更本质的东西，这也是为什么在自相似中，美国经济、文化对世界的征服比成吉思汗的铁蹄更有力。</p>
</li>
<li><p>由于市场经济在世界范围内的不平衡，必然导致当<strong>某些国家完成市场经济奴隶社会形态时，后来的国家才刚进入这种场经济奴隶社会形态</strong>，因此，一场如同历史上游牧与农耕民族的征服与被征服游戏就不断展开。 =&gt;一切都是征服和被征服。人之间的感情只是弱的时候，会受牵扯。</p>
</li>
<li><p>其实，在思想历史上，也有同样的情况出现。<strong>思想历史上的奴隶社会阶段，是所有文化形态中最有活力的时代，这个时代，也就是所谓思想历史上的轴心时代</strong>，人类其后的所有思想，从根本上，从来没有超越那个时代。</p>
</li>
<li><p>中国的崛起也离不开这如游牧对农耕的征服游戏。当中国制造、中国因数在全球涌动时，不过是市场经济自身演化法则的现实演示而已。</p>
</li>
<li><p>只要中国继续保持这种被汉奸主子称为野蛮的经济铁蹄的快速奔驰，成吉思汗席卷天下的一幕就会在经济领域再次上演。汉奸们叫床不爽，要怪，就怪那所谓的无形的手如此地辣手摧草。</p>
</li>
<li><p>所谓无形的手，实质就是人类欲望的肆意扩展进行有计划的调控。</p>
</li>
<li><p>游牧军事的强悍就在于，内在的欲望超越了现有的条件，在内部不能消耗这种能力，因此只能向外扩张去消耗，就如同那荷尔蒙所萌动的春情在夜色中无可阻挡地挥霍。</p>
</li>
<li><p>市场经济无形的手制造的动力，如同人的欲望制造的性能力。按道家的玩法，第一种方法就是肆意欲望，采阳补阴，广采面首而成就之。第二种就是控制转化欲望，采自身的大药而成就之。中国经济欲望萌动的性能量，如何通过内在的修炼而成就，就是第二种方法需要解决的问题。</p>
</li>
<li><p>中国今后的发展，最有现实意义的无非是两条路子： </p>
</li>
</ol>
<p>一、让贪婪去扩张疆土，以前是殖民，现在是经济的占领。社会意识形态层面，让市场经济的逻辑无情地贯彻下去，让所有人的欲望无限地扩展，让整个国家的经济、政治、文化、军事等等按照市场经济的逻辑继续扩展下去，成为一个战车，捣毁一切阻隔，在最后的大决战中成就霸主地位。</p>
<p>二、把人民币升值的压力转化为让中国最贫穷的1亿家庭成为10万元户。 </p>
<hr>
<br>

<p>全是先下手为强的强势文化。</p>
<p>好文章！把成吉思汗的蒙古王国力量也联系起来！一直到资本主义奴役和主导！就是和天狗不谋而合的么！除了这么透彻、这么能够串联本质，还有最关键的，长期价值思维，价值对比，经济与文化的力量远远大于军事！人与人，国与国，企业与企业，文化与文化，哪一样不是如此！</p>
<p>市场经济，资本主义，那么多名词，所指不过一样！资本主义，的原始阶段，就是资本野蛮的积累，奴隶交易、战争横财、贵族遗产，到白手起家的商业帝国，垄断帝国，在加上一些初始的国家垄断，都不过是没有规则的混沌时期里掠财的方式。虽然都在要钱，但是原始阶段的游戏规则又不一样了。彼时是斯大林，白色红色恐怖，现在多了人道主义的限制，做法有些收敛，不过从压迫转为金融上。</p>
<p>为什么人道主义上，不加上资本和权力的公平。利益集团的撼动太难了。科技革命让商业化有了更大的规模和空间，才足以让专制破产。=&gt;我们找到很多可能的思路和理由，但是没有框架。那么现在，去中心化的革命，更大程度的自由度和解放，才能够让资本这个庞然大物崩塌。=&gt;我们找到很多可能的思路和理由，都没有数学更清晰和确定。所以无法确定。谁能够量化呢。巴菲特只不过也只是在他已经熟悉的领域，并且能够接触到接近真实的情况，在其中去找确定性，找价值而已。</p>
<p>但是你说管理层自身，比如张总，他对业务的发展前景有多少理解呢。没有宏观上对这个行业的理解。不过也是在自己的圈子里找自己的立足。没有战略。因为很难。几乎无法量化。也需要理解力。</p>
<p>我们都没有超越。差劲和量上堆积的书，只是让思考的时间固定在那个维度和层级上。而更高质量，战略性和大局的思考力，还是缺乏的。一方面我们不够现实，一方面我们太缺乏创造和大局视野。悬浮着。逃不过是个创造的奴隶。</p>
<p>我说的不是高优先级。完全不与我的现实有任何启发。我想磨镜也是。文昭是兴趣，其实也是现实。我知道我更应该做什么。</p>
<p><br><br></p>
<p>25、29课：</p>
<p>总结：首先，中枢、盘整、趋势、背驰概念是确定的。再次基础上谈论转折。转折是趋势或盘整的转换，也需要考虑其级别属性。通俗地说背驰后的反弹有多种，我们从反弹的分类中发现转折。</p>
<p>根据反弹的力度，分为最后中枢的扩展 – 没有突破最后一个中枢的最低点、更大级别的盘整 、更大级别的反趋势。</p>
<br>

<ol>
<li>在某级别的盘整中，或者说<strong>围绕某级别中枢的震荡、延续中(盘整里面)，不存在转折的问题</strong>，除非站在次级别图形中，才有转折问题的探讨。</li>
<li>对于上涨的转折，有两种情况：下跌与盘整；对于下跌的转折，也有两种情况：上涨与盘整。</li>
<li>转折是有级别的，关于<strong>转折与背驰</strong>的关系，有如下定理：缠中说禅背驰-转折定理：<strong>某级别趋势的背驰</strong>将导致该趋势<strong>最后一个中枢的级别扩展、该级别更大级别的盘整或该级别以上级别的反趋势</strong>。例如，一个<strong>5分钟背驰段的下跌</strong>，最终通过<strong>1分钟以及1分钟以下级别的精确定位，最终可以找到背驰的精确点，其后就发生反弹</strong>。</li>
<li>反弹会有一个很明确的界定，就是包括三种情况：<strong>一、该趋势最后一个中枢的级别扩展、二、该级别更大级别的盘整、三该级别以上级别的反趋势</strong>。</li>
<li>一、该趋势最后一个中枢的级别扩展</li>
<li>只触及最后一个中枢的DD=min(dn)的反弹，就是背弛后最弱的反弹，这种反弹，将把最后一个中枢变成一个级别上的扩展，例如，把5分钟的中枢扩展成30分钟甚至更大的中枢。</li>
<li>第一类买点是绝对安全的，即使是这样一种最低级别的反弹，也有足够的空间让买入获利</li>
<li>很特殊的情况，不幸碰到这种情况，在资金利用率的要求下，当然是要找机会马上退出，否则就会浪费时间了。</li>
<li>注意，这种情况和盘整背驰中转化成第三类卖点的情况不同，那种情况下，反弹的级别一定比最后一个中枢低，而这种情况，反弹的级别一定等于或大于最后一个中枢的。因此，这两种情况，不难区分。</li>
<li>二、该级别更大级别的盘整</li>
<li>三、该级别以上级别的反趋势。</li>
<li>这二种情况就是发生转折的两种情况，原理是一样的，只是相应的力度有区别。当反弹至少要重新触及最后一个中枢，这样，将发生转折，也就是出现盘整与上涨两种情况，</li>
<li>对于上面5分钟下跌的例子，就意味着，将出现5分钟级别更大的盘整(背驰扩大了盘整，因为为更大)或5分钟级别以上的上涨，两段走势类型的连接，就有两种情况出现：下跌+盘整，或者下跌+上涨。</li>
<li>注意，这里的盘整的中枢级别(30min)一定大于下跌中的中枢级别(5min)，<strong>否则就和下跌的延伸或第一种该趋势最后一个中枢的级别扩展搞混了</strong>。而上涨的中枢，不一定大于上跌中的中枢，例如，一个5分钟级别的下跌后反过来是一个5分钟级别的上涨，这是很正常的，但如果是盘整，那就至少是30分钟级别的。</li>
<li>有人总是搞不明白为什么“下跌+盘整”中盘整的中枢级别一定大于下跌中的中枢，这里不妨用一个例子说明一下：例如，还是一个5分钟的下跌，那至少有两个中枢，整个下跌，最一般的情况就是a+A+b+B+c，其中的a\b\c，其级别最多就是1分钟级别的，甚至最极端的情况，可以就是一个缺口。而A、B，由于是5分钟级别的中枢，那至少由3段1分钟的走势类型构成，如果都按1分钟级别的走势类型来计量，而且不妨假设a\b\c都是1分钟的走势类型，那么a+A+b+B+c就有9个1分钟的走势类型。</li>
<li>而一个30分钟的盘整，至少有3个5分钟的走势类型，而1个5分钟的走势类型，至少有3个1分钟的走势类型，也就是一个30分钟的盘整，就至少有9个1分钟的走势类型，这和上面a+A+b+B+c的数量是一致的。从这数量平衡的角度，就知道为什么“下跌+盘整”中盘整的级别一定比下跌的级别大了，如果级别一样，例如一个5分钟的盘整，只有3个1分钟的走势类型，那和9就差远了，也不匹配。</li>
<li>当然，“下跌+盘整”中盘整的级别一定比下跌的级别大，最主要的原因还不是这个，而是上面说到的，如果该级别一样，那只有两种情况，下跌延伸或下跌最后一个中枢扩展，和“下跌+盘整”是不搭界的</li>
<li>有人可能还有疑问，如果下跌最后一个中枢扩展，例如5分钟扩展成30分钟，那和5分钟级别下跌+30分钟级别盘整有什么区别？这区别大了，因为在“5分钟级别下跌+30分钟级别盘整”，也就是“下跌+盘整”中，下跌和盘整都是完成的走势类型，这意味着是两个走势类型的连接。而下跌最后一个中枢扩展，是一个未完成的走势类型的延续，还在一个走势类型里</li>
<li>以上三种情况，就完全分类了某级别背驰后的级别与力度，也就是某级别的第一类买点后将发生怎么样的情况，而第一类卖点的情况是一样的，只是方向相反</li>
<li>那么，怎么分别这几种情况，关键就是看反弹中第1个前趋势最后一个中枢级别的次级别走势（例如前面的下跌是5分钟级别，就看1分钟级别的第1次反弹），是否重新回抽最后一个中枢里，如果不能，那第一种情况的可能就很大了，而且也证明反弹的力度值得怀疑，当然这种判别不是绝对的，但有效性很大。</li>
<li>这次20070206的反弹，用5分钟背驰段，然后考察1分钟以及1分钟以下级别的背驰进行精确定位，可以极为精确地把握这个底部，而且在实践中，很多人按照本ID的理论都把握住了，那么，其后的反弹，第一波是1分钟走势马上回到从2980开始的5分钟下跌的最后一个中枢里，这样就意味着第一种最弱的情况可能性可以完全排除了，其后，1分钟的走势继续完成，扩展成一个5分钟的上涨</li>
<li>在20070207的11点前后，一个1分钟的背驰制造了上涨的结束，其后进入一个中枢的震荡中，这个中枢，按照本章的定理，就可以断言，至少是5分钟级别的，而实际上演化成一个30分钟级别的，这意味着，一个快速的5分钟上涨的可能就没有了，后面只有两种演化的可能，就是一个30分钟以上级别的盘整，或者是一个30分钟以上的上涨，至于哪种情况，就必须看后面走势的演化。</li>
<li>而对于实际的操作，这两种情况并没有多大的区别，例如是盘整还是上涨，关键看突破第一个中枢后是否形成第三类买点，而<span style="color:red">操作中，是在第一、二类买点先买了，然后观察第三类买点是否出现，出现就继续持有，否则就可以抛出</span></li>
<li>如果是资金量特别小，或者对本ID的理论达到小学毕业水平，那么完全可以在<strong>突破的次级别走势背驰时先出掉</strong>，<strong>然后看回试是否形成第三类买点</strong>，形成就回补，不形成就不回补，就这么简单。</li>
<li>在第一次抄底时，最好就是买那些当下位置离最后一个中枢的DD=min(dn)幅度最大的，所谓的超跌，应该以此为标准。</li>
<li>因为本章的定理保证了，反弹一定达到DD=min(dn)之上，然后在反弹的第1波次级别背驰后出掉，如果这个位置还不能达到最后一个中枢，那么这个股票可以基本不考虑，当然，这可能有例外，但可能性很小。</li>
<li>然后在<strong>反弹的第一次次级别回试后买入那些反弹能达到最后一个中枢的股票而且最好是突破该中枢的而且回试后能站稳的</strong>，根据走势必完美，一定还有一个次级别的向上走势类型，如果这走势类型出现盘整背驰，那就要出掉，如果不出现，那就要恭喜你了，你买到了一个所谓V型反转的股票，其后的力度当然不会小。</li>
</ol>
<p><br><br></p>
<p>26、30课：</p>
<p>市场价格是否完全反映所有信息，可以随意假定，无论何种假定，都和实际的交易关系不大。交易中，你唯一需要明确的，就是无论市场价格是否完全反映信息，你都必须以市场的价格交易，而你的交易将构成市场的价格，对于交易来说，除了价格，一无所有（成交量可以看成是在一个最低的时间段内按该价格重复成交了成交数量个交易单位）。这一切，和市场价格是否反映所有信息毫无关系，因为所有价格都是当下的，如果当下的信息没被市场反映，那他就是没被市场当下反映的信息，至于会不会被另一个时间的价格反映是另外的事情。站在纯交易的角度，价格只有当下，当下只有价格，除了价格与依据时间延伸出来的走势，市场的任何其他东西都是可以忽略不计的。</p>
<p>价格也和人是否理智无关，无论你是否理智，都以价格交易，而交易也被价格，这是无论任何理论都必须接受的事实：交易，只反映为价格，以某种价格某个时间的交易，这就是交易的全部。至于交易后面的任何因数，如果假定其中一种或几种决定了交易的价格，无论这种因数是基本面、心理面、技术面、政治面还是什么，都是典型的上帝式思维，都是无聊勾当。其实，对于价格来说，时间并不需要特别指出，因为价格轨迹中的前后，就意味着时间的因数，也就是说，交易是可以按时间排序的，这就是交易另一个最大的特征：交易是有时间性的，而这时间，不可逆。在物理还在探讨时间是否可逆时，对于交易空间的探讨，这最困难的时间问题，就已经有了最不可动摇的答案。而本ID的理论，当然也是以这交易时间的不可逆为前提，如果今天的交易可以变成昨天的或者干脆不算了，那本ID的理论马上土崩瓦解。</p>
<p>交易，当然是有规律的，而且这规律是万古不变的，归纳上述就是：交易以时间的不可逆为前提完全等价地反映在价格轨迹上。当然，这万古不变也有其可变之处，例如交易突然因为某种原因可以随便更改，因此，在逻辑上更严谨的说法就是，把满足该条规律的市场称为价格充分有效市场，本ID的理论，就是针对这种价格充分有效市场的，而这种市场，至少对应了目前世界上所有正式的交易市场。那么，非价格充分有效市场是否存在？当然有。例如，你昨天一亿元钱买了一石头，今天卖石头的黑帮老大拿着枪顶着你说昨天的交易不算了，钱不给了，石头也收走了，这种存在类似交易的市场当然不可能是价格充分有效的。</p>
<p>以前所有市场理论的误区都在于去探讨决定价格的交易后面的因数，交易是人类的行为，没什么可探讨的，人类就像疯子一样，其行为即使可探讨，在交易层面也变得没什么可探讨的。所有企图解释交易动机、行为的理论都是没有交易价值的，不管人类的交易有什么理由，只要交易就产生价格，就有价格的轨迹，这就足够了。站在纯交易的角度，唯一值得数学化探讨的就是这轨迹，其他的研究都是误区，对交易毫无意义。</p>
<p>那么，价格是随机的吗？这又是一个上帝式的臆测。决定论和随机论，其背后的基础都是一个永恒因数论，一个永恒模式论，也就是，价格行为被某种神秘的理论所永恒模式化。无论这种模式是决定还是随机，这种假设的荒谬性是一样。交易，只来自现实，因此，价格是被现实的交易所决定的，相应，上面的顾虑就可以扩充为：交易是现实的行为，交易以时间的不可逆为前提完全等价地反映在价格轨迹上。</p>
<p>交易的现实性是交易唯一可以依赖的基础，那么交易的现实性反映了什么，有什么可能的现实推论？首先，人的反应是需要时间的，就算是脑神经的传输，也是需要时间的；其次，社会结构的现实多层性以及个体的差异性决定了，任何的群体性交易都不具有同时性，也就是说，即使是相同原因造成的相同买卖，都不可能同时出现，必然有先后，也就是说，交易具有延异性，不会完全地趋同，这是交易能形成可分析走势的现实基础。</p>
<p>由于交易具有延异性，没有绝对的同一性，那么即使对于严格一种因数决定交易行为的系统，也依然能产生可分析的价格轨迹。任何群体性的交易行为，不会出现完全的价格同一性，也就是说，不会永远出现所有人同一时刻的同一交易。而一个完全绝对趋同交易，就等价于一个赌博，所有的买卖和买大小没任何区别，这样的系统是否存在？当然，例如一个庄家百分百把所有股票都吃了，而且任何一笔的交易都只有他一个人参与，没有任何别的人参与，这时候，其走势等价于一个买大小的赌博。而只要有人买入或还持有这股票的1股，那么这个交易就可以用本ID的理论来描述，因为，一个不完全绝对趋同的交易就产生了，本ID理论的另一个界限就在此。</p>
<p>本ID的理论只有这两个界限，只要是价格充分有效市场里的非完全绝对趋同交易，那本ID的理论就永远绝对有效，这种绝对性就如同压缩影射不动点的唯一性对完备的距离空间一样。至于有多少人学习，应用这个理论，对理论本身并没有任何实质的影响，因为，即使所有人都应用本ID的理论，由于社会结构以及个体差异，依然不会造成一个完全绝对趋同交易，这样，本ID的理论依然有效。而更重要的是，本ID的理论，并不是一个僵化的操作，都是永远建立在当下之上的。例如，一个日线级别被判断进入背弛段，由于某种当下的绝对突发事件，例如突然有人无意按错键又给日本捎去一千几百颗原子弹，使得小级别产生突发性结构破裂最终影响到大级别的结构，这时候，整个的判断，就建立在一个新的走势基础上了，而往往这时，实际的交易并没有发生，除非你运气忒好，你刚按买入，那原子弹就飞起来了。一般人，总习惯于一种目的性思维，往往忽视了走势是当下构成中的，而本ID的理论判断，同样是建筑在当下构成的判断中，这是本ID理论又一个关键的特征。关于这种理论的当下性，在以后的课程中会重点介绍，按学历，这是初中的课程。</p>
<p>而本ID的理论，最终比的是人本身，就像乾坤大挪移的第八重肯定打不过第九重的，但任何非乾坤大挪移的，肯定打不过第八重一样，有一种武功是高出其它孤峰而上的，因为起点已经大大超越了，其他那些起点就错了，又怎么能比？显然，不可能所有人都相信应用本ID的理论，因此，那些不用本理论的人，就成了本ID理论吸血的对象，现实中，这种对象不是太少，而是太多了。其次，如果有庄家、基金偷学了这种方法，这就等于乾坤大挪移比第几重了，而且对于大资金来说，至少要比散户高出两重，才可能和散户打个平手，因为资金大，没有更高的功力，怎么能挪移起来？更重要的是，级别越大，企图控制干扰所需要的能量越大，对于周线级别以后，基本就没人能完全控制了，如果真是出现个个庄家、基金争学本ID理论的情况，那么除了在小级别比功力外，功力浅的完全可以把操作级别提高来加强安全性。更重要的是，应用相同的理论，在现实中也不会有相同的结果，现实就是一个典型的非完全绝对趋同系统，就像同样的核理论，并不会导致德国和美国同时造出原子弹，同样的理论，在不同的资金规模、资金管理水平，选股策略、基本面把握、交易者性格、气质等情况下，自然地呈现不同的面貌，这就保证了同一理论交易的非完全绝对趋同。</p>
<p>对本ID的理论有一点是必须明确的，就是本ID的理论是对价格充分有效市场非完全绝对趋同交易的一个完全的数学公理化理论，唯一需要监控的就是价格充分有效市场与非完全绝对趋同交易这两个前提是否还存在，更重要的是，这归根结底是一套关系人的理论，只能不断在交易中修炼，最后比的可是功力。例如，就算是背驰这么简单的事情，就算是同一种方法，当成为群体性行为时，比的就是心态与功力，心态不好、出手早或出手迟的，就会在价格上留下痕迹，甚至当趋同性较强时，会使得级别的延伸不断出现，那就让功力深的人得到一个更好的买入或卖出价格，这些细微的差别积累下来，足以使得赢利水平天差地别。这也是为什么本ID可以把理论公开的一个深层原因，因为本ID的理论是对价格充分有效市场非完全绝对趋同交易的一个客观理论，即使公开了，也不会让这理论有任何改变，就像牛顿力学不会让万有引力改变一样，美国的原子弹爆炸了不会影响中国的原子弹按照同样的理论出现一样。至于理论可能造成的趋同交易加大，也早在本ID理论的计算中，这里比的是当下的功力。</p>
<p>无论你用什么交易方法，只要是在价格充分有效市场非完全绝对趋同交易里，你就在本ID理论的计算中；而要在本ID的理论里功力日增，就首先要成为一个顶天立地的人，这也是本ID让各位多看本ID所解释论语的原因。交易，不过是人类行为的一种，要成为成功的交易者，首先要对人类的行为穷其源，得其智慧，否则，一个糊涂蛋，什么理论都是白搭。本ID理论的基础部分，只是把现实的真相解剖出来，但这远远不够，看明白与行得通，那是两回事情。当然，看都看不明白，是不可能真的行得通的。而行，就是修行，“见、闻、学、行”，缺一不可。本ID的理论如同大道，不需要私藏着，都可以学、都可以行，但能否行到不退转的位置，是否最终还是“学如不及，犹恐失之”，那就要靠每个人自身的修行了。</p>
<p>理论，只是把现实解剖，但真正的功力，都在当下，不光要用理论的眼睛看清楚现实，更要逐步让自己和走势合一。而行的初步功力是什么？归根结底就是“恰好”，这个“恰好”是动态的，无论多少人，每个人的行为当成一个向量，所有人的行为最终构成走势的向量，而所谓的“恰好”，就是这个总向量本身。而如何才能永远和这总向量一致？就要首先把自己变成一个零向量，有也只有当一个零向量加入到任何一个向量叠加系统里，才不会影响到最终的总向量的。把自己的贪婪与恐惧去掉，让市场的走势如同自己的呼吸一般，看走势如同看自己的呼吸，慢慢就可以下单如有神了，你的交易，就是顺着市场的总向量的方向增加其力度而已，这才是真正的顺势而为。只有这样，才算初步入门，才能逐步摆脱被走势所转的可悲境地，才能让自己和走势合一，和那永远变动的总向量一致而行。至于走势分析的学习，只不过是门外的热身而已。</p>
<p>有人可能要追问，如果所有人都变成零向量，那又如何？交易市场存在的基础，就是人的贪婪与恐惧，如果所有参与交易市场的人都没有贪婪与恐惧，那市场就没了，资本主义就没了，货币就被消灭了，那时候，本ID的理论自然就不存在了。只有对这个以人的贪婪、恐惧为基础的市场进行“不相”之，才能长期有效地吸取这市场的血。本ID理论的基础部分，在人类历史上第一次把交易市场建筑在严密的公理化体系上，就是要把市场的本来面目还原，让人的贪婪、恐惧无所遁形，只有明确地知道市场当下的行为，才可能逐步化解贪婪与恐惧，把交易行为建筑在一个坚实的现实基础上，而不是贪婪、恐惧所引发的臆测上。只有智慧才可以战胜贪婪、恐惧，而当所有的贪婪与恐惧被战胜后，贪婪与恐惧所物化的资本主义社会本身，也就丧钟敲响了。</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>读论语</title>
    <url>/2021/05/25/%E8%AF%BB%E8%AE%BA%E8%AF%AD/</url>
    <content><![CDATA[<br>

<br>



<h4 id="自己的理解"><a href="#自己的理解" class="headerlink" title="自己的理解"></a>自己的理解</h4><span id="more"></span>

<p>投资的用处相对大些。</p>
<p>有关于人性的智慧。</p>
<blockquote>
<p>子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？</p>
</blockquote>
<p>之指代道，圣人之道。时有按时、当时、时常、时运、时机等解。而字，你、你的、须臾、像、并列、假设、词缀与语气等解。这里当取并列之意。</p>
<p>《说文系传统论》写道，“悦，犹说也，拭也，解脱也。若人心有郁结能解释之也”。</p>
<p>学之、时习之，即闻道、见道，并且，按时付之于行动、时常付之于行动，成习惯。如同天下遇知己。都是解脱郁结的事情。正如，“天行健，君子以自强不息”，“君子敏于行”。</p>
<p>人，有人民、别人、人才之解。这里当指所有人。知，有了解、识别、交情、感觉、主持、智慧、有学识等解。而，如同第一句，取并列之意，而不取转折。愠为，内心燥热，即心不平静、情绪郁结或起伏。</p>
<p>即人们，不了解天地之道、不知道圣人之道，且内心无所执、没什么郁结，也是个君子世界。</p>
<p>整句话是说，要么学而时习，要么不知不愠，都可以让世界更朝着君子世界发展。不知不愠也是君子境界。君子不缺朋友，四海皆有友朋。</p>
<blockquote>
<p>有子曰:“其为人也孝弟而好犯上者，鲜矣；不好犯上而好作乱者，未之有也。君子务本，本立而道生。孝弟也者，其为仁之本与!”</p>
</blockquote>
<blockquote>
<p>子曰:“巧言令色，鲜矣仁！”</p>
</blockquote>
<p>讷于言的训诫。巧、令都为擅长之意。重视“纸上谈兵”(说话)，“涂脂抹粉”(粉饰)，只有空壳，这样的人，大概率不是君子。</p>
<blockquote>
<p>曾子曰“吾日三省吾身:为人谋而不忠乎?与朋友交而不信乎?传不习乎?”</p>
</blockquote>
<p>具体所复盘的内容。因人而异，因时而异。</p>
<p>彼时重视忠信与习道。当下，几乎是没有的。背信弃义而谋得利益，并且狐朋狗友夜夜笙歌，也是大把人在。但终究无法有厚度。所谓富而不仁，所见大多如此。看人心之所向。所向财富，利益，则复盘的，就都和谋得的手段相关。</p>
<p>当下的世界，并非君子世界。也无法和而不同。当下德草，随“时”风而偃，不随君子德风。不尚德的世界，就是齐。还没到鲁。因为现在有法律，维护私人财产，维护物质，而无约束德行。</p>
<p>子曰:“道千乘之国,敬事而信，节用而爱人，使民以时。”</p>
<p>子曰：“弟子入则孝，出则弟，谨而信，泛爱众，而亲仁。行有余力，则以学文。”</p>
<p>子夏曰：“贤贤易色；事父母，能竭其力；事君，能致其身；与朋友交，言而有信。虽曰未学，吾必谓之学矣。”</p>
<p>子曰：“君子不重则不威，学则不固。主忠信，无友不如己者，过则勿惮改。”<br>曾子曰：“慎终追远，民德归厚矣。”<br>子禽问于子贡曰：“夫子至于是邦也，必闻其政，求之与，抑与之与？”子贡曰：“夫子温、良、恭、俭、让以得之。夫子之求之也，其诸异乎人之求之与？”</p>
<blockquote>
<p>子曰：“父在，观其志。父没，观其行；三年无改于父之道，可谓孝矣。”</p>
</blockquote>
<p>不讲孝。是情。对人有情，则在乎她的志趣，在乎她的行事，并尽量应和。</p>
<p>有子曰：“礼之用，和为贵。先王之道，斯为美，小大由之。有所不行，知和而和，不以礼节之，亦不可行也。”<br>有子曰：“信近于义，言可复也。恭近于礼，远耻辱也。因不失其亲，亦可宗也。”</p>
<blockquote>
<p>子曰：“君子食无求饱，居无求安，敏于事而慎于言，就有道而正焉。可谓好学也已。”</p>
</blockquote>
<p>君子，即无郁结、不执之人。有食有居即可，不求更好。而对于言行，却有要求，”精神洁癖“。要求自己更加敏捷、谨慎，对照着道，”拨乱反正“。当下也一样，好学的人，工作狂，必然对外在的种种不予重视。</p>
<p>所思所念，皆在于道。或者在于知识、科学、客观世界、真理。而也因此不会对旁人有所侵犯，也不会纠结于得到别人的利益。</p>
<p>但是资本主义就是得到别人的利益。副产物是竞争带来的技术进步。</p>
<blockquote>
<p>子贡曰：“贫而无谄，富而无骄，何如？”子曰：“可也。未若贫而乐，富而好礼者也。”子贡曰：“《诗》云：‘如切如磋，如琢如磨’，其斯之谓与？”子曰：“赐也，始可与言《诗》已矣，告诸往而知来者。”</p>
</blockquote>
<p>不被”困“，不被贫富所控。不如即使窘困而乐在其中，即使富足而能有所节制。能够出离自己的当下，而保持君子之道。固守、时习。</p>
<p>切磋琢磨，一个典故就是贾岛的推敲。若能琢磨，则必不会谄骄。而越加琢磨，反而会寻得其乐。告往知来，即为联系。赐能够看到琢磨为”不困“的本质。</p>
<blockquote>
<p>子曰：“为政以德，譬如北辰，居其所而众星共之。”</p>
</blockquote>
<p>这个德不是虚幻的。不同位置，不同处境的人，有不同的德的标准。甚至每个人都不同。关键不在于德是什么。</p>
<p>当然有最基本的。即贫富的”不困“。贫富，不止物质上。任何有对比和差距的方面，都要”不困“。在于固守与琢磨，才能真的不受困。</p>
<blockquote>
<p>子曰：“《诗》三百，一言以蔽之，曰：‘思无邪’。”</p>
</blockquote>
<p>当下都是邪。现实都是邪。要求无邪，几乎是天堂。天堂一定是淳朴挚真的。当下能无邪么。</p>
<p>对弱势的人，真的要让利的话，也是可笑。</p>
<p>子曰：“道之以政，齐之以刑，民免而无耻。道之以德，齐之以礼，有耻且格。”</p>
<p>子曰：“吾十有五而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲，不逾矩。”</p>
<p>孟懿子问孝，子曰：“无违。”樊迟御，子告之曰：“孟孙问孝于我，我对曰‘无违’。”樊迟曰：“何谓也？”子曰：“生，事之以礼；死，葬之以礼，祭之以礼。”</p>
<p>孟武伯问孝。子曰：“父母唯其疾之忧。”</p>
<p>子游问孝，子曰：“今之孝者，是谓能养。至于犬马，皆能有养。不敬，何以别乎？”</p>
<p>子夏问孝。子曰：“色难。有事，弟子服其劳；有酒食，先生馔，曾是以为孝乎？”</p>
<p>子曰：“吾与回言终日，不违，如愚。退而省其私，亦足以发，回也不愚。”<br>子曰：“视其所以，观其所由，察其所安，人焉廋哉？人焉廋哉？”</p>
<blockquote>
<p>子曰：“温故而知新，可以为师矣。”</p>
</blockquote>
<p>为人师的，不温故不行，不知新不行。上级也一样。</p>
<p>当然老板，又是另一路径。</p>
<p>子曰：“君子不器。”<br>子贡问君子。子曰：“先行其言而后从之。”<br>子曰：“君子周而不比，小人比而不周。”<br>子曰：“学而不思则罔，思而不学则殆。”<br>子曰：“攻乎异端，斯害也已！”<br>子曰：“由，诲汝，知之乎！知之为知之，不知为不知，是知也。”<br>子张学干禄。子曰：“多闻阙疑，慎言其余，则寡尤；多见阙殆，慎行其余，则寡悔。言寡尤，行寡悔，禄在其中矣。”<br>哀公问曰：“何为则民服？”孔子对曰：“举直错诸枉，则民服；举枉错诸直，则民不服。”<br>季康子问：“使民敬、忠以劝，如之何？”子曰：“临之以庄，则敬；孝慈，则忠；举善而教不能，则劝。”<br>或谓孔子曰：“子奚不为政？”子曰：“《书》云：‘孝乎惟孝，友于兄弟，施于有政。’是亦为政，奚其为为政？”<br>子曰：“人而无信，不知其可也。大车无輗，小车无軏，其何以行之哉？”<br>子张问：“十世可知也？”子曰：“殷因于夏礼，所损益，可知也；周因于殷礼，所损益，可知也。其或继周者，虽百世，可知也。”<br>子曰：“非其鬼而祭之，谄也；见义不为，无勇也。”<br>八佾篇<br>孔子谓季氏:“八佾舞于庭，是可忍也，孰不可忍也?”<br>三家者以《雍》彻,子曰：‘相维辟公，天子穆穆’，奚取于三家之堂？”<br>子曰：“人而不仁，如礼何？人而不仁，如乐何？”<br>林放问礼之本,子曰：“大哉问！礼，与其奢也，宁俭；丧，与其易也，宁戚。”<br>子曰：“夷狄之有君，不如诸夏之亡也。”<br>季氏旅于泰山。子谓冉有曰：“女弗能救与？”对曰：“不能。”子曰：“呜呼！曾谓泰山不如林放乎？”<br>子曰：“君子无所争，必也射乎！揖让而升，下而饮。其争也君子。”<br>子夏问曰：“‘巧笑倩兮，美目盼兮，素以为绚兮’何谓也？”子曰：“绘事后素。”曰：“礼后乎？”子曰：“起予者商也，始可与言《诗》已矣。”<br>子曰：“夏礼，吾能言之，杞不足征也；殷礼吾能言之，宋不足征也。文献不足故也，足则吾能征之矣。”<br>子曰：“禘自既灌而往者，吾不欲观之矣。”<br>或问禘之说。子曰：“不知也。知其说者之于天下也，其如示诸斯乎！”指其掌。<br>祭如在，祭神如神在。子曰：“吾不与祭，如不祭。”<br>王孙贾问曰：“‘与其媚于奥，宁媚于灶’，何谓也？”子曰：“不然，获罪于天，无所祷也。”<br>子曰：“周监于二代，郁郁乎文哉！吾从周。”<br>子入太庙，每事问。或曰：“孰谓鄹人之子知礼乎？入太庙，每事问。”子闻之，曰：“是礼也。”<br>子曰：“射不主皮，为力不同科，古之道也。”<br>子贡欲去告朔之饩羊，子曰：“赐也！尔爱其羊，我爱其礼。”<br>子曰：“事君尽礼，人以为谄也。”<br>定公问：“君使臣，臣事君，如之何？”孔子对曰：“君使臣以礼，臣事君以忠。”<br>子曰：“《关雎》，乐而不淫，哀而不伤。”<br>哀公问社于宰我。宰我对曰：“夏后氏以松，殷人以柏，周人以栗，曰：使民战栗。”子闻之，曰：“成事不说，遂事不谏，既往不咎。”<br>子曰：“管仲之器小哉！”或曰：“管仲俭乎？”曰：“管氏有三归，官事不摄，焉得俭？”“然则管仲知礼乎？”曰：“邦君树塞门，管氏亦树塞门；邦君为两君之好，有反坫。管氏亦有反坫，管氏而知礼，孰不知礼？”<br>子语鲁大师乐，曰：“乐其可知也。始作，翕如也；从之，纯如也，皦如也，绎如也，以成。”<br>仪封人请见，曰：“君子之至于斯也，吾未尝不得见也。”从者见之。出曰：“二三子何患于丧乎？天下之无道也久矣，天将以夫子为木铎。”<br>子谓《韶》：“尽美矣，又尽善也。”谓《武》：“尽美矣，未尽善也。”<br>子曰：“居上不宽，为礼不敬，临丧不哀，吾何以观之哉！”<br>里仁篇<br>子曰：“里仁为美。择不处仁，焉得知？”<br>子曰：“不仁者不可以久处约，不可以长处乐。仁者安仁，知者利仁。”<br>子曰：“唯仁者能好人，能恶人。”<br>子曰：“苟志於仁矣，无恶也。”<br>子曰：“富与贵，是人之所欲也；不以其道得之，不处也。贫与贱，是人之所恶也；不以其道得之，不去也。君子去仁，恶乎成名？君子无终食之间违仁，造次必于是，颠沛必于是。”<br>子曰：“我未见好仁者，恶不仁者。好仁者，无以尚之；恶不仁者，其为仁矣，不使不仁者加乎其身。有能一日用其力于仁矣乎？我未见力不足者。盖有之矣，我未见也。”<br>子曰：“人之过也，各于其党。观过，斯知仁矣。”</p>
<blockquote>
<p>子曰：“朝闻道，夕死可矣。”</p>
</blockquote>
<p>闻道，并固守，时习之。</p>
<p>子曰：“士志于道，而耻恶衣恶食者，未足与议也。”<br>子曰：“君子之于天下也，无适也，无莫也，义之与比。”<br>子曰：“君子怀德，小人怀土；君子怀刑，小人怀惠。”<br>子曰：“放于利而行，多怨。”<br>子曰：“能以礼让为国乎？何有？不能以礼让为国，如礼何？”<br>子曰：“不患无位，患所以立。不患莫己知，求为可知也。”<br>子曰：“参乎！吾道一以贯之。”曾子曰：“唯。”子出，门人问曰：“何谓也？”曾子曰：“夫子之道，忠恕而已矣。”<br>子曰：“君子喻于义，小人喻于利。”<br>子曰：“见贤思齐焉，见不贤而内自省也。”<br>子曰：“事父母几谏，见志不从，又敬不违，劳而不怨。”<br>子曰：“父母在，不远游，游必有方。”<br>子曰：“三年无改于父之道，可谓孝矣。<br>子曰：“父母之年，不可不知也。一则以喜，一则以惧。<br>子曰．“古者言之不出，耻躬之不逮也。<br>子曰：“以约失之者鲜矣。</p>
<p>子曰：“德不孤，必有邻。”<br>子游曰：“事君数，斯辱矣；朋友数，斯疏矣。<br>公冶长篇<br>子谓公冶长：“可妻也，虽在缧绁之中，非其罪也！”以其子妻之。<br>子谓南容：“邦有道不废；邦无道免于刑戮。”以其兄之子妻之。<br>子谓子贱：“君子哉若人！鲁无君子者，斯焉取斯？”<br>子贡问曰：“赐也何如？”子曰：“女，器也。”曰：“何器也？”曰：“瑚琏也。”<br>或曰：“雍也仁而不佞。”子曰：“焉用佞？御人以口给，屡憎于人。不知其仁，焉用佞？”<br>子使漆雕开仕，对曰：“吾斯之未能信。”子说。<br>子曰：“道不行，乘桴浮于海，从我者其由与？”子路闻之喜，子曰：“由也好勇过我，无所取材。”<br>孟武伯问：“子路仁乎？”子曰：“不知也。”又问，子曰：“由也，千乘之国，可使治其赋也，不知其仁也。”“求也何如？”子曰：“求也，千室之邑、百乘之家，可使为之宰也，不知其仁也。”“赤也何如？”子曰：“赤也，束带立于朝，可使与宾客言也，不知其仁也。”<br>子谓子贡曰：“女与回也孰愈？”对曰：“赐也何敢望回？回也闻一以知十，赐也闻一以知二。”子曰：“弗如也，吾与女弗如也！”<br>宰予昼寝，子曰：“朽木不可雕也，粪土之墙不可杇也，于予与何诛？”子曰：“始吾于人也，听其言而信其行；今吾于人也，听其言而观其行。于予与改是。”<br>子曰：“吾未见刚者。”或对曰：“申枨。”子曰：“枨也欲，焉得刚。”<br>子贡曰：“我不欲人之加诸我也，吾亦欲无加诸人。”子曰：“赐也，非尔所及也。”<br>子贡曰：“夫子之文章，可得而闻也；夫子之言性与天道，不可得而闻也。”<br>子路有闻，未之能行，唯恐有闻。<br>子贡问曰：“孔文子何以谓之‘文’也？”子曰：“敏而好学，不耻下问，是以谓之‘文’也。”<br>子谓子产：“有君子之道四焉：其行己也恭，其事上也敬，其养民也惠，其使民也义。”<br>子曰：“晏平仲善与人交，久而敬之。”<br>子曰：“臧文仲居蔡，山节藻棁，何如其知也？”<br>子张问曰：“令尹子文三仕为令尹，无喜色，三已之无愠色，旧令尹之政必以告新令尹，何如？”子曰：“忠矣。”曰：“仁矣乎？”曰：“未知，焉得仁？”“崔子弑齐君，陈文子有马十乘，弃而违之。至于他邦，则曰：‘犹吾大夫崔子也。’违之。之一邦，则又曰：‘犹吾大夫崔子也。’违之，何如？”子曰：“清矣。”曰：“仁矣乎？”曰：“未知，焉得仁？”<br>季文子三思而后行，子闻之曰：“再斯可矣。”<br>子曰：“宁武子，邦有道，则知；邦无道，则愚。其知可及也，其愚不可及也。”<br>子在陈，曰：“归与！归与！吾党之小子狂简，斐然成章，不知所以裁之。”<br>子曰：“伯夷、叔齐不念旧恶，怨是用希。”<br>子曰：“孰谓微生高直？或乞醯焉，乞诸其邻而与之。”<br>子曰：“巧言、令色、足恭，左丘明耻之，丘亦耻之。匿怨而友其人，左丘明耻之，丘亦耻之。”<br>颜渊、季路侍，子曰：“盍各言尔志？”子路曰：“愿车马、衣轻裘与朋友共，敝之而无憾。”颜渊曰：“愿无伐善，无施劳。”子路曰：“愿闻子之志。”子曰：“老者安之，朋友信之，少者怀之。”<br>子曰：“已矣乎！吾未见能见其过而内自讼者也。”<br>子曰：“十室之邑，必有忠信如丘者焉，不如丘之好学也。”<br>雍也篇<br>子曰：“雍也可使南面。”<br>仲弓问子桑伯子，子曰：“可也简。”仲弓曰：“居敬而行简，以临其民，不亦可乎？居简而行简，无乃大简乎？”子曰：“雍之言然。”<br>哀公问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不迁怒，不贰过，不幸短命死矣，今也则亡，未闻好学者也。”<br>子华使于齐，冉子为其母请粟，子曰：“与之釜。”请益，曰：“与之庾。”冉子与之粟五秉。子曰：“赤之适齐也，乘肥马，衣轻裘。吾闻之也，君子周急不继富。”<br>原思为之宰，与之粟九百，辞。子曰：“毋以与尔邻里乡党乎！”<br>子谓仲弓曰：“犁牛之子骍且角，虽欲勿用，山川其舍诸？”<br>子曰：“回也，其心三月不违仁，其余则日月至焉而已矣。”<br>季康子问：“仲由可使从政也与？”子曰：“由也果，于从政乎何有？”曰：“赐也可使从政也与？”曰：“赐也达，于从政乎何有？”曰：“求也可使从政也与？”曰：“求也艺，于从政乎何有？”</p>
<p>季氏使闵子骞为费宰，闵子骞曰：“善为我辞焉。如有复我者，则吾必在汶上矣。”</p>
<p>伯牛有疾，子问之，自牖执其手，曰：“亡之，命矣夫！斯人也而有斯疾也！斯人也而有斯疾也！”<br>子曰：“贤哉回也！一箪食，一瓢饮，在陋巷，人不堪其忧，回也不改其乐。贤哉，回也！”<br>冉求曰：“非不说子之道，力不足也。”子曰：“力不足者，中道而废，今女画。”<br>子谓子夏曰：“女为君子儒，毋为小人儒。”<br>子游为武城宰，子曰：“女得人焉尔乎？”曰：“有澹台灭明者，行不由径，非公事，未尝至于偃之室也。”<br>子曰：“孟之反不伐，奔而殿，将入门，策其马曰：‘非敢后也，马不进也。’”<br>子曰：“不有祝鮀之佞，而有宋朝之美，难乎免于今之世矣。”<br>子曰：“谁能出不由户？何莫由斯道也？”<br>子曰：“质胜文则野，文胜质则史。文质彬彬，然后君子。”<br>子曰：“人之生也直，罔之生也幸而免。”<br>子曰：“知之者不如好之者；好之者不如乐之者。”<br>子曰：“中人以上，可以语上也；中人以下，不可以语上也。”<br>樊迟问知，子曰：“务民之义，敬鬼神而远之，可谓知矣。”问仁，曰：“仁者先难而后获，可谓仁矣。”<br>子曰：“知者乐水，仁者乐山。知者动，仁者静。知者乐，仁者寿。”<br>子曰：“齐一变至于鲁，鲁一变至于道。”<br>子曰：“觚不觚，觚哉！觚哉！”<br>宰我问曰：“仁者，虽告之曰：‘井有仁焉。’其从之也？”子曰：“何为其然也？君子可逝也，不可陷也；可欺也，不可罔也。”<br>子曰：“君子博学于文，约之以礼，亦可以弗畔矣夫。”<br>子见南子，子路不说，夫子矢之曰：“予所否者，天厌之！天厌之！”<br>子曰：“中庸之为德也，其至矣乎！民鲜久矣。”<br>子贡曰：“如有博施于民而能济众，何如？可谓仁乎？”子曰：“何事于仁，必也圣乎！尧、舜其犹病诸！夫仁者，己欲立而立人，己欲达而达人。能近取譬，可谓仁之方也已。”<br>述而篇<br>子曰：“述而不作，信而好古，窃比于我老彭。”<br>子曰：“默而识之，学而不厌，诲人不倦，何有于我哉？”<br>子曰：“德之不修，学之不讲，闻义不能徙，不善不能改，是吾忧也。”<br>子之燕居，申申如也，夭夭如也。<br>子曰：“甚矣，吾衰也！久矣，吾不复梦见周公。”<br>子曰：“志于道，据于德，依于仁，游于艺。”<br>子曰：“自行束脩以上，吾未尝无诲焉。”<br>子曰：“不愤不启，不悱不发，举一隅不以三隅反，则不复也。”<br>子食于有丧者之侧，未尝饱也。<br>子于是日哭，则不歌。<br>子谓颜渊曰：“用之则行，舍之则藏，惟我与尔有是夫！”子路曰：“子行三军，则谁与？”子曰：“暴虎冯河，死而无悔者，吾不与也。必也临事而惧，好谋而成者也。”<br>子曰：“富而可求也，虽执鞭之士，吾亦为之。如不可求，从吾所好。”<br>子之所慎：齐，战，疾。<br>子在齐闻《韶》，三月不知肉味，曰：“不图为乐之至于斯也。”<br>冉有曰：“夫子为卫君乎？”子贡曰：“诺，吾将问之。”入，曰：“伯夷、叔齐何人也？”曰：“古之贤人也。”曰：“怨乎？”曰：“求仁而得仁，又何怨？”出，曰：“夫子不为也。”<br>子曰：“饭疏食饮水，曲肱而枕之，乐亦在其中矣。不义而富且贵，于我如浮云。”<br>子曰：“加我数年，五十以学《易》，可以无大过矣。”<br>子所雅言，《诗》、《书》、执礼，皆雅言也。<br>叶公问孔子于子路，子路不对。子曰：“女奚不曰：其为人也，发愤忘食，乐以忘忧，不知老之将至云尔。”<br>子曰：“我非生而知之者，好古，敏以求之者也。”<br>子不语：怪、力、乱、神。<br>子曰：“三人行，必有我师焉。择其善者而从之，其不善者而改之。”<br>子曰：“天生德于予，桓魋其如予何？”<br>子曰：“二三子以我为隐乎？吾无隐乎尔！吾无行而不与二三子者，是丘也。”<br>子以四教：文，行，忠，信。<br>子曰：“圣人，吾不得而见之矣；得见君子者，斯可矣。”子曰：“善人，吾不得而见之矣，得见有恒者斯可矣。亡而为有，虚而为盈，约而为泰，难乎有恒乎。”<br>子钓而不纲，弋不射宿。<br>子曰：“盖有不知而作之者，我无是也。多闻，择其善者而从之；多见而识之，知之次也。”<br>互乡难与言，童子见，门人惑。子曰：“与其进也，不与其退也，唯何甚？人洁己以进，与其洁也，不保其往也。”<br>子曰：“仁远乎哉？我欲仁，斯仁至矣。”<br>陈司败问：“昭公知礼乎？”孔子曰：“知礼。”孔子退，揖巫马期而进之，曰：“吾闻君子不党，君子亦党乎？君取于吴，为同姓，谓之吴孟子。君而知礼，孰不知礼？”巫马期以告，子曰：“丘也幸，苟有过，人必知之。”<br>子与人歌而善，必使反之，而后和之。<br>子曰：“文，莫吾犹人也。躬行君子，则吾未之有得。”<br>子曰：“若圣与仁，则吾岂敢？抑为之不厌，诲人不倦，则可谓云尔已矣。”公西华曰：“正唯弟子不能学也。”<br>子疾病，子路请祷。子曰：“有诸？”子路对曰：“有之。《诔》曰：‘祷尔于上下神祇。’”子曰：“丘之祷久矣。”<br>子曰：“奢则不孙，俭则固。与其不孙也，宁固。”<br>子曰：“君子坦荡荡，小人长戚戚。”<br>子温而厉，威而不猛，恭而安。<br>泰伯篇<br>子曰：“泰伯，其可谓至德也已矣。三以天下让，民无得而称焉。”<br>子曰：“恭而无礼则劳；慎而无礼则葸；勇而无礼则乱；直而无礼则绞。君子笃于亲，则民兴于仁；故旧不遗，则民不偷。”<br>曾子有疾，召门弟子曰：“启予足，启予手。《诗》云：‘战战兢兢，如临深渊，如履薄冰。’而今而后，吾知免夫，小子！”<br>曾子有疾，孟敬子问之。曾子言曰：“鸟之将死，其鸣也哀；人之将死，其言也善。君子所贵乎道者三：动容貌，斯远暴慢矣；正颜色，斯近信矣；出辞气，斯远鄙倍矣。笾豆之事，则有司存。”<br>曾子曰：“以能问于不能；以多问于寡；有若无，实若虚，犯而不校。昔者吾友尝从事于斯矣。”<br>曾子曰：“可以托六尺之孤，可以寄百里之命，临大节而不可夺也。君子人与？君子人也。”<br>曾子曰：“士不可以不弘毅，任重而道远。仁以为己任，不亦重乎？死而后已，不亦远乎？”<br>子曰：“兴于《诗》，立于礼，成于乐。”<br>子曰：“民可使由之，不可使知之。”<br>子曰：“好勇疾贫，乱也。人而不仁，疾之已甚，乱也。”<br>子曰：“如有周公之才之美，使骄且吝，其余不足观也已。”<br>子曰：“三年学，不至于谷，不易得也。”<br>子曰：“笃信好学，守死善道。危邦不入，乱邦不居。天下有道则见，无道则隐。邦有道，贫且贱焉，耻也；邦无道，富且贵焉，耻也。”<br>子曰：“不在其位，不谋其政。”<br>子曰：“师挚之始，《关雎》之乱，洋洋乎盈耳哉！”<br>子曰：“狂而不直，侗而不愿，悾悾而不信，吾不知之矣。”<br>子曰：“学如不及，犹恐失之。”<br>子曰：“巍巍乎！舜、禹之有天下也而不与焉。”<br>子曰：“大哉尧之为君也！巍巍乎，唯天为大，唯尧则之。荡荡乎，民无能名焉。巍巍乎其有成功也，焕乎其有文章！”<br>舜有臣五人而天下治。武王曰：“予有乱臣十人。”孔子曰：“才难，不其然乎？唐虞之际，于斯为盛；有妇人焉，九人而已。三分天下有其二，以服事殷。周之德，其可谓至德也已矣。”<br>子曰：“禹，吾无间然矣。菲饮食，而致孝乎鬼神；恶衣服，而致美乎黻冕；卑宫室，而尽力乎沟洫。禹，吾无间然矣！”<br>子罕篇<br>子罕言利与命与仁。<br>达巷党人曰：“大哉孔子！博学而无所成名。”子闻之，谓门弟子曰：“吾何执？执御乎，执射乎？吾执御矣。”<br>子曰：“麻冕，礼也；今也纯，俭，吾从众。拜下，礼也；今拜乎上，泰也；虽违众，吾从下。”<br>子绝四：毋意、毋必、毋固、毋我。<br>子畏于匡，曰：“文王既没，文不在兹乎？天之将丧斯文也，后死者不得与于斯文也；天之未丧斯文也，匡人其如予何？”<br>太宰问于子贡曰：“夫子圣者与，何其多能也？”子贡曰：“固天纵之将圣，又多能也。”子闻之，曰：“太宰知我乎？吾少也贱，故多能鄙事。君子多乎哉？不多也。”<br>牢曰：“子云：‘吾不试，故艺。’”<br>子曰：“吾有知乎哉？无知也。有鄙夫问于我，空空如也。我叩其两端而竭焉。”<br>子曰：“凤鸟不至，河不出图，吾已矣夫！”<br>子见齐衰者、冕衣裳者与瞽者，见之，虽少，必作，过之必趋。<br>颜渊喟然叹曰：“仰之弥高，钻之弥坚。瞻之在前，忽焉在后。夫子循循然善诱人，博我以文，约我以礼，欲罢不能。既竭吾才，如有所立卓尔，虽欲从之，末由也已。”<br>子疾病，子路使门人为臣。病间，曰：“久矣哉，由之行诈也！无臣而为有臣，吾谁欺？欺天乎？且予与其死于臣之手也，无宁死于二三子之手乎！且予纵不得大葬，予死于道路乎？”<br>子贡曰：“有美玉于斯，韫椟而藏诸？求善贾而沽诸？”子曰：“沽之哉，沽之哉！我待贾者也。”<br>子欲居九夷。或曰：“陋，如之何？”子曰：“君子居之，何陋之有！”<br>子曰：“吾自卫反鲁，然后乐正，《雅》、《颂》各得其所。”<br>子曰：“出则事公卿，入则事父兄，丧事不敢不勉，不为酒困，何有于我哉？”<br>子在川上曰：“逝者如斯夫！不舍昼夜。”<br>子曰：“吾未见好德如好色者也。”<br>子曰：“譬如为山，未成一篑，止，吾止也；譬如平地，虽覆一篑，进，吾往也。”<br>子曰：“语之而不惰者，其回也与！”<br>子谓颜渊，曰：“惜乎！吾见其进也，未见其止也。”<br>子曰：“苗而不秀者有矣夫，秀而不实者有矣夫。”<br>子曰：“后生可畏，焉知来者之不如今也？四十、五十而无闻焉，斯亦不足畏也已。”<br>子曰：“法语之言，能无从乎？改之为贵。巽与之言，能无说乎？绎之为贵。说而不绎，从而不改，吾末如之何也已矣。”<br>子曰：“主忠信。毋友不如己者，过，则勿惮改。”<br>子曰：“三军可夺帅也，匹夫不可夺志也。”<br>子曰：“衣敝缊袍，与衣狐貉者立而不耻者，其由也与！‘不忮不求，何用不臧？’”子路终身诵之，子曰：“是道也，何足以臧？”<br>子曰：“岁寒，然后知松柏之后凋也。”<br>子曰：“知者不惑，仁者不忧，勇者不惧。”<br>子曰：“可与共学，未可与适道；可与适道，未可与立；可与立，未可与权。”<br>“唐棣之华，偏其反而。岂不尔思？室是远尔。”子曰：“未之思也，夫何远之有。”<br>乡党篇<br>孔子于乡党，恂恂如也，似不能言者；其在宗庙朝廷，便便言，唯谨尔。<br>朝，与下大夫言，侃侃如也；与上大夫言，訚訚如也。君在，踧踖如也，与与如也。<br>君召使摈，色勃如也，足躩如也。揖所与立，左右手，衣前后襜如也。趋进，翼如也。宾退，必复命曰：“宾不顾矣。”<br>入公门，鞠躬如也，如不容。立不中门，行不履阈。过位，色勃如也，足躩如也，其言似不足者。摄齐升堂，鞠躬如也，屏气似不息者。出，降一等，逞颜色，怡怡如也；没阶，趋进，翼如也；复其位，踧踖如也。<br>执圭，鞠躬如也，如不胜。上如揖，下如授。勃如战色，足蹜蹜如有循。享礼，有容色。私觌，愉愉如也。<br>君子不以绀緅饰，红紫不以为亵服。当暑,袗絺绤，必表而出之。缁衣羔裘，素衣麑裘，黄衣狐裘。亵裘长，短右袂。必有寝衣，长一身有半。狐貉之厚以居。去丧，无所不佩。非帷裳，必杀之。羔裘玄冠不以吊。吉月，必朝服而朝。<br>齐，必有明衣，布。齐必变食，居必迁坐。<br>食不厌精，脍不厌细。食饐而餲，鱼馁而肉败，不食；色恶，不食；臭恶，不食；失饪，不食；不时，不食；割不正，不食；不得其酱，不食。肉虽多，不使胜食气。唯酒无量，不及乱。沽酒市脯，不食。不撤姜食，不多食。<br>祭于公，不宿肉。祭肉不出三日，出三日不食之矣。<br>食不语，寝不言。<br>虽疏食菜羹，瓜祭，必齐如也。<br>席不正，不坐。<br>乡人饮酒，杖者出，斯出矣。<br>乡人傩，朝服而立于阼阶。<br>问人于他邦，再拜而送之。<br>康子馈药，拜而受之。曰：“丘未达，不敢尝。”<br>厩焚，子退朝，曰：“伤人乎？”不问马。<br>君赐食，必正席先尝之。君赐腥，必熟而荐之。君赐生，必畜之。侍食于君，君祭，先饭。<br>疾，君视之，东首，加朝服，拖绅。<br>君命召，不俟驾行矣。<br>入太庙，每事问。<br>朋友死，无所归，曰：“于我殡。”<br>朋友之馈，虽车马，非祭肉，不拜。<br>寝不尸，居不容。<br>见齐衰者，虽狎，必变。见冕者与瞽者，虽亵，必以貌。凶服者式之，式负版者。有盛馔，必变色而作。迅雷风烈，必变。<br>升车，必正立，执绥。车中不内顾，不疾言，不亲指。<br>色斯举矣，翔而后集。曰：“山梁雌雉，时哉时哉！”子路共之，三嗅而作。<br>先进篇<br>子曰：“先进于礼乐，野人也；后进于礼乐，君子也。如用之，则吾从先进。”<br>子曰：“从我于陈、蔡者，皆不及门也。”<br>德行：颜渊，闵子骞，冉伯牛，仲弓。言语：宰我，子贡。政事：冉有，季路。文学：子游，子夏。<br>子曰：“回也非助我者也，于吾言无所不说。”<br>子曰：“孝哉闵子骞！人不间于其父母昆弟之言。”<br>南容三复白圭，孔子以其兄之子妻之。<br>季康子问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不幸短命死矣，今也则亡。”<br>颜渊死，颜路请子之车以为之椁。子曰：“才不才，亦各言其子也。鲤也死，有棺而无椁，吾不徒行以为之椁。以吾从大夫之后，不可徒行也。”<br>颜渊死，子曰：“噫！天丧予！天丧予！”<br>颜渊死，子哭之恸，从者曰：“子恸矣！”曰：“有恸乎？非夫人之为恸而谁为？”<br>颜渊死，门人欲厚葬之，子曰：“不可。”门人厚葬之，子曰：“回也视予犹父也，予不得视犹子也。非我也，夫二三子也！”<br>季路问事鬼神，子曰：“未能事人，焉能事鬼？”,曰：“敢问死。”曰：“未知生，焉知死？”<br>闵子侍侧，訚訚如也；子路，行行如也；冉有、子贡，侃侃如也。子乐。“若由也，不得其死然。”<br>鲁人为长府，闵子骞曰：“仍旧贯如之何？何必改作？”子曰：“夫人不言，言必有中。”<br>子曰：“由之瑟，奚为于丘之门？”门人不敬子路，子曰：“由也升堂矣，未入于室也。”<br>子贡问：“师与商也孰贤？”子曰：“师也过，商也不及。”曰：“然则师愈与？”子曰：“过犹不及。”<br>季氏富于周公，而求也为之聚敛而附益之。子曰：“非吾徒也，小子鸣鼓而攻之可也。”<br>柴也愚，参也鲁，师也辟，由也喭。<br>子曰：“回也其庶乎，屡空。赐不受命而货殖焉，亿则屡中。”<br>子张问善人之道，子曰：“不践迹，亦不入于室。”<br>子曰：“论笃是与，君子者乎，色庄者乎？”<br>子路问：“闻斯行诸？”子曰：“有父兄在，如之何其闻斯行之？”冉有问：“闻斯行诸？”子曰：“闻斯行之。”公西华曰：“由也问：“闻斯行诸？”子曰：‘有父兄在’；求也问：‘闻斯行诸’。子曰‘闻斯行之’。赤也惑，敢问。”子曰：“求也退，故进之；由也兼人，故退之。”<br>子畏于匡，颜渊后。子曰：“吾以女为死矣！”曰：“子在，回何敢死！”<br>季子然问：“仲由、冉求可谓大臣与？”子曰：“吾以子为异之问，曾由与求之问。所谓大臣者，以道事君，不可则止。今由与求也，可谓具臣矣。”曰：“然则从之者与？”子曰：“弑父与君，亦不从也。”<br>子路使子羔为费宰，子曰：“贼夫人之子。”子路曰：“有民人焉，有社稷焉，何必读书然后为学。”子曰：“是故恶夫佞者。”<br>子路、曾皙、冉有、公西华侍坐，子曰：“以吾一日长乎尔，毋吾以也。居则曰‘不吾知也’如或知尔，则何以哉？”子路率尔而对曰：“千乘之国，摄乎大国之间，加之以师旅，因之以饥馑，由也为之，比及三年，可使有勇，且知方也。”夫子哂之。“求，尔何如？”对曰：“方六七十，如五六十，求也为之，比及三年，可使足民。如其礼乐，以俟君子。”“赤！尔何如？”对曰：“非曰能之，愿学焉。宗庙之事，如会同，端章甫，愿为小相焉。”“点，尔何如？”鼓瑟希，铿尔，舍瑟而作，对曰：“异乎三子者之撰。”子曰：“何伤乎？亦各言其志也。”曰：“暮春者，春服既成，冠者五六人，童子六七人，浴乎沂，风乎舞雩，咏而归。”夫子喟然叹曰：“吾与点也！”三子者出，曾皙后。曾皙曰：“夫三子者之言何如？”子曰：“亦各言其志也已矣。”曰：“夫子何哂由也？”曰：“为国以礼，其言不让，是故哂之。”“唯求则非邦也与？”“安见方六七十、如五六十而非邦也者？”“唯赤则非邦也与？”“宗庙会同，非诸侯而何？赤也为之小，孰能为之大？”<br>颜渊篇<br>颜渊问仁，子曰：“克己复礼为仁。一日克己复礼，天下归仁焉。为仁由己，而由人乎哉？”颜渊曰：“请问其目？”子曰：“非礼勿视，非礼勿听，非礼勿言，非礼勿动。”颜渊曰：“回虽不敏，请事斯语矣。”<br>仲弓问仁，子曰：“出门如见大宾，使民如承大祭。己所不欲，勿施于人。在邦无怨，在家无怨。”仲弓曰：“雍虽不敏，请事斯语矣。”<br>司马牛问仁，子曰：“仁者，其言也讱。”曰：“其言也讱，斯谓之仁已乎？”子曰：“为之难，言之得无讱乎？”<br>司马牛问君子，子曰：“君子不忧不惧。”曰：“不忧不惧，斯谓之君子已乎？”子曰：“内省不疚，夫何忧何惧？”<br>司马牛忧曰：“人皆有兄弟，我独亡。”子夏曰：“商闻之矣：死生有命，富贵在天。君子敬而无失，与人恭而有礼，四海之内皆兄弟也。君子何患乎无兄弟也？”<br>子张问明，子曰：“浸润之谮，肤受之愬，不行焉，可谓明也已矣；浸润之谮、肤受之愬不行焉，可谓远也已矣。”<br>子贡问政，子曰：“足食，足兵，民信之矣。”子贡曰：“必不得已而去，于斯三者何先？”曰：“去兵。”子贡曰：“必不得已而去，于斯二者何先？”曰：“去食。自古皆有死，民无信不立。”<br>棘子成曰：“君子质而已矣，何以文为？”子贡曰：“惜乎，夫子之说君子也！驷不及舌。文犹质也，质犹文也。虎豹之鞟犹犬羊之鞟。”<br>哀公问于有若曰：“年饥，用不足，如之何？”有若对曰：“盍彻乎？”曰：“二，吾犹不足，如之何其彻也？”对曰：“百姓足，君孰与不足？百姓不足，君孰与足？”<br>子张问崇德、辨惑，子曰：“主忠信，徙义，崇德也。爱之欲其生，恶之欲其死；既欲其生又欲其死，是惑也。‘诚不以富，亦只以异。’”<br>齐景公问政于孔子，孔子对曰：“君君，臣臣，父父，子子。”公曰：“善哉！信如君不君、臣不臣、父不父、子不子，虽有粟，吾得而食诸？”<br>子曰：“片言可以折狱者，其由也与？”子路无宿诺。<br>子曰：“听讼，吾犹人也。必也使无讼乎。”<br>子张问政，子曰：“居之无倦，行之以忠。”<br>子曰：“博学于文，约之以礼，亦可以弗畔矣夫。”<br>子曰：“君子成人之美，不成人之恶；小人反是。”<br>季康子问政于孔子，孔子对曰：“政者，正也。子帅以正，孰敢不正？”<br>季康子患盗，问于孔子。孔子对曰：“苟子之不欲，虽赏之不窃。”<br>季康子问政于孔子曰：“如杀无道以就有道，何如？”孔子对曰：“子为政，焉用杀？子欲善而民善矣。君子之德风，小人之德草，草上之风必偃。”<br>子张问：“士何如斯可谓之达矣？”子曰：“何哉尔所谓达者？”子张对曰：“在邦必闻，在家必闻。”子曰：“是闻也，非达也。夫达也者，质直而好义，察言而观色，虑以下人。在邦必达，在家必达。夫闻也者，色取仁而行违，居之不疑。在邦必闻，在家必闻。”<br>樊迟从游于舞雩之下，曰：“敢问崇德、修慝、辨惑。”子曰：“善哉问！先事后得，非崇德与？攻其恶，无攻人之恶，非修慝与？一朝之忿，忘其身，以及其亲，非惑与？”<br>樊迟问仁，子曰：“爱人。”问知，子曰：“知人。”樊迟未达，子曰：“举直错诸枉，能使枉者直。”樊迟退，见子夏，曰：“乡也吾见于夫子而问知，子曰：‘举直错诸枉，能使枉者直’，何谓也？”子夏曰：“富哉言乎！舜有天下，选于众，举皋陶，不仁者远矣。汤有天下，选于众，举伊尹，不仁者远矣。”<br>子贡问友，子曰：“忠告而善道之，不可则止，毋自辱焉。”<br>曾子曰：“君子以文会友，以友辅仁。”<br>子路篇<br>子路问政，子曰：“先之，劳之。”请益，曰：“无倦。”<br>仲弓为季氏宰，问政，子曰：“先有司，赦小过，举贤才。”曰：“焉知贤才而举之？”曰：“举尔所知。尔所不知，人其舍诸？”<br>子路曰：“卫君待子而为政，子将奚先？”子曰：“必也正名乎！”子路曰：“有是哉，子之迂也！奚其正？”子曰：“野哉由也！君子于其所不知，盖阙如也。名不正，则言不顺；言不顺，则事不成；事不成，则礼乐不兴；礼乐不兴，则刑罚不中；刑罚不中，则民无所错手足。故君子名之必可言也，言之必可行也。君子于其言，无所苟而已矣。”<br>樊迟请学稼，子曰：“吾不如老农。”请学为圃，曰：“吾不如老圃。”樊迟出。子曰：’小人哉，樊须也！上好礼，则民莫敢不敬；上好义，则民莫敢不服；上好信，则民莫敢不用情。夫如是，则四方之民襁负其子而至矣，焉用稼？”<br>子曰：“诵《诗》三百，授之以政，不达；使于四方，不能专对；虽多，亦奚以为？”<br>子曰：“其身正，不令而行；其身不正，虽令不从。”<br>子曰：“鲁卫之政，兄弟也。”<br>子谓卫公子荆，“善居室。始有，曰：‘苟合矣。’少有，曰：‘苟完矣。’富有，曰：‘苟美矣。’”<br>子适卫，冉有仆，子曰：“庶矣哉！”冉有曰：“既庶矣，又何加焉？”曰：“富之。”曰：“既富矣，又何加焉？”曰：“教之。”<br>子曰：“苟有用我者，期月而已可也，三年有成。”</p>
<blockquote>
<p>子曰：“‘善人为邦百年，亦可以胜残去杀矣。’诚哉是言也！”</p>
</blockquote>
<p>胜残去杀，残有为害的人、剩余、残破之意，杀有战斗、衰败、程度深、削除之意。胜为经得住、克服、超过，去为舍弃。理解为，去除那些暴戾为害的人、去除杀戮和斗争。</p>
<blockquote>
<p>子曰：“如有王者，必世而后仁。”</p>
</blockquote>
<p>子曰：“苟正其身矣，于从政乎何有？不能正其身，如正人何？”<br>冉子退朝，子曰：“何晏也？”对曰：“有政。”子曰：“其事也。如有政，虽不吾以，吾其与闻之。”<br>定公问：“一言而可以兴邦，有诸？”孔子对曰：“言不可以若是。其几也。人之言曰：‘为君难，为臣不易。’如知为君之难也，不几乎一言而兴邦乎？”曰：“一言而丧邦，有诸？”孔子对曰：“言不可以若是其几也。人之言曰：‘予无乐乎为君，唯其言而莫予违也。’如其善而莫之违也，不亦善乎？如不善而莫之违也，不几乎一言而丧邦乎？”<br>叶公问政，子曰：“近者说，远者来。”<br>子夏为莒父宰，问政，子曰：“无欲速，无见小利。欲速则不达，见小利则大事不成。”<br>叶公语孔子曰：“吾党有直躬者，其父攘羊，而子证之。”孔子曰：“吾党之直者异于是。父为子隐，子为父隐，直在其中矣。”<br>樊迟问仁，子曰：“居处恭，执事敬，与人忠。虽之夷狄，不可弃也。”<br>子贡问曰：“何如斯可谓之士矣？”子曰：“行己有耻，使于四方不辱君命，可谓士矣。”曰：“敢问其次。”曰：“宗族称孝焉，乡党称弟焉。”曰：“敢问其次。”曰：“言必信，行必果，踁踁然小人哉！抑亦可以为次矣。”曰：“今之从政者何如？”子曰：“噫！斗筲之人，何足算也！”<br>子曰：“不得中行而与之，必也狂狷乎！狂者进取，狷者有所不为也。”<br>子曰：“南人有言曰：‘人而无恒，不可以作巫医。’善夫！”“不恒其德，或承之羞。”子曰：“不占而已矣。”<br>子曰：“君子和而不同，小人同而不和。”<br>子贡问曰：“乡人皆好之，何如？”子曰：“未可也。”“乡人皆恶之，何如？”子曰：“未可也。不如乡人之善者好之，其不善者恶之。”<br>子曰：“君子易事而难说也，说之不以道不说也，及其使人也器之；小人难事而易说也，说之虽不以道说也，及其使人也求备焉。”<br>子曰：“君子泰而不骄，小人骄而不泰。”<br>子曰：“刚、毅、木、讷近仁。”<br>子路问曰：“何如斯可谓之士矣？”子曰：“切切偲偲，怡怡如也，可谓士矣。朋友切切偲偲，兄弟怡怡。”<br>子曰：“善人教民七年，亦可以即戎矣。”<br>子曰：“以不教民战，是谓弃之。”<br>宪问篇<br>宪问耻，子曰：“邦有道，谷；邦无道，谷，耻也。”“克、伐、怨、欲不行焉，可以为仁矣？”子曰：“可以为难矣，仁则吾不知也。”<br>子曰：“士而怀居，不足以为士矣。”<br>子曰：“邦有道，危言危行；邦无道，危行言孙。”<br>子曰：“有德者必有言，有言者不必有德。仁者必有勇，勇者不必有仁。”<br>南宫适问于孔子曰：“羿善射，奡荡舟，俱不得其死然；禹、稷躬稼而有天下。”夫子不答。南宫适出，子曰：“君子哉若人！尚德哉若人！”<br>子曰：“君子而不仁者有矣夫，未有小人而仁者也。”<br>子曰：“爱之，能勿劳乎？忠焉，能勿诲乎？”<br>子曰：“为命，裨谌草创之，世叔讨论之，行人子羽修饰之，东里子产润色之。”<br>或问子产，子曰：“惠人也。”问子西，曰：“彼哉，彼哉！”问管仲，曰：“人也。夺伯氏骈邑三百，饭疏食，没齿无怨言。”<br>子曰：“贫而无怨难，富而无骄易。”<br>子曰：“孟公绰为赵、魏老则优，不可以为滕、薛大夫。”<br>子路问成人，子曰：“若臧武仲之知、公绰之不欲、卞庄子之勇、冉求之艺，文之以礼乐，亦可以为成人矣。”曰：“今之成人者何必然？见利思义，见危授命，久要不忘平生之言，亦可以为成人矣。”<br>子问公叔文子于公明贾曰：“信乎，夫子不言，不笑，不取乎？”公明贾对曰：“以告者过也。夫子时然后言，人不厌其言；乐然后笑，人不厌其笑；义然后取，人不厌其取。”子曰：“其然？岂其然乎？”<br>子曰：“臧武仲以防求为后于鲁，虽曰不要君，吾不信也。”<br>子曰：“晋文公谲而不正，齐桓公正而不谲。”<br>子路曰：“桓公杀公子纠，召忽死之，管仲不死，曰未仁乎？”子曰：“桓公九合诸侯不以兵车，管仲之力也。如其仁，如其仁！”<br>子贡曰：“管仲非仁者与？桓公杀公子纠，不能死，又相之。”子曰：“管仲相桓公霸诸侯，一匡天下，民到于今受其赐。微管仲，吾其被发左衽矣。岂若匹夫匹妇之为谅也，自经于沟渎而莫之知也。”<br>公叔文子之臣大夫僎与文子同升诸公，子闻之,曰：“可以为‘文’矣。”<br>子言卫灵公之无道也，康子曰：“夫如是，奚而不丧？”孔子曰：“仲叔圉治宾客，祝鮀治宗庙，王孙贾治军旅，夫如是，奚其丧？”<br>子曰：“其言之不怍，则为之也难。”<br>陈成子弑简公，孔子沐浴而朝，告于哀公曰：“陈恒弑其君，请讨之。”公曰：“告夫三子。”,孔子曰：“以吾从大夫之后，不敢不告也，君曰‘告夫三子’者！”之三子告，不可。孔子曰：“以吾从大夫之后，不敢不告也。”<br>子路问事君，子曰：“勿欺也，而犯之。”<br>子曰：“君子上达，小人下达。”<br>子曰：“古之学者为己，今之学者为人。”<br>蘧伯玉使人于孔子，孔子与之坐而问焉，曰：“夫子何为？”对曰：“夫子欲寡其过而未能也。”使者出，子曰：“使乎！使乎！”<br>子曰：“不在其位，不谋其政。”曾子曰：“君子思不出其位。”<br>子曰：“君子耻其言而过其行。”<br>子曰：“君子道者三，我无能焉：仁者不忧，知者不惑，勇者不惧。”子贡曰：“夫子自道也。”<br>子贡方人，子曰：“赐也贤乎哉？夫我则不暇。”<br>子曰：“不患人之不己知，患其不能也。”<br>子曰：“不逆诈，不亿不信，抑亦先觉者，是贤乎！”<br>微生亩谓孔子曰：“丘何为是栖栖者与？无乃为佞乎？”孔子曰：“非敢为佞也，疾固也。”<br>曰：“骥不称其力，称其德也。”<br>或曰：“以德报怨，何如？”子曰：“何以报德？以直报怨，以德报德。”<br>子曰：“莫我知也夫！”子贡曰：“何为其莫知子也？”子曰：“不怨天，不尤人，下学而上达。知我者其天乎！”<br>公伯寮愬子路于季孙。子服景伯以告，曰：“夫子固有惑志于公伯寮，吾力犹能肆诸市朝。”子曰：“道之将行也与，命也；道之将废也与，命也。公伯寮其如命何？”<br>子曰：“贤者辟世，其次辟地，其次辟色，其次辟言。”子曰：“作者七人矣。”<br>子路宿于石门，晨门曰：“奚自？”子路曰：“自孔氏。”曰：“是知其不可而为之者与？”<br>子击磬于卫，有荷蒉而过孔氏之门者，曰：“有心哉，击磬乎！”既而曰：“鄙哉，硁硁乎！莫己知也，斯己而已矣。深则厉，浅则揭。”子曰：“果哉！末之难矣。”<br>子张曰：“《书》云，‘高宗谅阴，三年不言。’何谓也？”子曰：“何必高宗，古之人皆然。君薨，百官总己以听于冢宰三年。”<br>子曰：“上好礼，则民易使也。”<br>子路问君子，子曰：“修己以敬。”曰：“如斯而已乎？”曰：“修己以安人。”曰：“如斯而已乎？”曰：“修己以安百姓。修己以安百姓，尧、舜其犹病诸！”<br>原壤夷俟，子曰：“幼而不孙弟，长而无述焉，老而不死，是为贼！”以杖叩其胫。<br>阙党童子将命，或问之曰：“益者与？”子曰：“吾见其居于位也，见其与先生并行也。非求益者也，欲速成者也。”<br>卫灵公篇<br>卫灵公问陈于孔子，孔子对曰：“俎豆之事，则尝闻之矣；军旅之事，未之学也。”明日遂行。<br>在陈绝粮，从者病莫能兴。子路愠见曰：“君子亦有穷乎？”子曰：“君子固穷，小人穷斯滥矣。”<br>子曰：“赐也，女以予为多学而识之者与？”对曰：“然，非与？”曰：“非也，予一以贯之。”<br>子曰：“由，知德者鲜矣。”<br>子曰：“无为而治者其舜也与！夫何为哉？恭己正南面而已矣。”<br>子张问行，子曰：“言忠信，行笃敬，虽蛮貊之邦，行矣。言不忠信，行不笃敬，虽州里，行乎哉？立则见其参于前也，在舆则见其倚于衡也，夫然后行。”子张书诸绅。<br>子曰：“直哉史鱼！邦有道如矢，邦无道如矢。君子哉蘧伯玉！邦有道则仕，邦无道则可卷而怀之。”<br>子曰：“可与言而不与之言，失人；不可与言而与之言，失言。知者不失人亦不失言。”<br>子曰：“志士仁人无求生以害仁，有杀身以成仁。”<br>子贡问为仁，子曰：“工欲善其事，必先利其器。居是邦也，事其大夫之贤者，友其士之仁者。”<br>颜渊问为邦，子曰：“行夏之时，乘殷之辂，服周之冕，乐则《韶》、《舞》；放郑声，远佞人。郑声淫，佞人殆。”<br>子曰：“人无远虑，必有近忧。”<br>子曰：“已矣乎！吾未见好德如好色者也。”<br>子曰：“臧文仲其窃位者与！知柳下惠之贤而不与立也。”<br>子曰：“躬自厚而薄责于人，则远怨矣。”<br>子曰：“不曰‘如之何、如之何’者，吾末如之何也已矣。”<br>子曰：“群居终日，言不及义，好行小慧，难矣哉！”<br>子曰：“君子义以为质，礼以行之，孙以出之，信以成之。君子哉！”<br>子曰：“君子病无能焉，不病人之不己知也。”<br>子曰：“君子疾没世而名不称焉。”<br>子曰：“君子求诸己，小人求诸人。”<br>子曰：“君子矜而不争，群而不党。”<br>子曰：“君子不以言举人，不以人废言。”<br>子贡问曰：“有一言而可以终身行之者乎？”子曰：“其恕乎！己所不欲，勿施于人。”<br>子曰：“吾之于人也，谁毁谁誉？如有所誉者，其有所试矣。斯民也，三代之所以直道而行也。”<br>子曰：“吾犹及史之阙文也，有马者借人乘之，今亡矣夫！”<br>子曰：“巧言乱德，小不忍，则乱大谋。”<br>子曰：“众恶之，必察焉；众好之，必察焉。”<br>子曰：“人能弘道，非道弘人。”<br>子曰：“过而不改，是谓过矣。”<br>子曰：“吾尝终日不食、终夜不寝以思，无益，不如学也。”<br>子曰：“君子谋道不谋食。耕也馁在其中矣，学也禄在其中矣。君子忧道不忧贫。”<br>子曰：“知及之，仁不能守之，虽得之，必失之。知及之，仁能守之，不庄以涖之，则民不敬。知及之，仁能守之，庄以涖之，动之不以礼，未善也。”<br>子曰：“君子不可小知而可大受也，小人不可大受而可小知也。”<br>子曰：“民之于仁也，甚于水火。水火，吾见蹈而死者矣，未见蹈仁而死者也。”<br>子曰：“当仁不让于师。”<br>子曰：“君子贞而不谅。”<br>子曰：“事君，敬其事而后其食。”<br>子曰：“有教无类。”<br>子曰：“道不同，不相为谋。”<br>子曰：“辞达而已矣。”<br>师冕见，及阶，子曰：“阶也。”及席，子曰：“席也。”皆坐，子告之曰：“某在斯，某在斯。”师冕出。子张问曰：“与师言之道与？”子曰：“然，固相师之道也。”<br>季氏篇<br>季氏将伐颛臾，冉有、季路见于孔子，曰：“季氏将有事于颛臾。”孔子曰：“求，无乃尔是过与？夫颛臾，昔者先王以为东蒙主，且在邦域之中矣，是社稷之臣也。何以伐为？”冉有曰：“夫子欲之，吾二臣者皆不欲也。”孔子曰：“求，周任有言曰：‘陈力就列，不能者止。’危而不持，颠而不扶，则将焉用彼相矣？且尔言过矣，虎兕出于柙，龟玉毁于椟中，是谁之过与？”冉有曰：“今夫颛臾固而近于费，今不取，后世必为子孙忧。”孔子曰：“求，君子疾夫舍曰欲之而必为之辞。丘也闻，有国有家者，不患寡而患不均，不患贫而患不安。盖均无贫，和无寡，安无倾。夫如是，故远人不服则修文德以来之，既来之，则安之。今由与求也相夫子，远人不服而不能来也，邦分崩离析而不能守也，而谋动干戈于邦内。吾恐季孙之忧不在颛臾，而在萧墙之内也。”<br>孔子曰：“天下有道，则礼乐征伐自天子出；天下无道，则礼乐征伐自诸侯出。自诸侯出，盖十世希不失矣；自大夫出，五世希不失矣；陪臣执国命，三世希不失矣。天下有道，则政不在大夫；天下有道，则庶人不议。”<br>孔子曰：“禄之去公室五世矣，政逮于大夫四世矣，故夫三桓之子孙微矣。”<br>孔子曰：“益者三友，损者三友。友直、友谅、友多闻，益矣；友便辟、友善柔、友便佞，损矣。”<br>孔子曰：“益者三乐，损者三乐。乐节礼乐、乐道人之善、乐多贤友，益矣；乐骄乐、乐佚游、乐宴乐，损矣。”<br>孔子曰：“侍于君子有三愆：言未及之而言谓之躁，言及之而不言谓之隐，未见颜色而言谓之瞽。”<br>孔子曰：“君子有三戒：少之时，血气未定，戒之在色；及其壮也，血气方刚，戒之在斗；及其老也，血气既衰，戒之在得。”<br>孔子曰：“君子有三畏：畏天命，畏大人，畏圣人之言。小人不知天命而不畏也，狎大人，侮圣人之言。”<br>孔子曰：“生而知之者上也，学而知之者次也；困而学之又其次也。困而不学，民斯为下矣。”<br>孔子曰：“君子有九思：视思明，听思聪，色思温，貌思恭，言思忠，事思敬，疑思问，忿思难，见得思义。”<br>孔子曰：“见善如不及，见不善如探汤；吾见其人矣。吾闻其语矣。隐居以求其志，行义以达其道；吾闻其语矣，未见其人也。”<br>齐景公有马千驷，死之日，民无德而称焉；伯夷、叔齐饿于首阳之下，民到于今称之。其斯之谓与？”<br>陈亢问于伯鱼曰：“子亦有异闻乎？”对曰：“未也。尝独立，鲤趋而过庭，曰：‘学《诗》乎？’对曰：‘未也。’‘不学《诗》，无以言。’鲤退而学《诗》。他日，又独立，鲤趋而过庭，曰：‘学《礼》乎？’对曰：‘未也。’‘不学《礼》，无以立。’鲤退而学《礼》。闻斯二者。”陈亢退而喜曰：“问一得三，闻《诗》，闻《礼》，又闻君子之远其子也。”<br>邦君之妻，君称之曰夫人，夫人自称曰小童；邦人称之曰君夫人，称诸异邦曰寡小君；异邦人称之亦曰君夫人。<br>阳货篇<br>阳货欲见孔子，孔子不见，归孔子豚。孔子时其亡也而往拜之，遇诸涂。谓孔子曰：“来，予与尔言。”曰：“怀其宝而迷其邦，可谓仁乎？”曰：“不可。”“好从事而亟失时，可谓知乎？”曰：“不可！”“日月逝矣，岁不我与！”孔子曰：“诺，吾将仕矣。”<br>子曰：“性相近也，习相远也。”<br>子曰：“唯上知与下愚不移。”<br>子之武城，闻弦歌之声。夫子莞尔而笑，曰：“割鸡焉用牛刀？”子游对曰：“昔者偃也闻诸夫子曰：‘君子学道则爱人，小人学道则易使也。’”子曰：“二三子，偃之言是也！前言戏之耳。”<br>公山弗扰以费畔，召，子欲往。子路不说，曰：“末之也已，何必公山氏之之也？”子曰：“夫召我者而岂徒哉？如有用我者，吾其为东周乎！”<br>子张问仁于孔子，孔子曰：“能行五者于天下为仁矣。”请问之，曰：“恭、宽、信、敏、惠。恭则不侮，宽则得众，信则人任焉，敏则有功，惠则足以使人。”<br>佛肸召，子欲往。子路曰：“昔者由也闻诸夫子曰。亲于其身为不善者，君子不入也。’佛肸以中牟畔，子之往也，如之何？＂子曰：“然。有是言也。不曰坚乎，磨而不磷？不曰白乎，涅而不缁。焉岂匏瓜也哉？焉能系而不食？”<br>子曰：“由也，女闻六言六蔽矣乎？”对曰：“未也。”“居！吾语女。好仁不好学，其蔽也愚；好知不好学，其蔽也荡；好信不好学，其蔽也贼；好直不好学，其蔽也绞；好勇不好学，其蔽也乱；好刚不好学，其蔽也狂。”<br>子曰：“小子何莫学夫诗！诗，可以兴，可以观，可以群，可以怨：迩之事父，远之事君．多识于鸟兽草木之名。”<br>子谓伯鱼曰：“女为《周南》、《召南》矣乎？人而不为《周南》、《召南》，其犹正墙面而立也与？”<br>子曰：“礼云礼云，玉帛云乎哉？乐云乐云，钟鼓云乎哉？”<br>子曰：“色厉而内荏，譬诸小人，其犹穿窬之盗也与？”<br>子曰：“乡愿，德之贼也。”<br>子曰：“道听而途说，德之弃也。”<br>子曰：“鄙夫可与事君也与哉？其未得之也，患得之；既得之，患失之。苟患失之，无所不至矣。”<br>子曰：“古者民有三疾，今也或是之亡也。古之狂也肆，今之狂也荡；古之矜也廉，今之矜也忿戾；古之愚也直，今之愚也诈而已矣。”<br>子曰：“巧言令色，鲜矣仁。”<br>子曰：“恶紫之夺朱也，恶郑声之乱雅乐也，恶利口之覆邦家者。”<br>子曰：“予欲无言。”子贡曰：“子如不言，则小子何述焉？”子曰：“天何言哉？四时行焉，百物生焉，天何言哉？”<br>孺悲欲见孔子，孔子辞以疾。将命者出户，取瑟而歌，使之闻之。<br>宰我问：“三年之丧，期已久矣！君子三年不为礼，礼必坏；三年不为乐，乐必崩。旧谷既没，新谷既升，钻燧改火，期可已矣。”子曰：“食夫稻，衣夫锦，于女安乎？”曰：“安！”“女安则为之！夫君子之居丧，食旨不甘，闻乐不乐，居处不安，故不为也。今女安，则为之！”宰我出，子曰：“予之不仁也！子生三年，然后免于父母之怀。夫三年之丧，天下之通丧也，予也有三年之爱于其父母乎！”<br>子曰：“饱食终日，无所用心，难矣哉！不有博弈者乎？为之犹贤乎已。”<br>子路曰：“君子尚勇乎？”子曰：“君子义以为上。君子有勇而无义为乱，小人有勇而无义为盗。”<br>子贡曰：“君子亦有恶乎？”子曰：“有恶。恶称人之恶者，恶居下流而讪上者，恶勇而无礼者，恶果敢而窒者。”曰：“赐也亦有恶乎？”“恶徼以为知者，恶不孙以为勇者，恶讦以为直者。”<br>子曰：“唯女子与小人为难养也，近之则不孙，远之则怨。”<br>子曰：“年四十而见恶焉，其终也已。”<br>微子篇<br>微子去之，箕子为之奴，比干谏而死。孔子曰：“殷有三仁焉。”<br>柳下惠为士师，三黜。人曰：“子未可以去乎？”曰：“直道而事人，焉往而不三黜？枉道而事人，何必去父母之邦？”<br>齐景公待孔子曰：“若季氏，则吾不能。”以季、孟之间待之，曰：“吾老矣，不能用也。”孔子行。<br>齐人归女乐，季桓子受之，三日不朝，孔子行。”<br>楚狂接舆歌而过孔子曰：“凤兮凤兮，何德之衰？往者不可谏，来者犹可追。已而已而，今之从政者殆而！”孔子下，欲与之言，趋而辟之，不得与之言。<br>长沮、桀溺耦而耕，孔子过之，使子路问津焉。长沮曰：“夫执舆者为谁？”子路曰：“为孔丘。”曰：“是鲁孔丘与？”曰：“是也。”曰：“是知津矣。”问于桀溺，桀溺曰：“子为谁？”曰：“为仲由。”曰：“是鲁孔丘之徒与？”对曰：“然。”曰：“滔滔者天下皆是也，而谁以易之？且而与其从辟人之士也，岂若从辟世之士哉？”耰而不辍。子路行以告，夫子怃然曰：“鸟兽不可与同群，吾非斯人之徒与而谁与？天下有道，丘不与易也。”<br>子路从而后，遇丈人，以杖荷蓧。子路问曰：“子见夫子乎？”丈人曰：“四体不勤，五谷不分，孰为夫子？”植其杖而芸，子路拱而立。止子路宿，杀鸡为黍而食之，见其二子焉。明日，子路行以告，子曰：“隐者也。”使子路反见之，至则行矣。子路曰：“不仕无义。长幼之节不可废也，君臣之义如之何其废之？欲洁其身而乱大伦。君子之仕也，行其义也，道之不行已知之矣。”<br>逸民：伯夷、叔齐、虞仲、夷逸、朱张、柳下惠、少连。子曰：“不降其志，不辱其身，伯夷、叔齐与！”谓：“柳下惠、少连降志辱身矣，言中伦，行中虑，其斯而已矣。”谓：“虞仲、夷逸隐居放言，身中清，废中权。我则异于是，无可无不可。”<br>太师挚适齐，亚饭干适楚，三饭缭适蔡，四饭缺适秦，鼓方叔入于河，播鼗武入于汉，少师阳、击磬襄入于海。<br>周公谓鲁公曰：“君子不施其亲，不使大臣怨乎不以，故旧无大故则不弃也，无求备于一人。”<br>周有八士：伯达、伯适、仲突、仲忽、叔夜、叔夏、季随、季騧。<br>子张篇<br>子张曰：“士见危致命，见得思义，祭思敬，丧思哀，其可已矣。”<br>子张曰：“执德不弘，信道不笃，焉能为有？焉能为亡？”<br>子夏之门人问交于子张，子张曰：“子夏云何？”对曰：“子夏曰：‘可者与之，其不可者拒之。’”子张曰：“异乎吾所闻。君子尊贤而容众，嘉善而矜不能。我之大贤与，于人何所不容？我之不贤与，人将拒我，如之何其拒人也？”<br>子夏曰：“虽小道必有可观者焉，致远恐泥，是以君子不为也。”<br>子夏曰：“日知其所亡，月无忘其所能，可谓好学也已矣。”<br>子夏曰：“博学而笃志，切问而近思，仁在其中矣。”<br>子夏曰：“百工居肆以成其事，君子学以致其道。”<br>子夏曰：“小人之过也必文。”<br>子夏曰：“君子有三变：望之俨然，即之也温，听其言也厉。”<br>子夏曰：“君子信而后劳其民，未信，则以为厉己也；信而后谏，未信，则以为谤己也。”<br>子夏曰：“大德不逾闲，小德出入可也。”<br>子游曰：“子夏之门人小子，当洒扫应对进退则可矣。抑末也，本之则无，如之何？”子夏闻之，曰：“噫，言游过矣！君子之道，孰先传焉？孰后倦焉？譬诸草木，区以别矣。君子之道焉可诬也？有始有卒者，其惟圣人乎！”<br>子夏曰：“仕而优则学，学而优则仕。”<br>子游曰：“丧致乎哀而止。”<br>子游曰：“吾友张也为难能也，然而未仁。”<br>曾子曰：“堂堂乎张也，难与并为仁矣。”<br>曾子曰：“吾闻诸夫子，人未有自致者也，必也亲丧乎！”<br>曾子曰：“吾闻诸夫子，孟庄子之孝也，其他可能也；其不改父之臣与父之政，是难能也。”<br>孟氏使阳肤为士师，问于曾子。曾子曰：“上失其道，民散久矣。如得其情，则哀矜而勿喜！”<br>子贡曰：“纣之不善，不如是之甚也。是以君子恶居下流，天下之恶皆归焉。”<br>子贡曰：“君子之过也，如日月之食焉。过也人皆见之，更也人皆仰之。”<br>卫公孙朝问于子贡曰：“仲尼焉学？”子贡曰：“文武之道未坠于地，在人。贤者识其大者，不贤者识其小者，莫不有文武之道焉，夫子焉不学？而亦何常师之有？”<br>叔孙武叔语大夫于朝曰：“子贡贤于仲尼。”子服景伯以告子贡，子贡曰：“譬之宫墙，赐之墙也及肩，窥见室家之好；夫子之墙数仞，不得其门而入，不见宗庙之美、百官之富。得其门者或寡矣，夫子之云不亦宜乎！”<br>叔孙武叔毁仲尼，子贡曰：“无以为也，仲尼不可毁也。他人之贤者，丘陵也，犹可逾也；仲尼，日月也，无得而逾焉。人虽欲自绝，其何伤于日月乎？多见其不知量也。”<br>陈子禽谓子贡曰：“子为恭也，仲尼岂贤于子乎？”子贡曰：“君子一言以为知，一言以为不知，言不可不慎也。夫子之不可及也，犹天之不可阶而升也。夫子之得邦家者，所谓立之斯立，道之斯行，绥之斯来，动之斯和。其生也荣，其死也哀，如之何其可及也？”<br>尧曰篇<br>尧曰：“咨！尔舜！天之历数在尔躬，允执其中。四海困穷，天禄永终。”舜亦以命禹。曰：“予小子履，敢用玄牡，敢昭告于皇皇后帝：有罪不敢赦，帝臣不蔽，简在帝心。朕躬有罪，无以万方；万方有罪，罪在朕躬。”周有大赉，善人是富。“虽有周亲，不如仁人。百姓有过，在予一人。”谨权量，审法度，修废官，四方之政行焉。兴灭国，继绝世，举逸民，天下之民归心焉。所重：民、食、丧、祭。宽则得众，信则民任焉，敏则有功，公则说。<br>子张问于孔子曰：“何如斯可以从政矣？”子曰：“尊五美，屏四恶，斯可以从政矣。”子张曰：“何谓五美？”子曰：“君子惠而不费，劳而不怨，欲而不贪，泰而不骄，威而不猛。”子张曰：“何谓惠而不费？”子曰：“因民之所利而利之，斯不亦惠而不费乎？择可劳而劳之，又谁怨？欲仁而得仁，又焉贪？君子无众寡，无小大，无敢慢，斯不亦泰而不骄乎？君子正其衣冠，尊其瞻视，俨然人望而畏之，斯不亦威而不猛乎？”子张曰：“何谓四恶？”子曰：“不教而杀谓之虐；不戒视成谓之暴；慢令致期谓之贼；犹之与人也，出纳之吝谓之有司。”<br>孔子曰：“不知命，无以为君子也；不知礼，无以立也；不知言，无以知人也。”</p>
]]></content>
      <categories>
        <category>读后感</category>
      </categories>
      <tags>
        <tag>读后感</tag>
      </tags>
  </entry>
</search>
