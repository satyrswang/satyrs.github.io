<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2021.05</title>
    <url>/2021/07/28/2021-05/</url>
    <content><![CDATA[<br>

<h3 id="0523"><a href="#0523" class="headerlink" title="0523"></a>0523</h3><p>1、时间管理</p>
<p>!头脑里的问题和复盘，所思所想才能所悟</p>
<span id="more"></span>

<p>!没有计划和纪律就无主心骨</p>
<p>!不回溯自己走过的路径，出现的问题，会失去很多。</p>
<p>!功能繁杂就需要拆分，一个个解决。</p>
<p>!答案的找寻，需要自己去看具体的case</p>
<p>!要一次做到高质量 而不要缝缝补补</p>
<p>!<code>理性</code>，而<code>自信</code>– 对自我和信念坚持的信心。内心告诉自己不能的时候，就没有动力去研究如何可能，没有动力去找到正解</p>
<p>!思考的深度，让你行动时自信有纪律</p>
<p>!知识点的琐碎，在于理论框架的缺乏。自我体系的缺乏。</p>
<p>!工具不行，还是尽量得解决，不然只会拖延问题的最终效率</p>
<p>!不只看代码 还是得去了解思路后再看代码， 一个个看变量含义实在没有意义</p>
<p>!大部分时间的消耗，在于零散。零散的生活琐碎，零散的知识积累。没有理解到本质</p>
<p>!模型 是一个对问题的思考框架。如果没有细节上的关键点，或者局限或者突破，或者思路的逻辑性和数学上严谨性，那就漏洞百出了</p>
<p>!人文底蕴、语言的精炼、关注点的<code>深度</code>广度，以及<code>谦卑</code>，让他如此有魅力。</p>
<p>!要看放弃了什么</p>
<p>!所有以上的过程都不加判断地行动 愚昧</p>
<p>!头脑中对问题的预演，多个备选方案</p>
<p>!流畅的理解所形成的的记忆，一连串地因果链条</p>
<p>!每一个视角都可以深挖到许多细节和展开</p>
<p>!信号。而不是具体内容更关键</p>
<p>!大多抄来抄去</p>
<p>!写时思考</p>
<hr>
<br>

<p>2、零碎</p>
<p>是什么：</p>
<p>旁氏骗局，明斯基：拆东补西，夸张宣传，诱鱼上钩</p>
<p>幸存者偏差：成功者上总结的trait未必是必要条件 – 因为失败者同样可能有相同的trait然而并未成功。飞机弹孔的例子。</p>
<p>数字货币：纯数字化的 现实技术困难，想象OR设计实现的效果</p>
<p>国债轧空：</p>
<br>

<p>所有这些与我有什么关联？</p>
<p>我看到武去拆解一个概念的思路，以及材料来源。第二，和理论相结合和印证。</p>
<br>

<p>stl：deepcopy 重复地构造析构 异常</p>
<p>测试框架、过程监控</p>
<br>

<hr>
<p>3、tips</p>
<p>理性 信心 深度– 体现在细节 谦卑 精炼。慢慢地锤炼。</p>
<p>先预演、设计、想象、框架性、多角度、多方案，再行动（思考后看代码看资料等）。即计划（保证专注）与纪律（保证发力）。有所放弃。</p>
<p>方法论：拆解、温故、问题盘旋推敲 – 追求本质 、流畅地本质 口述逻辑链条、悬空观察、一次性高质量-不要修补、琐碎收束到体系中、陷入practice–case study、框架性思维问题、好工具事半功倍、语言或者事件背后的信号。</p>
<p>思考的角度可以带来很多不一样的感受。关注在运营、产品、变现、技术、手段、模式… …</p>
<hr>
<p>进一步：</p>
<br>

<p><strong>复盘细节与case、框架性与角度 –( 细节的上升 问题的抽象)、三思而行、悬空与慢、质量与效率、工具与程序与规范的重要性。</strong></p>
<br>

<blockquote>
<p>Case\ trick \ abstract \ detached \ one-time success\ tools \ steps \ time \  …</p>
</blockquote>
<br>

<p>记忆：</p>
<p>为了本质和质量，可以牺牲效率。但不能无节制。</p>
<p>本质即抽象、维度角度、细节等的层次。</p>
<p>5年后，10年后，20年后？</p>
<p>金融的10年后，会更加精彩。算法就没有10年后了么。需要对金融、互联网、产品、运营以及技术都有一定自己的理解和框架。世界是丰富的。如果是金融，从事研究、交易，一样的学习路线，一样去体悟。有一定的胜率。能够把握住精髓么。金融科班的含金量和算法，都高。都有价值和背书。都可以生成价值。</p>
<p>国家国际都缺乏天才。天才尚且无法解决一些问题。理论的局限性，还需要时间发展。</p>
<p>落脚到现实。还是不要选择从头开始。时间来不及。有余力，再去做吧。</p>
<br>

]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>2021.07</title>
    <url>/2021/07/28/2021-07/</url>
    <content><![CDATA[<br>



<h4 id="0708"><a href="#0708" class="headerlink" title="0708"></a>0708</h4><p>爷爷离开了。最后的阶段我也没有做点什么。没有陪伴，甚至没有电话，总觉得还有机会。</p>
<span id="more"></span>

<p>我以为自己会悲痛。然而没有。还是在追寻快乐。没有什么阻挡对快感的追求。有种抓紧时间的感觉。抓紧时间感受和热爱。</p>
<p>想离婚了。初心是什么呢。我贪恋的是陪伴和港湾，而不是这个人么。我心疼他累，心疼他在张总面前的委屈，心疼他为我做的一切。但是我还是不满足。不满足于他认知的一些空白，不满足于他不能再带给我见识和思想的冲击。不满足于，他对我而言似乎只是为了避免孤独。不满足于他时常只考虑我，而不够果断。不满足于激情的程度。我想要更多。</p>
<p>比如，他能够更自信，更聪明，更直接。更有魅力。</p>
<p>然而的确，我没有缺点么。为什么他能够包容。</p>
<p>爷爷和我说的最多就是好好工作。我不记得更多教诲了。死亡的肉身，只是皮包骨，枯瘦地。</p>
<p>大姑说爷爷前几个晚上，自己玩被子，玩很久，像个孩子。抓着大姑的手臂，把腿跷在大姑腿上。</p>
<p>回家那天晚上还和大姑吵了起来。一句没有过脑子的话脱口而出。这是我的毛病。但是我道歉了，也是我没想到的，也是我自己的确做的太少了。爷爷为我做的，远远多于我为他所做的。大姑会在背后说不好听的话，但是我也能接受，毕竟我和那些人的交集几乎为零，而大姑在意她的形象也是好的。没什么坏处。</p>
<p>是我太谦让了还是我没认识到利害。怪在没过脑子。</p>
<p>时间过得太快了。而根本记不住什么，也没能滚雪球地让时间给自己带来沉淀。似乎是这样的。感知不到早上和明天，后天的变化。</p>
<p>不要再继续这样消极没有激情的自己。</p>
<br>

<h4 id="0710"><a href="#0710" class="headerlink" title="0710"></a>0710</h4><p>昨天熬夜了，今天状态不太好。</p>
<p>1、整理每个点 – 提取和重复</p>
<p>2、熟悉后建立框架</p>
<p>3、视频输出、白纸上提取 </p>
<p>4、英文重要性</p>
<br>

<p>今天，1 “学习去学习”，讲了两种思维模式，专注和发散，在学习新事物的时候，对模式不够清晰的情况下，在两者之间进行切换反复。 2记忆分为工作记忆和长期记忆，对于长期记忆需要 多次少量的重复。根据时间间隔来重复多次。 3经济学人每天一篇 </p>
<br>



<p>4vue的router，从单html页面的跳转(to)，到工程里按钮点击后跳转到另一个link。后面是我需要的。</p>
<p>还有一些技巧，比如flex 和content，以及elementui和viewui有区别，以及es6的语法–缩写，以及scss，.+类名，scope不影响其他页面的样式。</p>
<p>5计网的视频</p>
<p>6猫的一篇讲私立幼儿园，结论是钱不够也没必要。说到底还是英语问题。</p>
<br>

<h4 id="0711"><a href="#0711" class="headerlink" title="0711"></a>0711</h4><p>吵架。去了奉贤、闵行。没有阅读。</p>
<p>1、不要吵架，非常耽误精力和时间。用沟通来解决问题。</p>
<p>2、利他思维、独立思考和判断</p>
<br>

<h4 id="0712"><a href="#0712" class="headerlink" title="0712"></a>0712</h4><p>1、经济学人</p>
<p>2、看看宁德时代和隆基股份、上海机场和美的的有没有印证</p>
<p>3、猫的文章，除了感受的部分，“基础知识” + “信息源” + “个股”</p>
<p>4、几个基础概念整理</p>
<p>5、打压热门item</p>
<p>6、sql统计标签</p>
<p>7、计网到ch4的第三个视频</p>
<br>

<h4 id="0713"><a href="#0713" class="headerlink" title="0713"></a>0713</h4><p>1、经济学人</p>
<p>2、计网到ch4的第9个</p>
<p>3、高数多元微分和定积分</p>
<p>4、整理昨天的</p>
<p>5、做个算法题、看篇论文-gpt</p>
<br>

<h4 id="0714"><a href="#0714" class="headerlink" title="0714"></a>0714</h4><p>1、ch4结束</p>
<p>2、极限结束</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>2021.08</title>
    <url>/2021/08/02/2021-08/</url>
    <content><![CDATA[<br>



<h4 id="0802"><a href="#0802" class="headerlink" title="0802"></a>0802</h4><span id="more"></span>

<ol>
<li>模型的很多问题还是需要思考的</li>
<li>考研的东西还需要抓住</li>
<li>工作要追求效率，抓紧时间，并且重视质量，各种细节需要严谨核实，避免无用功</li>
<li>家庭琐事尽量缩小花费的时间。</li>
</ol>
<br>



<h4 id="0806"><a href="#0806" class="headerlink" title="0806"></a>0806</h4><p>今天6号了。时间恍惚。</p>
<p>现在每次他出去我还是会不开心。很不开心。我的怨恨在哪呢，我觉得他应该陪伴我们，这里做到的不好，那里做的不好， 但是他总是看都没看见。他不觉得不好。或者觉得了，也只是视而不见。总之路径依赖了。</p>
<p>现在的想法就是他出门不顾家，我一样。有些任性吧，觉得不公平。我希望我做到的是，没有情绪，只是照着这个原则去做。</p>
<p>但是我还是没想明白我这样做的好处坏处。我知道坏处是他会觉得不好。那我应该沟通。我告知，不搞有情绪的事情。第二坏处就是，我能否做到是真的在做事情，而不是耽误时间在情绪里。我不希望他给我带来什么负面情绪。更希望自己有更多内生的正向能量。</p>
<p>我现在做的事情，大多还是想到啥就做点。</p>
<p>如果心不在焉，不如不做。尽量努力减少情绪的负面干扰。我希望在35岁前能够达到对世界的理解有一个较为高度的输出。需要读书和总结。更多地训练理性。即使是宽松的，也只是一种形式而已，反而没有自律和原则来的无负于光阴。</p>
<p>当然最终都是为了娱乐。我还是有些怀疑的。好像还是有缺点。虽然斯德哥尔摩状态的确是我想要的。但是那个天才说的 也对，并不是那样的状态就是全部的人生了。</p>
<p>最好的状态，还是心在焉而有所思、不仁、自如地切换，追求自己想要的也不伤害别人。</p>
<br>

<br>

<h4 id="0807"><a href="#0807" class="headerlink" title="0807"></a>0807</h4><p>来公司了。如果只是恐惧孤单或者未来的困难，而无法离开，那不如离开了。但是真的，我准备好了么。</p>
<p>离开后，我需要独自面对工作和压力。我需要给自己更多的鼓励而不依赖一个港湾。我需要开始割舍。这些勇气和毅力，切换和减少自己的情绪化，我能做到么。但我知道，在他那里，我已经习惯，已经无法改变对他和对自己施加痛苦。</p>
<p>我心里的结就一个，就是他孩子。我对阿豪有恐惧，有忍耐，有很多不快乐。积累起来。让我觉得他所有的做法，只是表面的手段而已。因为我的痛苦高过他，我的折磨也大，而他始终还是没有能力解决。我也没有能力。</p>
<p>这不是我想快乐，就快乐的起来的。有时候我的快乐，来自对他的折磨，折磨中我感觉到被爱而快乐，而不是来自他快乐我快乐。我缺乏勇气，缺乏安全感。也缺乏宽容。我没办法做到简简单单生活。不如让他找个其他简简单单的女人，幸福地过。</p>
<p>我一个人的时候，一个人的痛苦，都还是需要我自己偿。有才华的他，有抒发的地方。而我不过是普通的人。有些悲哀，无法抒发。悲哀来自欲望和情绪，来自索取和不安。人生找另一个人，如果是投资，至少他能让我开眼界。但是我没有能力一起去。我不习惯在那样的场合去逢迎。这样的眼界，似乎和我无关。</p>
<p>或者找另一个人，他真的能够理解你的不安。他的理解表现在哪呢。你都不理解你自己呢。</p>
<p>并且我们的交流模式也充满了距离。负面表达非常多。</p>
<p>我没能力解决。我想离开。我不想面对，几十年后的难以融入，几十年后的被抛弃和忍耐。年纪大了还要忍耐，还无法从心所欲，实在是有些悲哀。</p>
<p>1他孩子以后，他以后，我不信任。2我和他的相处模式出很大问题。我不喜欢直接表达情感。并且有痛苦，甚至莫名其妙。我一夜没睡，他倒好，这样的经历让我认清楚很多。还是要更爱自己。我痛苦而他快乐，让我更加感到冰冷。和倾向于折磨。而他是乐观派，偏向于无心肺。3我的初心。他的初心。4我一定可以考上研，一定可以更好。一定可以得到我想要的，成为我我想成为的。但是我要有相应的付出。</p>
<p>1金钱上少很多。时间多了。2少了快乐也少了痛苦。但不要总往坏处想。3一个人，但是还是要有热爱和乐观。4对孩子要付出。</p>
<br>

<br>

<h4 id="0808"><a href="#0808" class="headerlink" title="0808"></a>0808</h4><p>看下怎么搞模拟盘。以及代码买卖。以及模型。</p>
<p>周日整一天待在公司。还是经济学人看了几篇。效率非常低。不是真的想做事。想了很多杂乱的。</p>
<p>晚上回家了。</p>
<p>周六晚上又熬了一夜。我还是一直在看已经了解的那些东西。还是在一个层次里反复挣扎。而且没有沉淀地去付出。也没有什么思考。希望以后能坚持输出。</p>
<p>概念很重要，定义很重要。但都是从无知到慢慢地体味出来的。</p>
<br>

<h4 id="0809"><a href="#0809" class="headerlink" title="0809"></a>0809</h4><p>一天忙工作。充实算不上。但是整理代码也非常繁杂，多个场景要测，于是就耗了很多时间。</p>
<p>晚上回家了。路上看了一些文章。印象比较深的就是对时代的划分，农业到工业到信息化，到现在数据化的海量信息和自动化、ai。</p>
<p>以及对于短视频业务的认可。</p>
<p>还有一些创业的想法，都是从不会一步步走来的。</p>
<p>还有达叔里一些小故事。做事情看重不重要。</p>
<p>看了王国维的一篇文章。哲学不会让你更理性，也不会让你过好一生。更不会让你有钱。无非是一种思维，重视抽象。</p>
<br>

<h4 id="0810"><a href="#0810" class="headerlink" title="0810"></a>0810</h4><p>上班很累。看经济学人。</p>
<p>晚上回来看了无穷级数的一些题目。</p>
<p>发散地想了不少。回味了之前总结的技巧和写的一些日志。</p>
<p>看是为了什么？</p>
<p>如果不能带来本质上有益的概念和认知、现实上的价值、创新的，而只是一团脑子一转都会想的比他还深的。何必看了。无非是打发时间。而大部分的工作，不过也是如此，大部分的问题，也是如此。</p>
<p>看东西要求本质，要真的确认背后的事实。</p>
<br>

<h4 id="0811"><a href="#0811" class="headerlink" title="0811"></a>0811</h4><p>一天都在补数据、起模型。重新补数、起模型。</p>
<p>还是没能预先地思考可能错误的地方。所以就非常地耗费、手工。</p>
<p><strong>如果你需要量化一些事情的影响程度，能够对发生的事有一定的把握。对逻辑不确切的能够求证，而得到本质和确认。</strong></p>
<p>那需要一方面有知识和理论框架，得以有支撑。其次，需要你对事实去验证。看看是否自洽和严谨。</p>
<p>否则只是一厢情愿的相信而已。</p>
<p>自己熟悉的未必是风险低的。屁股可以影响脑袋，但是不能决定。</p>
<p>已经跌了的，未必是风险低的。</p>
<p>已经涨很高的，未必是风险高的。看你怎么衡量。</p>
<p>已经发生的，会有一定惯性。</p>
<p>当前已知的，和未来将发生的，影响程度没你想的那么大。</p>
<p>真正风险低的，是未来将有发展的、备受重视的、资金青睐的。</p>
<p>为什么资本不青睐小米？因为股价不只是当前的业绩。资本是流动的。股价是不确定的。围绕估值的波动，在一般等价物中是的，但是在股市中不是。股市是围绕钱波动，围绕信心波动。</p>
<br>

<h4 id="0812"><a href="#0812" class="headerlink" title="0812"></a>0812</h4><p>尝试预想。尝试不带偏见地、多分类地场景预设。</p>
<p>尝试探寻，寻找漏洞并证明。</p>
<br>



<h4 id="0823"><a href="#0823" class="headerlink" title="0823"></a>0823</h4><p>“喜欢和别人聊天，想知道他们的反应。一方面想通过文字表达自己，一方面想看到他们的反馈，像一场实验。人越老，藏的越多，但是说出来的如果是有意义的，带着好的目的，那么有何不可。”</p>
<p>“只是，我还会喜欢去咀嚼那些话，反复地感受里面的自我表达。有些自恋而敏感。其实大可不必。信以为真，未尝不是难得糊涂。何以更宽容呢。不过是难得糊涂。”</p>
<p>“如果以这种方式你能够找到自己的一种平衡，虽然不合适。我能预想到的不好的地方是，不坦诚，没有真的让自己去宽容，不够隐忍。都在谋取自己的利益，非常地无有远志。”</p>
<p>我知道什么是对的什么是错的。只是还是会自欺欺人。</p>
<br>

<ol>
<li><p>财政部的国债太高，达到ceiling。并且还需要撒币。因为现金太多，国会批不下来更高的ceiling。</p>
</li>
<li><p>撒币带来的影响是，隔夜利率为负，货币基金的大量现金无处安放，只能放在逆回购市场，借回给央行。</p>
</li>
<li><p>带来逆回购市场供给大于需求，逆回购市场中国债作为抵押品需求上升、价格上升，利率下降。</p>
</li>
<li><p>为了解决隔夜利率为负的问题，央行提高逆回购市场利率，并允许更多货币基金参与者。这样就为隔夜回购利率设置了隐形下限。也导致，qe和财政部的cash都流入到逆回购市场。货币基金和外国央行握有的美国国债的利息。</p>
</li>
<li><p>如果想把钱，从银行的准备金账户移到逆回购市场，需要多久 –  政府货币基金持有的加权国债到期日。准备金的钱和货币基金的、保险的钱等，是买了流动性高资产的，一些挪动需要时间。肯定不会卖出国债，因为这时候国债市场卖出是亏钱的 – the Fed pays 0.05% (annual yield) for overnight cash via RRPs, which is more than the yield of 3-month and shorter-term Treasury bills, that’s where some of the cash goes. </p>
</li>
<li><p>所以整个行动，更像是央行作为财政部的最后贷款人的角色，避免政府破产。一方面回笼现金，一方面吸收国债作为抵押品。</p>
</li>
<li><p>但是逆回购市场又有危机了，现金太多已经超过1兆。所以又出来SRF，就是短期你手里的抵押品给我作抵押，我借你钱 – 这是短期的，所以不会影响美联储的资产负债表。这样，大家都买长期国债好了，钱不够了来抵押。那么钱就被吸引到国债上。成为新兴吸收国债的qe，又不会导致国债在美联储的资负表上，导致爆表。</p>
</li>
<li><p>Q</p>
<ol>
<li><p> 货币基金主要是谁的钱，主要有哪些资产可以购买？</p>
</li>
<li><p>逆回购市场带来的美联储的负债债主主要是哪些机构 – 即哪些机构主要是逆回购的交易对手以及他们会如何操作。 – 主要是： $479 billion in overnight RRPs on the Fed’s balance sheet. Of them, 80% ($384 billion) were with just five financial institutions ： Fidelity ($195 billion), Goldman Sachs ($60 billion); Morgan Stanley ($44 billion), JP Morgan ($43 billion), and Blackrock ($42 billion).</p>
</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>Apriori_fptree</title>
    <url>/2021/03/11/Apriori-fptree/</url>
    <content><![CDATA[<h2 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h2><ul>
<li>input<br>{ID1:[item1,item2…],ID2:[item3,item2…],…}</li>
</ul>
<span id="more"></span>
<ul>
<li><p>output<br>item1-&gt;item2,item3…</p>
</li>
<li><p>math<br>频率及条件概率的简单应用<br>支持度: 频数/样本总量<br>置信度: 条件概率<br>如果子集不为频繁集则超集一定不为；反之如果超集是频繁集则子集均为频繁集。</p>
</li>
<li><p>parameters<br><code>minSupport</code><br><code>minConf</code></p>
</li>
<li><p>过程<br>1-4完成了频繁集的搜寻，5及之后产生规则。</p>
</li>
</ul>
<p>1、<code>list</code>存放每个用户的  <code>itemset</code>，生成[[],[]]存放所有item并排序，<code>map(frozenset, C1)</code><br>2、第一轮遍历，生成一个<code>list</code>存放所有支持度大于<code>minSupport</code>，大于则<code>insert(0,key)</code>。遍历的函数<code>scanD</code>所生成<code>list</code>的元素是<code>frozenset</code>。如果<code>can.issubset(tid)</code>且<code>！ssCnt.has_key(can)</code>则加入到<code>supportData</code>。<br>3、list存放所有频繁物品集，<code>supportData</code>存放所有计算过的物品集的支持度，以<code>frozenset</code>作为<code>key</code>。两者更新的过程是在<code>apriori</code>中循环调用<code>aprioriGen</code>和<code>scanD </code>，再更新。直到<code>scanD</code>得到的频繁物品集为空。<br>4、<code>aprioriGen</code>的过程，两层循环，遍历上一个(即物品集中个数为目前需要产生的个数-1)频繁物品集，<code>L1 = list(Lk[i])[:k-2];L2 = list(Lk[j])[:k-2]</code>取前k-2项比较，若相同则<code>retList.append(Lk[i] | Lk[j])</code>将物品集union。是一个merging过程。</p>
<p>5、如果频繁集<code>frozenset</code>中item多于2，先<code>rulesFromConseq</code>再<code>calcConf</code>。<code>rulesFromConseq</code>是个递归过程，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rulesFromConseq(freqSet, H, supportData, brl, minConf&#x3D;0.7):</span><br><span class="line">    m &#x3D; len(H[0])</span><br><span class="line">    if (len(freqSet) &gt; (m + 1)): #停止条件</span><br><span class="line">        Hmp1 &#x3D; aprioriGen(H, m+1)</span><br><span class="line">        Hmp1 &#x3D; calcConf(freqSet, Hmp1, supportData, brl, minConf)</span><br><span class="line">        if (len(Hmp1) &gt; 1):   #至少需要两个元素才能够merge</span><br><span class="line">            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)</span><br></pre></td></tr></table></figure>
<p>这里一个问题是，如果<code>len(Hmp1) =1</code>，但是<code>len(freqSet) &gt; (len(Hmp1[0]) + 1)</code>仍然成立，因此修改为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def rulesFromConseq(freqSet, H, supportData, brl, minConf&#x3D;0.7):</span><br><span class="line">    m &#x3D; len(H[0])</span><br><span class="line">    if (len(freqSet) &gt; (m + 1)): #停止条件</span><br><span class="line">        if(len(H) &gt; 1):</span><br><span class="line">            Hmp1 &#x3D; aprioriGen(H, m+1)</span><br><span class="line">        Hmp1 &#x3D; calcConf(freqSet, Hmp1, supportData, brl, minConf)</span><br><span class="line">        rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)</span><br></pre></td></tr></table></figure>

<br>


<h2 id="fptree"><a href="#fptree" class="headerlink" title="fptree"></a>fptree</h2><ul>
<li><p>概念<br>闭项：超集的支持度为s，子集的支持度都不超过s。<br>条件模式基： 即一个item追溯到root的所有路径<br>条件fp树 ： 即根据条件模式基创建的tree<br><code>myCondTree, myHead = createTree(condPattBases, minSup)</code></p>
</li>
<li><p>准备<br><code>headertable</code> : <code>dict</code>，<code>value</code>为<code>[count,treeNode]</code><br><code>treeNode</code> : <code>class</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def __init__(self, nameValue, numOccur, parentNode):</span><br><span class="line">     self.name &#x3D; nameValue</span><br><span class="line">     self.count &#x3D; numOccur</span><br><span class="line">     self.nodeLink &#x3D; None</span><br><span class="line">     self.parent &#x3D; parentNode      </span><br><span class="line">     self.children &#x3D; &#123;&#125; </span><br></pre></td></tr></table></figure>
<p>主要方法：<code>createTree</code> <code>updateTree</code>  <code>mineTree</code><br>辅助方法：<code>updateHeader</code> <code>findPrefixPath</code> <code>ascendTree</code></p>
</li>
<li><p>过程</p>
</li>
</ul>
<ol>
<li><code>createTree</code>首先第一次遍历计算<code>count</code>，<code>headerTable[item] = headerTable.get(item, 0) + dataSet[trans]</code>将<code>count</code>简单赋值为1。注意到这里的dataSet类型。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def createInitSet(dataSet):</span><br><span class="line">    retDict &#x3D; &#123;&#125;</span><br><span class="line">    for trans in dataSet:</span><br><span class="line">        retDict[frozenset(trans)] &#x3D; 1</span><br><span class="line">    return retDict</span><br></pre></td></tr></table></figure>
获得<code>freqItemSet = set(headerTable.keys())</code>后第二次遍历。先对item排序，<code>orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)] </code>，再<code>updateTree(orderedItems, retTree, headerTable, count)</code>。</li>
<li><code>updateTree</code>，这里就是普通的对树的操作，也是递归。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if len(items) &gt; 1:#call updateTree() with remaining ordered items</span><br><span class="line">        updateTree(items[1::], inTree.children[items[0]], headerTable, count)</span><br></pre></td></tr></table></figure>
唯一注意之处，需要<code>updateHeader</code>。</li>
<li><code>mineTree</code>和<code>Apriori</code>中的<code>rulesFromConseq </code>一样较复杂，包含递归。先生成条件模式基，再生成conditional-fptree，再递归。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if myHead !&#x3D; None:                  </span><br><span class="line">   mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)</span><br></pre></td></tr></table></figure>
循环递归mineTree的过程即生成规则过程，直到频繁2项集都得到。</li>
</ol>
]]></content>
      <categories>
        <category>算法</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF</title>
    <url>/2021/03/09/CRF/</url>
    <content><![CDATA[<br>

<p>本文主要介绍 线性链的CRF。应用于标注问题。<br>介绍：概率图、MEMM问题、势函数、CRF的定义和表示方法，以及基本问题。</p>
<span id="more"></span>

<br>



<h3 id="00概率无向图模型"><a href="#00概率无向图模型" class="headerlink" title="00概率无向图模型"></a>00概率无向图模型</h3><p>1、无向图表示的随机变量之间：成对马尔科夫性、局部马尔科夫性、全局马尔科夫性。</p>
<p>2、概率无向图模型定义：<br>联合概率满足上面的三个马尔科夫性，则称此联合概率分布为概率无向图模型或者马尔科夫随机出。</p>
<p>3、团与最大团</p>
<p>4、因子分解<br>联合分布表示为 最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解。</p>
<p>这个函数，就是 势函数。要求是正的。一般为指数函数。</p>
<p>5、条件随机场</p>
<p>给定随机变量x的条件下，随机变量y的马尔科夫随机场。</p>
<p>学习时，学习条件概率模型。预测时利用条件概率，求得最大输出序列y。</p>
<p>定义：</p>
<p><img src="/2021/03/09/CRF/crf.png" alt="crf"></p>
<!-- <img src=crf.png width=80% height=300> -->

<p><img src="/2021/03/09/CRF/lccrf.png" alt="lccrf"></p>
<!-- <img src=lccrf.png width=80% height=300> -->

<br>

<br>



<h3 id="01引入"><a href="#01引入" class="headerlink" title="01引入"></a>01引入</h3><p>MEMM有的缺陷是标签偏向。</p>
<p>标签偏向问题的原因是 – MEMM在每一步都用softmax进行了归一化</p>
<p>CRF通过全局归一化解决了标签偏向问题。</p>
<p>这样讲是比较费解的。论文里的例子容易理解。两个词，rib rob。</p>
<p>概率图如下(来自论文)</p>
<p><img src="/2021/03/09/CRF/bias.png" alt="img"></p>
<p>这里隐状态1到2只有一条路，4到5也只有一条路，所以整个模型没有考虑观测值i 和 o不同，所以导致词性标注过程中，不考虑观测变量。这个就不符合逻辑了。</p>
<p>因此局部的归一化，容易受隐变量影响，而不去考虑观测变量。这个带来的问题是：</p>
<p>如果training数据，3个都是rib，一个是rob。那么在预测过程中，求max P(y1y2y3|rob)时，viterbi算法走出来的，将会是0-&gt;1-&gt;2-&gt;3。</p>
<p>即，<strong>如果上一个状态是低熵的，那么很容易忽略观测变量，而在预测时，受样本影响较大。</strong></p>
<p>即，无论观测值，当State1比State2转移少，State1会一直转移到State1，即使全局看转移到State2概率更高。即有更少转移的状态、拥有的转移概率普遍偏高，概率最大路径更容易出现转移少的状态。</p>
<h3 id="02CRF解决label-bias"><a href="#02CRF解决label-bias" class="headerlink" title="02CRF解决label bias"></a>02CRF解决label bias</h3><p>一，crf将输入序列和输出标注映射为一个d维实数向量，而MEMM的特征函数拥有的信息只是输入序列和当前状态以及上一个状态，也就是说CRF的特征函数掌握信息量更多，从而表达能力更强。</p>
<p>第二个的改进是它不再每一次状态转移进行归一化，而是在全局进行归一化，这样完美解决Label Bias问题。即CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。</p>
<p>有得必有失，注意到模型的分母需要罗列所有的状态序列，对于序列长度为n的输入序列，状态序列的个数为S^n ，对于这种指数增长问题，在实际应用中一般都是intractable的，只能付诸于近似求解，比如我们之前提过的Variational Bayes或者Gibbs Sampling等等。</p>
<p>不过有一种特殊结构的CRF – linear chain crf，精确快速求解的方式是存在的。</p>
<p>(注：马尔科夫随机场就是概率无向图，而crf是马尔科夫随机场特殊一种，而线性链是crf的特殊一种。隐状态是随机场，而观测变量是条件。条件随机场还有其他多种形式。)</p>
<br>

<br>



<h3 id="03最大图、势函数"><a href="#03最大图、势函数" class="headerlink" title="03最大图、势函数"></a>03最大图、势函数</h3><h4 id="举个例子"><a href="#举个例子" class="headerlink" title="举个例子"></a>举个例子</h4><p>例子更清楚：<br><img src="/2021/03/09/CRF/%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E4%BE%8B%E5%AD%901.png" alt="矩阵形式例子1"><br><img src="/2021/03/09/CRF/%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E4%BE%8B%E5%AD%902.png" alt="矩阵形式例子2"></p>
<h4 id="linear-crf-是如何定义特征函数的？如何将原来intractable的问题变为tractable？"><a href="#linear-crf-是如何定义特征函数的？如何将原来intractable的问题变为tractable？" class="headerlink" title="linear crf 是如何定义特征函数的？如何将原来intractable的问题变为tractable？"></a>linear crf 是如何定义特征函数的？如何将原来intractable的问题变为tractable？</h4><p>linear让最大团的个数为序列长度-1，并且，最大团内部的特征函数为 t1 个状态特征函数 和 t2个转移特征函数 之和的形式。</p>
<h4 id="那，无向图的条件概率怎么表达呢？"><a href="#那，无向图的条件概率怎么表达呢？" class="headerlink" title="那，无向图的条件概率怎么表达呢？"></a>那，无向图的条件概率怎么表达呢？</h4><p>从MEMM可见，本质上有两点，一是x到y的函数，即自定义的特征函数，二是loss。和MEMM不同的是，CRF是无向图。无向图的条件概率怎么表达呢？</p>
<p>来自google图片</p>
<p><img src="/2021/03/09/CRF/crftu.png" alt="img"></p>
<p>这里就引入了最大团概念。由上文介绍。得到</p>
<p><img src="/2021/03/09/CRF/crfp.jpg" alt="img"></p>
<p>这个公式，本质是求 观测变量下隐状态序列的概率。c为最大团个数，k为自定义的k个特征函数。</p>
<p>对于CRF，可以为他定义两款特征函数：转移特征&amp;状态特征。 即将建模总公式展开，见下面👇的参数化表达。</p>
<p>省得写了，下面大量贴图。</p>
<br>

<br>





<h3 id="04三种表达形式"><a href="#04三种表达形式" class="headerlink" title="04三种表达形式"></a>04三种表达形式</h3><p>1、参数化形式</p>
<p>公式：<br><img src="/2021/03/09/CRF/%E5%8F%82%E6%95%B0%E5%8C%96%E5%85%AC%E5%BC%8F.png" alt="参数化公式"></p>
<p>解释如下：<br><img src="/2021/03/09/CRF/%E5%8F%82%E6%95%B0%E5%8C%96%E5%85%AC%E5%BC%8F%E8%A7%A3%E9%87%8A2_1.png" alt="参数化公式解释2_1"></p>
<p><img src="/2021/03/09/CRF/%E5%8F%82%E6%95%B0%E5%8C%96%E5%85%AC%E5%BC%8F%E8%A7%A3%E9%87%8A2_2.png" alt="参数化公式解释2_2"></p>
<p>即：<br><img src="/2021/03/09/CRF/%E8%A7%92%E5%BA%A6.png" alt="角度"></p>
<p>把两种特征合在一起：</p>
<p><img src="/2021/03/09/CRF/hebing1.jpg" alt="1"></p>
<p>合并的公式中，我们并没有显示将边和节点区分开来，而只是写出了边的特征函数，因为从某种程度上边包含的信息已经涵盖了节点所拥有的信息，将两者统一起来可以有利于我们数学公式表达的方便性，另一方面，将边和节点进行单独讨论，从理论上可能有一点冗余，但是从实际效果中，节点信息可以充当一种backoff，起到一定的平滑效果(Smoothing)。</p>
<p>特征函数部分和MEMM一样记做score：</p>
<p><img src="/2021/03/09/CRF/hebingscore.jpg" alt="2"></p>
<p>可见：</p>
<p>我们为 token(隐状态) 打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值。对数线性的思路。</p>
<p>这个的推导在视频里去看。<a href="https://www.bilibili.com/video/BV19t411R7QU?p=4">https://www.bilibili.com/video/BV19t411R7QU?p=4</a></p>
<br>

<p>2、参数化简化的形式推导<br>略</p>
<br>

<p>3、矩阵形式</p>
<p>矩阵形式推导：<br><img src="/2021/03/09/CRF/%E7%9F%A9%E9%98%B5%E6%8E%A8%E5%AF%BC.png" alt="矩阵推导"></p>
<!-- <img src=矩阵推导.png width=70% height=500> -->

<br>
<br>


<h3 id="05三个问题"><a href="#05三个问题" class="headerlink" title="05三个问题"></a>05三个问题</h3><h4 id="模型基本问题"><a href="#模型基本问题" class="headerlink" title="模型基本问题"></a>模型基本问题</h4><p>模型问题包含 learning 和 inference。<br>learning即 parameter estimation。求 θ。<br>inference：<br>(以下 y 为隐变量)<br>1 marginal prob ： 当建模对象为joint distribution，求p(y_t|x)即求边缘概率问题。<br>2 conditional prob ： 求 p(x|y)  –此为生成模型才有的问题，在crf中不考虑<br>3 MAP inference ： decoding ， 求  y_pred = argmax p(y|x)          其中y_pred = y_1y_2y_3 …</p>
<p><img src="/2021/03/09/CRF/wenti.jpg" alt="问题定义"></p>
<br>

<h4 id="CRF-learning"><a href="#CRF-learning" class="headerlink" title="CRF learning"></a>CRF learning</h4><p>1、问题定义<br><img src="/2021/03/09/CRF/learning%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89.png" alt="learning问题定义"></p>
<p>CRF也是极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。能用在log-linear models上的求参方法都可以用过来。</p>
<p>2、todo – 数学推导</p>
<br>




<h4 id="CRF-marginal-prob"><a href="#CRF-marginal-prob" class="headerlink" title="CRF marginal prob"></a>CRF marginal prob</h4><p>1、即求 P(yt = i | x)<br>给定x条件下的 隐变量yt 的边缘概率。<br>即 词性标注中，给定一句话，判断出 y1(第一个词)是动词(i)的概率是多少。</p>
<p>2、硬算，复杂度高。指数级计算不可行。<br>简化：</p>
<p>数学转化。<br>用到了变量消除法， sum+product，又叫信念传播，belief propagate。<br>HMM的前向 后向传播也就是belief propagate。</p>
<p><img src="/2021/03/09/CRF/sumproduct.png" alt="sumproduct"><br><img src="/2021/03/09/CRF/sumproduct2.png" alt="sumproduct2"></p>
<h4 id><a href="#" class="headerlink" title></a><br></h4><h4 id="CRF-decoding"><a href="#CRF-decoding" class="headerlink" title="CRF decoding"></a>CRF decoding</h4><p>求 y的序列，使得 argmax p(y|x)<br>– 类似于 HMM 问题。vertbi，既然是dp，核心要梳理清楚dp的转移函数。</p>
<p>我们就定义i处的局部状态为 f(I) ,表示在位置i处的隐状态的各种取值可能为 I，然后递推位置i+1处的隐状态，写出来DP转移公式。</p>
<br>

<p>reference</p>
<p><img src="https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico" alt="img"><a href="https://arxiv.org/abs/1011.4088">https://arxiv.org/abs/1011.4088</a> </p>
<p><a href="http://www.ai.mit.edu/courses/6.891-nlp/ASSIGNMENT1/t11.1.pdf">http://www.ai.mit.edu/courses/6.891-nlp/ASSIGNMENT1/t11.1.pdf</a></p>
<p><a href="https://www.cnblogs.com/en-heng/p/6201893.html">https://www.cnblogs.com/en-heng/p/6201893.html</a></p>
<p><a href="https://blog.csdn.net/aws3217150/article/details/68935789">https://blog.csdn.net/aws3217150/article/details/68935789</a></p>
]]></content>
      <categories>
        <category>算法</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF后续</title>
    <url>/2021/04/14/CRF%E5%90%8E%E7%BB%AD/</url>
    <content><![CDATA[<br>

<p>介绍下BiLSTM+CRF吧</p>
<br>

<span id="more"></span>



<h4 id="Q1、为啥搞一个LSTM-CRF的hybrid-model"><a href="#Q1、为啥搞一个LSTM-CRF的hybrid-model" class="headerlink" title="Q1、为啥搞一个LSTM+CRF的hybrid model?"></a>Q1、为啥搞一个LSTM+CRF的hybrid model?</h4><p>用LSTM，整体的预测accuracy是不错indeed, 但是会出现上述的错误：在B之后再来一个B。这个错误在CRF中是不存在的，因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。</p>
<p>那就把CRF接到LSTM上面，把LSTM在time<em>step上把每一个hidden</em>state的tensor输入给CRF，让LSTM负责在CRF的特征限定下，依照新的loss function，学习出一套新的非线性变换空间。</p>
<h4 id="Q2、CRF-源码"><a href="#Q2、CRF-源码" class="headerlink" title="Q2、CRF++源码"></a>Q2、CRF++源码</h4><ul>
<li>输入</li>
</ul>
<p>He reckons the current account deficit will narrow to only #1.8 billion in September .”代表一个训练句子xx，而CRF++要求将这样的句子拆成 <strong>每一个词一行并且是固定列数的数据</strong>，其中列除了原始输入，还可以包含一些其他信息，比如第二列包含了POS信息，最后一列是Label信息。而不同的训练序列与序列之间的相隔，就靠一个空白行来区分。</p>
<ul>
<li>特征模版</li>
</ul>
<p>1、“%x[row, column]” 代表获得<strong>当前指向位置向上或向下偏移|row|行</strong>，并指向第column列的值。</p>
<p>2、CRF++中主要有两种特征模版，<strong>Unigram和Bigram 模版</strong>，注意Unigram和Bigram是<strong>相对于输出序列</strong>而言，而不是相对于输入序列。对于”U01:%x[0,1]”这样一个模版，上面例子的输入数据会产生如下的特征函数：</p>
<p>3、</p>
]]></content>
      <categories>
        <category>算法</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>CRF前续知识</title>
    <url>/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<br>

<p>从HMM MEMM介绍概率图、生成和判别模型</p>
<br>

<span id="more"></span>

<h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>我理解，机器学习，本质是一种反向求解过程。数据的具象世界里找到抽象的规律。比由规律推演出具象要复杂。</p>
<p>并且反向求解还有各种现实的局限，数据上、算力上、可行性上、性能约束上。就会有各种trick。就像工程中缓存击穿穿透问题，通过小trick来避免。而其根基还是在于统计。</p>
<p>由机器学习到深度学习，根基和理论框架都没有变。</p>
<p>CRF即是一种对序列数据的建模。放松了HMM的1-gram依赖假设和观测独立假设。转移和发射矩阵也进一步抽象为函数。因此就比HMM更实际些。也由于放松， 需要各种理论(和SVM一样涉及到了许多定理，更多是概率统计里分布相关的定理)支持，就比HMM 更加复杂。</p>
<p>很容易从概率图，看到机器学习为何向深度转化。本质都是对现实世界里的学习任务进行建模。</p>
<h2 id="00-生成和判别模型"><a href="#00-生成和判别模型" class="headerlink" title="00 生成和判别模型"></a>00 生成和判别模型</h2><p>概率图为什么那么难。核心在 是否理解 生成和判别模型的区别。</p>
<p>神经网络模型、SVM、perceptron、LR、DT……</p>
<p>NB、LDA ……</p>
<p>核心区别在于： 对 联合概率  还是对 条件概率 建模。</p>
<p><strong>角度一</strong>用inference过程举例：假设我知道P(X) P(X,Y) ，那我就根据条件概率公式预测出来y_pred 了。</p>
<p>这个是 生成模型。</p>
<p>而判别 ： 假设我知道P(Y|X) ，那我直接代入X就可得到y_pred</p>
<p><strong>角度二</strong>用learning过程举例：生成我要对 P(X,Y) 建模，判别我要对P(Y|X) 建模。</p>
<p>其实是很简单的。但是我们 对于不同模型去整体把握的时候 ，忽略了top down思维方式。bottom up的确是不容易想清楚。</p>
<p>进一步： 你能从这里思考出，生成和判别的各自局限吗？</p>
<p>判别边界和(结果的)生成过程、先验假设、数据量和性能</p>
<h2 id="01-概率图"><a href="#01-概率图" class="headerlink" title="01 概率图"></a>01 概率图</h2><p>序列数据 – 标注 分类 走势分析 –&gt;其中概率图解决标注和分类</p>
<p>概率图的话，需要去学习基本的有向图表示含义，图画出来(如下)则能判断，给出隐状态c，a\b之间是否独立。</p>
<p>来自知乎某文：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/tu.png" alt="img"></p>
<p>focus在HMM MEMM CRF –&gt;如何解决标注问题</p>
<h2 id="02-马尔科夫随机场定义-概率无向图定义"><a href="#02-马尔科夫随机场定义-概率无向图定义" class="headerlink" title="02 马尔科夫随机场定义(概率无向图定义)"></a>02 马尔科夫随机场定义(概率无向图定义)</h2><p>马尔科夫假设 – 马尔科夫模型的前提与局限</p>
<p>马尔科夫性质 – a. 成对，b. 局部，c. 全局。</p>
<p>这个性质主要有助于无向图里势函数的推导。不在乎数学，可忽略。势函数见下04。</p>
<h2 id="03-有向图和无向图"><a href="#03-有向图和无向图" class="headerlink" title="03 有向图和无向图"></a>03 有向图和无向图</h2><p>图模型 – 有向(贝叶斯网络)和无向(马尔科夫网络)  </p>
<p>HMM、 Karman filter是有向，boltzman 、CRF是无向</p>
<p>核心区别：在于建模过程如何求 node 代表的token ，该随机变量的联合概率</p>
<h2 id="04-无向图的联合概率-–-势函数"><a href="#04-无向图的联合概率-–-势函数" class="headerlink" title="04 无向图的联合概率 – 势函数"></a>04 无向图的联合概率 – 势函数</h2><p>势函数哪里来的or为啥这个样子？ – 或者说为啥无向图的联合概率是这个样子的因子分解？</p>
<p>： 由 Hammersley-Clifford定理 保证。</p>
<h2 id="05-HMM"><a href="#05-HMM" class="headerlink" title="05 HMM"></a>05 HMM</h2><h3 id="HMM之前的NB"><a href="#HMM之前的NB" class="headerlink" title="HMM之前的NB"></a>HMM之前的NB</h3><p>来自google图片：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/nb.png" alt="img"></p>
<h3 id="HMM五要素"><a href="#HMM五要素" class="headerlink" title="HMM五要素"></a>HMM五要素</h3><p>第一个隐状态的概率分布、转移矩阵、发射矩阵、状态集、观测集</p>
<p>来自google图片：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/hmm.png" alt="img"></p>
<p>进一步： 你能从这里思考出，HMM是生成还是判别？</p>
<p>这也是HMM痛点。假设不合实际、生成自身局限、没有更多特征信息。</p>
<h3 id="HMM问题"><a href="#HMM问题" class="headerlink" title="HMM问题"></a>HMM问题</h3><p>1、学习过程 ，即计算出五要素(建模过程)</p>
<p>监督学习：</p>
<p>MLE – 本质就是频率来估计概率</p>
<p>非监督学习：</p>
<p> Baum-Welch– 本质即EM</p>
<p>2、概率计算过程 ，即建好的模型下(五要素已知下)，当前观测序列的联合概率</p>
<p>直接计算 – 本质就是个暴力穷举</p>
<p>前向算法、后向算法  – 本质和viterbi一样也是DP优化</p>
<p>3、预测过程 ，即预测出当前时刻，哪个隐状态概率最大</p>
<p>近似算法 – 复杂度高且是局部最优：即使当转移概率为0时，仍可能出现在预测出来的状态序列里</p>
<p>viterbi算法 – 本质就是个动态规划</p>
<p>4、filter问题</p>
<p>略</p>
<p>5、smoothing问题</p>
<p>略</p>
<p>这些算法并不需要太复杂的证明，都比较直观。这也是HMM好理解的原因。</p>
<p>感兴趣来bili  一键三连。</p>
<h2 id="06-MEMM最大熵马尔科夫模型"><a href="#06-MEMM最大熵马尔科夫模型" class="headerlink" title="06 MEMM最大熵马尔科夫模型"></a>06 MEMM最大熵马尔科夫模型</h2><p>整个模型的框架和HMM一样，不同之处在于：</p>
<p>1、观测独立的假设被打破</p>
<p>2、局部采用LR(最大熵模型)</p>
<p>3、因为2，建模过程就通过梯度下降</p>
<h3 id="MEMM的概率图"><a href="#MEMM的概率图" class="headerlink" title="MEMM的概率图"></a>MEMM的概率图</h3><p>来自google图片：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memm.png" alt="img"></p>
<p>从图中可见：</p>
<p>1、根据01概率图的基础知识，MEMM中，当y_t已知，这种是个v字形的有向图，则x_t和y_t-1 、x_t-1 是相关的–即路径是连通的。因此打破了HMM的观测独立假设。</p>
<p>2、这里是判别模型，因此对P(Y|X,θ)建模</p>
<p>3、“x_t-&gt;y_t” 每个这个箭头的过程，都是一个逻辑回归LR。因此，每一个这样的局部都需要进行一次softmax，而softmax中e的指数是：w*f( h, y_t) ，也叫做质量分数(mass score)(这个mass来自pmf的m)，其中h是 (y_t-1, x_1:t)</p>
<p>4、概率计算：</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memmp.jpg" alt="img"></p>
<p>这里，就是softmax得到的隐状态y的概率，加上log就是最大似然</p>
<p>其中，t为第几个隐状态，i为隐状态值，o为观测值，a为不同特征函数个数–自定义，Z为softmax的归一化。</p>
<p>特征函数是自己定义的，可求导即可。(本质即为，从某个词到某个词性的函数)。特征函数权重是可训练参数。</p>
<p>相当于dfm的子网、graphsage或者GCN的聚合子网。</p>
<p>又找到了一个图，这个更清楚：👈🏻左边为HMM 右边👉🏻为MEMM</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memm3.png" alt="img"></p>
<h3 id="MEMM问题"><a href="#MEMM问题" class="headerlink" title="MEMM问题"></a>MEMM问题</h3><p>和HMM一样，涉及到建模、预测。</p>
<p>区别的地方是：</p>
<p>建模：MEMM的loss是-log极大似然，然后梯度下降去求。</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/memmloss.png" alt="img"></p>
<p>这里，对n个样本，每个样本长度为mi，的softmax结果求log，然后加了正则。</p>
<p>最大熵推导出softmax。整个loss的本质框架是MLE。</p>
<h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2><p>补充：</p>
<p>来自google图片：</p>
<p>NB在sequence建模下拓展到了HMM；LR在sequence建模下拓展到了CRF。</p>
<p><img src="/2021/04/14/CRF%E5%89%8D%E7%BB%AD%E7%9F%A5%E8%AF%86/more.png" alt="img"></p>
<p>reference：</p>
<p><a href="http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf">http://www.cs.columbia.edu/~mcollins/fall2014-loglineartaggers.pdf</a></p>
<p><a href="http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf">http://www.ai.mit.edu/courses/6.891-nlp/READINGS/maxent.pdf</a></p>
<p><a href="https://blog.csdn.net/taoqick/article/details/102672110">https://blog.csdn.net/taoqick/article/details/102672110</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/37163081">https://zhuanlan.zhihu.com/p/37163081</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/33397147">https://zhuanlan.zhihu.com/p/33397147</a></p>
<p><a href="https://www.quora.com/What-is-the-best-resource-to-understand-Conditional-Random-Fields">https://www.quora.com/What-is-the-best-resource-to-understand-Conditional-Random-Fields</a></p>
<p>谷歌图片</p>
<p><a href="https://zhuanlan.zhihu.com/p/113187662">https://zhuanlan.zhihu.com/p/113187662</a></p>
<p><a href="https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://en.wikipedia.org/&amp;httpsredir=1&amp;article=1162&amp;context=cis_papers">https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://en.wikipedia.org/&amp;httpsredir=1&amp;article=1162&amp;context=cis_papers</a></p>
<p><a href="https://www.bilibili.com/video/BV19t411R7QU?p=3">https://www.bilibili.com/video/BV19t411R7QU?p=3</a></p>
]]></content>
      <categories>
        <category>算法</category>
        <category>ml</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>Decorator</title>
    <url>/2021/02/04/Decorator/</url>
    <content><![CDATA[<h2 id="functional-programming-concepts"><a href="#functional-programming-concepts" class="headerlink" title="functional programming concepts"></a><strong>functional programming concepts</strong></h2><p>分两类：</p>
<ul>
<li>Function decorators</li>
<li>Class decorators<span id="more"></span></li>
</ul>
<blockquote>
<p>A decorator in Python is any callable Python object that is used to <strong>modify a function or a class</strong>. A reference to a function “func” or a class “C” is passed to a decorator and the decorator returns a modified function or class. The modified functions or classes usually contain calls to the original function “func” or class “C”. </p>
</blockquote>
<h2 id="show-me-the-code"><a href="#show-me-the-code" class="headerlink" title="show me the code"></a><strong>show me the code</strong></h2><ul>
<li><p><em>参数检查</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def argument_test_natural_number(f):</span><br><span class="line">    def helper(x):</span><br><span class="line">        if type(x) &#x3D;&#x3D; int and x &gt; 0:</span><br><span class="line">            return f(x)</span><br><span class="line">        else:</span><br><span class="line">            raise Exception(&quot;Argument is not an integer&quot;)</span><br><span class="line">    return helper</span><br><span class="line">    </span><br><span class="line">@argument_test_natural_number</span><br><span class="line">def factorial(n):</span><br><span class="line">    if n &#x3D;&#x3D; 1:</span><br><span class="line">        return 1</span><br><span class="line">    else:</span><br><span class="line">        return n * factorial(n-1)</span><br><span class="line"></span><br><span class="line">for i in range(1,10):</span><br><span class="line">	print(i, factorial(i))</span><br><span class="line"></span><br><span class="line">print(factorial(-1))</span><br></pre></td></tr></table></figure></li>
<li><p><em>统计次数</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def call_counter(func):</span><br><span class="line">    def helper(*args, **kwargs):</span><br><span class="line">        helper.calls +&#x3D; 1</span><br><span class="line">        return func(*args, **kwargs)</span><br><span class="line">    helper.calls &#x3D; 0</span><br><span class="line"></span><br><span class="line">    return helper</span><br><span class="line"></span><br><span class="line">@call_counter</span><br><span class="line">def succ(x):</span><br><span class="line">    return x + 1</span><br><span class="line"></span><br><span class="line">@call_counter</span><br><span class="line">def mul1(x, y&#x3D;1):</span><br><span class="line">    return x*y + 1</span><br><span class="line"></span><br><span class="line">print(succ.calls)</span><br><span class="line">for i in range(10):</span><br><span class="line">    succ(i)</span><br><span class="line">mul1(3, 4)</span><br><span class="line">mul1(4)</span><br><span class="line">mul1(y&#x3D;3, x&#x3D;2)</span><br><span class="line">    </span><br><span class="line">print(succ.calls)</span><br><span class="line">print(mul1.calls)</span><br></pre></td></tr></table></figure></li>
<li><p><em>含参数的decorator</em></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def greeting(expr):</span><br><span class="line">    def greeting_decorator(func):</span><br><span class="line">        def function_wrapper(x):</span><br><span class="line">            print(expr + &quot;, &quot; + func.__name__ + &quot; returns:&quot;)</span><br><span class="line">            func(x)</span><br><span class="line">        return function_wrapper</span><br><span class="line">    return greeting_decorator</span><br><span class="line"></span><br><span class="line">@greeting(&quot;-wyq-&quot;)</span><br><span class="line">def foo(x):</span><br><span class="line">    print(42)</span><br><span class="line"></span><br><span class="line">foo(&quot;Hi&quot;)</span><br></pre></td></tr></table></figure></li>
<li><p><em>using import 要注意变量的域</em></p>
</li>
</ul>
<p>greeting_decorator.py  没用functools的版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def greeting(func):</span><br><span class="line">    def function_wrapper(x):</span><br><span class="line">        &quot;&quot;&quot; function_wrapper of greeting &quot;&quot;&quot;</span><br><span class="line">        print(&quot;Hi, &quot; + func.__name__ + &quot; returns:&quot;)</span><br><span class="line">        return func(x)</span><br><span class="line">    function_wrapper.__name__ &#x3D; func.__name__</span><br><span class="line">    function_wrapper.__doc__ &#x3D; func.__doc__</span><br><span class="line">    function_wrapper.__module__ &#x3D; func.__module__</span><br><span class="line">    return function_wrapper</span><br></pre></td></tr></table></figure>
<p>anotherfile.py:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from greeting_decorator import greeting</span><br><span class="line"></span><br><span class="line">@greeting</span><br><span class="line">def f(x):</span><br><span class="line">    &quot;&quot;&quot; just some silly function &quot;&quot;&quot;</span><br><span class="line">    return x + 4</span><br><span class="line"></span><br><span class="line">f(10)</span><br><span class="line">print(&quot;function name: &quot; + f.__name__)</span><br><span class="line">print(&quot;docstring: &quot; + f.__doc__)</span><br><span class="line">print(&quot;module name: &quot; + f.__module__)</span><br></pre></td></tr></table></figure>
<p>greeting_decorator.py</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from functools import wraps</span><br><span class="line"></span><br><span class="line">def greeting(func):</span><br><span class="line">    @wraps(func)</span><br><span class="line">    def function_wrapper(x):</span><br><span class="line">        &quot;&quot;&quot; function_wrapper of greeting &quot;&quot;&quot;</span><br><span class="line">        print(&quot;Hi, &quot; + func.__name__ + &quot; returns:&quot;)</span><br><span class="line">        return func(x)</span><br><span class="line">    return function_wrapper</span><br></pre></td></tr></table></figure>
<p>否则将返回</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">function name: function_wrapper</span><br><span class="line">docstring:  function_wrapper of greeting </span><br><span class="line">module name: greeting_decorator</span><br></pre></td></tr></table></figure>
<ul>
<li><em><code>__call__</code></em><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Fibonacci:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.cache &#x3D; &#123;&#125;</span><br><span class="line">    def __call__(self, n):</span><br><span class="line">        if n not in self.cache:</span><br><span class="line">            if n &#x3D;&#x3D; 0:</span><br><span class="line">                self.cache[0] &#x3D; 0</span><br><span class="line">            elif n &#x3D;&#x3D; 1:</span><br><span class="line">                self.cache[1] &#x3D; 1</span><br><span class="line">            else:</span><br><span class="line">                self.cache[n] &#x3D; self.__call__(n-1) + self.__call__(n-2)</span><br><span class="line">        return self.cache[n]</span><br><span class="line"></span><br><span class="line">fib &#x3D; Fibonacci()</span><br><span class="line"></span><br><span class="line">for i in range(15):</span><br><span class="line">    print(fib(i), end&#x3D;&quot;, &quot;)</span><br></pre></td></tr></table></figure></li>
<li><em>using class</em><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class decorator2:</span><br><span class="line">    </span><br><span class="line">    def __init__(self, f):</span><br><span class="line">        self.f &#x3D; f</span><br><span class="line">        </span><br><span class="line">    def __call__(self):</span><br><span class="line">        print(&quot;Decorating&quot;, self.f.__name__)</span><br><span class="line">        self.f()</span><br><span class="line"></span><br><span class="line">@decorator2</span><br><span class="line">def foo():</span><br><span class="line">    print(&quot;inside foo()&quot;)</span><br><span class="line"></span><br><span class="line">foo()</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepFm</title>
    <url>/2021/03/08/DeepFm/</url>
    <content><![CDATA[<br>

<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>在推荐领域较流行的深度模型方案：</p>
<p>实战：阿里MLR、阿里DIN、阿里ESSM、京东强化学习推荐、facebook个性化推荐dlrm、Deep Neural Networks for YouTube Recommendations、华为DeepFM、google2016 wide&amp;deep learning、googleDeep&amp;Cross Network等。</p>
<span id="more"></span>
<p>DeepFm模型及进化：XDeepFM(deep进化)、AFM(加入attention)、FFM(field-aware)、PNN、FNN、NFM等。</p>
<p>其中，DeepFM在论文中通过大量实验证明，DeepFM的AUC和Logloss都优于目前的最好效果。效率上，DeepFM和目前最优的效果的深度模型相当。在Benchmark数据集和商业数据集上，DeepFM效果超过目前所有模型。</p>
<h3 id="Q1：FM解决什么问题？"><a href="#Q1：FM解决什么问题？" class="headerlink" title="Q1：FM解决什么问题？"></a>Q1：FM解决什么问题？</h3><p>1、普通的线性模型，我们都是将各个特征独立考虑的，并没有考虑到特征与特征之间的相互关系。<br>为了表述特征间的相关性，我们采用多项式模型。</p>
<p>在多项式模型中，特征xi与xj的组合用xixj表示。为了简单起见，我们讨论二阶多项式模型。<br>2、与线性模型相比，FM的模型就多了后面特征组合的部分。<br><img src="/2021/03/08/DeepFm/%E4%BA%8C%E9%A1%B9%E5%BC%8F.png" alt="二项式"></p>
<h3 id="Q2：FM参数求解？"><a href="#Q2：FM参数求解？" class="headerlink" title="Q2：FM参数求解？"></a>Q2：FM参数求解？</h3><p>1、公式中，组合部分的特征相关参数共有n(n−1)/2个。在数据很稀疏的情况下xi,xj都不为0的情况非常少，这样将导致ωij无法通过训练得出。</p>
<p>为了求出ωij，我们对每一个特征分量xi引入辅助向量Vi=(vi1,vi2,⋯,vik)。然后，利用vivj^T对ωij进行求解。</p>
<p><img src="/2021/03/08/DeepFm/%E5%BC%95%E5%85%A5V.png" alt="引入V"></p>
<p>2、如何求解V？</p>
<p>推导公式网上到处都有。</p>
<p>3、得到公式推导结果后，对w求导，梯度下降进行训练。</p>
<h4 id="Q3：FFM？"><a href="#Q3：FFM？" class="headerlink" title="Q3：FFM？"></a>Q3：FFM？</h4><p>公式：</p>
<p><img src="/2021/03/08/DeepFm/ffm_gs.png" alt="ffm_gs"></p>
<p>举例：</p>
<p><img src="/2021/03/08/DeepFm/ffm.png" alt="ffm"></p>
<h4 id="Q4：why-DeepFm？"><a href="#Q4：why-DeepFm？" class="headerlink" title="Q4：why DeepFm？"></a>Q4：why DeepFm？</h4><p>1、因子分解机(Factorization Machines, FM)通过对于每一维特征的隐变量内积来提取特征组合。最终的结果也非常好。<br>但是，虽然理论上来讲FM可以对高阶特征组合进行建模，但实际上因为计算复杂度的原因一般都只用到了二阶特征组合。<br>那么对于高阶的特征组合来说，通过多层的神经网络即DNN去解决。</p>
<p>2、One-hot类型的特征输入到DNN中，会导致网络参数太多。<br>如何解决这个问题呢，类似于FFM中的思想，将特征分为不同的field：让Dense Vector进行组合，来表示高阶特征。</p>
<p><img src="/2021/03/08/DeepFm/field.png" alt="field"></p>
<p>3、但是低阶和高阶特征组合隐含地体现在隐藏层中，如果我们希望把低阶特征组合单独建模，然后融合高阶特征组合。<br>=&gt;就得到了DeepFm。</p>
<h4 id="Q5：what-DeepFm？"><a href="#Q5：what-DeepFm？" class="headerlink" title="Q5：what DeepFm？"></a>Q5：what DeepFm？</h4><p>1、有两种融合方式，分别为串行和并行的结构。<br>这里介绍并行结构。</p>
<p><img src="/2021/03/08/DeepFm/structure.png" alt="structure"><br><img src="/2021/03/08/DeepFm/deepfm.png" alt="deepfm"></p>
<p>2、emb部分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#embeddings</span></span><br><span class="line"><span class="comment">#weights[&#x27;feature_embeddings&#x27;] 存放的每一个值其实就是FM中的vik</span></span><br><span class="line">weights[<span class="string">&#x27;feature_embeddings&#x27;</span>] = tf.Variable(</span><br><span class="line">    tf.random_normal([self.feature_size,self.embedding_size],<span class="number">0.0</span>,<span class="number">0.01</span>),</span><br><span class="line">    name=<span class="string">&#x27;feature_embeddings&#x27;</span>)</span><br><span class="line"></span><br><span class="line">weights[<span class="string">&#x27;feature_bias&#x27;</span>] = tf.Variable(tf.random_normal([self.feature_size,<span class="number">1</span>],<span class="number">0.0</span>,<span class="number">1.0</span>),name=<span class="string">&#x27;feature_bias&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">self.embeddings = tf.nn.embedding_lookup(self.weights[<span class="string">&#x27;feature_embeddings&#x27;</span>],self.feat_index) <span class="comment"># N * F * K</span></span><br><span class="line">feat_value = tf.reshape(self.feat_value,shape=[-<span class="number">1</span>,self.field_size,<span class="number">1</span>])</span><br><span class="line">self.embeddings = tf.multiply(self.embeddings,feat_value)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果是离散值，则embedding lookup之后，每个维度✖️1，连续值则✖️连续值。<br>✖️后的结果就是公式里的 Vi,f · Xi</p>
<p>3、dnn部分<br>为了更好的发挥DNN模型学习high-order特征的能力，文中设计了一套子网络结构，将原始的稀疏表示特征映射为稠密的特征向量。<br>子网络设计时的两个要点：</p>
<p>不同field特征长度不同，但是子网络输出的向量需具有相同维度；<br>利用FM模型的隐特征向量V作为网络权重初始化来获得子网络输出向量。文中将FM的预训练V向量作为网络权重初始化替换为直接将FM和DNN进行整体联合训练，从而实现了一个端到端的模型。 （即lookup）</p>
<p>4、fm</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># second order term</span></span><br><span class="line"><span class="comment"># sum-square-part</span></span><br><span class="line">self.summed_features_emb = tf.reduce_sum(self.embeddings,<span class="number">1</span>) <span class="comment"># None * k</span></span><br><span class="line">self.summed_features_emb_square = tf.square(self.summed_features_emb) <span class="comment"># None * K</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># squre-sum-part</span></span><br><span class="line">self.squared_features_emb = tf.square(self.embeddings)</span><br><span class="line">self.squared_sum_features_emb = tf.reduce_sum(self.squared_features_emb, <span class="number">1</span>)  <span class="comment"># None * K</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#second order</span></span><br><span class="line">self.y_second_order = <span class="number">0.5</span> * tf.subtract(self.summed_features_emb_square,self.squared_sum_features_emb)</span><br><span class="line">self.y_second_order = tf.nn.dropout(self.y_second_order,self.dropout_keep_fm[<span class="number">1</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="Q6：扩展，自定义算子？"><a href="#Q6：扩展，自定义算子？" class="headerlink" title="Q6：扩展，自定义算子？"></a>Q6：扩展，自定义算子？</h4><p>比如我们输入是每个item的idor一些离散特征时候。需要对离散特征的各个值–&gt;index构建一个table。然后每次输入转换成对应的index，再根据index去tf.nn.embedding_lookup。离散特征对应的weight dict大小，需要给个预估值，大一些，囊括各种离散、连续的取值总数。</p>
<p>这里就可以对tensorflow里的hashtable算子进行扩展。构建一个table，动态增长index，来了一个新的取值，就对应index++。</p>
<p>实现：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function">Status <span class="title">Find</span><span class="params">(OpKernelContext* ctx, <span class="keyword">const</span> Tensor&amp; key, Tensor* value,</span></span></span><br><span class="line"><span class="function"><span class="params">              <span class="keyword">const</span> Tensor&amp; default_value)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Grab the input tensor</span></span><br><span class="line">	<span class="keyword">const</span> V default_val = default_value.flat&lt;V&gt;()(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">auto</span> A = key.flat&lt;K&gt;();</span><br><span class="line">    <span class="comment">// Create an output tensor</span></span><br><span class="line">	<span class="keyword">auto</span> output_flat = value-&gt;flat&lt;V&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set all but the first element of the output tensor to 0.</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> N = A.size();</span><br><span class="line">    <span class="keyword">int</span> pos = <span class="number">0</span>;</span><br><span class="line">  </span><br><span class="line">    K find_key;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function">tf_shared_lock <span class="title">sl</span><span class="params">(mu_)</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (; pos &lt; N; pos++) &#123;</span><br><span class="line">            find_key = SubtleMustCopyIfIntegral(A(pos));</span><br><span class="line">            <span class="keyword">auto</span> it = table_.find(find_key);</span><br><span class="line">            <span class="keyword">if</span>(it == table_.end()) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                output_flat(pos) = it-&gt;second;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(pos &lt; N) &#123;</span><br><span class="line">        <span class="keyword">int64_t</span> hsize;</span><br><span class="line">        <span class="function">mutex_lock <span class="title">l</span><span class="params">(mu_)</span></span>;</span><br><span class="line">        <span class="keyword">for</span>(; pos &lt; N; pos++) &#123;</span><br><span class="line">            find_key = SubtleMustCopyIfIntegral(A(pos));</span><br><span class="line">            <span class="keyword">auto</span> it = table_.find(find_key);</span><br><span class="line">            <span class="keyword">if</span>(it == table_.end()) &#123;</span><br><span class="line">               hsize = table_.size();</span><br><span class="line">               <span class="keyword">if</span>(hsize &gt;= max_size_) &#123;</span><br><span class="line">                <span class="keyword">return</span> errors::ResourceExhausted(<span class="string">&quot;max size limit&quot;</span>);</span><br><span class="line">               &#125;</span><br><span class="line">               table_.insert(&#123;find_key, hsize&#125;);</span><br><span class="line">               output_flat(pos) = hsize;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                output_flat(pos) = it-&gt;second;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Status::OK();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> dynamic_unique_table <span class="keyword">import</span> DynamicUniqueTable</span><br><span class="line"></span><br><span class="line">table = DynamicUniqueTable(key_dtype=tf.string,</span><br><span class="line">                     value_dtype=tf.int64,</span><br><span class="line">                     default_value=-<span class="number">1</span>,</span><br><span class="line">                     max_ids=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(table.lookup(<span class="string">&quot;asdfetryuijkn&quot;</span>)))</span><br><span class="line">    print(sess.run(table.lookup(<span class="string">&quot;hahahahah&quot;</span>)))</span><br><span class="line">    print(sess.run(table.export()))</span><br><span class="line">    tf.train.Saver().save(sess, <span class="string">&#x27;index_test_model2/&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p>相关命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">本地：</span><br><span class="line">python pai&#x2F;main.py --train_tables&#x3D;.&#x2F;dub_train.csv --test_tables&#x3D;.&#x2F;dub_test.csv --data_type&#x3D;file --discrete_size&#x3D;15 --continue_size&#x3D;14 --epoch&#x3D;30 --buckets&#x3D;.&#x2F; --save_predict&#x3D;True</span><br><span class="line">pai:</span><br><span class="line">组件无法支持多张表输入， 直接在odps的sql页面执行：</span><br><span class="line"></span><br><span class="line">PAI-name tensorflow1120_ext</span><br><span class="line"></span><br><span class="line">-project algo_public</span><br><span class="line">-Dscript&#x3D;&#39;oss:&#x2F;&#x2F;datadrive.oss-cn-shanghai-internal.aliyuncs.com&#x2F;tmp&#x2F;tl_deepfm.tar.gz&#39;</span><br><span class="line">-DentryFile&#x3D;&#39;pai&#x2F;main.py&#39;</span><br><span class="line">-DgpuRequired&#x3D;100</span><br><span class="line">-Dtables&#x3D;&#39;odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_fit_normal,odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_eval_normal&#39;</span><br><span class="line">-Dbuckets&#x3D;&#39;oss:&#x2F;&#x2F;datadrive.oss-cn-shanghai-internal.aliyuncs.com&#x2F;tmp&#x2F;&#39;</span><br><span class="line">-Darn&#x3D;&#39;acs:ram::1872928906167841:role&#x2F;aliyunodpspaidefaultrole&#39;</span><br><span class="line">-DossHost&#x3D;&quot;oss-cn-shanghai-internal.aliyuncs.com&quot;</span><br><span class="line">-DuserDefinedParameters&#x3D;&quot;--train_tables&#x3D;odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_fit_normal--test_tables&#x3D;odps:&#x2F;&#x2F;ypp_recommend&#x2F;tables&#x2F;rec_dub_model_train_data_eval_normal--discrete_size&#x3D;20 --continue_size&#x3D;49 --data_type&#x3D;odps --input_need_hash&#x3D;True --use_input_bn&#x3D;False --epoch&#x3D;100&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>




]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>GAN</title>
    <url>/2021/10/06/GAN/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>GPT</title>
    <url>/2021/07/14/GPT/</url>
    <content><![CDATA[<br>

<br>

<h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><span id="more"></span>

<ol>
<li>GPT 1.0采取预训练+FineTuning两个阶段，它采取<strong>Transformer作为特征抽取器</strong>。</li>
<li>预训练阶段采用<strong>“单向语言模型”作为训练任务</strong>，把语言知识编码到Transformer里。第二阶段，在第一阶段训练好的模型基础上，通过Finetuning来做具体的NLP任务。</li>
<li>Bert基本就是GPT 1.0的结构，除了预训练阶段采取的是“双向语言模型”之外，它们并没什么本质差异。</li>
<li>首先把Transformer模型参数扩容，常规的Transformer Big包含24个叠加的Block，扩容到48层。其次，大量的不同领域的数据，以及数据筛选。之后，GPT 2.0用这些网页做“单向语言模型”。最后，第二阶段的finetune，2.0采用的是无监督地进行下游任务的学习(只是GPT作者想说明在第一阶段Transformer学到了很多通用的包含各个领域的知识)。</li>
<li>BPE输入改动</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>LPR</title>
    <url>/2021/05/09/LPR/</url>
    <content><![CDATA[<br>

<h3 id="LPR"><a href="#LPR" class="headerlink" title="LPR"></a>LPR</h3><br>

<p>目的，替换贷款利率在贷款定价中发挥基准作用。</p>
<br>

<h4 id="影响"><a href="#影响" class="headerlink" title="影响"></a>影响</h4><p>1】利率市场化改革</p>
<p>​    开始LPR的13年，也取消了贷款利率的下限。</p>
<span id="more"></span>

<p>​    没有起到作用：</p>
<p>​        1、政策属性强，盯住的是贷款基准利率而不是货币市场利率 – 盯住MLF 投放规模大、期限合适、使用频率高、央行选定的中期政策利率</p>
<p>​        2、中小行参与度低 ，报价和lpr的使用方面都只有少量银行参与 – 加入银行</p>
<p>​        3、只有1年期限的报价，对其他期限的利率无法提供参照 – 增加5y期</p>
<br>    

<p>意义-新增贷款盯住LPR，LPR盯住MLF。MLF由央行市场化招标形成，兼具灵活(相对于贷款基准利率)和稳定性(相对于货币市场利率)。</p>
<p>提高了LPR的市场化程度，淡化了贷款基准利率影响，促进解决货币市场化利率和贷款基准利率并存的 利率双轨制问题。推动贷款利率市场化改革。</p>
<br>

<p>2】提高利率传导效率</p>
<p>18年通过MLF等扩张性操作投放了流动性后，货币市场利率下降后，贷款利率并没有随之下降。价格型传导机制受阻。</p>
<p>欧美经验中，市场化利率传导路径：</p>
<p>央行制定政策利率（欧元区利率走廊、降准、MLF、美国公开市场操作、隔夜逆回购等操作）-&gt;货币市场利率（同业拆借、回购）-&gt;存款利率（银行的负债分为存款性和借款性，两者具有替代性。如货币市场利率低时瑞银对存款征收年费）-&gt;货币市场利率和存款利率导致负债成本降低，决定了银行的贷款利率（贷款的成本加成定价模型FTP 贷款利率=资金成本+风险成本+业务成本+合理利润）</p>
<br>

<p>传导成立条件：1存款和 同业负债的替代性（监管机构对银行负债结构的考核使得替代性弱，有专门针对存款的考核如存贷比&lt;=3/4、同业负债占比&lt;1/3。因为借款占比更高时则有流动性和交叉风险。处于对稳定性考虑，银行人才稀缺，放开这样的约束不太现实。） 2银行内部完善的FTP（我国存在双轨制，银行内部存在资产负债部和金融市场部，资产负债部进行存贷款定价，参考存贷款基准利率，然后将多余的资金给金融市场部交易，金融市场参考货币市场利率。货币市场作为后发一方对于存贷款利率影响较小）。我国不满足。</p>
<br>

<p>在我国不太可行的欧美路线，央行考虑跳过欧美传导的路径。通过MLF影响LPR，然后LPR传导给贷款利率。”子午谷奇谋“。1、增加了利率的可控性，货币调控向价格型转变创造空间 2增加了MLF中期货币政策利率的影响力，完善了短期利率走廊+中期政策利率的调控框架</p>
<br>

<p>3】降低实体经济融资成本</p>
<p>货币政策传导受阻，中小企业融资问题仍然突出；贷款基准利率环境下，协同保留贷款利率0.9倍隐性下限。</p>
<p>改革后的LPR报价方式，相较于4.35%，新增贷款的基准利率的确有所下降。</p>
<p>基准利率是贷款客户和银行讨价还价的锚，基准利率下降使得企业议价能力上升。并使得银行难以再设置隐形下限。</p>
<br>

<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>1、期限 2、参与度与报价广泛性 3、mlf的政策属性强 4、央行借款占比低，对整体负债成本影响小，传导有限。5、融资贵更多贵在风险成本和业务成本。而非资金成本。需要其他配套措施解决融资问题。6、存款利率不下，仍然参考基准存款利率。存贷差收缩，压低银行利润，扩大基差风险。银行需要下沉资质，增强风控能力。</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM</title>
    <url>/2021/01/31/LSTM/</url>
    <content><![CDATA[<ul>
<li>篇幅稍长，分为四个部分</li>
</ul>
<ol>
<li>background</li>
<li>step-by-step</li>
<li>show me the code</li>
<li>deep thinking</li>
</ol>
<p>codes<br><a href="https://github.com/satyrswang/blog-jianshu/blob/master/LSTM.lua">https://github.com/satyrswang/blog-jianshu/blob/master/LSTM.lua</a></p>
<h2 id="background"><a href="#background" class="headerlink" title="background"></a>background</h2><ul>
<li><p>what?<br>rnn和feedforward network有嘛不同？</p>
<blockquote>
<p>It’s the easiest to implement an RNN just as a feedforward network with some parts of the input feeding into the middle of the stack, and a bunch of outputs coming out from there as well. There is no magic internal state kept in the network. It’s provided as a part of the input!</p>
</blockquote>
<span id="more"></span>
<p>只是把隐层有拎出来作为下一个隐层的input。=_=<br>然而，理论支持吗？</p>
</li>
<li><p>Problem</p>
<ul>
<li>视频那么多帧，前一帧连着后一帧，间隔又短，那么是否可用前一帧来预测后一帧？<br>看情况。</li>
<li>完形填空 the clouds are in the ___<br>I grew up in France… I speak fluent <em>French</em>.<br>当gap变大，France和<em>French</em>距离那么远，RNN没用了。</li>
<li>为什么gap大了，就没用了？理论证明如下：<br><a href="http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf">Bengio, et al. (1994)</a><br><a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf">Hochreiter (1991) German</a></li>
</ul>
</li>
<li><p>然而，LSTMs 不会因为gap惹事儿。<br>Long Short Term Memory networks<br><img src="/2021/01/31/LSTM/chain.webp" alt="chain"></p>
</li>
</ul>
<h2 id="step-by-step"><a href="#step-by-step" class="headerlink" title="step-by-step"></a>step-by-step</h2><ul>
<li>图第二个干嘛了？你先别看图，听我讲：<br>注意这里横着看，看的是chain中第t个</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#过程1 --名字是 input transform</span><br><span class="line">输入 ：input中的x(t)，chain中前一个x输出的结果h(t-1)</span><br><span class="line">参数 ：x的权重w1，h的权重w2，加一个bias</span><br><span class="line">激活函数 ：tanh</span><br></pre></td></tr></table></figure>
<p>以上得到一个结果记为c_in</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#过程2 --名字是 三个gates，每个gate如下</span><br><span class="line">输入 ：input中的x(t)，chain中前一个x输出的结果h(t-1)</span><br><span class="line">参数 ：x的权重w1，h的权重w2，加一个bias</span><br><span class="line">激活函数 ：g</span><br></pre></td></tr></table></figure>
<p>得到三个结果记为i , f , o<br>先保留一个问题： 过程1、2的输入虽然都是x h变量，但是是一样的吗？还是x h这两个向量的部分值呢？<br>有了c_in,i , f , o 之后干嘛，我怎么得到这一层的h？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#过程3 -- 名字是 state update </span><br><span class="line">输入 ：c_in,i , f , o ,c_out(t-1)</span><br><span class="line">输出 ：新的h(t)， c_out(t)</span><br></pre></td></tr></table></figure>
<p>c_out(t-1) 是chain中前一个的输出呗。h、c_out怎么计算的？<br>c_out(t)  =  f * c_out(t-1) + i * c_in<br>h(t)  =  o * tanh(c_out(t))</p>
<ul>
<li>就这么简单？<br>是的。为什么能这样呢？<blockquote>
<p>Because of the <strong>gating mechanism</strong> the cell can keep a piece of information for long periods of time during work and <strong>protect the gradient inside the cell from harmful changes during the training</strong>. Vanilla LSTMs don’t have <strong>a forget gate</strong> and add unchanged cell state during the update (it can be seen as a recurrent connection with a constant weight of 1), what is often referred to as a Constant Error Carousel (CEC). It’s called like that, because <strong>it solves a serious RNN training problem of vanishing and exploding gradients</strong>, which in turn makes it possible to learn long-term relationships.</p>
</blockquote>
</li>
</ul>
<p>原来，因为有个<strong>gating mechanism</strong> 就是 过程2 嘛，解决了RNN的gradient的问题。为什么能解决<strong>vanishing and exploding gradients</strong>的问题呢？理论支持去看论文。</p>
<h2 id="show-me-the-code"><a href="#show-me-the-code" class="headerlink" title="show me the code"></a>show me the code</h2><p>基于 Torch7</p>
<ul>
<li>snippet1: inputs<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local inputs &#x3D; &#123;&#125;</span><br><span class="line">table.insert(inputs, nn.Identity()())   -- x(t)</span><br><span class="line">table.insert(inputs, nn.Identity()())   -- c_out(t-1)</span><br><span class="line">table.insert(inputs, nn.Identity()())   -- h(t-1)</span><br><span class="line">local input &#x3D; inputs[1]</span><br><span class="line">local prev_c &#x3D; inputs[2]</span><br><span class="line">local prev_h &#x3D; inputs[3]</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li><p>  想想看我们要什么？你回答完了之后，听我讲：<br>三个变量 ：过程1、2要的x(t) h(t-1)和过程3还要的c_out(t-1)</p>
</li>
<li><p>  怎么得到？<br>这里用到了<code>nn.Identity()()</code> 和 <code>table.insert</code></p>
<blockquote>
<p>The array-like objects in lua are called tables.<br>nn.Identity() - passes on the input (used as a placeholder for input)</p>
</blockquote>
</li>
</ol>
<p>如果你用tf，那么nn.Identify就是placeholder</p>
<ul>
<li>snippet2: Computing gate values<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local i2h &#x3D; nn.Linear(input_size, 4 * rnn_size)(input) </span><br><span class="line">local h2h &#x3D; nn.Linear(rnn_size, 4 * rnn_size)(prev_h)   </span><br><span class="line">local preactivations &#x3D; nn.CAddTable()(&#123;i2h, h2h&#125;)    </span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li> <code>4 * rnn_size</code>什么鬼？<br>过程1、2在激活前是不是都是x(t) h(t-1)的线性变换？即<code>nn.Linear</code>。<br><code>preactivations</code>将i2h, h2h作加法运算返回一个vector。<br>我们将线性变换的结果分成4份，每份<code>rnn_size</code>多个值。为什么分为4份？记得我们有三个gates吗 ，得到i,f,o？<blockquote>
<p>The first will be used for <strong>i</strong>n gates, second for <strong>f</strong>orget gates, third for <strong>o</strong>ut gates and the last one <strong>as a cell input</strong> .</p>
</blockquote>
</li>
</ol>
<p>就跟玩儿似的。这里<strong>as a cell input</strong>就是直赋值给了h(t)，作为chain下一个的输入。也解释了之前的保留问题，即输入并不是一样的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local pre_sigmoid_chunk &#x3D; nn.Narrow(2, 1, 3 * rnn_size)(preactivations)</span><br><span class="line">local all_gates &#x3D; nn.Sigmoid()(pre_sigmoid_chunk)</span><br><span class="line">local in_chunk &#x3D; nn.Narrow(2, 3 * rnn_size + 1, rnn_size)(preactivations)</span><br><span class="line">local in_transform &#x3D; nn.Tanh()(in_chunk)</span><br><span class="line">local in_gate &#x3D; nn.Narrow(2, 1, rnn_size)(all_gates)</span><br><span class="line">local forget_gate &#x3D; nn.Narrow(2, rnn_size + 1, rnn_size)(all_gates)</span><br><span class="line">local out_gate &#x3D; nn.Narrow(2, 2 * rnn_size + 1, rnn_size)(all_gates)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><p> <code>nn.Narrow</code>什么鬼？</p>
<blockquote>
<p>select appropriate parts of the preactivation vector.</p>
</blockquote>
</li>
<li><p>  其他很简单啊，前3份传入gates要<code>nn.Sigmoid()</code>激活。3另一份只需要<code>nn.Tanh()</code>激活。</p>
</li>
</ol>
<ul>
<li><p>snippet3: Cell and hidden state<br>gates结果i f o也有了。进入过程3了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local c_forget &#x3D; nn.CMulTable()(&#123;forget_gate, prev_c&#125;)</span><br><span class="line">local c_input &#x3D; nn.CMulTable()(&#123;in_gate, in_transform&#125;)</span><br><span class="line">local next_c &#x3D; nn.CAddTable()(&#123; c_forget, c_input&#125;)</span><br><span class="line">local c_transform &#x3D; nn.Tanh()(next_c)</span><br><span class="line">local next_h &#x3D; nn.CMulTable()(&#123;out_gate, c_transform&#125;)</span><br></pre></td></tr></table></figure>
<p>按公式计算。没说的。得到<code>next_c</code>和<code>next_h</code></p>
</li>
<li><p>snippet4: define module</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">outputs &#x3D; &#123;&#125;</span><br><span class="line">table.insert(outputs, next_c)</span><br><span class="line">table.insert(outputs, next_h)</span><br><span class="line">return nn.gModule(inputs, outputs)</span><br></pre></td></tr></table></figure></li>
<li><p>手残党的snippet5: 栗子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">require &#39;nn&#39;</span><br><span class="line">require &#39;nngraph&#39;</span><br><span class="line">LSTM &#x3D; require &#39;LSTM.lua&#39;  --以上snippet</span><br><span class="line">--创建3层LSTM，输入3输出3</span><br><span class="line">network &#x3D; &#123;LSTM.create(3, 4), LSTM.create(4, 4), LSTM.create(4, 3)&#125;</span><br><span class="line">--准备</span><br><span class="line">local x &#x3D; torch.randn(1, 3)</span><br><span class="line">local previous_state &#x3D; &#123;</span><br><span class="line">  &#123;torch.zeros(1, 4), torch.zeros(1,4)&#125;,</span><br><span class="line">  &#123;torch.zeros(1, 4), torch.zeros(1,4)&#125;,</span><br><span class="line">  &#123;torch.zeros(1, 3), torch.zeros(1,3)&#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#x3D; nil</span><br><span class="line">next_state &#x3D; &#123;&#125;</span><br><span class="line"></span><br><span class="line">--feed数据</span><br><span class="line">local layer_input &#x3D; &#123;x, table.unpack(previous_state[1])&#125;</span><br><span class="line">for l &#x3D; 1, #network do</span><br><span class="line">  local layer_output &#x3D; network[l]:forward(layer_input)</span><br><span class="line">  table.insert(next_state, layer_output)</span><br><span class="line">  local layer_h &#x3D; layer_output[2]</span><br><span class="line">  if l &lt; #network then</span><br><span class="line">    layer_input &#x3D; &#123;layer_h, table.unpack(previous_state[l + 1])&#125;</span><br><span class="line">  else</span><br><span class="line">    output &#x3D; layer_h</span><br><span class="line">  end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">print(next_state)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="deep-thinking"><a href="#deep-thinking" class="headerlink" title="deep thinking"></a>deep thinking</h2><p>尽管已经很长了。还是要写理解。这时你可以看图了。</p>
<blockquote>
<p>what information we’re going to throw away from the cell state<br> what new information we’re going to store in the cell state</p>
</blockquote>
<p>1、 什么是forget gate？</p>
<ul>
<li>其实就是将x h线性变换后做一个sigmoid， 如果结果是0，代表forget  c_out(t-1)。</li>
<li>这个例子非常好：<blockquote>
<p>the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.</p>
</blockquote>
</li>
</ul>
<p>2、 i 和 c_in?</p>
<ul>
<li>两步，第一步i，i = 1相当于是确定哪些值我们需要update或者说需要更新输入的多大成分，想象为将c_in scale了i倍；而tanh相当于为需要更新的值确定了更新成什么c_in。</li>
<li>相乘，则确定了新的候选值，再与f相加，我们便确定了新的状态。</li>
</ul>
<blockquote>
<p>we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.</p>
</blockquote>
<p>3、 那么输出什么？</p>
<ul>
<li>首先我们需要确定哪些更新后的状态需要输出，用sigmoid，得到的o就是我们想要输出的部分。 </li>
<li> 然后 基于更新好的状态c_out(t)，将其tanh控制在[-1,1]之间。乘以o，输出我们要输出的。<blockquote>
<p>since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output <strong>whether the subject is singular or plural</strong>, so that we know what form a verb should be conjugated into if that’s what follows next.</p>
</blockquote>
</li>
</ul>
<p>4、 各类变种<br> <a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf">Gers &amp; Schmidhuber (2000)</a><br><a href="http://arxiv.org/pdf/1406.1078v3.pdf">Cho, et al. (2014)</a><br><a href="http://arxiv.org/pdf/1508.03790v2.pdf">Yao, et al. (2015)</a><br><a href="http://arxiv.org/pdf/1402.3511v1.pdf">Koutnik, et al. (2014)</a></p>
<p>5、 比对各类变种的结论<br><a href="http://arxiv.org/pdf/1503.04069.pdf">Greff, et al. (2015)</a><br><a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz, et al. (2015)</a></p>
<p>欢迎补充材料。<br>reference:<br><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>News投资逻辑摘抄</title>
    <url>/2021/05/12/News%E6%8A%95%E8%B5%84%E9%80%BB%E8%BE%91%E6%91%98%E6%8A%84/</url>
    <content><![CDATA[<br>
<br>



<h1 id="David-Swensen："><a href="#David-Swensen：" class="headerlink" title="David Swensen："></a>David Swensen：</h1><span id="more"></span>

<p>1、Led by Tobin’s ideas, he stressed<strong>asset allocation rather than stock picking, or attempts to time the market</strong> — beyond the mechanical market timing that came with his policy of regularly rebalancing Yale’s portfolio. At the margin, that entailed <strong>buying more of assets that had done badly and selling some of those that had done well</strong>.</p>
<p>2、With the public markets deeply liquid and exhaustively researched, there was no point in trying to beat them. But <strong>private markets were less efficient</strong>, and he could reasonably hope to find bargains if his team was smart enough. </p>
<p>3、The first was that equities were indubitably better than bonds or cash for the longer term — and that “equities” should not be restricted merely to shares traded on public stock exchanges but should include <strong>any investment with a non-guaranteed upside for the investor</strong>. His second was that diversification was important. </p>
<p>4、 He regularly attacked excessive fees in his later years. If his team couldn’t find any place where Yale’s long-term horizon might give them a chance to beat the market, he might even have left money in index funds.</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>Report读后感</title>
    <url>/2021/05/09/Report%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<br>

<br>

<h3 id="Financial-Stability-Report读后感"><a href="#Financial-Stability-Report读后感" class="headerlink" title="Financial Stability Report读后感"></a>Financial Stability Report读后感</h3><p>1、Asset Valuations</p>
<p>Size of Selected Asset Markets各类资产价格涨幅</p>
<span id="more"></span>

<p>Yields on Nominal Treasury Securities 2、10年名义国债利率</p>
<p>Term Premium on 10-Year Nominal Treasury Securities 10年国债期限溢价</p>
<p>Implied volatility of 10-Year Swap rate 110互换利率隐含波动率</p>
<p>Treasury market Depth –</p>
<p>1、时限，给定价格和数量，多快成交 </p>
<p>2、深度，给定时间和价格，多大数量</p>
<p> 3、宽度，给定时限和数量，多低价格</p>
<p><strong>多样性</strong>是指两个队伍中的人不是因为同一原因而排在这个队的，否则整个队伍会一起出现或者一起离开，对流动性不利<br><strong>恢复性</strong>是指对如果有一个队伍的人突然被清空，那么重新排成类似的队伍需要的时间，时间越短流动性越好。</p>
<p>Corporate Bond Yields</p>
<p>Corporate Bond Spreads to Similar-maturity Treasury Securities   Source: ICe Data Indices, LL</p>
<p>低评级公司债和可比到期国债利率差价变低。</p>
<p>excess Bond Premium</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow源码解读01</title>
    <url>/2021/04/16/TensorFlow%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB01/</url>
    <content><![CDATA[<br>

<p>架构</p>
<span id="more"></span>



<p>1、在网络通信、硬件上开发变量、基础op和一些函数。在之上有执行器、分布式。然后是api接口和应用。</p>
<p>2、核心为，通信、设备管理、op对数据操作、图计算。</p>
<p>3、core目录包含了核心模块。public和client，一个是api接口头文件、一个是api接口实现。platform为和os相关接口文件。protobuf为数据传输的结构化序列。framework包含log、tensor、memory、registey等。graph，distributed，都好理解。kernels是核心的op，ops为梯度、io等op。lib为公共库，如hash等。stream_executor是并行计算框架。common_runtime有session、threadpool、executor管理等。contrib是contributor贡献。gpus封装了cuda cudnn编程库。</p>
<p>4、tensor相关操作，slice add reshape reduce shuffle 等。<code>tensorbuffer</code>指针，指向Eigen::Tensor</p>
<p>5、</p>
<p>Python所构建好的graph模型，会在底下悄悄地生成一个由GraphDef表示的图结构来。然后我们使用Python等语言里的Session具体去分配内存，初使化参数，运行计算图时，TF的后端会将我们前一步所构建的GraphDef转化为一个可执行的Graph。</p>
<p>构建scope，即new graph，包含着op registry。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//static OpRegistry* global_op_registry = new OpRegistry; graph里包含 registry</span></span><br><span class="line">Graph* graph = <span class="keyword">new</span> Graph(OpRegistry::Global());</span><br><span class="line"></span><br><span class="line"><span class="comment">// ShapeRefiner performs shape inference for TensorFlow Graphs.  It is</span></span><br><span class="line"><span class="comment">// responsible for instantiating InferenceContext objects for each</span></span><br><span class="line"><span class="comment">// Node in the Graph, and providing/storing the &#x27;input_tensor&#x27; Tensors</span></span><br><span class="line"><span class="comment">// used by Shape Inference functions, when available at graph</span></span><br><span class="line"><span class="comment">// construction time.</span></span><br><span class="line">ShapeRefiner* refiner =</span><br><span class="line">      <span class="keyword">new</span> ShapeRefiner(graph-&gt;versions(), graph-&gt;op_registry());</span><br><span class="line">  <span class="keyword">return</span> Scope(<span class="keyword">new</span> Impl(graph, <span class="keyword">new</span> Status, <span class="keyword">new</span> Impl::NameMap, refiner,</span><br><span class="line">                        <span class="comment">/* disable_shape_inference */</span> <span class="literal">false</span>));</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>tensorflow源码</category>
      </categories>
      <tags>
        <tag>tensorflow源码</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow源码解读02</title>
    <url>/2021/04/16/TensorFlow%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB02/</url>
    <content><![CDATA[<br>

<p>Tensor、op相关</p>
<span id="more"></span>



<p>从api开始挖。然后再上升到框架。然后再topdown地整合。</p>
<p>友好小白。</p>
<br>

<p>1、Tensor\TensorShape</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建tensor - constructor包含TensorShape</span></span><br><span class="line"><span class="comment">//TensorShape继承自Base ，constructor传入一个initializer_list</span></span><br><span class="line"><span class="function"><span class="keyword">explicit</span> <span class="title">TensorShapeBase</span><span class="params">(gtl::ArraySlice&lt;int64&gt; dim_sizes)</span></span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<br>

<p>2、DeepCopy</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//主要两个步骤 ，在tensor_utils中</span></span><br><span class="line"><span class="comment">//1 数据的memcpy -- 这里用到了string_view，use StringPiece as a convenient map over the tensor buffer</span></span><br><span class="line"><span class="keyword">using</span> StringPiece = absl::string_view;</span><br><span class="line">StringPiece(<span class="keyword">static_cast</span>&lt;<span class="keyword">char</span>*&gt;(buf_-&gt;data()), TotalBytes());</span><br><span class="line"><span class="built_in">memcpy</span>(<span class="keyword">const_cast</span>&lt;<span class="keyword">char</span>*&gt;(output_data.data()), input_data.data(), input_data.size());</span><br><span class="line"></span><br><span class="line"><span class="comment">//2 类型 和 维度</span></span><br><span class="line">output-&gt;unaligned_flat&lt;Variant&gt;() =input.unaligned_flat&lt;Variant&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//这里unaligned_flat，为unaligned_shaped&lt;T, 1&gt;(&#123;NumElements()&#125;);</span></span><br><span class="line"><span class="comment">//主要检查type ，用span来填充维度：FillDimsAndValidateCompatibleShape，添入Eigen的dims并返回Eigen的tensor</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> ArraySlice = absl::Span&lt;<span class="keyword">const</span> T&gt;;</span><br><span class="line"><span class="keyword">typedef</span> Eigen::TensorMap&lt;Eigen::Tensor&lt;T, NDIMS, Eigen::RowMajor, IndexType&gt; &gt;</span><br><span class="line">      UnalignedTensor;</span><br></pre></td></tr></table></figure>

<p>类似的操作如 slice concat split等不再赘述。</p>
<br>

<p>3、gtest</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">   <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values_to_test = LoadValuesFromConfig();</span><br><span class="line">   RegisterMyTests(values_to_test);</span><br><span class="line">   ...</span><br><span class="line">   <span class="keyword">return</span> RUN_ALL_TESTS();</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//写法如</span></span><br><span class="line">TEST(TensorProtoUtil, CompressTensorProtoInPlaceTooSmall) &#123;</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">int</span> kLength = <span class="number">63</span>;</span><br><span class="line">  TensorProto tensor_proto =</span><br><span class="line">      tensor::CreateTensorProto(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt;(kLength), &#123;kLength&#125;);</span><br><span class="line">  EXPECT_FALSE(tensor::CompressTensorProtoInPlace(&amp;tensor_proto));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//CreateTensorProto的测试</span></span><br><span class="line"><span class="keyword">auto</span> proto = tensor::CreateTensorProto(values, shape);</span><br><span class="line">EXPECT_EQ(proto.DebugString(),</span><br><span class="line">          <span class="string">&quot;dtype: DT_STRING\n&quot;</span></span><br><span class="line">          <span class="string">&quot;tensor_shape &#123;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  dim &#123;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;    size: 1\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  &#125;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  dim &#123;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;    size: 3\n&quot;</span></span><br><span class="line">          <span class="string">&quot;  &#125;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;&#125;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;string_val: \&quot;a\&quot;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;string_val: \&quot;b\&quot;\n&quot;</span></span><br><span class="line">          <span class="string">&quot;string_val: \&quot;c\&quot;\n&quot;</span>);</span><br></pre></td></tr></table></figure>

<p>tensorflow 的test非常多，都是基于gtest框架。</p>
<br>

<p>4、CreateTensorProto</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//同样两个部分，shape和value进行create。tensor_util.h中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//shape部分是protobuf相关操作</span></span><br><span class="line">internal::SetTensorProtoShape(shape, &amp;tensor_shape_proto);</span><br><span class="line"><span class="comment">//set的方式主要是：循环获得shapeproto中的RepeatedPtrField&lt; ::tensorflow::TensorShapeProto_Dim &gt;</span></span><br><span class="line"><span class="comment">// RepeatedPtrField is like RepeatedField, but used for repeated strings or Messages.</span></span><br><span class="line"><span class="comment">//获得后，再调用其Add方法，将shape的每一维 生成一个element并且set值</span></span><br><span class="line"><span class="function">Element* <span class="title">Add</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//value用internal::TensorProtoHelper&lt;Type&gt; 中的AddValues这个static方法，把value中的值一个个加进proto</span></span><br><span class="line">tensor.set_dtype(TypeHelper::GetDataType());</span><br><span class="line">tensor.mutable_tensor_shape()-&gt;Swap(&amp;tensor_shape_proto);</span><br><span class="line">TypeHelper::AddValues(values.begin(), values.end(), &amp;tensor);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>还有诸如CompressTensorProtoInPlace（主要是CompressRepeatedField，核心两个阈值<code> static const int64 kDefaultMinNumElements = 64;static const float kDefaultMinCompressionRatio = 2.0f;</code>）。</p>
<br>

<p>5、protobuf</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//tensor、op、graph等的proto 涉及到很多protobuf操作，不赘述</span></span><br><span class="line"><span class="comment">//比如Arena中</span></span><br><span class="line"><span class="keyword">auto</span>* p = CreateMaybeMessage&lt;::tensorflow::TensorShapeProto&gt;(GetArenaNoVirtual());</span><br></pre></td></tr></table></figure>

<br>

<p>6、OpRegistry</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// The standard implementation of OpRegistryInterface, along with a</span></span><br><span class="line"><span class="comment">// global singleton used for registering ops via the REGISTER</span></span><br><span class="line"><span class="comment">// macros below.  Thread-safe.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Example registration:</span></span><br><span class="line"><span class="comment">//   OpRegistry::Global()-&gt;Register(</span></span><br><span class="line"><span class="comment">//     [](OpRegistrationData* op_reg_data)-&gt;Status &#123;</span></span><br><span class="line"><span class="comment">//       // Populate *op_reg_data here.</span></span><br><span class="line"><span class="comment">//       return Status::OK();</span></span><br><span class="line"><span class="comment">//   &#125;);</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>









<br>
总结：
1、A `Span<T>` is somewhat analogous to an `absl::string_view`, but for an array of elements of type `T`. A user of `Span` must ensure that the data being pointed to outlives the `Span` itself.

<p>2、string_view </p>
<p>3、# ##  </p>
<p>4、gtest</p>
<p>5、initializer_list</p>
</T>]]></content>
      <categories>
        <category>tensorflow源码</category>
      </categories>
      <tags>
        <tag>tensorflow源码</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow源码解读03</title>
    <url>/2021/05/02/TensorFlow%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB03/</url>
    <content><![CDATA[<p>1、</p>
]]></content>
  </entry>
  <entry>
    <title>batchnorm</title>
    <url>/2021/04/05/batchnorm/</url>
    <content><![CDATA[<br>

<p>bn相关问题</p>
<span id="more"></span>
<ul>
<li><p>白化</p>
<ul>
<li><p>目的</p>
<ul>
<li><p>1）去除特征之间的相关性 —&gt; 独立；</p>
</li>
<li><p>2）使得所有特征具有相同的均值和方差 —&gt; 同分布。</p>
</li>
</ul>
</li>
<li><p>PCA白化保证了所有特征分布均值为0，方差为1</p>
</li>
<li><p>ZCA白化则保证了所有特征分布均值为0，方差相同；</p>
</li>
<li><p>白化操作，固定了每一层网络输入分布，加速网络训练过程的收敛</p>
</li>
</ul>
</li>
<li><p>Internal Covariate Shift</p>
<ul>
<li><p>内容</p>
<ul>
<li><p>covariate shift 就是分布不一致假设之下的一个分支问题<br>它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同</p>
</li>
<li><p>对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大</p>
</li>
<li><p>可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由</p>
</li>
</ul>
</li>
<li><p>问题</p>
<ul>
<li><p>简而言之，每个神经元的输入数据不再是“独立同分布”。</p>
</li>
<li><p>学习速度、饱和区(早停)、影响其他层</p>
</li>
</ul>
</li>
<li><p>解决</p>
<ul>
<li>bn框架<ul>
<li>h = f ( g * {(x-μ)/θ} + b)</li>
</ul>
</li>
<li>经过这么的变回来再变过去，会不会跟没变一样<ul>
<li>再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数</li>
<li>但是新参数有不同的学习动态。</li>
<li>在旧参数中，x的均值取决于下层神经网络的复杂关联；<br>但在新参数中， 仅由 b 来确定，去除了与下层计算的密切耦合。</li>
</ul>
</li>
<li>这样的 Normalization 与标准的白化区别<ul>
<li>标准白化操作的目的是“独立同分布”</li>
<li>变换为均值为 b  、方差为 g^2 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>BN</p>
<ul>
<li><p>分类</p>
<ul>
<li>Batch Normalization<ul>
<li>每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的</li>
<li>分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性</li>
<li>但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。</li>
</ul>
</li>
<li>Layer Normalization </li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>防止模型梯度爆炸</li>
<li>为什么加速收敛<ul>
<li>梯度的方向为垂直等高线的方向而走之字形路线，这样会使迭代很 </li>
</ul>
</li>
</ul>
</li>
<li><p>常见的方法有</p>
<ul>
<li>min-max标准化（min-max normalization）</li>
<li>归一化</li>
<li>log函数转换</li>
<li>atan函数转换</li>
<li>z-score标准化（zero-mena normalization，此方法比较常用）- 模糊量化法</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>算法</category>
        <category>dl</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>bean解析及注册源码</title>
    <url>/2021/03/11/bean%E8%A7%A3%E6%9E%90%E5%8F%8A%E6%B3%A8%E5%86%8C%E6%BA%90%E7%A0%81/</url>
    <content><![CDATA[<br>

<p>S1<code>xml文件等资源类 </code> – 对各种资源类的封装+encode</p>
<span id="more"></span>
<p><img src="https://upload-images.jianshu.io/upload_images/8716089-f03da4d523f26bbb.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="resource.jpg"></p>
<p>S2<code>读取xml文件</code> – 对xml文件的校验、load、read(这里的read调用BeanDefinitionDocumentReader)<br>主要在<code>XmlBeanDefinitionReader</code>中</p>
<p>S3<code>解析属性(xml中标签)</code> –从xml到Bean<br>实现在<code>DefaultBeanDefinitionDocumentReader</code><br><code>doRegisterBeanDefinitions()</code> 中解析了<code>profile</code>属性，并且其中的<code>parseBeanDefinitions()</code>是解析xml的开始。</p>
<p>根据不同的<code>namespace</code>和<code>nodename</code>，分别不同处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123;</span><br><span class="line">	importBeanDefinitionResource(ele);</span><br><span class="line">&#125;</span><br><span class="line">else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123;</span><br><span class="line">	processAliasRegistration(ele);</span><br><span class="line">&#125;</span><br><span class="line">else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123;</span><br><span class="line">	processBeanDefinition(ele, delegate);</span><br><span class="line">&#125;</span><br><span class="line">else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123;</span><br><span class="line">	&#x2F;&#x2F; recurse</span><br><span class="line">	doRegisterBeanDefinitions(ele);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>处理过程涉及到<code>BeanDefinitionParserDelegate BeanDefinitionHolder BeanDefinitionReaderUtils XmlReaderContext</code></p>
<blockquote>
<p>delegate中对元素(属性)进行解析，结果放入holder中，此时holder已经包含了各种属性。再由Utils中将holder进行注册。最后由context将注册结果通知监听器。</p>
</blockquote>
<p>S4<code>注册</code></p>
]]></content>
      <categories>
        <category>工程</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>工程</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>c++tips上</title>
    <url>/2021/04/11/c-tips/</url>
    <content><![CDATA[<br>
<br>


<p>基础汇总</p>
<span id="more"></span>

<br>

<p>c++总的思想角度是，告诉编译器要干嘛。以及内存上的管理。</p>
<p>相关关键词都在指示，将内存如何安排。</p>
<ul>
<li><p>extern</p>
<p> means it is only a declaration, its definition is later or external to this file, asking the compiler not to assign memory or generate code</p>
</li>
</ul>
<ul>
<li><p>内存对齐</p>
</li>
<li></li>
<li><p>构造与析构函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、是否在析构函数抛出异常：</span><br><span class="line">	1异常点之后的程序如果有释放资源，会造成诸如资源泄漏</span><br><span class="line">	2通常异常发生时，c++的机制会调用已经构造对象的析构函数来释放资源，此时若析构函数本身也抛出异常，则前一个异常尚未处理，又有新的异常，会造成程序崩溃的问题。</span><br><span class="line">	3把异常完全封装在析构函数内部，决不让异常抛出函数之外</span><br><span class="line">	</span><br><span class="line">2、构造函数是否可以抛出异常：</span><br><span class="line">	1构造函数可以抛出异常。构造函数中尽量不要抛出异常。</span><br><span class="line">	2既需要分配内存，又需要抛出异常时要特别注意防止内存泄露的情况发生。因为在构造函数中抛出异常，在概念上将被视为该对象没有被成功构造，因此当前对象的析构函数就不会被调用，就会造成内存泄漏。</span><br><span class="line">	3同时，由于构造函数本身也是一个函数，在函数体内抛出异常将导致当前函数运行结束，并释放已经构造的成员对象，包括其基类的成员，即执行直接基类和成员对象的析构函数</span><br><span class="line">	</span><br><span class="line">3、构造函数和析构函数可以调用虚函数吗：</span><br><span class="line">	1当创建某个派生类的对象时，如果在它的基类的构造函数中调用虚函数，那么此时派生类的构造函数并未执行，所调用的函数可能操作还没有被初始化的成员，将导致灾难的发生。</span><br><span class="line">	2即使想在构造函数中实现动态联编，在实现上也会遇到困难。这涉及到对象虚指针（vptr）的建立问题。一个类的构造函数在执行时，并不能保证该函数所能访问到的虚指针就是当前被构造对象最后所拥有的虚指针，因为后面派生类的构造函数会对当前被构造对象的虚指针进行重写，因此无法完成动态联编</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>堆栈</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、堆栈和缓存的区别</span><br><span class="line">	1栈使用的是一级缓存， 他们通常都是被调用时处于存储空间中，调用完毕立即释放；</span><br><span class="line">	2堆是存放在二级缓存中，堆的首地址放在一级缓存缓存中，分配和释放会产生系统调用，由用户态进入内核态，所以速度会慢一些</span><br></pre></td></tr></table></figure></li>
<li><p>安全漏洞、内存越界</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  </span><br></pre></td></tr></table></figure></li>
<li><p>野指针</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">“野指针”不是NULL指针，是指指向“垃圾”内存的指针。即指针指向的内容是不确定的。</span><br><span class="line">产生的原因：</span><br><span class="line">1）指针变量没有初始化。因此，创建指针变量时，该变量要被置为NULL或者指向合法的内存单元。</span><br><span class="line">2）指针p被free之后，没有置为NULL，让人误以为p是个合法的指针。</span><br><span class="line">3）指针跨越合法范围操作。不要返回指向栈内存(非静态局部变量）的指针或引用。</span><br><span class="line"></span><br><span class="line">可能后果：</span><br><span class="line">若操作系统将这部分已经释放的内存重新分配给另外一个进程，而原来的程序重新引用现在的迷途指针，向其中写入数据，则这部分程序内容将被破坏，而导致程序错误。这种类型的程序错误，通常会导致segment fault和一般的保护错误。</span><br><span class="line">其他常见错误：</span><br><span class="line">返回一个基于栈分配的局部变量的地址时，一旦调用的函数返回，分配给这些变量的空间将回收，此时它们拥有的是垃圾值，如return &amp;num，如果要使它的生命周期加长，应该将其声明为static</span><br></pre></td></tr></table></figure></li>
<li><p>STL容器及常见算法</p>
</li>
</ul>
<blockquote>
<p><a href="http://vernlium.github.io/2019/12/29/C-STL%E5%B8%B8%E7%94%A8%E5%AE%B9%E5%99%A8API%E6%80%BB%E7%BB%93/">http://vernlium.github.io/2019/12/29/C-STL%E5%B8%B8%E7%94%A8%E5%AE%B9%E5%99%A8API%E6%80%BB%E7%BB%93/</a></p>
<p><a href="https://blog.csdn.net/weixin_43150428/article/details/82469933">https://blog.csdn.net/weixin_43150428/article/details/82469933</a></p>
</blockquote>
<ul>
<li><p>tuple</p>
</li>
<li><p>array container</p>
</li>
<li><p>range-base for</p>
</li>
<li><p>initializer lists</p>
</li>
<li><p>delegate/inheriting constructors</p>
</li>
<li><p>nullptr</p>
</li>
<li><p>inline</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、内联展开避免函数中断调用开销</span><br><span class="line">2、宏：内联函数在运行时可调试，而宏定义不可以；编译器会对内联函数的参数类型做安全检查或自动类型转换（同普通函数），而宏定义则不会；内联函数可以访问类的成员变量，宏定义则不能；在类中声明同时定义的成员函数，自动转化为内联函数</span><br><span class="line"></span><br><span class="line">inline一般只用于如下情况：</span><br><span class="line">一个函数不断被重复调用函数只有简单的几行，且函数不包含for、while、switch语句，递归。</span><br></pre></td></tr></table></figure></li>
<li><p>unordered_table</p>
</li>
<li><p>shared_ptr/weak_ptr</p>
</li>
<li><p>regexp</p>
</li>
<li><p>cost of exception handling</p>
</li>
<li><p>type trait</p>
</li>
<li><p>strong exception gaurantee</p>
</li>
<li><p>CRTP</p>
</li>
<li><p>smart pointer</p>
</li>
<li><p>std::function and function pointer</p>
</li>
<li><p>runtime cost of lambda function</p>
</li>
<li><p>虚继承</p>
</li>
<li><p>Rvalue reference</p>
</li>
</ul>
<ul>
<li>function/bind 适配器、取反器等</li>
</ul>
<p>call operator被重写了，即()被重写。避免函数调用开销，作为inline。可以加入全局变量，以面向对象的角度考虑。支持泛型。</p>
<p>bind即将某个参数固定为，第一个或第二个操作数。</p>
<p>函数适配器，就像复合函数，将各个类型的函数进行组合。进行一些自定义来能够将自己的函数可以用在适配器里。</p>
<ul>
<li><p>Lambda expression and closure</p>
</li>
<li><p>auto/decltype</p>
</li>
<li><p>static_assert</p>
</li>
<li><p>可变模板参数</p>
</li>
<li><p>指针和引用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、指针可以是null，是对象</span><br><span class="line">2、引用必须初始化，不是对象，且不能重新赋值</span><br><span class="line"></span><br><span class="line">Using Reference when you find </span><br><span class="line">1、It always represents a non-null object</span><br><span class="line">2、And it will not represent any other objects</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>全局作用域</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">::</span><br></pre></td></tr></table></figure></li>
<li><p>static</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">1、定义一个全局变量有许多缺点，最明显的缺点是破坏了此变量的访问范围（使得在此函数中定义的变量，不仅仅只受此函数控制）。static 关键字则可以很好的解决这个问题。</span><br><span class="line"></span><br><span class="line">2、在 C++ 中，需要一个数据对象为整个类而非某个对象服务,同时又力求不破坏类的封装性,即要求此成员隐藏在类的内部，对外不可见时，可将其定义为静态数据。</span><br><span class="line"></span><br><span class="line">即，需要class享有且隐藏在类内，或者，只受某个函数控制而不是受类控制。</span><br><span class="line"></span><br><span class="line">3、</span><br><span class="line">全局（静态）存储区：分为 DATA 段和 BSS 段。这段数据在程序刚开始运行时就完成初始化。</span><br><span class="line"></span><br><span class="line">DATA 段（全局初始化区）存放初始化的全局变量和静态变量；</span><br><span class="line">BSS 段（全局未初始化区）存放未初始化的全局变量和静态变量。程序执行之前已经为0。</span><br><span class="line"></span><br><span class="line">4、面向过程：</span><br><span class="line">静态全局变量 -- 只能在本文件中访问，不能在其它文件中访问，即便是 extern 外部声明也不可以</span><br><span class="line">静态局部变量 -- 程序运行结束以后才释放</span><br><span class="line">静态函数 -- 只能在本文件中调用，不能被其他文件调用</span><br><span class="line"></span><br><span class="line">面向对象：</span><br><span class="line">静态成员变量</span><br><span class="line">静态成员函数</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">static 修饰的变量存放在全局数据区的静态变量区，包括全局静态变量和局部静态变量，都在全局数据区分配内存。初始化的时候自动初始化为 0。</span><br><span class="line">（4）不想被释放的时候，可以使用static修饰。比如修饰函数中存放在栈空间的数组。如果不想让这个数组在函数调用结束释放可以使用 static 修饰。</span><br><span class="line">（5）考虑到数据安全性（当程序想要使用全局变量的时候应该先考虑使用 static）。</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>malloc free 和new delete</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、malloc只分配指定大小的堆内存空间，而new可以根据对象类型分配合适的堆内存空间，当然还可以通过重载operator new 自定义内存分配策略，其次还能够构造对象</span><br><span class="line">2、free释放对应的堆内存空间，delete,先执行对象的析构函数，在释放对象所占空间。</span><br><span class="line">3、malloc与free是C++&#x2F;C 语言的标准库函数，new&#x2F;delete 是C++的运算符。</span><br><span class="line">4、malloc返回类型是void*,使用时需要类型转换，而new在分配时，编译器能够根据对象类型自动计算出大小，返回类型是指向对象类型的指针，其封装了sizeof和类型转换功能，实际上new分为两步，第一步是通过调用operator new函数分配一块合适，原始的，未命名的内存空间，返回类型也是void *,而且operator new可以重载，可以自定义内存分配策略，甚至不做内存分配，甚至分配到非内存设备上，而malloc无能为力，第二步，调用构造函数构造对象，new将调用constructor，而malloc不能；delete将调用destructor，而free不能</span><br><span class="line"></span><br><span class="line">总结：1、大小or分配策略是否可自定义 2、析构和构造函数 3、库函数和运算符(重载) </span><br></pre></td></tr></table></figure></li>
<li><p>const</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、变量 初始化后，不被改变</span><br><span class="line"></span><br><span class="line">2、函数 </span><br><span class="line">1. 修饰返回值 const int func() -- 不能改变返回值</span><br><span class="line">2. 修饰参数 int func(const ) -- 函数体内不能改变参数</span><br><span class="line">3. 修饰成员函数 int func() const -- 函数体内不能改变成员变量的值</span><br><span class="line"></span><br><span class="line">3、指针</span><br><span class="line">1. const int* 、int const*-- 常量指针，pointer指向的变量的值不能变，pointer可指向其他对象</span><br><span class="line">2. int* const -- 指针常量，可以改变指针指向变量的值，pointer不能指向其他对象</span><br><span class="line"></span><br><span class="line">4、对象 const对象只能调用const成员函数，不能调用普通函数</span><br><span class="line"></span><br><span class="line">const int* const p&#x3D;&amp;i;</span><br></pre></td></tr></table></figure></li>
<li><p>多态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、虚函数的类 至少有一个(多继承会有多个)一维的虚函数表叫做虚表(virtual table)，属于类成员，虚表的元素值是虚函数的入口地址，在编译时就已经为其在数据端分配了空间</span><br><span class="line"></span><br><span class="line">2、确定的虚函数对应virtual table中一个固定位置n，n是一个在编译时期就确定的常量，所以，使用vptr加上对应的n，就可以得到对应的函数入口地址。C++采用的这种绝对地址+偏移量的方法调用虚函数，查找速度快执行效率高，时间复杂度为O(1)</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/11/c-tips/vptr.jpg" alt="vptr"></p>
</li>
<li><p>c++ 与java .NET</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一，C++的难不仅仅在于其静态结构体系，还有很多源于语言设计上的包袱，比如对C的兼容，比如没有垃圾收集机制，比如对效率的强调，等等。一旦把这些包袱丢掉，设计的难度确实可以大大下降。</span><br><span class="line"></span><br><span class="line">第二，Java和.NET的核心类库是在C++十几年成功和失败的经验教训基础之上，结合COM体系优点设计实现的，自然要好上一大块。事实上，在Java和.NET核心类库的设计中很多地方，体现的是基于接口的设计，和真正的基于对象的设计。有了这两个主角站台，“面向类的设计”不能喧宾夺主，也能发挥一些好的作用。</span><br><span class="line"></span><br><span class="line">第三，Java和.NET中分别对C++最大的问题——缺少对象级别的delegate机制做出了自己的回应，这就大大弥补了原来的问题。</span><br><span class="line"></span><br><span class="line">尽管如此，Java还是沾染上了“面向类设计”的癌症，基础类库里就有很多架床叠屋的设计，而J2EE&#x2F;Java EE当中，这种形而上学的设计也很普遍，所以也引发了好几次轻量化的运动。</span><br><span class="line"></span><br><span class="line">至于.NET，我听陈榕介绍过，在设计.NET的时候，微软内部对于是否允许继承爆发了非常激烈的争论。很多资深高人都强烈反对继承。至于最后引入继承，很大程度上是营销需要压倒了技术理性。尽管如此，由于有COM的基础，又实现了非常彻底的delegate，所以 .NET 的设计水平还是很高的。它的主要问题不在这，在于太急于求胜，更新速度太快，基础不牢。当然，根本问题还是微软没有能够在Web和Mobile领域里占到多大的优势，也就使得.NET没有用武之地。</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>命名规范</p>
</li>
</ul>
<p>指针：p；指针指针：pp；句柄：h；引用：ref；array rg</p>
<p>成员变量：m_ ；全局变量 ：g_  ；静态变量：s_  ；类class：C ；接口：I</p>
<p>i fl ui ch ul(ULONG) dw fn w(USHORT, SHORT, WORD) b v d f(BOOL)  hr(HRESULT)</p>
<ul>
<li><p>c++ 与 c</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、C面向过程，C++面向对象--Support data abstraction</span><br><span class="line">2、Protect legacy codes </span><br><span class="line">3、More safe</span><br><span class="line">4、c++支持面向对象、基于对象、基于过程、泛型编程</span><br></pre></td></tr></table></figure></li>
</ul>
<p>reference：<br><a href="https://blog.csdn.net/myan/article/details/5928531">https://blog.csdn.net/myan/article/details/5928531</a></p>
]]></content>
      <categories>
        <category>工程</category>
        <category>c++</category>
      </categories>
      <tags>
        <tag>工程</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>c++感受</title>
    <url>/2021/06/19/c-%E6%84%9F%E5%8F%97/</url>
    <content><![CDATA[<br>

<br>

<h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><span id="more"></span>

<br>

<p>1、面向过程、面向对象、泛型编程。</p>
<p>过程是topdown思维，像递归，或者动态规划里的转移方程，将问题拆解成子问题，直到可以解决。而问题规模是从大问题到子问题的唯一区别，那么通过递归解决问题规模上的转化。</p>
<p>过程里，有 模块 对应为class 、 调用 对应为message–通信 、records 对应为 objects 、 procedure对应着成员函数。关于数据，过程编程里的数据，直到需要才有，而对象编程里，数据是已经初始化，存放在内存。 </p>
<p>对象则是需要将系统拆分考虑。对象自身，也就需要考虑构造、析构、new&amp;delete、this、成员变量、成员函数，以及一些operator、重载和重写。访问级别、static、以及const。别名 – typedef。对象之间的关系，则需要考虑继承多态、以及关系，是is 还是 has。</p>
<p>有意思的是，function object。其实也只是正常的class，重载operator ()，可以传入到泛型函数里。函数相关还有成员函数指针，也算语法糖。而引用和指针、深浅拷贝的问题，也都是语法层面的问题，本质从内存管理上去考虑就很容易理解。</p>
<p>函数有几种，一种是只有一个实现，一种是多种实现方式，但是input不变，另一种是input的类型也在变化。对应就可以通过默认参数、重写以及template模板，来实现。</p>
<p>2、</p>
]]></content>
      <categories>
        <category>工程</category>
        <category>c++</category>
      </categories>
      <tags>
        <tag>工程</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title>ctr的负样本</title>
    <url>/2021/08/02/ctr%E7%9A%84%E8%B4%9F%E6%A0%B7%E6%9C%AC/</url>
    <content><![CDATA[<br>



<h3 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h3><span id="more"></span>

<ul>
<li>RLNF: Reinforcement Learning based Noise Filtering for Click-Through Rate Prediction</li>
<li><ol>
<li>几种提升ctr模型的思路<ol>
<li>新的模型结构</li>
<li>incorporating feature interactions</li>
<li>可解释性</li>
<li>数据和行为的稀疏性问题</li>
<li>用户兴趣爱好随时间变迁(时间 维度)</li>
<li>样本不平衡性</li>
</ol>
</li>
<li>RLNF的步骤<ol>
<li>s1根据特征向量生成action a(是否选择)；s2根据很多的a生成reward；s3根据reward生成；s4所有负样本中选择部分a为选择的N，用N和正样本来训练ctr模型==&gt;这样交替地进行noise filter和ctr model的训练</li>
</ol>
</li>
<li></li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>cuda编程</title>
    <url>/2021/04/12/cuda%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<br>

<p>cuda c的hello world</p>
<span id="more"></span>

<br> 

<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">*hello_world.cu</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">hello_world</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;GPU: Hello world!\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;CPU: Hello world!\n&quot;</span>);</span><br><span class="line">  hello_world&lt;&lt;&lt;<span class="number">1</span>,<span class="number">10</span>&gt;&gt;&gt;();</span><br><span class="line">  cudaDeviceReset();<span class="comment">//if no this line ,it can not output hello world from gpu</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<br>

<h4 id="异构计算"><a href="#异构计算" class="headerlink" title="异构计算"></a>异构计算</h4><p>1、一台 intel i7-4790 CPU加上两台Titan x GPU构成的工作站，GPU插在主板的PCIe卡口上，运行程序的时候，CPU像是一个控制者，指挥两台Titan完成工作后进行汇总，和下一步工作安排，所以CPU我们可以把它看做一个指挥者，主机端，host，而完成大量计算的GPU是我们的计算设备，device。</p>
<p>2、一个四核CPU一般有四个ALU，ALU是完成逻辑计算的核心，也是我们平时说四核八核的核，控制单元，缓存也在片上，DRAM是内存，一般不在片上，CPU通过总线访问内存。</p>
<p>3、GPU，排列成行的一组ALU 公用一个Control单元和Cache，这个部分相当于一个完整的多核CPU。但是不同的是ALU多了，control部分变小，可见计算能力提升了，控制能力减弱了。</p>
<p>4、CPU和GPU之间通过PCIe总线连接，用于传递指令和数据，这部分也是后面要讨论的性能瓶颈之一。</p>
<p>5、低并行逻辑复杂的程序适合用CPU；高并行逻辑简单的大数据计算适合GPU</p>
<h4 id="衡量GPU计算能力"><a href="#衡量GPU计算能力" class="headerlink" title="衡量GPU计算能力"></a><br>衡量GPU计算能力</h4><p>容量：CUDA核心数量（越多越好）；内存大小（越大越好）</p>
<p>性能：峰值计算能力；内存带宽</p>
<br>

<h4 id="CPU和GPU线程的区别："><a href="#CPU和GPU线程的区别：" class="headerlink" title="CPU和GPU线程的区别："></a>CPU和GPU线程的区别：</h4><p>1、CPU线程是重量级实体，操作系统交替执行线程，线程上下文切换花销很大</p>
<p>2、GPU线程是轻量级的，GPU应用一般包含成千上万的线程，多数在排队状态，线程之间切换基本没有开销。</p>
<p>3、CPU的核被设计用来尽可能减少一个或两个线程运行时间的延迟，而GPU核则是大量线程，最大幅度提高吞吐量</p>
<br>

<h4 id="cuda-c"><a href="#cuda-c" class="headerlink" title="cuda c"></a>cuda c</h4><p>1、CUDA C 是标准ANSI C语言的扩展，扩展出一些语法和关键字来编写设备端代码，而且CUDA库本身提供了大量API来操作设备完成计算。</p>
<p>2、驱动API是低级的API，使用相对困难，运行时API是高级API使用简单，其实现基于驱动API。<br>这两种API是互斥的，也就是你只能用一个，两者之间的函数不可以混合调用，只能用其中的一个库。</p>
<p>3、一个CUDA应用通常可以分解为两部分</p>
<p>CPU 主机端代码和GPU 设备端代码</p>
<p>4、nvcc 是从LLVM开源编译系统为基础开发的。CUDA nvcc编译器会自动分离你代码里面的不同部分，如图中主机代码用C写成，使用本地的C语言编译器编译，设备端代码，也就是核函数，用CUDA C编写，通过nvcc编译，链接阶段，在内核程序调用或者明显的GPU设备操作时，添加运行时库。</p>
<h4 id="cuda程序步骤"><a href="#cuda程序步骤" class="headerlink" title="cuda程序步骤"></a><br>cuda程序步骤</h4><ol>
<li>分配GPU内存</li>
<li>拷贝内存到设备</li>
<li>调用CUDA内核函数来执行计算</li>
<li>把计算完成数据拷贝回主机端</li>
<li>内存销毁</li>
</ol>
<br>

<h4 id="CUDA抽象了硬件实现"><a href="#CUDA抽象了硬件实现" class="headerlink" title="CUDA抽象了硬件实现"></a>CUDA抽象了硬件实现</h4><ol>
<li>线程组的层次结构</li>
<li>内存的层次结构</li>
<li>障碍同步</li>
</ol>
<br>

<h4 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h4><ul>
<li>Nvidia Nsight集成开发环境</li>
<li>CUDA-GDB 命令行调试器</li>
<li>性能分析可视化工具</li>
<li>CUDA-MEMCHECK工具</li>
<li>GPU设备管理工具</li>
</ul>
]]></content>
      <categories>
        <category>工程</category>
        <category>gpu</category>
      </categories>
      <tags>
        <tag>工程</tag>
        <tag>gpu</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>cuda编程模型</title>
    <url>/2021/04/12/cuda%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>工程</category>
        <category>gpu</category>
      </categories>
      <tags>
        <tag>工程</tag>
        <tag>gpu</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title>debug训诫</title>
    <url>/2021/05/26/debug%E8%AE%AD%E8%AF%AB/</url>
    <content><![CDATA[<br>

<br>

<p>1、感觉很难，错误都在细节。</p>
<span id="more"></span>

<p>2、error好好看。</p>
<p>3、并不是难。而是繁复。改动处有很多。但少量。</p>
<p>4、改了一处，考虑是否对其他地方有影响。等到发现问题，又难以和之前的改动联系起来。导致锁定时间不能更快。</p>
<p>5、除了业务逻辑，还有 nil 数组越界 除以0 之类的小错误</p>
<p>6、还有你用的接口，虽然是别人提供的，但未必正确 – check功能即可。mock数据。即业务数据是否准确。</p>
<p>7、能配置的 是否都尽量配置化了。方便线上出问题不用发版。</p>
<p>8、日志和性能监控的，时间打点</p>
<p>9、kibana用法：性能上、error上</p>
<p>10、更多时间消耗在：定位，不仔细地排查到，而是根据感觉、设想找到错误的地方然后改动，结果又需要进一步排查。这个使得时间成倍地被消耗。</p>
]]></content>
      <categories>
        <category>工程</category>
        <category>训诫</category>
      </categories>
      <tags>
        <tag>工程</tag>
        <tag>训诫</tag>
      </tags>
  </entry>
  <entry>
    <title>dl技巧</title>
    <url>/2021/10/06/dl%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<br>

<h2 id="加速"><a href="#加速" class="headerlink" title="加速"></a>加速</h2><ul>
<li>minibatch</li>
</ul>
<span id="more"></span>

<p>batch将一个batch里的example组成matrix进行运算。<br>GPU使同时运算。GPU+minibatch</p>
<h2 id="提升performance"><a href="#提升performance" class="headerlink" title="提升performance"></a>提升performance</h2><h2 id="bn"><a href="#bn" class="headerlink" title="bn"></a>bn</h2><p>1、bn是因为，1 线性变换加上激活的非线性变换，导致中间层的输入远离初始分布，后面的网络由于每次输入分布变化，导致网络的学习比较慢。2 由于一些激活函数求导随着层数增加，乘积越来越小，导致梯度接近0，模型近似饱和。又有情况是，w的梯度越来越大，w值也越来越大。此时可以改变激活函数为非饱和激活函数如Relu，也可以通过让激活函数的输入分布保持在一个稳定状态，尽可能避免它们陷入梯度饱和区，这也就是Normalization的思路。</p>
<p>即，一个是分布变化，一个是梯度问题。</p>
<p>2、解决：<br>1 白化，通过使得输入特征的均值为0，方差为1或者相同，来保证分布的稳定并去除特征相关性，从而加速收敛<br>2 bn ： 白化的缺点是计算量并且改变了特征的分布导致信息丢失，所以引入了bn– 简化、进化版的白化。</p>
<p>3、bn<br><a href="https://zhuanlan.zhihu.com/p/34879333">bn原理、效果测试</a><br><a href="https://zhuanlan.zhihu.com/p/33173246">bn ln wn</a></p>
<p>4、 test上bn<br>无偏估计、每个层的bn进行指数</p>
<p>5、 优点<br>1分布稳定，避免梯度消失，加速收敛 2对网络参数不敏感 3引入随机噪声，正则化效果</p>
<p>尽管每一个batch中的数据都是从总体样本中抽样得到，但不同mini-batch的均值与方差会有所不同，这就为网络的学习过程中增加了随机噪音，与Dropout通过关闭神经元给网络训练带来噪音类似，在一定程度上对模型起到了正则化的效果。原作者通过也证明了网络加入BN后，可以丢弃Dropout，模型也同样具有很好的泛化效果。</p>
<h2 id="trick"><a href="#trick" class="headerlink" title="trick"></a>trick</h2><p>lr：<br>learning rate ，推荐了解 cosine learning rate</p>
<p>weight：<br>weight decay。</p>
<p>loss：<br>当你的模型还不错的时候，可以试着做数据增广和改损失函数锦上添花了。</p>
<p>全连接：<br>1、能用全卷积的任务，少用全连接层，参数量小。<br>2、highway net：可以考虑将全连接层（MLP）替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动</p>
<p>参数初始化：<br>1、uniform均匀分布初始化：w = np.random.uniform(low=-scale, high=scale, size=[n_in,n_out])<br>    Xavier初始法，适用于普通激活函数(tanh,sigmoid)：scale = np.sqrt(3/n)<br>    He初始化，适用于ReLU：scale = np.sqrt(6/n)<br>2、normal高斯分布初始化：w = np.random.randn(n_in,n_out) * stdev # stdev为高斯分布的标准差，均值设为0<br>    Xavier初始法，适用于普通激活函数 (tanh,sigmoid)：stdev = np.sqrt(n)He初始化，适用于ReLU：stdev = np.sqrt(2/n)<br>3、svd初始化：对RNN有比较好的效果。<br>4、LSTM 的forget gate的bias,用1.0或者更大的值做初始化</p>
<p>batch_size：<br>embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.</p>
<p>数据预处理：<br>bn、pca白化</p>
<p>激活函数：<br>sigmoid函数在-4到4的区间里，才有较大的梯度。之外的区间，梯度接近0。输入0均值，sigmoid函数的输出不是0均值的。<br>把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类</p>
<p>ensemble：<br>cv<br>不同模型线性融合</p>
<p>优化器：<br>用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.据说adadelta一般在分类问题上效果比较好，adam在生成问题上效果比较好。<br>sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞</p>
<p>dropout：<br>dropout对小数据防止过拟合有很好的效果,值一般设为0.5<br>小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显<br>因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&gt;RNN与RNN-&gt;输出的位置</p>
<p>梯度：<br>要做梯度归一化,即算出来的梯度除以minibatch size<br>clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15<br>RNN 不加 gradient clipping，导致训练一段时间以后 loss 突然变成 Nan。</p>
<p>正则：<br>一轮加正则，一轮不加正则，反复进行。</p>
<p>数据集：<br>判断测试集靠不靠谱，包括采样/数据分布的线上一致性、时效性、标注正确率、评测方差/置信度<br>除了标准和采样问题外，数据问题上还可以组合大量预处理策略和训练策略</p>
<h2 id="性能关注"><a href="#性能关注" class="headerlink" title="性能关注"></a>性能关注</h2><p>运行速度，准确率，召回率，auc，显存利用率，显存占用，cpu利用率，内存占用，并发路数</p>
<h2 id="业务场景和模型是否合适"><a href="#业务场景和模型是否合适" class="headerlink" title="业务场景和模型是否合适"></a>业务场景和模型是否合适</h2><p>陌生社交和graphsage的相似top推荐思路的逻辑不恰</p>
<h2 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h2><p>当一个复杂的前馈神经网络被训练在小的数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。</p>
<p>1、取平均，相当于训练了很多个模型然后投票<br>2、正则化<br>3、避免共适应、共线性</p>
<h2 id><a href="#" class="headerlink" title></a></h2>]]></content>
      <tags>
        <tag>算法</tag>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>dssm</title>
    <url>/2021/03/08/dssm/</url>
    <content><![CDATA[<h4 id="DSSM"><a href="#DSSM" class="headerlink" title="DSSM"></a>DSSM</h4><ul>
<li><p>任务<br>用来预测两个句子的语义相似度，又可以获得某句子的低维语义Embedding向量。</p>
<span id="more"></span></li>
<li><p>场景<br>DSSM 模型的最大特点就是 Query 和 Document 是两个独立的子网络，后来这一特色被移植到推荐算法的召回环节，即对用户端（User）和物品端（Item）分别构建独立的子网络塔式结构。<br>两个子网络产生的 Embedding 向量可以独自获取及缓存。</p>
</li>
</ul>
<p>当模型训练完成时，物品的 Embedding 是可以保存成词表的，线上应用的时候只需要查找对应的 Embedding 即可。因此线上只需要计算 （用户，上下文） 一侧的 Embedding，基于 Annoy 或 Faiss 技术索引得到用户偏好的候选集。</p>
<ul>
<li>word hashing<br>word hashing方法是用来减少输入向量的维度，该方法基于字母的n-gram。给定一个单词（good），我们首先增加词的开始和结束部分（#good#），然后将该词转换为字母 [公式] -gram的形式（假设为trigrams：#go，goo，ood，od#）。最后该词使用字母 n-gram的向量来表示。</li>
</ul>
<p>这种方法的问题在于有可能造成冲突，因为两个不同的词可能有相同的n-gram向量来表示。与原始的ont-hot向量表示的词典大小相比，word hashing明显降低了向量表示的维度。</p>
<ul>
<li>优点</li>
</ul>
<p>1、解决了LSA、LDA、Autoencoder等方法存在的一个最大的问题：字典爆炸（导致计算复杂度非常高），因为在英文单词中，词的数量可能是没有限制的，但是字母n-gram的数量通常是有限的.<br>2、基于词的特征表示比较难处理新词，字母的 n-gram可以有效表示，鲁棒性较强<br>3、使用有监督方法，优化语义embedding的映射问题<br>4、省去了人工的特征工程</p>
<ul>
<li>缺点</li>
</ul>
<p>1、word hashing可能造成冲突<br>2、DSSM采用了词袋模型，损失了上下文信息<br>3、在排序中，搜索引擎的排序由多种因素决定，由于用户点击时doc的排名越靠前，点击的概率就越大，如果仅仅用点击来判断是否为正负样本，噪声比较大，难以收敛<br>4、对于中文而言，处理方式与英文有很多不一样的地方。中文往往需要进行分词，但是我们可以仿照英文的处理方式，将中文的最小粒度看作是单字（在某些文献里看到过用偏旁部首，笔画，拼音等方法）</p>
<ul>
<li><p>扩展<br>对DSSM的优化出现了很多的变种，有CNN-DSSM，LSTM-DSSM，MV-DSSM等。</p>
</li>
<li><p>trick<br><img src="/2021/03/08/dssm/trick.png" alt="trick"></p>
</li>
</ul>
<hr>
<ul>
<li>架构<br><img src="/2021/03/08/dssm/dssm.png" alt="dssm"></li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>dubbo-spi</title>
    <url>/2021/01/31/dubbo-spi/</url>
    <content><![CDATA[<h2 id="spi"><a href="#spi" class="headerlink" title="spi"></a>spi</h2><p><img src="/2021/01/31/dubbo-spi/jdbc.png" alt="jdbc实现"></p>
<p><strong>SPI 的缺点</strong><br>JDK 标准的 SPI 会一次性加载实例化扩展点的所有实现，JDK 启动的时候会一次性全部加载。<br>1如果有的扩展点实现初始化很耗时或者如果有些实现类并没有用到， 会很浪费资源。<br>2如果扩展点加载失败，会导致调用方报错，而且这个错误很难定位到。</p>
<span id="more"></span>

<p><strong>dubbo spi</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Protocol  p &#x3D; ExtensionLoader.getExtensionLoader(xxx.class).getAdaptiveExtension();</span><br><span class="line">ExtensionLoader.getExtensionLoader(xxx.class).getExtension(name);</span><br><span class="line">ExtensionLoader.getExtensionLoader(xxx.class).getActivateExtension(url, key);</span><br></pre></td></tr></table></figure>
<p><code>protocol</code>会在运行的时候判断一下应该选用这个Protocol接口的哪个实现类来实例化对象。<br>动态的根据配置去找到对应的实现类。如果你没有配置，那就走默认的实现类。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@SPI(&quot;dubbo&quot;)  </span><br><span class="line">public interface Protocol &#123;  </span><br><span class="line">      </span><br><span class="line">    int getDefaultPort();  </span><br><span class="line">  </span><br><span class="line">    @Adaptive  </span><br><span class="line">    &lt;T&gt; Exporter&lt;T&gt; export(Invoker&lt;T&gt; invoker) throws RpcException;  </span><br><span class="line">  </span><br><span class="line">    @Adaptive  </span><br><span class="line">    &lt;T&gt; Invoker&lt;T&gt; refer(Class&lt;T&gt; type, URL url) throws RpcException;  </span><br><span class="line"></span><br><span class="line">    void destroy();  </span><br><span class="line">  </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">dubbo&#x3D;org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol</span><br></pre></td></tr></table></figure>
<p><code>@SPI(“dubbo”)</code>：通过 SPI 机制来提供实现类，实现类是通过 dubbo 作为默认 key 去配置文件里找到的，配置文件名称与接口全限定名一样的，通过 dubbo 作为 key 可以找到默认的实现类就是 org.apache.dubbo.rpc.protocol.dubbo.DubboProtocol</p>
<p><code>@Adaptive</code>：如果想要动态替换掉默认的实现类，需要使用 @Adaptive 。表示动态代理实现。在运行的时候会针对 Protocol 生成<code>代理类</code>，这个代理类的那俩方法里面会有<code>代理代码</code>，代理代码会在运行的时候动态根据 url 中的 protocol 来获取那个 key，默认是 dubbo，自己指定则获取相应的实现。</p>
<h2 id="扩展dubbo组件"><a href="#扩展dubbo组件" class="headerlink" title="扩展dubbo组件"></a>扩展dubbo组件</h2><p><strong>step1</strong><br><img src="/2021/01/31/dubbo-spi/implement.png" alt="自定义方法"></p>
<p><strong>step2</strong><br><img src="/2021/01/31/dubbo-spi/properties.png" alt="添加配置"></p>
<p><strong>step3</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class App </span><br><span class="line">&#123;</span><br><span class="line">    public static void main( String[] args )</span><br><span class="line">    &#123;</span><br><span class="line">&#x2F;&#x2F;        调用方代码</span><br><span class="line">    	Protocol protocol &#x3D; ExtensionLoader.getExtensionLoader(Protocol.class).getExtension(&quot;myProtocol&quot;);</span><br><span class="line">    	System.out.println(protocol.getDefaultPort());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h2 id="实现原理探析"><a href="#实现原理探析" class="headerlink" title="实现原理探析"></a>实现原理探析</h2><p>getExtension方法获取一个SPI接口的扩展类实例的流程: 分为解析配置文件、加载并缓存扩展类、创建并加工(属性注入与层层包装)扩展类实例 几个步骤。</p>
<p>通过getAdaptiveExtension方法的流程可以发现，要想获得一个SPI接口的自适应扩展类实例，有2种方式：<br>1在SPI接口的配置文件中配置具有@Adaptive注解的扩展类，在执行解析SPI接口配置文件方法getExtensionClasses时，它会调用loadClass方法，该方法判断扩展类是否具有@Adaptive注解，如果有，则将该类Class缓存到ExtensionLoader的字段“cachedAdaptiveClass”中，然后直接实例化该Class的实例并进行自动装配；<br>2如果未配置@Adaptive修饰的扩展类，则Dubbo会使用字节码技术创建一个自适应扩展类，前提是SPI接口上至少有一个被@Adaptive注解的方法；</p>
<p>todo</p>
]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title>dubbo</title>
    <url>/2021/04/05/dubbo/</url>
    <content><![CDATA[<p>dubbo–rpc框架相关</p>
<span id="more"></span>

<ul>
<li><p>分层</p>
<ul>
<li>service、config、cluster、 monitor、protocol、exchange、registry、proxy、transport、serialize等</li>
</ul>
</li>
<li><p>网络通信协议、序列化协议</p>
<ul>
<li>dubbo 长连接、nio、高并发、数据量小</li>
<li>hessian </li>
<li>还有其他 json 、java二进制、rmi等</li>
</ul>
</li>
<li><p>负载均衡</p>
<ul>
<li>权重、轮询、自动感知、一致性hash</li>
<li>方法级别配置</li>
</ul>
</li>
<li><p>spi</p>
<ul>
<li>插件扩展</li>
</ul>
</li>
</ul>
<ul>
<li><p>动态代理</p>
<ul>
<li><p>静态代理</p>
<ul>
<li>编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件</li>
<li>代理类中包含具体代理的接口类</li>
</ul>
</li>
<li><p>动态</p>
<ul>
<li><p>可以不需要针对每个目标类都创建一个代理类，静态代理中接口一旦新增加方法，目标对象和代理对象都要进行修改，非常麻烦</p>
</li>
<li><p>jdk动态</p>
<ul>
<li> InvocationHandler 接口和 Proxy 类 (InvocationHandler里面会用到Proxy类)</li>
<li><code>SmsService smsService = (SmsService) JdkProxyFactory.getProxy(new SmsServiceImpl());</code></li>
<li>JDK 动态代理只能只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类</li>
</ul>
</li>
<li><p>cglib动态代理</p>
<ul>
<li>自定义 MethodInterceptor 并重写 intercept 方法，intercept 用于拦截增强被代理类的方法</li>
<li><code>AliSmsService aliSmsService = (AliSmsService) CglibProxyFactory.getProxy(AliSmsService.class);</code></li>
<li> CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/basis/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%E8%AF%A6%E8%A7%A3.md">代理模式详解.</a></p>
</li>
</ul>
</li>
<li><p>服务治理</p>
<ul>
<li>链路、压力、时长、可用性(成功率)、服务分层(循环依赖)</li>
<li>服务降级<ul>
<li>dubbo中的mock<ul>
<li>类加上Mock后缀，作为降级策略</li>
</ul>
</li>
</ul>
</li>
<li>失败重试、超时重试<ul>
<li>200ms超时，重试3次</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title>elasticsearch</title>
    <url>/2021/04/27/elasticsearch/</url>
    <content><![CDATA[<br>

<h2 id="相关笔记"><a href="#相关笔记" class="headerlink" title="相关笔记"></a>相关笔记</h2><p>1、搜索引擎或者elk(es、logstash、kibana)系统，都是根据内容中的关键字建立倒排索引即反向索引，便于搜索。</p>
<span id="more"></span>



<p>2、 在 Lucene 的基础上进行封装，实现了分布式搜索引擎。</p>
<p>3、es的索引、类型和文档的概念比较重要，类似于 MySQL 中的数据库、表和行。</p>
<p>4、Elasticsearch 也是 Master-slave 架构，也实现了数据的分片和备份。写入index和type时是和master打交道，然后同步到slave，但是写入doc数据，不需要如此。为了提高性能，写数据是采取routing，将写入压力分散。</p>
<p>5、</p>
<p>Reference:</p>
<p><a href="https://zhuanlan.zhihu.com/p/62892586">https://zhuanlan.zhihu.com/p/62892586</a></p>
]]></content>
      <categories>
        <category>es</category>
      </categories>
      <tags>
        <tag>es</tag>
      </tags>
  </entry>
  <entry>
    <title>flink</title>
    <url>/2021/03/08/flink/</url>
    <content><![CDATA[<br>


<h1 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h1><h2 id="state"><a href="#state" class="headerlink" title="state"></a>state</h2><ul>
<li><p>场景</p>
<ul>
<li>有状态的逻辑是因为数据之间存在关联，单条数据是没有办法把所有的信息给表现出来。<span id="more"></span>
<ul>
<li>去重</li>
<li>窗口计算</li>
<li>机器学习参数</li>
<li>访问历史数据</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么要管理状态</p>
<ul>
<li>内存<ul>
<li>流式作业：24 小时的数据都放到内存，可能会出现内存不足。</li>
</ul>
</li>
<li>高可用<ul>
<li>机器若出现故障或者宕机，需要考虑如何备份及从备份中去恢复，</li>
</ul>
</li>
<li>扩展性<ul>
<li>单节点无法处理全部访问数据，增加几个节点进行横向扩展，这时数据的状态如何平均分配到新增加的节点。</li>
</ul>
</li>
</ul>
</li>
<li><p>方案</p>
<ul>
<li><p>Managed State &amp; Raw State</p>
<ul>
<li>自定义operator用 raw</li>
<li>raw 必须能够转成字节数组</li>
<li>managed，flink自动存储和恢复，并进行内存优化<br>支持已知的数据结构，如 Value、List、Map</li>
</ul>
</li>
<li><p>Managed State</p>
<ul>
<li><p>Keyed State </p>
<ul>
<li><p>每个 Key 对应一个 State</p>
</li>
<li><p>整个程序中没有 keyBy 的过程就没有办法使用KeyedStream。</p>
</li>
<li><p>并发改变时状态重新分配：内置了 2 种分配方式</p>
</li>
<li><p>Keyed State 通过 RuntimeContext 访问，这需要 Operator 是一个 Rich Function。</p>
</li>
<li><p>几种 Keyed State 的差异</p>
<ul>
<li><p>ReducingState 和 AggregatingState 与 ListState 都是同一个父类，但状态数据类型上是单个值</p>
</li>
<li><p> AggregatingState 输入的 IN，输出的是 OUT。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Operator State </p>
<ul>
<li>可以用于所有算子，常用于 Source</li>
<li>一个 Operator 实例对应一个 State</li>
<li>Operator  State 需要自己实现 CheckpointedFunction 或 ListCheckpointed 接口。</li>
<li>支持的数据结构相对较少 </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>使用示例</p>
<ul>
<li><p><a href="https://github.com/apache/flink/blob/master/flink-examples/flink-examples-streaming/src/main/java/org/apache/flink/streaming/examples/statemachine/StateMachineExample.java">写状态机是如何实现</a></p>
</li>
<li><p>实现的是：首先下订单，订单生成后状态为待付款，当再来一个事件状态付款成功，则事件的状态将会从待付款变为已付款，待发货…</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>状态的保存和恢复</p>
<ul>
<li><p>保存</p>
<ul>
<li>Checkpoint 会定时制作分布式快照</li>
</ul>
</li>
<li><p>恢复</p>
<ul>
<li><p>checkpoint</p>
<ul>
<li>数据源需要支持数据重新发送</li>
<li>两种一致性语义，一种是恰好一次，一种是至少一次</li>
<li>1、把进程或者线程移到 active 的 其他台机器上<br>2、整个作业的所有 Task 都回滚到最后一次成功 Checkpoint 中的状态</li>
</ul>
</li>
<li><p>savepoint</p>
<ul>
<li>手动调整并发，必须要重启作业并会提示 Checkpoint 已经不存在–&gt; 此时savepoint</li>
<li>比较持久，以标准格式存储</li>
<li>允许代码或配置发生改变，恢复需要启动作业手动指定一个路径恢复</li>
</ul>
</li>
</ul>
</li>
<li><p>checkpoint实现</p>
<ul>
<li>运行环境 env.enableCheckpointing 传入间隔时间。越频繁，恢复时追数据就会相对减少，IO 消耗增加。</li>
<li>设置了 Exactly_Once 语义，并且需要 Barries 对齐，这样可以保证消息不会丢失也不会重复。</li>
<li>setMinPauseBetweenCheckpoints 防止 Checkpoint 太过于频繁</li>
<li>setCheckpointTimeout 表示做 Checkpoint 多久超时</li>
<li>setMaxConcurrentCheckpoints</li>
<li>enableExternalizedCheckpoints。默认 Checkpoint 会在整个作业 Cancel 时被删除。Checkpoint 是作业级别的保存点。</li>
</ul>
</li>
<li><p>checkpoint可选的状态存储方式</p>
<ul>
<li><p>MemoryStateBackend</p>
<ul>
<li>构造方法是设置最大的 StateSize，选择是否做异步快照</li>
<li>且需要注意 maxStateSize &lt;= akka.framesize 默认 10 M</li>
<li>Checkpoint 存储在 JobManager 内存中，因此总大小不超过 JobManager 的内存。- 本地测试、几乎无状态的作业，比如 ETL、JobManager 不容易挂，或挂掉影响不大的情况。不推荐在生产场景使用。</li>
</ul>
</li>
<li><p>FsStateBackend</p>
<ul>
<li>需要传一个文件路径和是否异步快照</li>
<li>State 依然在 TaskManager 内存中- Checkpoint 存储在外部文件系统（本地或 HDFS）</li>
<li>常规使用状态的作业、例如分钟级窗口聚合或 join、需要开启 HA 的作业。</li>
</ul>
</li>
<li><p>RocksDBStateBackend</p>
<ul>
<li>key/value 的内存存储系统</li>
<li>不支持同步的 Checkpoint</li>
<li>支持增量的 Checkpoint</li>
<li>存储在外部文件系统（本地或 HDFS）</li>
<li>单个 TaskManager 上 State 总量不超过它的内存+磁盘，单 Key 最大 2G，总大小不超过配置的文件系统容量即可</li>
<li>超大状态的作业，例如天级窗口聚合、需要开启 HA 的作业、最好是对状态读写性能要求不高的作业。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h2><ul>
<li>todo</li>
</ul>
<h2 id="watermark"><a href="#watermark" class="headerlink" title="watermark"></a>watermark</h2><ul>
<li>todo</li>
</ul>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><ul>
<li>todo</li>
</ul>
<h2 id="CET"><a href="#CET" class="headerlink" title="CET"></a>CET</h2><ul>
<li>todo</li>
</ul>
<h2 id="概念-amp-角色"><a href="#概念-amp-角色" class="headerlink" title="概念&amp;角色"></a>概念&amp;角色</h2><ul>
<li>TaskManager &amp; slot<ul>
<li>每一个 TaskManager 都是一个JVM进程</li>
<li>每个task slot表示TaskManager拥有资源的一个固定大小的子集</li>
<li>将其管理的内存均分给各个slot</li>
<li>一个TaskManager一个slot时，那么每个task group运行在独立的JVM中</li>
<li>多个slot时，多个subtask可以共同享有一个JVM</li>
<li>在同一个JVM进程中的task将共享TCP连接和心跳消息，也可能共享数据集和数据结构，从而减少每个task的负载。</li>
</ul>
</li>
</ul>
<ul>
<li>todo</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li><a href="https://www.infoq.cn/article/vgkza-s9fmbgabp71pgh">状态管理及容错机制</a></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>流处理</tag>
      </tags>
  </entry>
  <entry>
    <title>graphsage</title>
    <url>/2021/03/08/graphsage/</url>
    <content><![CDATA[<h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h4><ul>
<li>卷积<br>数学上卷积的两个例子：</li>
</ul>
<p>一个对象（吃冰淇凌）对一个系统（体重）的作用效果满足线性原理、累加原理。该对象对这个系统连续作用了一段时间后，求该系统的状态。这个时候，一个卷积就可以求出来了！</p>
<span id="more"></span>

<br>
第二个例子：
![卷积](卷积.jpg)


<hr>
<p>DL中的卷积：</p>
<p>CNN卷积本质是，共享参数的filter过滤器，像素点加权构成feature map 实现特征提取。<br>a）平滑滤波 b）边缘提取，很容易通过设计特定的“卷积核”，然后将其与像素矩阵的对应元素（不进行旋转）相乘得到。<br>a）就是将中心像素值与周围临近的像素进行平均，自然就能“削峰填谷”，实现平滑处理<br>b) 中心像素复制n份，减去周围n个临近的像素值。相近的则减为0，边缘才被留下。<br>卷积神经网络中“卷积”，是为了提取图像的特征，其实只借鉴了数学卷积中“加权求和”的特点。</p>
<ul>
<li><p>为什么需要GCN<br>CNN LSTM等 对非欧几里得空间数据(eg：社交网络、信息网络等)进行处理上却存在一定的局限性。<br>用GCN：拓扑图中每个node相邻的个数不同，不能用同样大小的filter进行平移提取feature。任何数据在赋范空间内都可以建立拓扑关联，如谱聚类。GCN是区别于CV NLP的任务的模型。</p>
</li>
<li><p>图学习任务<br>1、图节点分类任务：图中每个节点都有对应的特征，当我们已知一些节点的类别的时候，可以设计分类任务针对未知节点进行分类。我们接下来要介绍的 GCN、GraphSAGE、GAT模型都是对图上的节点分类。<br>2、图边结构预测任务：图中的节点和节点之间的边关系可能在输入数据中能够采集到，而有些隐藏的边需要我们挖掘出来，这类任务就是对边的预测任务，也就是对节点和节点之间关系的预测。<br>3、图的分类：对于整个图来说，我们也可以对图分类，图分类又称为图的同构问题，基本思路是将图中节点的特征聚合起来作为图的特征，再进行分类。</p>
</li>
</ul>
<p>如：<br>1、节点分类—反欺诈：因为图中每个节点都拥有自己的特征信息。通过该特征信息，我们可以构建一个风控系统，如果交易节点所关联的用户 IP 和收货地址与用户注册 IP 和注册地址不匹配，那么系统将有可能认为该用户存在欺诈风险。<br>2、边结构预测—商品推荐：图中每个节点都具有结构信息。如果用户频繁购买某种类别商品或对某种类别商品评分较高，那么系统就可以认定该用户对该类商品比较感兴趣，所以就可以向该用户推荐更多该类别的商品。</p>
<ul>
<li>拉普拉斯矩阵<br><img src="/2021/03/08/graphsage/laplacian.png" alt="laplacian"></li>
</ul>
<hr>
<ul>
<li><p>GCN主要贡献<br>这篇文章的主要贡献是为图半监督分类任务设计了一个简单并且效果好的神经网络模型，这个模型由谱图卷积(spectral graph convolution)的一阶近似推导而来，具有理论基础。</p>
</li>
<li><p>GCN学习策略</p>
</li>
</ul>
<p><img src="/2021/03/08/graphsage/GCN%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5.png" alt="GCN学习策略"></p>
<ul>
<li><p>多层图卷积网络(Graph Convolutional Network, GCN)的逐层传播公式<br><img src="/2021/03/08/graphsage/%E9%80%90%E5%B1%82%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F.png" alt="逐层传播公式"></p>
</li>
<li><p>谱图卷积(Spectral Graph Convolutions)</p>
</li>
<li><p>逐层线性模型</p>
</li>
<li><p>半监督学习节点分类</p>
</li>
<li><p>传播公式解释</p>
</li>
</ul>
<p>todo</p>
<hr>
<ul>
<li>gcn聚合时，如果考虑邻居信息和自身信息的占比，用attention呢。但是attention的问题是，fitting训练集，对测试集效果不佳。</li>
</ul>
<h4 id="graphsage"><a href="#graphsage" class="headerlink" title="graphsage"></a>graphsage</h4><p>1、比GCN进步之处<br>GCN的训练方式需要将邻接矩阵和特征矩阵一起放到内存或者显存里，在大规模图数据上是不可取的。其次，GCN在训练时需要知道整个图的结构信息(包括待预测的节点), 这在现实某些任务中也不能实现(比如用今天训练的图模型预测明天的数据，那么明天的节点是拿不到的)。GraphSAGE的出现就是为了解决这样的问题。</p>
<p>GraphSAGE采用了采样的机制，使得图模型可以应用到大规模的图结构数据中，是目前几乎所有工业上图模型的雏形。<br>进一步：每个节点这么多邻居，采样能否考虑到邻居的相对重要性呢，或者我们在聚合计算中能否考虑到邻居的相对重要性? </p>
<p>2、inductive 还是 transductive<br>如果训练时用到了测试集或验证集样本的信息(或者说，测试集和验证集在训练的时候是可见的), 我们把这种学习方式叫做transductive learning, 反之，称为inductive learning. 显然，我们所处理的大多数机器学习问题都是inductive learning, 因为我们刻意的将样本集分为训练/验证/测试，并且训练的时候只用训练样本。然而，在GCN中，训练节点收集邻居信息的时候，用到了测试或者验证样本，所以它是transductive的。</p>
<p>3、简单过程<br>思路一个网络里，我们知道部分点的分类，我们希望通过各种方法，知道其他未知点的属性。解决对未知节点的泛化问题。</p>
<p>GraphSAGE是一个inductive框架，在具体实现中，训练时它仅仅保留训练样本到训练样本的边。inductive learning 的优点是可以利用已知节点的信息为未知节点生成Embedding. GraphSAGE 取自 Graph SAmple and aggreGatE, SAmple指如何对邻居个数进行采样。aggreGatE指拿到邻居的embedding之后如何汇聚这些embedding以更新自己的embedding信息。<br><br></p>
<p><img src="/2021/03/08/graphsage/visual_graphsage.webp" alt="visual_graphsage"></p>
<hr>
<p>4、具体步骤及伪代码</p>
<p>步骤：<br>1.对邻居采样<br>2.采样后的邻居embedding传到节点上来，并使用一个聚合函数聚合这些邻居信息以更新节点的embedding<br>3.根据更新后的embedding预测节点的标签</p>
<p><img src="/2021/03/08/graphsage/graphsage.png" alt="graphsage"></p>
<br>


<p>描述：初始化各个节点emb，对每个节点emb采样邻居的emb，对邻居进行聚合，将自己的emb和聚合后的emb做一个非线性变换，更新为自己的emb。</p>
<p>5、K的解释</p>
<p>K：聚合器数量，权重矩阵数量，层数。</p>
<p><img src="/2021/03/08/graphsage/K%E8%A7%A3%E9%87%8A.jpg" alt="K解释"></p>
<br>

<p>6、采样<br>定长抽样。定义邻居个数，进行有放回的重采样/负采样达到个数。每个node采样个数一致，为了把多个邻居拼成tensor放入gpu批量训练。</p>
<p>7、聚合器<br>平均效果最好。<br>也有用lstm聚合器和pooling聚合器，</p>
<p>8、学习过程<br>有监督：交叉熵<br>无监督：学习出来的相邻node的emb应该尽可能接近。此时的loss如下。<br><img src="/2021/03/08/graphsage/loss.jpg" alt="loss"><br><img src="/2021/03/08/graphsage/loss%E8%A7%A3%E9%87%8A.jpg" alt="loss解释"></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>hive</title>
    <url>/2021/03/03/hive/</url>
    <content><![CDATA[<br>


<ul>
<li><p>组件架构：<br>hiveserver2（beeline）,hive,metadb</p>
<span id="more"></span>
<blockquote>
<p>Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.</p>
</blockquote>
</li>
<li><p>连接hiveserver2  <br>GUI CLI JDBC (beeline)</p>
</li>
<li><p>数据源<br>用kafka，sqoop等获得data，放入hdfs，这些数据各种结构都有。<br>关系数据库的表，MongoDB 或json数据，或日志</p>
</li>
<li><p>执行hql<br>背后运行的是mapreduce or Tez jobs(类似于pig latin脚本执行pig)<br><code>insert into test values(&quot;wangyuq&quot;,&quot;123&quot;);</code><br>查看tracking url</p>
</li>
<li><p>stage<br>将你的数据移到目的位置之前，将会staing 那儿一段时间。staging文件最终丢弃。</p>
</li>
<li><p>比对<br>pig是对非结构化数据处理的好的etl。<br>hive不是关系数据库，只是维护存储在HDFS的数据的metadata，使得对大数据操作就像sql操作表一样，只不过hql和sql稍有出入。使我们能用sql来执行mr。可以对hdfs数据进行query。<br>hive使用metastore存表。hive默认derby但是可自定义更换。</p>
</li>
<li><p>劣<br>hive不能承诺优化，只是简单，因此hive不能支持实时，性能差<br>index view有限制（partition bucket 弥补）<br>和sql 的datatype不完全一样</p>
</li>
<li><p>与hdfs关系<br>hdfs里有hive，data在hdfs上，schema在metastore里。<br>load语句： 将hdfs搬运到hive，hdfs不再有该数据。只是将真正的data转到了hive目录下。</p>
</li>
</ul>
<ul>
<li> Making Multiple Passes over the Same Data<blockquote>
<p>Hive has a special syntax for producing multiple aggregations from a single pass through a source of data, rather than rescanning it for each aggregation. This change can save considerable processing time for large input data sets. </p>
</blockquote>
</li>
</ul>
<p>因此如下方式更加高效,并且可开启并行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM pv_users</span><br><span class="line">    INSERT OVERWRITE TABLE pv_gender_sum</span><br><span class="line">        SELECT pv_users.gender, count_distinct(pv_users.userid)</span><br><span class="line">        GROUP BY pv_users.gender</span><br><span class="line"></span><br><span class="line">    INSERT OVERWRITE DIRECTORY &#39;&#x2F;user&#x2F;data&#x2F;tmp&#x2F;pv_age_sum&#39;</span><br><span class="line">        SELECT pv_users.age, count_distinct(pv_users.userid)</span><br><span class="line">        GROUP BY pv_users.age;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set hive.exec.parallel&#x3D;true;   &#x2F;&#x2F;打开任务并行执行</span><br><span class="line">set hive.exec.parallel.thread.number&#x3D;16; &#x2F;&#x2F;同一个sql允许最大并行度，默认为8。</span><br></pre></td></tr></table></figure>

<ul>
<li><p>日期处理<br>查看N天前的日期：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select from_unixtime(unix_timestamp(&#39;20111102&#39;,&#39;yyyyMMdd&#39;) - N*86400,&#39;yyyyMMdd&#39;) from t_lxw_test1 limit 1;  </span><br></pre></td></tr></table></figure>
<p>获取两个日期之间的天数/秒数/分钟数等等：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select ( unix_timestamp(&#39;2011-11-02&#39;,&#39;yyyy-MM-dd&#39;)-unix_timestamp(&#39;2011-11-01&#39;,&#39;yyyy-MM-dd&#39;) ) &#x2F; 86400  from t_lxw_test limit 1; </span><br></pre></td></tr></table></figure></li>
<li><p>left outer join</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--query 1</span><br><span class="line">select count(id) from  </span><br><span class="line">(select id  from   a  left outer join   b  </span><br><span class="line">on a.id&#x3D;b.id and  b.date&#x3D;&#39;2017-10-27&#39;    </span><br><span class="line">where to_date(a.adate) &gt;&#x3D; &#39;2017-10-27&#39;   and a.date&#x3D;&#39;2017-07-24&#39;  </span><br><span class="line">) a </span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--query 2</span><br><span class="line">select count(id) from  </span><br><span class="line">(select id  from   a  left outer join   b  </span><br><span class="line">on a.id&#x3D;b.id and  b.date&#x3D;&#39;2017-10-27&#39;  and a.date&#x3D;&#39;2017-07-24&#39;  </span><br><span class="line">where to_date(a.adate) &gt;&#x3D; &#39;2017-10-27&#39;  </span><br><span class="line">) a </span><br></pre></td></tr></table></figure>
<p>区别？where 后面跟的是过滤条件，query 1 中的a.date=’2017-07-24’, 在table scan之前就会Partition Pruner 过滤分区，所以只有’2017-07-24’下的数据会和b进行join。<br>而query 2中会读入所有partition下的数据，再和b join，并且根据join的关联条件只有a.date=’2017-07-24’  的时候才会真正执行join，其余情况下又由于是left outer join, 右面会留NULL</p>
</li>
<li><p><a href="http://beadooper.com/?page_id=313">配置文件</a></p>
</li>
<li><p> <a href="http://superlxw1234.iteye.com/blog/1751216">正则</a><br>java中的正则匹配即可:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name rlike &#39;^[\\u4e00-\\u9fa5]+$&#39;</span><br><span class="line">select mobile from phone where mobile rlike &#39;^\\d+$&#39; ;  </span><br></pre></td></tr></table></figure></li>
<li><p><a href="http://superlxw1234.iteye.com/blog/1582880">控制hive任务中的map数和reduce数</a></p>
</li>
<li><p><a href="https://github.com/hbutani/SQLWindowing"> SQLWindowing</a></p>
</li>
<li><p><a href="http://blog.csdn.net/yeweiouyang/article/details/42082663">hdfs目录创建hive表,指定分区</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE if not exists push_log(</span><br><span class="line">     hostid STRING, dayid STRING</span><br><span class="line">     plmn STRING)</span><br><span class="line"> COMMENT &#39; log table&#39;</span><br><span class="line"> PARTITIONED BY (hostid STRING, dayid STRING) </span><br><span class="line"> ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\001&#39;</span><br><span class="line"> STORED AS TEXTFILE</span><br><span class="line"> LOCATION &#39;&#x2F;user&#x2F;data&#x2F;push&#39;;</span><br><span class="line">alter table push_log add partition(hostid&#x3D;&#39;$hostid&#39;, dayid&#x3D;&#39;$dayid&#39;) location &#39;&#x2F;user&#x2F;data&#x2F;push&#x2F;$hostid&#x2F;$dayid&#39;;</span><br></pre></td></tr></table></figure>
<p>testtext 数据<code>wer 46 weree   78 wer 89 rr  89</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create table d_part(name string)  partitioned by(value string) row format delimited fields terminated by &#39;\t&#39;  lines terminated by &#39;\n&#39; stored as textfile;</span><br><span class="line"></span><br><span class="line">set hive.exec.dynamic.partition&#x3D;true;</span><br><span class="line">set hive.exec.dynamic.partition.mode&#x3D;nonstrick;</span><br><span class="line"></span><br><span class="line">insert overwrite table d_part partition(value) select name,addr as value from testtext;</span><br><span class="line"></span><br><span class="line">select * from d_part;</span><br><span class="line">show partitions d_part;</span><br></pre></td></tr></table></figure></li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; create table d_part2(</span><br><span class="line">    &gt; name string</span><br><span class="line">    &gt; )</span><br><span class="line">    &gt; partitioned by(value string,dt string)</span><br><span class="line">    &gt; row format delimited fields terminated by &#39;\t&#39; </span><br><span class="line">    &gt; lines terminated by &#39;\n&#39;</span><br><span class="line">    &gt; stored as textfile;</span><br><span class="line">hive&gt; insert overwrite table d_part2 partition(value,dt)</span><br><span class="line">    &gt; select &#39;test&#39; as name,  </span><br><span class="line">    &gt; addr as value,</span><br><span class="line">    &gt; name as dt</span><br><span class="line">    &gt; from testtext;</span><br><span class="line">show partitions d_part2;</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="http://superlxw1234.iteye.com/blog/1568739">hive中转义特殊字符</a></li>
<li>schema tool<br><a href="https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive_schema_tool.html">https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive_schema_tool.html</a></li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>juc</title>
    <url>/2021/04/05/juc/</url>
    <content><![CDATA[<p>线程池、各种锁、高并发解决方案相关</p>
<span id="more"></span>


<ul>
<li><p>线程池</p>
<ul>
<li>阻塞队列<ul>
<li>任务队列中没有任务时阻塞获取任务的线程，使得线程进入wait状态，释放cpu资源。</li>
<li>有任务时才唤醒对应线程从队列中取出消息进行执行。</li>
</ul>
</li>
<li>任务类型<ul>
<li>CPU密集型任务<ul>
<li>尽量使用较小的线程池，一般为CPU核心数+1</li>
</ul>
</li>
<li>IO密集型任务<ul>
<li>可以使用稍大的线程池，一般为2*CPU核心数。</li>
</ul>
</li>
<li>混合<ul>
<li>分别用不同的线程池去处理</li>
</ul>
</li>
</ul>
</li>
<li>4类<ul>
<li>newCachedThreadPool, newFixedThreadPool, newScheduledThreadPool, newSingleThreadExecutor</li>
</ul>
</li>
</ul>
</li>
<li><p>乐观锁 悲观锁</p>
<ul>
<li>悲观<ul>
<li>db里的行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。</li>
</ul>
</li>
<li>乐观<ul>
<li>在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制</li>
<li>乐观锁适用于多读的应用类型，这样可以提高吞吐量</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
        <category>java</category>
        <category>并发</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>lexicons</title>
    <url>/2021/07/20/lexicons/</url>
    <content><![CDATA[<br>

<br>

<p>spared by幸免于</p>
<p>upheaval</p>
<span id="more"></span>

<p>maintain its work- force at full force</p>
<p>help us maintain our level of excellence through this complicated time despite the new challenges that confront us all</p>
<p> our current economic woes 经济困难</p>
<p>we wish you well as we all plot our courses through the current difficulties.</p>
<p>an abiding influence 持续的影响</p>
<p>commenced the day-long event</p>
<p>delivering the keynote address</p>
<p>attends to the problems facing practitioners </p>
<p>be caricatured as  an Ivory Tower academic被讽刺为象牙塔学术</p>
<p>Consul General of Egypt 埃及总领事</p>
<p>reiterate 重申</p>
<p>crowd out 挤出</p>
<p>cabinet内阁</p>
<p>upper eche-lons of 高层</p>
<p> wonk 专家</p>
<p>newsletter 时事通讯</p>
<p>liquidate 清盘</p>
<p>scrutinize</p>
<p>the July Politburo meeting</p>
<p>catalyst</p>
<p>economy tilting over</p>
<p>extradition request</p>
<p>stiff competition</p>
<p>coerce强迫</p>
<p>The comments sit awkwardly with China’s existing diplomatic disputes</p>
<p>State Department 国务院</p>
<p>span from</p>
<p> after a volatile session that saw it swing between gains and losses </p>
<p>put it on course for a bear market</p>
<p>talk up the market 讨论/哄抬市场</p>
<p>in the crosshairs of Beijing’s sweeping regulatory crackdowns 全面监管镇压的焦点</p>
<p>venting of sentiment </p>
<p>jawboning 嘘声</p>
<p>regulatory onslaught 监管冲击</p>
<p>property sectors房地产行业</p>
<p> tighten its grip on </p>
<p>exacerbate</p>
<p>a pillar of support that</p>
<p> anti-trust</p>
<p>bailed out</p>
<p>once priced in</p>
<p> scrutiny</p>
<p>sweep up</p>
<p>a slew of </p>
<p>auto-hailing</p>
<p>the gauge slumped 8.2% </p>
]]></content>
      <categories>
        <category>英文积累</category>
      </categories>
      <tags>
        <tag>英文积累</tag>
      </tags>
  </entry>
  <entry>
    <title>linux&amp;mac命令</title>
    <url>/2021/04/13/linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<br> 

<p>记录常见linux mac命令</p>
<span id="more"></span>

<br> 

<h4 id="端口、进程"><a href="#端口、进程" class="headerlink" title="端口、进程"></a>端口、进程</h4><p>1、查看进程号</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ps -ef | grep 进程名</span><br></pre></td></tr></table></figure>



<p>2、查看端口被哪个进程监听</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo lsof -i :端口</span><br></pre></td></tr></table></figure>



<p>3、查看进程监听的端口</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo lsof -nP -p 进程号 | grep LISTEN</span><br><span class="line">sudo lsof -nP | grep LISTEN | grep 进程号</span><br></pre></td></tr></table></figure>



<p>4、查看监听端口的进程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo lsof -nP | grep LISTEN | grep 端口号</span><br></pre></td></tr></table></figure>



<p>5、看到一个新的方法（MacOS统计TCP/UDP端口号与对应服务）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;### TCP LISTEN ###&quot;</span><br><span class="line">lsof -nP -iTCP -sTCP:LISTEN</span><br></pre></td></tr></table></figure>



<p>6、关于环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Mac系统的环境变量，加载顺序为：</span><br><span class="line">&#x2F;etc&#x2F;profile &#x2F;etc&#x2F;paths ~&#x2F;.bash_profile ~&#x2F;.bash_login ~&#x2F;.profile ~&#x2F;.bashrc</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;profile和&#x2F;etc&#x2F;paths是系统级别的，系统启动就会加载，后面几个是当前用户级的环境变量。后面3个按照从前往后的顺序读取，如果&#x2F;.bash_profile文件存在，则后面的几个文件就会被忽略不读了，如果&#x2F;.bash_profile文件不存在，才会以此类推读取后面的文件。~&#x2F;.bashrc没有上述规则，它是bash shell打开的时候载入的。</span><br><span class="line"></span><br><span class="line">&#x2F;etc&#x2F;bashrc （一般在这个文件中添加系统级环境变量）</span><br><span class="line">全局（公有）配置，bash shell执行时，不管是何种方式，都会读取此文件</span><br><span class="line"></span><br><span class="line">https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;acb1f062a925</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>mac</tag>
        <tag>cheetsheet</tag>
      </tags>
  </entry>
  <entry>
    <title>listwise</title>
    <url>/2021/10/06/listwise/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>macd</title>
    <url>/2021/06/20/macd/</url>
    <content><![CDATA[<br>
<br>



<h4 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h4><p><strong>DIF=12日EMA-26日EMA。</strong></p>
<span id="more"></span>

<p><strong>DIF绝对值大小，代表着长短期均线距离（开口）的大小。</strong>当12日均线在26日均线之上时，股价处于上升状态，DIF在0轴之上；当12日均线在26日均线之下时，股价处于下跌状态，DIF在0轴之下。</p>
<p><strong>当DIF上穿0轴时，即12均线与26日均线金叉；当DIF下穿0轴时，即12日均线与26日均线死叉。在趋势行情中，金叉与死叉是有效的买卖信号，但在震荡行情中，金叉与死叉基本上都是假信号。</strong></p>
<p>股价在上涨，但DIF却在不断下跌，即<strong>随着股价上涨，两条均线之间的距离未能创出新高。</strong>也就意味着上涨的势头越来越弱，虽然从股价上看还是上涨的，但随时都可能开始下跌。</p>
<p><strong>DEA：DIF值的移动平均线，一般是软件默认是9日平均线。</strong></p>
<p>如果DIF上穿DEA（金叉），意味着最近的DIF正在变大；如果DIF下穿DEA（死叉），意味着最近的DIF正在变小。</p>
<p>当DIF在零轴之上时：</p>
<p><strong>DIF与DEA金叉，意味着DIF正在变大，即股价的长短期均线距离在变大，股价上涨势头越来越猛。</strong></p>
<p><strong>DIF与DEA死叉，意味着DIF正在变小，即股价的长短期均线的距离正在变小，股价目前上涨势头正在变弱。</strong></p>
<p>当DIF在零轴之下时：</p>
<p><strong>DIF与DEA金叉，此时DIF是负值，也就是说DIF的绝对值在变小，即股价的长短期均线距离在变小，股价下跌势头正在变弱。</strong></p>
<p><strong>DIF与DEA死叉，此时DIF是负值，也就是说DIF的绝对值在变大，即股价的长短期均线距离在变大，股价下跌势头越来越强。</strong></p>
<p><strong>红柱与绿柱：（DIF-DEA）*2，即是柱子的数值，红柱为正值，绿柱为负值。</strong></p>
<p>当红柱变为绿柱时，对应着DIF与DEA的死叉；当绿柱变为红柱时，对应着DIF与DEA的金叉。</p>
<p><br><br></p>
<h4 id="均值系统"><a href="#均值系统" class="headerlink" title="均值系统"></a>均值系统</h4><p>1、三种情况，一是交织缠绕，二是女上，三是男上。</p>
<p>2、响应地，就有 从女上减弱进入缠绕，男上减弱进入缠绕，以及由缠绕突破成女上或者男上。</p>
<p>3、而形状按照力量分有，走平后按照原来的情况继续、靠近而不突破后按照原来的情况继续、突破后缠绕。</p>
<p>4、买卖点：男上位最后一次缠绕后背弛构成的空头陷阱抄底进入，这是第一个值得买入的位置，而第二个值得买入或加码的位置，就是女上位后第一次缠绕形成的低位。</p>
<p>5、买尽量买男上的转折，卖尽量卖女上的缠绕背驰。  </p>
<p>6、</p>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><h4 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h4><p>1、本质上都是一个评价系统，也就是告诉你在这个系统的标准下，评价对象的强弱</p>
<p>2、</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>ml基础</title>
    <url>/2021/04/05/ml%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<br>

<p>ml知识点</p>
<span id="more"></span>

<ul>
<li><p>softmax和logistic</p>
<ul>
<li>sigmoid: 导数在(0,1/4)，当x&gt;1/4或者&lt;-1/4时，导数非常小 1/1+e^(-x)</li>
<li>当分类数为2时，可以推导出两个一致</li>
<li>softmax建模使用的分布是多项式分布，而logistic则基于二项式分布</li>
<li>softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类</li>
<li>多个logistic回归进行多分类，”苹果”这个词语既属于”水果”类也属于”3C”类别。</li>
</ul>
</li>
<li><p>二项式和多项式分布</p>
<ul>
<li>二项<ul>
<li>二项分布是n重伯努利试验中正例发生次数的离散概率分布</li>
<li><code>import numpy; a = numpy.random.binomial(n=10, p=0.7, size = 1)</code></li>
</ul>
</li>
<li>多项<ul>
<li>二项分布是单变量分布，而多项分布是多变量分布</li>
<li>多项分布的例子是扔骰子，每次试验有多种可能，进行多次试验，多项分布描述的是每种可能发生次数的联合概率分布</li>
<li><code>a = numpy.random.multinomial(n=10, pvals=[0.2,0.4,0.4], size = 1)</code></li>
</ul>
</li>
</ul>
</li>
<li><p>逻辑回归梯度下降伪代码</p>
<ul>
<li><a href="https://blog.csdn.net/qq_33391629/article/details/108711228">https://blog.csdn.net/qq_33391629/article/details/108711228</a></li>
</ul>
</li>
<li><p>最小均方误差和最小二乘</p>
<ul>
<li>算术平均数可以让均方误差误差最小</li>
<li>如果误差的分布是正态分布，那么最小二乘法得到的就是最有可能的值。即：最小二乘法是概率密度(误差的概率密度)是高斯分布的最大似然估计的状况</li>
<li><a href="https://www.zhihu.com/question/396712527/answer/1243682221">线性回归模型用最小二乘法得到的估计的均方误差怎么得到</a></li>
</ul>
</li>
<li><p>并行直方图</p>
<ul>
<li>分裂节点时，数据在block中按列存放，而且已经经过了预排序，因此可以并行计算，即同时对各个属性遍历最优分裂点</li>
</ul>
</li>
<li><p>特征点分割</p>
<ul>
<li>树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点</li>
<li>当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，XGBoost采用了一种近似的算法</li>
<li>近似的算法对每维特征加权分位进行分桶，具体的算法利用到了损失函数关于待求树的二阶导数。</li>
</ul>
</li>
<li><p>偏导数与残差</p>
<ul>
<li>偏导数的计算可以确定每个相关数据对最终结果的局部/全局最小值的一个梯度。<br>顺着梯度进行调整，会让结果逐步趋向局部/全局最小值。</li>
<li>gbdt是用梯度来近似残差：损失函数L对当前所学模型F预测值的负梯度，所以模型的更新是沿着梯度下降方向的。</li>
</ul>
</li>
</ul>
<ul>
<li><p>auc计算</p>
<ul>
<li>(auc计算)[<a href="https://blog.csdn.net/qq_22238533/article/details/78666436]">https://blog.csdn.net/qq_22238533/article/details/78666436]</a></li>
<li>为啥auc<ul>
<li>随机抽出一对样本（一个正样本，一个负样本），然后用训练得到的分类器来对这两个样本进行预测，预测得到正样本的概率大于负样本概率的概率。</li>
</ul>
</li>
</ul>
</li>
<li><p>模型评估</p>
<ul>
<li>精确率<ul>
<li>分类正确的正样本 占 判定为正样本的总数 。</li>
</ul>
</li>
<li>召回率<ul>
<li>分类正确的正样本 占 真正正样本总数 。</li>
</ul>
</li>
<li>问题<ul>
<li>提高精确率，会更倾向于 “更有把握才把样本判定为正” ，此时保守而降低了召回率</li>
<li>即排序的TOPN，精确率很高，但是会漏掉 很多其他正样本。导致用户找不到自己想要的。</li>
<li>综合考虑<ul>
<li>PR 曲线<ul>
<li>x召回，y精确。</li>
</ul>
</li>
<li>f1 score<ul>
<li>调和平均值</li>
</ul>
</li>
<li>ROC 曲线</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql性能优化</title>
    <url>/2021/04/05/mysql%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<br>

<p>background知识、性能优化</p>
<span id="more"></span>
<ul>
<li><p>MyISAM 与 InnoDB </p>
<ul>
<li>InnoDB 支持事务、能回滚，MyISAM 不支持</li>
<li>MyISAM 适合查询以及插入为主的应用，修改多或者要安全则innodb</li>
<li>InnoDB 中必须包含只有自增长字段的索引，MyISAM可以和其他字段一起建立联合索引</li>
<li>清空整个表时，InnoDB 是一行一行的删除，效率非常慢。MyISAM 则会重建表</li>
<li>InnoDB 支持外键，MyISAM 不支持   <ul>
<li>外键<ul>
<li>定义外键约束，关系数据库可以保证无法插入无效的数据</li>
<li>可以把数据与另一张表关联起来，这种列称为外键</li>
<li>外键约束会降低数据库的性能，追求速度并不设置外键约束</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>多对多关系：通过一个中间表，关联两个一对多关系</p>
</li>
</ul>
<ul>
<li><p>分库分表</p>
<ul>
<li><p>原因</p>
<ul>
<li>单表数据大，sql效率低</li>
<li>并发高，单库撑不住；磁盘使用高</li>
</ul>
</li>
<li><p>分表</p>
<ul>
<li>经常读取和不经常读取的字段分开</li>
<li>把一个大的用户表分拆为用户基本信息表user_info和用户详细信息表user_profiles。大部分时候，只需要查询user_info表，提高了查询速度。</li>
</ul>
</li>
<li><p>中间件：proxy方案 or client层方案 sharding-jdbc mycat</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>crud</p>
</li>
<li><p>分布式id生成</p>
</li>
</ul>
<ul>
<li><p>垂直、水平拆分<br>中间件解决对某个字段值的自动路由，路由到相应的库、表<br>range、hash</p>
</li>
<li><p>迁移方案<br>数据库中间件的调研学习、设计分库分表方案、测试环境分库代码的正常读写<br>开始迁移：<br>1部署分库策略 2双写 3导数的时候，判断最后修改时间，确保只能新数据覆盖老数据 4自动校验，反复读写，直到新旧库数据一致 5停掉老库，部署分库分表的代码</p>
</li>
</ul>
<ul>
<li>动态扩容缩容分库分表<br>一般都够用：32库，每个库1500个写并发，qps可以承受4.8w。加上一个MQ削峰，qps接受8w，每秒消费5w。一个库32张表，1024表，每个表500w，一个mysql放50亿条数据。</li>
</ul>
<p>1确定机器数量、每台机器多少个库、多少表 2路由规则 3机器上装好mysql 4导数工具导数 5修改配置调整数据库服务器地址 6重发系统上线</p>
<ul>
<li><p>分库分表后的全局id<br>1并发不高，qps几百–直接往一个库写入，获得自增的id；起一个服务，专门地获取最大的id，返回一批新的id<br>2sequence+固定步长<br>3uuid –不具有有序性，b+树过多随机读写<br>4系统时间 + 业务数据<br>5雪花算法<br>第一个bit是0。41bit-时间，10bit是工作机器id，12bits序列号<br>同一ms内有12bits可表达的id</p>
</li>
<li><p>mysql读写分离<br>Case1在binlog没有被从拉到本地的中继日志中、串行地执行relaylog中的命令时，master宕机<br>Solution半同步，至少一个从返回ack才认为写操作完成。<br>Case2由于串行重发， 会有延迟几十到百毫秒不等<br>Solution2并行同步，从开启多线程，并行重放日志，库级别并行<br>mysql中查看延迟，严重解决：<br>分库、并行复制、新插入的数据查询不到时处理、查询直连主库</p>
</li>
<li><p>单库单表和分库分表的join</p>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql索引</title>
    <url>/2021/04/05/mysql%E7%B4%A2%E5%BC%95/</url>
    <content><![CDATA[<br>

<p>性能优化之 索引</p>
<span id="more"></span>


<ul>
<li><p>优点</p>
<ul>
<li>分组、排序操作</li>
<li>加速表与表之间的连接</li>
</ul>
</li>
<li><p>分类</p>
<ul>
<li><p>普通索引</p>
<ul>
<li>允许重复值和空值</li>
</ul>
</li>
<li><p>唯一索引</p>
<ul>
<li>必须唯一，允许空值</li>
<li>组合索引，则列值的组合必须唯一</li>
</ul>
</li>
<li><p>主键索引</p>
<ul>
<li>不允许值重复或者值为空</li>
</ul>
</li>
<li><p>空间索引</p>
<ul>
<li>空间数据类型的字段建立的索引</li>
</ul>
</li>
<li><p>全文索引</p>
<ul>
<li> CHAR、VARCHAR 或 TEXT 类型的列上创建</li>
<li>只有 MyISAM 存储引擎支持全文索引</li>
</ul>
</li>
</ul>
</li>
<li><p>位图索引</p>
<ul>
<li>　“select * from table where Gender=‘男’ and Marital=“未婚”;”<br>  首先取出男向量10100…，然后取出未婚向量00100…，<br>  将两个向量做and操作，这时生成新向量0010</li>
</ul>
</li>
<li><p>B与B+ </p>
<ul>
<li>B：树内的每个节点都存储数据；叶子节点之间无指针连接</li>
<li>B+：数据只出现在叶子节点；所有叶子节点增加了一个链指针</li>
</ul>
</li>
<li><p>聚集索引、非聚集索引 </p>
<ul>
<li>区别<ul>
<li>聚集：行中数据的物理顺序与键值的逻辑（索引）顺序相同</li>
<li>非聚集索引叶子节点上存储了索引字段自身值和主键索引</li>
</ul>
</li>
<li>聚集相关<ul>
<li>聚簇索引默认就是主键索引；第一个唯一非空索引被作为聚集索引；内部会生成一个隐藏的主键</li>
<li>uuid作为主键，写入性能低了</li>
</ul>
</li>
<li>非聚集<ul>
<li>索引不能随意增加。在做写库操作的时候，需要同时维护这几颗树的变化，导致效率降低</li>
<li>回表或者二次查询<ul>
<li>使用聚集索引查询可以直接定位到记录，而普通索引通常需要扫描两遍索引树</li>
<li>解决<ul>
<li>联合索引</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>联合索引</p>
<ul>
<li>最左匹配</li>
</ul>
</li>
<li><p>查询失效</p>
<ul>
<li>条件<ul>
<li>最左</li>
<li>OR条件中的每个列都加上索引</li>
<li>not in</li>
<li>前置通配符</li>
<li>is null</li>
</ul>
</li>
<li>函数<ul>
<li>where age-1=17</li>
<li>内置函数，索引失效 </li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title>node2vec</title>
    <url>/2021/04/05/node2vec/</url>
    <content><![CDATA[<ul>
<li><p>node2vec的随机游走</p>
<span id="more"></span>
<ul>
<li><p><a href="https://blog.csdn.net/rover2002/article/details/106760664">Alias Method: 非均匀随机抽样算法</a></p>
<ul>
<li>空间换时间的方法，在常数时间内，完成非均匀到均匀采样的映射。</li>
<li>主要思想： 概率转换为面积，然后把面积填到其他地方，总体长方形面积最小。由于有填到其他地方的，就有alias。这样之后再掷筛子，随机。</li>
</ul>
</li>
<li><p>跳转概率</p>
<ul>
<li><p>Node2vec 的跳转概率为 自定义的权重*有向边权重。</p>
</li>
<li><p>自定义权重，考虑了当前点vi的所有相关点 和 vi前一个node的距离。距离为0则概率1/p(即重复访问刚刚访问过的node)，为1则1，为2则1/q。</p>
</li>
<li><p>根据论文做法，对p q在{0.25,0.5,1,2,4}中进行grid search。</p>
</li>
<li><p>参数p控制重复访问刚刚访问过的顶点的概率。若p较大，则访问刚刚访问过的顶点的概率会变低。</p>
</li>
<li><p>参数q控制着游走是向外还是向内：<br>  若q&gt;1，随机游走倾向于访问和上一次的t接近的顶点(偏向BFS)；<br>  若q&lt;1，倾向于访问远离t的顶点(偏向DFS)</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>python</title>
    <url>/2021/04/05/python/</url>
    <content><![CDATA[<br>

<p>语言、语法糖相关</p>
<span id="more"></span>


<ul>
<li><p>python2 3 区别</p>
<ul>
<li><p>map filter，由function变为class，前者返回列表，后者为object</p>
</li>
<li><p>print由命令变为函数</p>
</li>
<li><p>编码由ASCII变为utf8</p>
</li>
<li><p>xrange</p>
<ul>
<li>要生成很大的数字序列的时候，用xrange会比range性能优很多，因为不需要一上来就开辟一块很大的内存空间。</li>
<li>range创建列表，xrange是生成器</li>
<li>3只保留了生成器的方式并命名为range</li>
</ul>
</li>
<li><p>除法 1/2 = 0 0.5</p>
</li>
</ul>
</li>
<li><p>滑动窗口算法 </p>
<ul>
<li>单调栈</li>
<li><a href="https://leetcode-cn.com/problems/sliding-window-maximum/">https://leetcode-cn.com/problems/sliding-window-maximum/</a></li>
</ul>
</li>
<li><p>abc</p>
<ul>
<li>注解abstractmethod，抽象基类</li>
</ul>
</li>
<li><p>标记清除</p>
<ul>
<li>对执行删除（-1）后的每个引用-1，为0的放到死亡容器，否则放到存活容器</li>
<li>循环存活容器，复活死亡中的变量放到存活容器内</li>
<li>删除死亡容器内的所有对象</li>
</ul>
</li>
<li><p>分代</p>
<ul>
<li>新创建的对象做为0代。每执行一个【标记-删除】，存活的对象代数就+1</li>
<li>代数越高的对象（存活越持久的对象），进行【标记-删除】的时间间隔就越长</li>
<li>一个对象10次检测都没给它干掉, 就认定这个对象一定很长寿, 就减少这货的”检测频率”</li>
</ul>
</li>
<li><p>触发垃圾回收</p>
<ul>
<li>调用gc.collect()</li>
<li>GC达到阀值时</li>
<li>程序退出时</li>
</ul>
</li>
<li><p>intern机制</p>
</li>
<li><p>copy deepcopy</p>
<ul>
<li>复制的值是不可变对象（数值，字符串，元组）,对象的id值与等式左边的id值相同。</li>
<li>复制的值是可变对象（列表和字典）<ul>
<li>浅拷贝两种情况：<ul>
<li>复制的 对象中无 复杂 子对象，浅复制的值改变也并不会影响原来的值反之也不影响</li>
<li>复制的对象中有 复杂 子对象， 改变原来的值 中的复杂子对象的值 ，会影响浅复制的值</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>cookie和session</p>
<ul>
<li>session 在服务器端，cookie 在客户端（浏览器）</li>
<li>session id 是存在 cookie 中</li>
<li>浏览器禁用了 cookie ，同时 session 也会失效</li>
<li>cookie安全性比session差</li>
</ul>
</li>
<li><p>正则</p>
<ul>
<li>正则表达式匹配中，（.<em>）和（.</em>?）匹配区别？<ul>
<li>贪婪、非贪婪</li>
</ul>
</li>
<li>正则re.complie<ul>
<li>re.compile是将正则表达式编译成一个对象，加快速度，并重复使用</li>
</ul>
</li>
</ul>
</li>
<li><p>动态类型dynamic typing</p>
<ul>
<li>将string传入mod，compilier并不会报错，仍然是原来compile出来的步骤。</li>
<li>mod(“%s%s”, (“py”,”thon”) )结果将是python，在c中，遇到BINARY_MOULO指令操作，如果是string类型则直接调用PyString_Format()。</li>
</ul>
</li>
</ul>
<ul>
<li><p>一切发生在运行时、命名空间。   </p>
<ul>
<li><p>命名空间分模块、类、方法。</p>
</li>
<li><p>编译只是从将代码在运行时转成成code object，当执行时转成function object。</p>
</li>
<li><p>def是一个assignment function(将return 的结果assign给变量)，遇到def 的时候实则就是call这个function并对参数进行assign。</p>
</li>
<li><p>类在创建时就开始执行，在一个字典所表示的命名空间中执行。这个命名空间用来创建类对象。</p>
</li>
<li><p>都是 对象/引用。变量只是名称，不是容器。字典中将名称映射到对象上。</p>
</li>
<li><p>三个scopes，local，global:module，builtin</p>
</li>
<li><p>not pthon object hooks：1，赋值，赋值将改变命名空间，而非对象自身 2，类型检查 3，is运算 4，and or not 都是布尔运算 5，调用，需要实现getattr来返回属性对象，该属性对象在call时执行某方法</p>
</li>
<li><p>如果要对object 移动，需要delete并清楚所有的references。</p>
</li>
<li><p>弱引用是当object删除之后被通知，即callback。</p>
</li>
<li><p>互相引用的reference cycle–&gt;需要cycle gc</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Cython PyPy</p>
<ul>
<li>Cpython中，PyObject都是以struct实现，包含type pointer、reference counting或其他C type。PyType struct有对type的一些描述信息。</li>
</ul>
</li>
<li><p>常用到的文本提取，任意字符串</p>
<ul>
<li>.* .*?</li>
</ul>
</li>
<li><p>编译 解释</p>
<ul>
<li>compiler部分很少，interpreter工作量远远多。</li>
<li>Python interpreter是一个virtual machine(模拟物理机)，是个stack machine(操作stacks)，和register machine(从内存不同位置读写)不同。</li>
<li>且是byte interpreter，input是instruction sets，元素为byte codes，lexing&amp;parsing将python codes 转为byte codes。相当于C codes和assembly codes。</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>编程语言</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>编程语言</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>redis_mindroad</title>
    <url>/2021/04/05/redis-mindroad/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>汇总</strong></p>
<span id="more"></span>


<ul>
<li><p>基础数据结构5种及场景</p>
</li>
<li><p>高可用</p>
<ul>
<li>持久化</li>
<li>集群<ul>
<li>主从</li>
<li>cluster</li>
<li>sentinel</li>
</ul>
</li>
</ul>
</li>
<li><p>高级数据结构</p>
<ul>
<li>bitmap</li>
<li>geo</li>
<li>hyperloglog</li>
</ul>
</li>
<li><p>并发竞争</p>
</li>
<li><p>分布式锁</p>
</li>
<li><p>内存淘汰机制</p>
</li>
<li><p>读写，cache aside pattern</p>
</li>
<li><p>单线程</p>
</li>
<li><p>选型</p>
</li>
<li><p>事务</p>
</li>
<li><p>分区</p>
</li>
<li><p>性能</p>
<ul>
<li>expire时间</li>
<li>pipeline</li>
<li>keys scan</li>
</ul>
</li>
<li><p>缓存</p>
<ul>
<li>缓存击穿</li>
<li>缓存穿透</li>
<li>缓存雪崩</li>
</ul>
</li>
<li><p>新特性</p>
</li>
<li><p>底层</p>
</li>
<li><p>ziplist、skiplist…</p>
</li>
</ul>
<p><img src="/2021/04/05/redis-mindroad/redis.png" alt="脑图"></p>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>redis</title>
    <url>/2021/01/31/redis/</url>
    <content><![CDATA[<h2 id="相关笔记"><a href="#相关笔记" class="headerlink" title="相关笔记"></a>相关笔记</h2><p>1、从自己拉还是定期拉 还是master发<br>第一次全量复制的时候，从发命令后主发rdb和写缓存<br>续传的时候：master维护backlog里有offset、master run id,从发送offset给主，如果没有则全量<br>其他情况，master会异步发送</p>
<span id="more"></span>
<p>2、单机写 读的并发量、集群并发量<br>几万、十万qps、几十万<br>单个value 1g</p>
<p>3、哨兵没有检测到主failure， 主自动重启，导致数据清空</p>
<p>4、通信及复制：<br>启动slave时：PSYNC给master<br>第一次连接—全量复制中，两个点：1rdb快照 2写命令缓存<br>全量参数：1时间60s 2内存缓冲区持续消耗和一次性超过<br>断点续传：网络故障的部分复制，offset</p>
<p>5、哨兵+主从复制<br>不保证0丢失，保证高可用</p>
<p>6、heartbeat<br>互相发送，主每10s、从每1s</p>
<p>7、主备切换前提、选举算法、哨兵master信息同步<br>选举前提 quorum和majority,至少满足max(quorum,majority)<br>选举算法：四个参数。<br>切换后其他的哨兵更新master配置：通过监听的channel中的version号，version号是负责切换的哨兵从新的master中获得的configuration epoch</p>
<p>8、持久化方式、优缺点、实现<br>Rdb：定期地冷备数据 — fork子进程进行磁盘io，不影响高性能，但如果rdb文件特别大则会影响服务，每隔5min宕机丢数据<br>aof：每1s后台fsync，最多丢1s数据。append写入文件，无磁盘寻址时间，文件尾部破损容易修复(?)、命令基于内存的数据重新构建而不是基于旧的指令日志 – rewrite log（指令压缩、旧的仍然提供服务，新的好了后替换），灾难性误删除.</p>
<p>9、并发竞争<br>redis的cas？zookeeper分布式锁？<br>采用CAS协议，则是如下的情景。<br> •第一步，A取出数据对象X，并获取到CAS-ID1；<br>•第二步，B取出数据对象X，并获取到CAS-ID2； <br>•第三步，B修改数据对象X，在写入缓存前，检查CAS-ID与缓存空间中该数据的CAS-ID是否一致。结果是“一致”，就将修改后的带有CAS-ID2的X写入到缓存。<br> •第四步，A修改数据对象Y，在写入缓存前，检查CAS-ID与缓存空间中该数据的CAS-ID是否一致。结果是“不一致”，则拒绝写入，返回存储失败。<br>这样CAS协议就用了“版本号”的思想，解决了冲突问题。（乐观锁概念）</p>
<p>在使用redis的setnx方法和memcace的add方法时，如果指定的key已经存在，则返回false。利用这个特性，实现全局锁<br>每次生成全局id前，先检测指定的key是否存在，如果不存在则使用redis的incr方法或者memcache的increment进行加1操作。这两个方法的返回值是加1后的值，如果存在，则程序进入循环等待状态。循环过程中不断检测key是否还存在，如果key不存在就执行上面的操作。</p>
<p>10、数据恢复<br>放到指定目录，然后重启redis，redis会恢复内存中的数据然后继续提供服务</p>
<p>11、哨兵–分布式<br>监控、修改地址(确保slave连接正确的master)、主从切换（确保潜在master的slave复制了所有的数据）、故障通知<br>两个配置：quorum、majority<br>quorum是至少多少哨兵认为宕机，才是master真的宕机<br>majority是必须满足大多数的哨兵是运行，才能进行故障转移。<br>=&gt;主备切换至少满足max(quorum,majority)<br>sdown odown</p>
<p>12、丢失：<br>case1slave没有同步完master的数据，master宕机<br>case2master机器脱离了集群，但是仍然运行，哨兵又选举了新的master。但是client还是在旧的写，旧的成为slave去新的master更新数据。这部分写丢失。<br>解决：两个参数。master宕机控制在丢失数据10s内。一点超过10s的数据复制，则master停止写请求。</p>
<p>13、哨兵自动发现<br>1往自己监控的channel里发消息 ，包括： runid、master监控配置、hostip<br>2监听channel，感知其他的哨兵<br>3监控配置的同步</p>
<p>14、cluster<br>Cluster bus通信、gossip协议</p>
<p>15、集群元数据维护方式<br>集中式：zookeeper作为实现，时效性高，存储更新有压力<br>gossip：分散更新，滞后</p>
<p>16、一致性hash、hash slot<br>hash的值空间组成一个环，将master的ip进行hash，确定在环的位置。数据找到位置后，存入顺时针走的第一个遇到的master。<br>当master宕机，则master和前一个master之间的数据受到影响。<br>热点问题：master计算多个hash值，在环中增加虚拟节点<br>slot：每个key计算crc16值，然后对16384取模，对应到相应的hash slot。每个master持有部分的slot</p>
<p>17、cluster中的选举、复制和哨兵<br>大于一半的master投票给该slave则该slave可以替换为master<br>cluster直接集成了复制和哨兵功能</p>
<p>18、缓存雪崩<br>所有的请求在redis都没有命中<br>解决：<br>redis高可用(主从+哨兵)、hystrix限流+本地ehcache缓存、redis持久化<br>先查本地、再redis、再限流，未通过的请求则降级</p>
<p>19、穿透<br>没查到则写一个空值到redis</p>
<p>20、击穿<br>热点key失效时缓存被瞬时击穿<br>1不用更新则永不过期 2更新频率高或者时间长则定时线程提前主动重新构建缓存或者延后过期时间 3更新频率低或者时间短，则分布式互斥锁或者本地锁保证少量请求能重新构建缓存，其余则锁释放后再访问新的缓存</p>
<p>21、双写一致性<br>Cache aside：读-先读缓存、再读数据库、再写入缓存，更新则先更新数据库、再删除缓存<br>为什么是删除不是更新？lazy<br>删除缓存失败？–先删除缓存再更新数据库<br>先删除缓存再更新数据库–还是会有不一致，每秒并发几万：当更新没完成，一个请求来了，写入缓存的是旧数据，然后又更新了数据库。</p>
<p>解决：串行化。<br>更新的数据都附带上数据标识，如果不在缓存中，则将读和更新发到一个队列中，线程从队列中串行地拿操作执行。<br>注意：1请求操作的超时（过多的写操作积压，一般单机20个队列，qps500的写可以支持，200ms里100个写，每个队列5个，每个20ms完成，那么读请求也可以在200ms内返回。否则扩容机器） 2读并发高 3热点商品的请求倾斜 4路由到同一台机子</p>
<p>22、线上部署情况<br>cluster模式，10台机器，5台master，一主一从<br>每个master高峰的qps 5w<br>master配置：32g8core+1T，分给redis内存最多10g<br>内存中放商品数据，每条数据大概10kb，10w条则1g，一般200w条，占20g。目前的qps高峰是3500左右请求量。</p>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>spark</title>
    <url>/2021/04/05/spark/</url>
    <content><![CDATA[<br>

<p>spark job相关</p>
<span id="more"></span>

<ul>
<li><p>groupby reduceby区别</p>
<ul>
<li>reduceByKey 会进行分区内聚合，函数内部调用了combineByKey，然后再进行网络传输</li>
<li>groupByKey 不会进行局部聚合</li>
</ul>
</li>
<li><p>依赖</p>
<ul>
<li>NarrowDependency分为OneToOneDependency和RangeDependency两种</li>
<li>ShuffleDependency：子依赖多个父RDD</li>
</ul>
</li>
<li><p>stage &amp; task</p>
<ul>
<li><p>DAGScheduler将Stage划分-&gt;把Stage转换为TaskSet-&gt;TaskScheduler将计算任务最终提交到集群</p>
</li>
<li><p>最后的Stage包含了一组ResultTask</p>
</li>
<li><p>task</p>
<ul>
<li>task和partition是一对一。一组Task就组成了一个Stage</li>
<li>Task<ul>
<li>ShuffleMapTask<ul>
<li>根据Task的partitioner将计算结果放到不同的bucket</li>
</ul>
</li>
<li>ResultTask<ul>
<li>计算结果发送回Driver</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>如何将stage划分为taskset</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Spark Standalone-cluster</p>
<ul>
<li>流程<ul>
<li>集群启动后worker向Master汇报资源，Master掌握集群资源</li>
<li>client提交sparkApplication，向Master申请启动Driver</li>
<li>Master收到请求后随机找一台节点启动Driver，Driver向Master申请executor进程资源</li>
<li>driver 里scheduler将DAG划分为stage，taskscheduler划分stage为taskset</li>
<li>Master找到满足资源的worker节点，启动Excutor，反向注册给Driver</li>
<li>Driver的context taskscheduler再把task发给executor，监控task，回收结果（collect方法）</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>缓存</p>
<ul>
<li>persist() 会把数据以序列化的形式缓存在 JVM 的堆空间中</li>
<li>触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用</li>
<li>StorageLevel</li>
<li>shuffle会涉及到网络传输，可能会丢失数据，shuffle之前做persist，框架默认将数据持久化到磁盘</li>
</ul>
</li>
<li><p>checkpoint</p>
<ul>
<li>在checkpoint的时候强烈建议先进行cache</li>
<li>当你checkpoint执行成功了,那么前面所有的RDD依赖都会被销毁</li>
<li>区别persist<ul>
<li>persist可以将 RDD 的 partition 持久化到磁盘，但该 partition 由 blockManager 管理, blockManager stop，文件夹被删除</li>
<li>checkpoint 将 RDD 持久化到 HDFS 或本地文件夹，是一直存在的，也就是说可以被下一个 driver program 使用</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>rdd</p>
<ul>
<li>本质上是多个Partition组成的List。一个RDD可以包含多个分区，每个分区就是一个dataset片段。分区数决定了并行计算的数量<br>默认情况下，HDFS上面一个Block就是一个Partition。</li>
<li>RDD如何保障数据处理效率<ul>
<li>通过persist与patitionBy函数来控制RDD的分区与持久化</li>
<li>底层接口则是基于迭代器的</li>
</ul>
</li>
</ul>
</li>
<li><p>容错</p>
<ul>
<li><p>rdd记住构建它的操作图（Graph of Operation），Worker失败时重新计算，无需replication</p>
</li>
<li><p>需要恢复执行过程的中间状态：通过Spark提供的checkpoint</p>
</li>
<li><p>lineAge：每次更新都会记录下来，比较复杂且比较耗费性能。适用于DAG中重算太耗费时间的</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>数据倾斜<ul>
<li>先局部聚合，再全局聚合。</li>
<li>并行度太少了，导致个别Task的压力太大</li>
<li>自定义partition，分散key的分布，使其更加均匀</li>
</ul>
</li>
</ul>
<ul>
<li><p>累加器和广播变量</p>
<ul>
<li><p>累加器</p>
<ul>
<li>累计计数等场景</li>
<li>Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新。</li>
</ul>
</li>
<li><p>广播变量</p>
<ul>
<li>每个executor一个副本，普通变量每个task一个副本</li>
<li>driver定义和修改</li>
<li>节点间高效分发大对象</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>序列化</p>
<ul>
<li><p>java序列化</p>
<ul>
<li>static和transient修饰的变量不会被序列化</li>
<li>readObject()方法和writeObject()自定义实现序列化</li>
</ul>
</li>
<li><p>spark序列反序列化过程</p>
<ul>
<li>代码中对象在driver本地序列化；</li>
<li>对象序列化后传输到远程executor节点；</li>
<li>远程executor节点反序列化对象</li>
</ul>
</li>
<li><p>SerializationDebugger在日志中出问题的类和属性</p>
</li>
<li><p>kryo </p>
<ul>
<li>Kryo serialization 性能和序列化大小都比默认提供的 Java serialization 要好</li>
<li><code>.registerKryoClasses(Array(classOf[Student])) // 将自定义的类注册到Kryo</code></li>
<li>Spark 2.0.0以来,sparkcontext初始化时某些类型已被注册进去</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>repartition和coalesce的区别、partitionby</p>
<ul>
<li>coalesce<ul>
<li>repartition底层为coalesce</li>
<li>一般增大rdd分区用repartition，减小用coalesce</li>
<li>repartition一定涉及shuffle，coalesce根据传入参数判断是否发生shuffle</li>
</ul>
</li>
<li>partitionby<ul>
<li>repartition 和 partitionBy 都是对数据进行重新分区，默认都是使用 HashPartitioner，区别在于partitionBy 只能用于 PairRDD</li>
<li>repartition 随机生成的数来当做 Key，partitionby是自己的key</li>
</ul>
</li>
</ul>
</li>
<li><p>map()和mapPartition()的区别</p>
<ul>
<li>map()：每次处理一条数据；mapPartition()：每次处一个分区的数据，可能导致OOM。 </li>
<li>当内存空间较大的时候建议使用mapPartition()，以提高处理效率。</li>
<li>mapPartitionsWithIndex类似mapPartitions，但func带有一个整数参数表示分片的索引值</li>
</ul>
</li>
<li><p>glom</p>
<ul>
<li>glom会把每个批次每个分区的数据从Iterator类型转换为Array类型，所以如果每个分区的数据非常大的话会出现OOM的情况。</li>
</ul>
</li>
<li><p>pipe</p>
<ul>
<li>每个分区一个脚本</li>
</ul>
</li>
<li><p>比hive快</p>
<ul>
<li>HQL 引擎还比 Spark SQL 的引擎更快</li>
<li>Hadoop 每次 shuffle 操作后，必须写到磁盘，spark内存</li>
<li>消除了冗余的 MapReduce 阶段</li>
<li>Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM<br> Spark 基于线程，只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。</li>
</ul>
</li>
<li><p>aggregateByKey foldByKey combineByKey </p>
<ul>
<li>aggregateByKey <ul>
<li><code>rdd.aggregateByKey(0)(math.max(_,_),_+_)</code></li>
<li>每个分区的各个key的value和初始值0，进行max，获得到每个key对应value的max<br>  然后每个分区进行combine即相加</li>
</ul>
</li>
<li> 计算相同key对应值的相加结果：<code>rdd.foldByKey(0)(_+_)</code></li>
<li>combineByKey<ul>
<li><code>input.combineByKey((_,1), (acc:(Int,Int),v)=&gt;(acc._1+v,acc._2+1),//v为当前值 (acc1:(Int,Int),acc2:(Int,Int))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2))</code></li>
<li>key对应的value先映射成一个二元组(value,1)，同一个分区内的相同key将二元组相加<br>再将不同分区的相同key的二元组相加。</li>
<li>相加时，元组的第一位和第二位都分别累加</li>
</ul>
</li>
</ul>
</li>
<li><p>cogroup</p>
<ul>
<li>第一个RDD元素是(1,”Allen”)，(2,”Bob”)，(3,”Carl”)，第二个RDD元素是(1,10000)，(1,5000)，(2,11000)，(2,6000)，(3,12000)，(3,6000)</li>
<li>join的结果是： (1,(Allen,10000)) (1,(Allen,5000)) (2,(Bob,11000)) (2,(Bob,6000)) (3,(Carl,12000)) (3,(Carl,6000)) </li>
<li>cogroup的结果是： (1,([Allen],[10000, 5000])) (2,([Bob],[11000, 6000])) (3,([Carl],[12000, 6000])) </li>
</ul>
</li>
<li><p>mapValues </p>
<ul>
<li>mapValues(_+”|||”)–每个value后加上”|||”</li>
</ul>
</li>
</ul>
<ul>
<li><p>T级别数据</p>
<ul>
<li><p>单个executor进程内RDD的分片数据是用Iterator流式访问的</p>
</li>
<li><p>RDD lineage上各个transformation携带的闭包函数复合而成的Iterator，<br>  每访问一个元素，就对该元素应用相应的复合函数，得到的结果再流式地落地<br>  对于shuffle stage是落地到本地文件系统留待后续stage访问，<br>  对于result stage是落地到HDFS或送回driver端等等，视选用的action而定</p>
</li>
<li><p>用户要求Spark cache该RDD，且storage level要求在内存中cache时，<br>Iterator计算出的结果才会被保留，通过cache manager放入内存池</p>
</li>
<li><p><a href="https://www.zhihu.com/question/23079001/answer/23569986">内存有限的情况下 Spark 如何处理 T 级别的数据</a></p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>spring</title>
    <url>/2021/10/06/spring/</url>
    <content><![CDATA[<h2 id="bean生命周期"><a href="#bean生命周期" class="headerlink" title="bean生命周期"></a>bean生命周期</h2><p>1、需要初始化<span id="more"></span>bean时，factory调用createBean进行实例化。<br>2、获取BeanDefinition对象中的信息进行实例化。并且这一步仅仅是简单的实例化，并未进行依赖注入。<br>3、实例化对象被包装在BeanWrapper对象中，接着Spring根据BeanDefinition中的信息进行依赖注入。<br>4、 注入Aware接口紧接着，Spring会检测该对象是否实现了xxxAware接口，并将相关的xxxAware实例注入给bean。<br>5、BeanPostProcessor当经过上述几个步骤后，bean对象已经被正确构造，但如果你想要对象被使用前再进行一些自定义的处理，就可以通过BeanPostProcessor接口实现。<br>6、InitializingBean与init-method<br>7、DisposableBean和destroy-method和init-method一样，通过给destroy-method指定函数，就可以在bean销毁前执行指定的逻辑。</p>
<p>keyword： factory createbean beandefinition wrapper aware postprocessor initializing destory</p>
<h2 id><a href="#" class="headerlink" title></a></h2>]]></content>
  </entry>
  <entry>
    <title>tensorflow</title>
    <url>/2021/04/05/tensorflow/</url>
    <content><![CDATA[<br>

<p>基础、遇到的问题、源码相关汇总</p>
<span id="more"></span>

<ul>
<li>tensorflow的调试<ul>
<li>nan灾难<ul>
<li>Debugger V2</li>
<li>tensorboard Debugger V2 GUI</li>
<li>tf.print()</li>
<li>tfdbg</li>
</ul>
</li>
<li>显存不断增长<ul>
<li>释放临时节点  auto reuse\是不是需要train的变量</li>
<li>避免在run里面进行运算产生临时节点，即出了结果再计算</li>
</ul>
</li>
<li>emb lookup耗时</li>
<li>多个id统一hash </li>
<li>gpu cpu copy</li>
</ul>
</li>
</ul>
<ul>
<li><p>神经网络八股<br>就不搞了。太多人做过了。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">checkpoint_save_path &#x3D; &quot;.&#x2F;checkpoint&#x2F;fashion.ckpt&quot;</span><br><span class="line">if os.path.exists(checkpoint_save_path + &#39;.index&#39;):</span><br><span class="line">    print(&#39;-------------load the model-----------------&#39;)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line"></span><br><span class="line">cp_callback &#x3D; tf.keras.callbacks.ModelCheckpoint(filepath&#x3D;checkpoint_save_path,</span><br><span class="line">                                                 save_weights_only&#x3D;True,</span><br><span class="line">                                                 save_best_only&#x3D;True)</span><br><span class="line"></span><br><span class="line">history &#x3D; model.fit(x_train, y_train, batch_size&#x3D;32, epochs&#x3D;5, validation_data&#x3D;(x_test, y_test), validation_freq&#x3D;1,</span><br><span class="line">                    callbacks&#x3D;[cp_callback])</span><br><span class="line">model.summary()</span><br><span class="line"></span><br><span class="line">print(model.trainable_variables)</span><br><span class="line">file &#x3D; open(&#39;.&#x2F;weights.txt&#39;, &#39;w&#39;)</span><br><span class="line">for v in model.trainable_variables:</span><br><span class="line">    file.write(str(v.name) + &#39;\n&#39;)</span><br><span class="line">    file.write(str(v.shape) + &#39;\n&#39;)</span><br><span class="line">    file.write(str(v.numpy()) + &#39;\n&#39;)</span><br><span class="line">file.close()</span><br><span class="line"></span><br><span class="line"># 显示训练集和验证集的acc和loss曲线</span><br><span class="line">acc &#x3D; history.history[&#39;sparse_categorical_accuracy&#39;]</span><br><span class="line">val_acc &#x3D; history.history[&#39;val_sparse_categorical_accuracy&#39;]</span><br><span class="line">loss &#x3D; history.history[&#39;loss&#39;]</span><br><span class="line">val_loss &#x3D; history.history[&#39;val_loss&#39;]</span><br><span class="line"></span><br><span class="line">plt.subplot(1, 2, 1)</span><br><span class="line">plt.plot(acc, label&#x3D;&#39;Training Accuracy&#39;)</span><br><span class="line">plt.plot(val_acc, label&#x3D;&#39;Validation Accuracy&#39;)</span><br><span class="line">plt.title(&#39;Training and Validation Accuracy&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(1, 2, 2)</span><br><span class="line">plt.plot(loss, label&#x3D;&#39;Training Loss&#39;)</span><br><span class="line">plt.plot(val_loss, label&#x3D;&#39;Validation Loss&#39;)</span><br><span class="line">plt.title(&#39;Training and Validation Loss&#39;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>todo</title>
    <url>/2021/06/10/todo/</url>
    <content><![CDATA[<br>

<br>

<h3 id="always-to-do"><a href="#always-to-do" class="headerlink" title="always to do"></a>always to do</h3><p>专利 办理居住证 </p>
<span id="more"></span>

<p>transformer、命令整理、配置个虚拟账户，开始开发自己有思考的代码</p>
<ul>
<li><p>工作问题repo</p>
</li>
<li><p>读论语</p>
</li>
<li><p>阶段性总结</p>
</li>
<li><p>投资逻辑</p>
</li>
<li><p>News投资逻辑摘抄</p>
</li>
<li><p>行为分析</p>
</li>
<li><p>c++-tips</p>
</li>
<li><p>debug训诫</p>
</li>
<li><p>Report读后感</p>
</li>
<li><p>transformer</p>
</li>
</ul>
<h3 id="系统"><a href="#系统" class="headerlink" title="系统"></a>系统</h3><ul>
<li>获得历史每一笔买卖，然后想办法做一些分析。每天的分时图、每一定比例的涨幅中累积的量是多少、统计出所有的筹码分布 – (所有买卖出来了 持仓自然出来了)、每一个价位上的成交笔数、做一些阈值过滤后的成交图形</li>
</ul>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>transformer</title>
    <url>/2021/06/07/transformer/</url>
    <content><![CDATA[<br>

<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>lstm的门机制对长依赖的解决不够彻底。更长的依赖关系无法学习到。并且门机制并行度提不上去。</p>
<span id="more"></span>

<br>

<h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>encoder-decoder架构，本质上是seqtoseq加上attention。</p>
<p>整体思想是，encoder接收初始序列，六层的最终输出，再输入到decoder的每一层模块。decoder能输出翻译后的句子。</p>
<p>encoder：输入序列进行embedding，然后加上position的encoding，进入self attention。self attention后面加入一个前馈网络层。（selfattention + 前馈FNN）为encoder的一个模块。</p>
<p>decoder：接收encoder的输入以及上一模块的输入。（self attention +encoderdecoder 的atten+ 前馈FNN）为decoder的一个模块。</p>
<br>

<h3 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h3><h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self attention"></a>self attention</h4><p>假设输入词序列A B，对应的embedding为 a b。三个矩阵，Q K V。</p>
<p>a b分别和Q K V相乘，各得到三个向量，aq ak av，bq bk bv。</p>
<p>aq * ak = aq_ak_score ;aq * bk = aq_bk_score。两个➗8。再将这些分数softmax。得到0-1之间。</p>
<p>这里，得到了a对所有其他词的分数向量。</p>
<p>再将av  bv分别和这些分数相乘后加和，得到了第一词A的输出。</p>
<p>类似得到B的输出。</p>
<br> 

<h4 id="矩阵形式"><a href="#矩阵形式" class="headerlink" title="矩阵形式"></a>矩阵形式</h4><p>inputX * WQ = Q;inputX * WK = K;inputX * WV = V</p>
<p>softmax( Q*K / 8 ) * V = attentionOut</p>
<br> 



<h4 id="多头"><a href="#多头" class="headerlink" title="多头"></a>多头</h4><p>多个attentionOut进行concat然后再*W0 = 新的out</p>
<p>W0进入模型一起训练</p>
<br> 



<h4 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h4><p><br> <br> </p>
<h4 id="ref"><a href="#ref" class="headerlink" title="ref"></a>ref</h4><p><a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a><br>        <a href="https://zhuanlan.zhihu.com/p/127774251">https://zhuanlan.zhihu.com/p/127774251</a></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>xgboost</title>
    <url>/2021/03/11/xgboost/</url>
    <content><![CDATA[<br>

<h1 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ul>
<li>xgboost是Gradient Boosting的一种高效系统实现</li>
</ul>
<span id="more"></span>
<h2 id="基学习器"><a href="#基学习器" class="headerlink" title="基学习器"></a>基学习器</h2><ul>
<li>tree(gbtree)，也可用线性分类器(gblinear)。GBDT则特指梯度提升决策树算法</li>
</ul>
<h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><ul>
<li><p>xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数</p>
</li>
<li><p>xgboost工具支持自定义代价函数</p>
</li>
<li><p>代价函数里加入了正则项</p>
<ul>
<li>树的叶子节点个数</li>
<li>每个叶子节点上输出的score的L2模的平方和</li>
<li>正则项降低了模型的variance</li>
</ul>
</li>
<li><p>Shrinkage（xgboost中的eta）</p>
<ul>
<li>xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。</li>
<li>一般把eta设置得小一点，然后迭代次数设置得大一点</li>
</ul>
</li>
<li><p>gamma </p>
<ul>
<li>当增益大于阈值时才让节点分裂，gamma即阈值</li>
<li>它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数的同时相当于做了预剪枝。</li>
</ul>
</li>
<li><p>lambda</p>
<ul>
<li>正则项里leaf score的L2模平方的系数，对leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性。</li>
</ul>
</li>
</ul>
<h2 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h2><ul>
<li>缺失值<ul>
<li>对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向</li>
</ul>
</li>
<li>列抽样<ul>
<li>xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算</li>
</ul>
</li>
</ul>
<h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><ul>
<li><p>在特征粒度上</p>
</li>
<li><p>最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点）</p>
</li>
<li><p>预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构</p>
</li>
<li><p>在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
</li>
<li><p>可并行的近似直方图算法</p>
<ul>
<li>用贪心法枚举所有可能的分割点，当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低</li>
</ul>
</li>
</ul>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ul>
<li><a href="https://www.zhihu.com/question/41354392">GBDT 和 XGBOOST 的区别有哪些</a></li>
<li>这个总结的也不错 <a href="https://zhuanlan.zhihu.com/p/38946959">https://zhuanlan.zhihu.com/p/38946959</a></li>
</ul>
<hr>
<p>mind:</p>
<p><img src="/2021/03/11/xgboost/xgboost.png" alt="xgboost"></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>《做生意的艺术》读后感</title>
    <url>/2021/05/23/%E3%80%8A%E5%81%9A%E7%94%9F%E6%84%8F%E7%9A%84%E8%89%BA%E6%9C%AF%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<p>1、夸张大胆，想象力</p>
<span id="more"></span>

<p>2、专注而游戏，能够悬空</p>
<p>3、重视宣传</p>
<p>4、对拒绝和批评，不太过放在心上</p>
<p>5、直觉判断。非常强悍的判断力</p>
<p>有了一个眼光后，认识到宝地。就开始找人商议，筹划合作。肯定是顶着压力的。但是大方向没有错。</p>
<p>6、手段</p>
<p>7、道德 ？ 生意？</p>
<p>很大程度是演戏和忽悠。但是有一定的真实度。演戏和演说的说服力，是一种能力。和道德无关。</p>
]]></content>
      <categories>
        <category>金融</category>
        <category>读后感</category>
      </categories>
      <tags>
        <tag>金融</tag>
        <tag>读后感</tag>
      </tags>
  </entry>
  <entry>
    <title>书list</title>
    <url>/2021/06/29/%E4%B9%A6list/</url>
    <content><![CDATA[<br>

<br>

<ol>
<li>Balanced Asset Allocation</li>
</ol>
<span id="more"></span>

<p>大幅提及的金属在RPAR中被弱化（被黄金和金属股票代替），如果拉出数据跑一跑，就知道虽然金属在2008年前表现不错，但2008年危机和之后一直在亏，如果RPAR用了20%的金属就无法宣传自己是equity-like return了。如果对历史再有了解，就知道金属之前的表现部分是因为其本身原是小众市场，而当其变为主流后是否能够复制之前的表现是值得商榷的（而这是Milken垃圾债券的逻辑悖论）。</p>
<p>自己其实可以复制、甚至是改良策略，同时把VOO、VBINX、RPAR当做标杆，看看backtesting是否能够照进现实。</p>
<ol start="2">
<li>BrianW.Kernighan ,Dennjs M Ritchie的c语言</li>
<li>Principles of Compiler Design</li>
<li>最醒目的是《可可·香奈儿的传奇一生》，以及压在一摞书底下、凯文·凯利写的《失控》</li>
<li>生活之道，威廉奥斯勒</li>
<li>Structure and Interpretation of Computer Programs   Harold Abelson/ Gerald Jay Sussman。<a href="https://book.douban.com/review/4559081/">https://book.douban.com/review/4559081/</a></li>
<li>肖星的财务思维课</li>
<li>secrets of tomb</li>
<li>三边委员会，毛泽东和耶鲁大学Johnathan Spence</li>
<li>Technical analysis of the futures markets JOHN J .MURPHY</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2021/02/21/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<br>
让我联想到博弈论中的决策树。根据多种可能地情况路径，利用概率和已知知识，进行决策判断。
<br>
<br>

<h4 id="基本问题"><a href="#基本问题" class="headerlink" title="基本问题"></a>基本问题</h4><p>Q1：如果建立一颗树，使得经过路径判断，走到叶子时候，能够得到最终样本归属的label(类别)或者y(连续值)？</p>
<ul>
<li>分析这棵树需要满足的基本性质<br>互斥、完备。即通过各条到叶子的不同路径，能够最终覆盖绝多数的样本。</li>
</ul>
<span id="more"></span>

<ul>
<li><p>生成这棵树的过程<br>1、要得到中间节点(特征)。分类能力越强的特征，越靠近根。<br>2、要找到当前特征的切分点，而进行分叉。<br>即涉及到特征选择(特征间重要性比较以及当前特征的切分点)。<br>而构建的过程，则是递归地选择特征的过程。<br>直到，没有特征可选、或者样本点已经完全覆盖(特征选择的标准达到阈值，不再分叉)。</p>
</li>
<li><p>分析回归树<br>1、回归树对特征选择和切分点选择的标准，肯定是区别于分类树的。<br>2、回归树的y如何生成？<br>分类树的label来自于多数投票。回归树则来自于，落入叶子的样本的均值。</p>
</li>
</ul>
<p>综上，<br>1、特征选择只在当前考虑选择最优，属于贪心策略，为局部最优。<br>2、本质上，是将样本空间进行直线(线性棱的空间体)切割。是概率中的条件概率。多个特征规则组合的条件下，样本所属类别的判定。<br>3、由于1，生成的树易产生过拟合。<br>4、回归树的回归结果，相比于lr等回归预测模型，结果数量是少的。</p>
<p>Q2：如何进行特征选择？</p>
<ul>
<li><p>对于分类树：<br>1、例如性别、年龄，如果已知性别能够比已知年龄，是更好的信息–即可以更大概率地判断出所属类别，那么就是更重要的特征。<br>2、其中，更大概率地判断出所属类别 – 即降低经验条件熵。<br>3、而经验条件熵的缺点是，对于取值比较多的特征，具有偏向性。其公式导致，取值多则经验条件熵值会小。可以考虑极端情况，当每个样本的该特征都取值不同，则经验条件熵为0。就有了C4.5，通过信息增益比来选择特征。<br>4、CART的gini系数本质和熵类似。越小，熵越小，确定性越高，特征越重要。</p>
</li>
<li><p>对于回归树：<br>1、cart回归树是二叉，要么是要么否。<br>2、回归树的切分选择不以熵入手考虑，而以均方误差作为判断标准。<br>3、对于当前特征，找到这样的切分点 – 使得此切分下的两个叶子样本集中，均方误差和最小。误差为：y-所有样本y的均值。<br>4、生成树过程和分类树一样。</p>
</li>
<li><p>为什么不用相关性？<br>余弦相似度是特征向量在label向量方向上的投影长度。一定程度也是特征重要的体现。<br>但是，在树模型中，由于本质是条件概率模型，熵就更能反映出概率模型的好坏。</p>
</li>
</ul>
<p>Q3：过拟合处理？</p>
<ul>
<li><p>剪枝<br>1、ID3,C4.5的剪枝是一个动态规划算法。对于某个非叶节点，计算剪去子树后的树的loss和不剪的loss。根据loss小的选择相应动作。而loss大小的比较，可以进行局部计算。因此可用动规实现。<br>2、loss为，每棵树的所有叶子上样本的熵之和，加上惩罚项(叶子数量或者节点数量)。<br>3、本质为，正则化的极大似然函数，来选择概率模型。</p>
</li>
<li><p>CART剪枝<br>1、区别于ID3&amp;C4.5，利用了递归思想，对所生成的树进行剪枝。<br>2、对于某个中间节点，找到进行剪枝动作的阈值–a的大小，a大于阈值则剪枝。即对于每一个中间节点，都有这样的阈值thre所决定的区间[thre,正无穷），剪枝后的树优于生成树。<br>3、对于所有的最优字数，交叉验证得到最终的最优树，并得到响应的a的thre。</p>
</li>
</ul>
<br>


<h4 id="扩展问题"><a href="#扩展问题" class="headerlink" title="扩展问题"></a>扩展问题</h4><p>Q4：信息增益比的缺点？<br>偏好取值少的特征。<br>C4.5不是直接选择增益比最大的特征，而是之前先把信息增益低于均值的属性剔除，然后在剩下的特征中选择而信息增益比最大的。得到兼顾。</p>
<p>Q5：预剪枝？<br>1、对当前节点在划分时，进行估计，如果不能够提升泛化能力则不进行划分。<br>2、本质是基于贪心，对当前节点进行判断是否划分。<br>3、坏处：当前虽然不能提升泛化能力，但可能划分后子树能够提升泛化能力。导致模型欠拟合。</p>
<p>Q6：对比LR和决策树？<br>1、y的结果上，决策树是固定的几个值。<br>2、LR比决策树慢，时间复杂度高。<br>3、LR无法处理缺失值，需要赋值。并且对极端值更敏感<br>4、LR对线性关系、全局拟合较好。决策树则是局部最优、局部数据探查更细致，不能更好地对多个特征同时考量。</p>
<p>Q7：CART做了哪些简化？<br>1、log函数的计算量大，而gini由图形可知，是对熵模型的很好的近似。<br>2、CART是二叉树，对每个特征进行二分而非多分，减少了特征选择时的计算。</p>
<p>Q8：测试集上缺失值的处理？<br>1、赋平均值或出现频次最高的值<br>2、走特征的常用分支<br>3、特征值专门处理的分支<br>C4.5的方式是，探查所有的分支，然后得到每个类别的概率，取最大的赋给该样本。</p>
<p>Q9：CART对于离散特征处理、连续特征处理的不同之处？<br>1、ID3&amp;C4.5都是多叉，特征只出现在一个中间节点；CART是对离散特征不断地二分。<br>2、连续特征：遍历每个特征，对某特征找到最小化均方误差(loss)的thre点，该点作为该特征的切分点。在所有特征中找到最小的loss，得到最优的(特征，特征切分点)作为中间节点。二分后，对每个孩子继续进行最优的(特征，特征切分点)的查找。同离散特征一样，连续特征可以重复出现，作为中间节点。直到达到停止条件。<br>3、CART连续特征：换种表述：对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。<br>4、ID3不支持连续特征划分。C4.5的做法是，将连续值进行排序，取两个值的中间值作为划分点，然后算信息增益比，(连续和离散的)最大的信息增益比为该中间节点。当连续值较多时，计算量的大。并且和CART一样，该连续特征同样可以继续作为中间节点，而非和离线一样一次性地生成多个孩子分叉。</p>
<p>Q10：缺点或者优化？<br>1、分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1。<br>2、如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。</p>
<p>其他–都可以在上述问题中找到答案：<br>如何找到切分点？</p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式mindroad</title>
    <url>/2021/04/05/%E5%88%86%E5%B8%83%E5%BC%8Fmindroad/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>汇总</strong></p>
<span id="more"></span>

<h2 id="分布式系统"><a href="#分布式系统" class="headerlink" title="分布式系统"></a>分布式系统</h2><p>1、层次结构</p>
<p>2、通信协议<br>默认dobbo<br>还有rmi hessian http webservice </p>
<p>3、序列化协议<br>默认hessian<br>java二进制序列化、json、soap</p>
<p>4、hessian数据结构<br>8种原始、3种递归、一种特殊类型</p>
<p>5、pb为什么最高性能<br>1编译器 2数据压缩</p>
<p>6、负载均衡策略<br>随机–根据权重，大则流量高<br>自动感知<br>一致性hash</p>
<p>7、集群容错<br>Failover<br>Failfast<br>Failsafe<br>Failback<br>Forking<br>Broadcast</p>
<p>8、动态代理<br>动态字节码生成</p>
<p>9、spi思想及扩展dubbo组件</p>
<p>10、服务治理<br> 1接口的调用次数和耗时 TP50/90/99、调用成功率、失败的监控<br> 2全链路的次数和耗时<br> 3timeout和retry<br> 4 Mock中实现降级逻辑<br> 5 分布式服务接口的幂等：redis+插入unique key<br> 6 接口调用的顺序性：会提升系统复杂度，降低效率-热点压力。dubbo进行hash负载均衡，然后打到一台机器，将order_id相同的请求放到该机器的一个线程内存队列中。更好的策略是合并请求。</p>
<hr>
<p>11、rpc框架设计</p>
<ul>
<li>zookeeper注册中心，本地动态代理发出请求，hessian序列化、长连接协议，负载均衡，服务端动态代理监听并代理实现</li>
<li>再加上 监控、配置化</li>
</ul>
<hr>
<p>12、zookeeper使用场景<br>分布式协调、分布式锁、注册中心(元数据管理)、HA高可用</p>
<ul>
<li>zk<ul>
<li>分布式协调<ul>
<li>A系统在zk建立一个监听，当系统B从MQ消费完订单消息，将zk状态改掉，zk则通知监听的系统A</li>
</ul>
</li>
<li>分布式锁<ul>
<li>a获取锁（尝试创建临时节点），b没得到就注册监听器（zk会将变化情况反向推送b），释放则获取到</li>
<li>临时节点保证，a宕机能够把临时节点自动删除，避免死锁</li>
</ul>
</li>
<li>配置中心/元数据<ul>
<li>dubbo的服务端地址都放到zk，消费从zk获取最新的地址，服务地址改动，zk及时地变化</li>
</ul>
</li>
<li>高可用<ul>
<li>a宕机了，删除zk的临时节点，监听的b及时发现则插入新的临时节点，a恢复发现有了b临时节点则注册监听</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>13、分布式锁及其效率<br>三点： 互斥、无死锁(业务意外情况下仍然可以获取锁)、高可用(锁的机子宕了，能够继续获取锁)</p>
<ul>
<li><p>zk</p>
<ul>
<li>while +  countdownlatch–await卡着 + 临时节点</li>
<li>临时顺序节点更好</li>
<li>创建临时的znode，别的客户端注册监听器，释放锁就是删除znode，释放了就会通知客户端</li>
</ul>
</li>
<li><p>redis<br>1如何保证原子性：lua<br>2业务时间&gt;定期删除时间：守护线程延长定期时间并设置超时放弃<br>3锁的误删除：uuid+threadid<br>4防止死锁：key加上过期时间 setnx expire</p>
</li>
<li><p>set orderic:lock 随机值 nx 3000 （nx代表无锁才能set返回ok），其他nil的节点每隔1s看是否还能set成功</p>
</li>
<li><p>释放：lua获取value对比是否是自己的，如果是则删除，否则不处理</p>
</li>
<li><p>redis主从架构时，由于异步更新到从，可能导致set的分布式锁失效</p>
</li>
</ul>
<p>5redis集群(三种:主从、哨兵、cluster)无法满足高可用：主从是为了读写分离，锁还是在master上面。哨兵机制的集群，当master上的锁没有同步到slave时，此时有加锁请求则仍能够获得锁，不满足互斥性。三主三从的cluster模式(hash槽分配)，超过了10w+的并发，则一个主的槽不可用，则该redis集群全不可用。</p>
<p>6时间漂移：硬件解决</p>
<p>7 redlock：多个实例，但是不是集群关系。锁的有效时间= ttl-时间漂移(所有实例加锁花费的时间)。</p>
<p>获取锁只需要超过一半的实例获取成功。避免死锁：重试机制-加锁不成功则撤回请求(避免死锁)，并且在重试时加上随机时间，避免同时加锁的请求都重试结果都无法获得。避免不互斥：延迟重启-如果master挂了，延迟ttl时间对slave重启替换为master。<br>TTL&gt; 业务执行时间+redis加锁时间+时钟漂移<br>删除锁则所有实例都执行删除。</p>
<ul>
<li>redlock 不健壮、无效请求<ul>
<li>至少一半以上节点set成功且在超时时间内，才算创建成功。否则说明失败，通过lua删除已经set的节点。</li>
<li>没成功就每隔1s去看是否被释放</li>
</ul>
</li>
</ul>
<hr>
<p>14、分布式接口幂等性</p>
<ul>
<li>数据库unique key</li>
<li>redis ：set orderid payed</li>
</ul>
<hr>
<p>15、分布式接口顺序性</p>
<ul>
<li>接入服务进行分发到(hash分发)不同机子，然后机子分发到一个线程的内存队列<br>  或者接入服务分发到MQ，然后其他机子从MQ拉消费</li>
<li>但如果接入服收到的顺序不是你要的，那还是无法完全保证顺序</li>
<li>完全保证顺序<ul>
<li>分布式锁<ul>
<li>不仅以orderid标识，还需要有标识请求序号的seq。<br>  请求先获取zookeeper锁，查库，如果seq不对则释放锁。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>16、分布式会话</p>
<ul>
<li>spring session + Redis，解耦web容器</li>
</ul>
<hr>
<p>17、分布式事务</p>
<ul>
<li><p>事务ACID </p>
<ul>
<li>全部结束或者回滚、和5000转账不影响和、并发事务相互不影响(脏读(看到另一个事务的中间状态)、commit读、重复读、串行化)、宕机之后恢复数据和事务成功结束后一致</li>
</ul>
</li>
<li><p>CAP </p>
<ul>
<li>获得准确数据、及时响应返回、单节点挂了不影响其他</li>
<li>一般选择AP – 柔性事务 base理论，达到最终一致。CA是一个必须有分布式锁，一个可以没有</li>
</ul>
</li>
<li><p>2pc</p>
<ul>
<li>依赖数据库、不适合高并发</li>
<li>第一阶段，每个节点记录log，并且返回给总协调是否成功，第二阶段，有一个不成功则协调让所有的人回滚。</li>
<li>SEATA <ul>
<li>第一阶段各个节点就提交然后释放锁，第二阶段如果成功则删除log，否则按照log回滚。（全局事务id和分支事务id，定位到log）</li>
</ul>
</li>
</ul>
</li>
<li><p>tcc</p>
<ul>
<li>转账的例子</li>
<li>严格、业务代码繁琐</li>
<li>try 系统a预留系统bc资源，锁</li>
<li>confirm  rpc调用系统b扣减，调用系统c转账，本地记下log</li>
<li>cancel 回滚</li>
</ul>
</li>
<li><p>可靠消息最终一致性</p>
<ul>
<li>本地消息表<ul>
<li>todo</li>
</ul>
</li>
<li>rocketmq事务消息</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式事务</title>
    <url>/2021/02/04/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-%E4%B8%8A/</url>
    <content><![CDATA[<p><strong>摘要</strong></p>
<p>1、什么是事务、分布式事务，分布式事务的场景<br>2、CAP理论，及BASE理论，柔性事务</p>
<span id="more"></span>
<p>3、分布式事务解决模型:2PC TCC （概念、区别）<br>4、2PC解决方案：XA、AT（角色、流程、实战、缺点、区别）<br>5、TCC解决方案：三种异常处理、Hmily实现</p>
<h2 id="1、事务、分布式事务及场景"><a href="#1、事务、分布式事务及场景" class="headerlink" title="1、事务、分布式事务及场景"></a>1、事务、分布式事务及场景</h2><ul>
<li><p>一般利用数据可事务特性，为数据库事务。数据库和应用在一个服务器，则为本地事务。</p>
</li>
<li><p>事务则需要有ACID性质。</p>
<ul>
<li>原子性、一致性、隔离性(一个事务不能看到其他事务的中间状态)、持久性</li>
</ul>
</li>
<li><p>而分布式事务则和分布式架构相关，当服务被拆分，并通过网络进行协作，则一个事务就涉及到多个服务以及远程调用。此时为分布式事务。</p>
</li>
<li><p>场景：</p>
<ul>
<li>微服务架构中，即需要跨JVM进程。无论是多个服务访问一个实例，还是多个服务多个实例，本地事务都无法解决。</li>
<li>单个系统访问多个数据库实例，就需要进行不同的数据库连接</li>
</ul>
</li>
</ul>
<br>

<h2 id="2、CAP"><a href="#2、CAP" class="headerlink" title="2、CAP"></a>2、CAP</h2><h3 id="分布式事务控制目标"><a href="#分布式事务控制目标" class="headerlink" title="分布式事务控制目标"></a>分布式事务控制目标</h3><p>场景：mysql一主一从，商品写主读从。</p>
<p>一致性、可用性、分区容错性</p>
<ul>
<li><p>consistency：写操作后的读(任意节点读)可以读到最近状态。</p>
<ul>
<li>在从同步数据的过程中，将从锁住，同步完再允许查询。 需要1写响应有延迟；2资源锁定与释放；3同步失败则返回错误信息而不能返回旧数据</li>
</ul>
</li>
<li><p>availability：任何事务操作都可以得到响应，且不会响应错误或超时。</p>
<ul>
<li>需要能够立即响应并非错误、非超时，可以允许旧数据。需要1同步时不能锁定；2返回旧数据或者默认值</li>
</ul>
</li>
<li><p>partitio tolerance：分布式各个节点在不同的子网，就形成了网络分区，彼此需要通过网络进行交互，当网络通信失败时，仍能够提供服务。</p>
<ul>
<li>单个节点挂了不影响其他节点，同步时不影响读写操作。需要1添加主备从备节点、避免主或从挂了；2异步进行数据从主到从的同步</li>
</ul>
</li>
</ul>
<blockquote>
<p>三个特性不能共存。一般选择AP，达到最终一致性即可。</p>
</blockquote>
<ul>
<li>AP，通常实现AP都会保证最终一致性</li>
<li>CP，zookeeper追求的就是强一致</li>
<li>CA，不进行分区，本地事务隔离级别即可。</li>
</ul>
<br>

<h2 id="BASE理论与柔性事务"><a href="#BASE理论与柔性事务" class="headerlink" title="BASE理论与柔性事务"></a>BASE理论与柔性事务</h2><p>在AP中，满足1 基本可用 2 软状态 3 最终一致。即满足base，为柔性事务。</p>
<ul>
<li>软状态: 中间状态– 当同步过程中来了查询，给出“支付中”状态，一致后再返回“成功”。</li>
</ul>
<br>

<h2 id="3、2PC"><a href="#3、2PC" class="headerlink" title="3、2PC"></a>3、2PC</h2><h3 id="3-1-2PC概念"><a href="#3-1-2PC概念" class="headerlink" title="3.1 2PC概念"></a>3.1 2PC概念</h3><ul>
<li><p>两阶段提交协议，prepare准备阶段和commit提交阶段。</p>
</li>
<li><p>包含 事务管理器和事务参与者(数据库实例)。管理器决定整个事务的提交和回滚，参与者负责本地事务的提交和回滚。</p>
</li>
<li><p>过程：</p>
<ul>
<li>prepare： 管理器向每个实例发送prepare消息，每个实例写本地的undo(修改前的数据)和redo(修改后的数据)日志。此时没有提交。</li>
<li>commit： 管理器收到参与者的失败或超时，则向每个参与者发送rollback消息。否则向每个实例发送commit。每个参与者进行执行指令并释放锁。</li>
</ul>
</li>
</ul>
<br>

<h3 id="3-2-2PC解决方案"><a href="#3-2-2PC解决方案" class="headerlink" title="3.2 2PC解决方案"></a>3.2 2PC解决方案</h3><h4 id="3-2-1-XA方案"><a href="#3-2-1-XA方案" class="headerlink" title="3.2.1 XA方案"></a>3.2.1 XA方案</h4><p>规范数据库实现2pc协议的分布式事务处理模型DTP。<br>定义了角色：AP RM TM,TM 和 RM 通讯接口为XA。即数据库提供的2pc接口协议，基于该协议的2pc实现为xa方案。</p>
<p>缺点：<br>1、资源锁需要事务的两个阶段结束才能释放<br>2、本地数据库需要支持XA协议<br><br></p>
<h4 id="3-2-2-Seata方案"><a href="#3-2-2-Seata方案" class="headerlink" title="3.2.2 Seata方案"></a>3.2.2 Seata方案</h4><p>Seata是提供AT和TCC模式的分布式事务解决方案。</p>
<ul>
<li><p>与XA区别：</p>
<ul>
<li>1、XA是两阶段后释放锁，而AT模式(2pc)第一阶段则提交释放锁</li>
<li>2、AT是应用层的中间件，对业务0侵入。</li>
</ul>
</li>
<li><p>实现：</p>
<ul>
<li>三个角色，TC TM RM。</li>
<li>TC负责，接收TM的全局事务提交或回滚指令，和RM通信协调分支事务。</li>
<li>TM，开启全局事务，向TC发出提交或回滚指令。</li>
<li>RM，分支注册、状态汇报、接收TC指令、驱动本地事务提交或回滚的执行。</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li>具体的执行流程如下:</li>
</ul>
<ol>
<li>用户服务的 TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的XID。</li>
<li>用户服务的 RM 向 TC 注册 分支事务，该分支事务在用户服务执行新增用户逻辑，并将其纳入 XID 对应全局<br>事务的管辖。</li>
<li>用户服务执行分支事务，向用户表插入一条记录。</li>
<li>逻辑执行到远程调用积分服务时(XID 在微服务调用链路的上下文中传播)。积分服务的RM 向 TC 注册分支事<br>务，该分支事务执行增加积分的逻辑，并将其纳入 XID 对应全局事务的管辖。</li>
<li>积分服务执行分支事务，向积分记录表插入一条记录，执行完毕后，返回用户服务。</li>
<li>用户服务分支事务执行完毕。</li>
<li>TM 向 TC 发起针对 XID 的全局提交或回滚决议。</li>
<li>TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。</li>
</ol>
<hr>
<ul>
<li>详解流程：<ul>
<li>每个RM使用DataSourceProxy连接数据库，其目的是使用ConnectionProxy，使用数据源和数据连接代理的目 的就是在第一阶段将undo_log和业务数据放在一个本地事务提交，这样就保存了只要有业务操作就一定有 undo_log。</li>
<li>在第一阶段undo_log中存放了数据修改前和修改后的值，为事务回滚作好准备，所以第一阶段完成就已经将分 支事务提交，也就释放了锁资源。</li>
<li>TM开启全局事务开始，将XID全局事务id放在事务上下文中，通过feign调用也将XID传入下游分支事务，每个 分支事务将自己的Branch ID分支事务ID与XID关联。</li>
<li>第二阶段全局事务提交，TC会通知各各分支参与者提交分支事务，在第一阶段就已经提交了分支事务，这里各各参与者只需要删除undo_log即可，并且可以异步执行，第二阶段很快可以完成。</li>
<li>第二阶段全局事务回滚，TC会通知各各分支参与者回滚分支事务，通过 XID 和 Branch ID 找到相应的回滚日志，通过回滚日志生成反向的 SQL 并执行，以完成分支事务回滚到之前的状态，如果回滚失败则会重试回滚操作。</li>
</ul>
</li>
</ul>
<ul>
<li><p>实战step：</p>
<ul>
<li>下载，解压并启动seata服务器 /bin/seata-server.bat -p 8888 -m file 。端口和文件方式存储信息。(TC)</li>
<li>添加discover-server子模块，discover-server基于Eureka实现。</li>
<li>添加微服务子模块，子模块pom中引入spring-cloud-alibaba-seata（包含了RM TM），并配置TC的地址：registry.conf、file.conf中：</li>
</ul>
<blockquote>
<p>在file.conf中更改service.vgroup_mapping.[springcloud服务名]-fescar-service-group = “default”，并修改 service.default.grouplist =[seata服务端地址]</p>
</blockquote>
<ul>
<li>创建代理数据源（配置mysql连接）</li>
</ul>
<blockquote>
<p>Seata的RM通过DataSourceProxy才能在业务代码的事务提交时，通过这个切<br>  入点，与TC进行通信交互、记录undo_log等。每个RM使用DataSourceProxy连接数据库，其目的是使用ConnectionProxy，使用数据源和数据连接代理的目 的就是在第一阶段将undo_log和业务数据放在一个本地事务提交，这样就保存了只要有业务操作就一定有 undo_log。</p>
</blockquote>
<ul>
<li>代码部分细节：<code>FeignClient</code>、<code>@GlobalTransactional</code>、<code>@Transactional</code></li>
</ul>
<blockquote>
<p>将@GlobalTransactional注解标注在全局事务发起的Service实现方法上，开启全局事务: GlobalTransactionalInterceptor会拦截@GlobalTransactional注解的方法，生成全局事务ID(XID)，XID会在整个分布式事务中传递。 在远程调用时，spring-cloud-alibaba-seata会拦截Feign调用将XID传递到下游服务。</p>
</blockquote>
</li>
</ul>
<br>

<h3 id="4、TCC"><a href="#4、TCC" class="headerlink" title="4、TCC"></a>4、TCC</h3><h4 id="4-1-TCC相关解决方案"><a href="#4-1-TCC相关解决方案" class="headerlink" title="4.1 TCC相关解决方案"></a>4.1 TCC相关解决方案</h4><p>tcc-transaction、<br>Hmily、<br>ByteTcc、<br>EasyTransaction</p>
<br>

<h4 id="4-2-TCC的三种异常处理"><a href="#4-2-TCC的三种异常处理" class="headerlink" title="4.2 TCC的三种异常处理"></a>4.2 TCC的三种异常处理</h4><ol>
<li><strong>空回滚</strong><br> 没有执行try就执行了cancel</li>
<li><strong>幂等</strong><br> cancel和commit会重试</li>
<li><strong>悬挂</strong><br> RPC 调用分支事务try时，先注册分支事务，再执行RPC调用，如果此时 RPC 调用的网络发生拥堵， 通常 RPC 调用是有超时时间的，RPC 超时以后，TM就会通知RM回滚该分布式事务，可能回滚完成后，RPC 请求 才到达参与者真正执行，而一个 Try 方法预留的业务资源，只有该分布式事务才能使用，该分布式事务第一阶段预 留的业务资源就再也没有人能够处理了，对于这种情况，我们就称为悬挂，即业务资源预留后没法继续处理。</li>
</ol>
<p>转账最终方案：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;A</span><br><span class="line">try: </span><br><span class="line">	try幂等校验</span><br><span class="line">	try悬挂处理 </span><br><span class="line">	检查余额是否够30元 </span><br><span class="line">	扣减30元</span><br><span class="line">confirm: </span><br><span class="line">	空</span><br><span class="line">cancel: </span><br><span class="line">	cancel幂等校验</span><br><span class="line">	cancel空回滚处理 </span><br><span class="line">	增加可用余额30元</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;B</span><br><span class="line">try: </span><br><span class="line">	空</span><br><span class="line">confirm: </span><br><span class="line">	confirm幂等校验</span><br><span class="line">	正式增加30元 </span><br><span class="line">cancel:</span><br><span class="line">	空</span><br></pre></td></tr></table></figure>

<h4 id="4-3-Hmily实现TCC事务"><a href="#4-3-Hmily实现TCC事务" class="headerlink" title="4.3 Hmily实现TCC事务"></a>4.3 Hmily实现TCC事务</h4><ul>
<li><p>新增配置类接收application.yml中的Hmily配置信息，并创建HmilyTransactionBootstrap Bean</p>
</li>
<li><p>A账户</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;DAO 略</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;service </span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">    @Transactional</span><br><span class="line">    @Hmily(confirmMethod &#x3D; &quot;commit&quot;, cancelMethod &#x3D; &quot;rollback&quot;)</span><br><span class="line">	public  void updateAccountBalance(String accountNo, Double amount) &#123;</span><br><span class="line">		&#x2F;&#x2F;事务id</span><br><span class="line">		&#x2F;&#x2F;try幂等校验</span><br><span class="line">		&#x2F;&#x2F;try悬挂处理</span><br><span class="line">		&#x2F;&#x2F;从账户扣减及扣减失败</span><br><span class="line">		&#x2F;&#x2F;增加本地事务try成功记录，用于幂等性控制标识 accountInfoDao.addTry(transId);</span><br><span class="line">		&#x2F;&#x2F;远程调用bank2</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;feign</span><br><span class="line"></span><br><span class="line">	@FeignClient(value &#x3D; &quot;seata‐demo‐bank2&quot;, fallback &#x3D; Bank2Fallback.class)</span><br><span class="line">	public interface Bank2Client &#123;</span><br><span class="line">		@GetMapping(&quot;&#x2F;bank2&#x2F;transfer&quot;)</span><br><span class="line">		@Hmily</span><br><span class="line">		Boolean transfer(@RequestParam(&quot;amount&quot;) Double amount);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;controller 略</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>B账户 略</li>
</ul>
<h4 id="4-4-TCC与2PC对比"><a href="#4-4-TCC与2PC对比" class="headerlink" title="4.4 TCC与2PC对比"></a>4.4 TCC与2PC对比</h4><ol>
<li>2PC通常在DB层面，TCC在应用层面</li>
<li>2PC无侵入性，TCC自定义数据操作粒度，锁冲突降低，提高吞吐，但业务逻辑每个分支都需要实现TCC三个方法，且需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。</li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式事务-下</title>
    <url>/2021/02/04/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-%E4%B8%8B/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>摘要</strong></p>
<p>1、可靠消息最终一致性，概念特点、问题、针对问题的方案(本地消息表、Rocketmq)<br>2、本地消息表流程图<br>3、Rocketmq流程、实战<br>4、最大努力通知，流程、</p>
<span id="more"></span>

<h2 id="可靠消息最终一致性"><a href="#可靠消息最终一致性" class="headerlink" title="可靠消息最终一致性"></a>可靠消息最终一致性</h2><p>当事务发起方执行完成本地事务后并发出一条消息，事务参与方(消息消费者)一定能 够接收消息并处理事务成功，此方案强调的是只要消息发给事务参与方最终事务要达到一致。</p>
<ul>
<li><p>特点</p>
<ul>
<li>适合执行周期长且实时性要求不高的场景</li>
<li>引入消息机制后，同步的事务操作变为基于消 息执行的异步操作, 避免了分布式事务中的同步阻塞操作的影响，并实现了两个服务的解耦。</li>
</ul>
</li>
<li><p>网络通信的不确定性会导致分布式事务问题：</p>
</li>
</ul>
<ol>
<li>事务发起方(消息生产方)将消息发给消息中间件</li>
<li>事务参与方从消息中间件接收消息</li>
</ol>
<h3 id="可靠消息最终一致性的问题"><a href="#可靠消息最终一致性的问题" class="headerlink" title="可靠消息最终一致性的问题"></a>可靠消息最终一致性的问题</h3><p>1、本地事务与消息发送的原子性问题</p>
<ul>
<li>发送消息成功，数据库操作失败  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">begin transaction; </span><br><span class="line">		&#x2F;&#x2F;1.发送MQ</span><br><span class="line">		&#x2F;&#x2F;2.数据库操作 </span><br><span class="line">commit transation;</span><br></pre></td></tr></table></figure></li>
<li>如果发送MQ消息失败，就会抛出异常，导致数据库事务回滚。但如果是超时异常，数 据库回滚，但MQ其实已经正常发送了，同样会导致不一致。  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">begin transaction; </span><br><span class="line">	&#x2F;&#x2F;1.数据库操作</span><br><span class="line">	&#x2F;&#x2F;2.发送MQ </span><br><span class="line">commit transation;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>2、事务参与方接收消息的可靠性，需要如果接收消息失败可以重复接收消息</p>
<p>3、消息重复消费的问题，要解决消息重复消费的问题就要实现事务参与方的方法幂等性。 </p>
<h2 id="本地消息表方案"><a href="#本地消息表方案" class="headerlink" title="本地消息表方案"></a>本地消息表方案</h2><p><img src="/2021/02/04/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1-%E4%B8%8B/bendixiaoxibiao.png" alt="本地消息表方案"></p>
<ul>
<li>交互流程规范</li>
</ul>
<ol>
<li>用户服务在本地事务新增用户和增加 ”积分消息日志“。(用户表和消息表通过本地事务保证一致)</li>
<li>定时任务扫描日志–&gt;保证将消息发送给消息队列。启动独立的线程，定时对消息日志表中的消息进行扫描并发送至消息中间件，在消息中间件反馈发送成功后删除该消息日志，否则等待定时任务下一周期重试。</li>
<li>MQ的ack(即消息确认)机制–&gt;幂等，收到ack，MQ将不再向消费者推送消息，否则消费者会不断重 试向消费者来发送消息。</li>
</ol>
<h2 id="RocketMQ事务消息方案"><a href="#RocketMQ事务消息方案" class="headerlink" title="RocketMQ事务消息方案"></a>RocketMQ事务消息方案</h2><ul>
<li>交互流程实现</li>
</ul>
<ol>
<li>Producer (MQ发送方)发送事务消息至MQ Server，MQ Server将消息状态标记为Prepared(预备状态)，注意此时这条消息消费者(MQ订阅方)是无法消费到的。</li>
<li>MQ Server接收到Producer 发送给的消息则回应发送成功表示MQ已接收到消息。</li>
<li>Producer 端执行业务代码逻辑，通过本地数据库事务控制。</li>
<li>若Producer 本地事务执行成功则自动向MQServer发送commit消息，MQ Server接收到commit消息后将状态标记为可消费，此时MQ订阅方(积分服务)即正常消费消息。若Producer 本地事务执行失败则自动向MQServer发送rollback消息，MQ Server接收到rollback消息后 将删除消息。</li>
<li>如果执行Producer端本地事务过程中，执行端挂掉或者超时，MQ Server将会不停的询问同组的其他 Producer来获取事务执行状态，这个过程叫事务回查。MQ Server会根据事务回查结果来决定是否投递消息。 </li>
</ol>
<br>

<ul>
<li>以上主干流程已由RocketMQ实现，对用户侧来说，用户需要：</li>
</ul>
<ol>
<li>实现本地事务执行</li>
<li>本地事务回查方法，因此关注本地事务的执行状态</li>
</ol>
<h3 id="RocketMq事务"><a href="#RocketMq事务" class="headerlink" title="RocketMq事务"></a>RocketMq事务</h3><p>RocketMQ主要解决了两个功能:<br>1、本地事务与消息发送的原子性问题。<br>2、事务参与方接收消息的可靠性。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;回调</span><br><span class="line"></span><br><span class="line"> public interface RocketMQLocalTransactionListener &#123;</span><br><span class="line">      &#x2F;**</span><br><span class="line">		‐ 发送prepare消息成功此方法被回调，该方法用于执行本地事务</span><br><span class="line">		‐ @param msg 回传的消息，利用transactionId即可获取到该消息的唯一Id</span><br><span class="line">		‐ @param arg 调用send方法时传递的参数，当send时候若有额外的参数可以传递到send方法中，这里能获取到</span><br><span class="line">		‐ @return 返回事务状态，COMMIT:提交 ROLLBACK:回滚 UNKNOW:回调</span><br><span class="line">		*&#x2F;</span><br><span class="line">          RocketMQLocalTransactionState executeLocalTransaction(Message msg, Object arg);</span><br><span class="line"></span><br><span class="line">      	&#x2F;**</span><br><span class="line">		‐ @param msg 通过获取transactionId来判断这条消息的本地事务执行状态</span><br><span class="line">		‐ @return 返回事务状态，COMMIT:提交 ROLLBACK:回滚 UNKNOW:回调</span><br><span class="line">		*&#x2F;</span><br><span class="line">          RocketMQLocalTransactionState checkLocalTransaction(Message msg);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> &#x2F;&#x2F; 发送事务消息API  </span><br><span class="line">TransactionMQProducer producer &#x3D; new TransactionMQProducer(&quot;ProducerGroup&quot;); producer.setNamesrvAddr(&quot;127.0.0.1:9876&quot;);</span><br><span class="line">producer.start();</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;设置TransactionListener实现 producer.setTransactionListener(transactionListener);</span><br><span class="line">&#x2F;&#x2F;发送事务消息</span><br><span class="line">SendResult sendResult &#x3D; producer.sendMessageInTransaction(msg, null);  </span><br></pre></td></tr></table></figure>


<ul>
<li><p>实践：</p>
<ul>
<li>在application-local.propertis中配置rocketMQ nameServer地址及生产组</li>
<li>service</li>
</ul>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;从event构建消息体</span><br><span class="line">	public void sendUpdateAccountBalance(AccountChangeEvent accountChangeEvent) </span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;发送消息并收到回应后执行本地事务</span><br><span class="line">public void doUpdateAccountBalance(AccountChangeEvent accountChangeEvent)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;实现执行本地事务和事务回查两个方法</span><br><span class="line"> 	@RocketMQTransactionListener(txProducerGroup &#x3D; &quot;producer_group_txmsg_bank1&quot;)</span><br><span class="line"> 	public class ProducerTxmsgListener implements RocketMQLocalTransactionListener </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>controller 略</li>
<li>B账户的MQ 监听类</li>
</ul>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;需要实现幂等</span><br><span class="line">@RocketMQMessageListener(topic &#x3D; &quot;topic_txmsg&quot;,consumerGroup &#x3D; &quot;consumer_txmsg_group_bank2&quot;)</span><br><span class="line">public class TxmsgConsumer implements RocketMQListener&lt;String&gt; </span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="最大努力通知"><a href="#最大努力通知" class="headerlink" title="最大努力通知"></a>最大努力通知</h2><ul>
<li>流程</li>
</ul>
<ol>
<li></li>
</ol>
]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title>创意</title>
    <url>/2021/09/20/%E5%88%9B%E6%84%8F/</url>
    <content><![CDATA[<br>

<br>



<h2 id="产品"><a href="#产品" class="headerlink" title="产品"></a>产品</h2><span id="more"></span>

<p>1、像打怪升级一样的技术学习路线和课程。很简单。一个个视频刷过去。一个个关卡过去。每次只能选择两个课程同时进行。</p>
<p>2、普通人也能用的量化模型。如何把这个简单化 – 投研帮，联系，利用他的知识，实现产品。并有渠道。</p>
<p>3、股市大v的正反观点，论据论证app。非常清晰。简明。</p>
<p>4、大嫂的小程序。3个月左右搞定。是不是能有两三家印刷厂需要、如果真的存在这个市场。这样最低可以卖30w。也值得。后续还可以卖给其他印刷厂。我们自己开始干。</p>
<p>5、</p>
<h1 id="行业"><a href="#行业" class="headerlink" title="行业"></a>行业</h1><p>1、给时间作用力20w。两年内能通过业余时间(早晨1h，中午1h)有副业。开第二个工作室的时候，合资合股。编辑视频、一起拍视频。我的目的很简单，一个是健康(精神状态和身体素质)，二个是有自己兴趣方面的副业。很重要的是，这个行业也是周期性的，有朝阳有下坡，但是用心做事的人可以打败周期。</p>
<p>你做这个的目的是什么。你有什么想法、志向。你的能力是什么。不要介意。本质上只是辛苦钱+服务。</p>
<p>而我相信这个行业是因为，现在消费升级，新消费下品牌有溢出效应。健康方面的需求都在往上走。品牌有溢出性。但是健身的连锁做的很差， 口碑不好，关键在于人。我希望我们做起来之后能做出品牌。能有经验和市场方面的积累。去拉融资。真的把好的健身习惯和知识做普及、惠于民。</p>
<p>像欧美看齐。其实健身是引入的。中国人也是跟风。因为大v和明星都这样。而大v和明星学的也是好莱坞。</p>
<p>我在寻找机会。技术上的商业化，和价值的商业化。</p>
<p>身材管理是持续性的。</p>
<p>场所、监督和氛围。专业和服务。终归落到服务而不是专业性。大部分是希望效果加上服务态度。</p>
<p>2、</p>
<h2 id="营销"><a href="#营销" class="headerlink" title="营销"></a>营销</h2><p>1、微博营销、名人转发，关键：内容优质</p>
<p>2、热点比赛的名次</p>
<p>3、</p>
<h1 id="心得"><a href="#心得" class="headerlink" title="心得"></a>心得</h1><p>1、一定要关注视频。视频比文字和图像的凝聚力更大，因为普及普世大众化</p>
]]></content>
  </entry>
  <entry>
    <title>利率</title>
    <url>/2021/05/09/%E5%88%A9%E7%8E%87/</url>
    <content><![CDATA[<br>



<h3 id="利率"><a href="#利率" class="headerlink" title="利率"></a>利率</h3><h5 id="定义与分类"><a href="#定义与分类" class="headerlink" title="定义与分类"></a>定义与分类</h5><span id="more"></span>

<p>定义：金融资产的到期收益率</p>
<p>别名：贴现率、要求回报率</p>
<br>

<p>构成： 无风险收益率+风险溢价(系统性风险的回报) </p>
<p>风险构成： <code>期限、违约(信用)、流动性、通胀</code></p>
<p>结构分析：利率期限结构、利率风险结构(控制期限变量分析)</p>
<br>

<p>分类：官定/市场，基准利率/一般利率，名义利率/实际利率、固定利率/浮动利率</p>
<br>

<p>其他：</p>
<p>​    违约评级机构：标普、穆迪、惠誉</p>
<p>​    国家信用：税收、债务货币化(内债)、政局稳定性</p>
<p>​    经济衰退-&gt;违约风险溢价上升</p>
<p>​    流动性风险：交易佣金、买卖价差(做市商 <code>市场交易的活跃度</code> 影响来bid 和 ask)、市场冲击(<code>市场规模</code>大则冲击压力越小) …</p>
<p>​    税后收益率 = 税前收益率 * (1- 税率) ，税收优惠的债券，可以接受更低的税前收益率</p>
<br>

<h5 id="利率决定论"><a href="#利率决定论" class="headerlink" title="利率决定论"></a>利率决定论</h5><br>

<p>1、古典利率决定理论(实际利率决定理论)</p>
<p>储蓄和利率正相关  投资和利率负相关 投资和储蓄决定均衡利率(产品市场出清)</p>
<p>货币只是交易媒介，不影响经济行为。货币和商品的二元无关论。</p>
<br>



<p>2、凯恩斯利率决定理论(流动性偏好理论)</p>
<p>货币供给与利率无关 货币需求与利率成负相关 货币需求和货币供给决定均衡利率(货币市场出清)</p>
<p>预防、交易、投机都影响货币需求。</p>
<p>古典的不成立：均衡利率取决于S位置，S取决于收入，收入取决于I，I取决于均衡利率。</p>
<p>延期消费带来利息不成立：等待或者延期不能带来利息，而是 流动性，也即放弃周转的灵活换来利息。</p>
<br>



<p>3、可贷资金理论</p>
<p>可贷资金的需求包括投资和新增货币需求，与利率负相关</p>
<p>可贷资金的供给包括储蓄和新增货币供给，与利率正相关</p>
<p>可贷资金供求决定均衡利率(可贷资金市场出清)</p>
<p>从微观出发，怎样保护自己的资产。</p>
<br>

<p>和流动性偏好，相似之处：</p>
<br>



<p>4、IS-LM 模型</p>
<br>

<h5 id="一般影响因素"><a href="#一般影响因素" class="headerlink" title="一般影响因素"></a>一般影响因素</h5><br>

<p>长期因素、短期因素；行为主体；国内国际；模型中 经济或金融变量(P I S C π e Ms Md …)；供需两面；</p>
<br>

<p>财政支出(预算赤字↑) - 总投资↑ - 货币需求↑ </p>
<p>国债↑ - 国债价格↓ 利率↑</p>
<p>放水(准备金率、回购) - 货币供给↑ - 名义利率↓</p>
<p>疫情 - 分情况看</p>
<p>价格↑ - 货币需求↑ -（货币供给不变时）名义利率↑</p>
<p>收入↑ - 货币需求↑ -（货币供给不变时）名义利率↑</p>
<p>通胀预期↑ - 现金持有↓ - 购买国债或其他权益证券↑ - 利率↓ ； 通胀预期↑ - (实际利率不变时)名义利率↓</p>
<p>经济上行(边际消费倾向、投资) ↑- (收入水平给定时)储蓄(货币供给)↓ - 资金成本↑ ，实际利率↑ - (通胀不变)名义利率↑</p>
<br>

<p>结论：利率是顺周期变量</p>
<p>…</p>
<h5 id="利率市场化改革"><a href="#利率市场化改革" class="headerlink" title="利率市场化改革"></a>利率市场化改革</h5><p>why</p>
<br>

<p>利率传导效率、货币政策价格型转变(对基准利率的可控性、可测性；货币供应量为中介目标影响越来越弱)、信贷配给的腐败(银行国企盆满锅满 – LPR倒逼)、资源配置效率低下 … </p>
<p>金融与互联网的结合，更是利率市场化和金融脱媒的推手。人们越来越难以忍受这种双轨制的损失。</p>
<p>利率是市场经济中最核心也是最主要的<code>生产要素的价格</code>。<code>提高生产力的办法之一就是提高生产要素的供给，降低生产要素的价格。</code>这就是利率市场化的本质原因。</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>前端</title>
    <url>/2021/07/20/%E5%89%8D%E7%AB%AF/</url>
    <content><![CDATA[<br>

<br>

<h4 id="路线"><a href="#路线" class="headerlink" title="路线"></a>路线</h4><p>HTML CSS3 JS</p>
<span id="more"></span>

<p>三个基础后面开框架：JQuery 、 Vue、 React、Angular</p>
<p>UI库：bootstrap 、 element…</p>
<p>构建平台：Node.js(服务器上运行的js环境)及其优化Expressjs、npm(下载各种依赖)</p>
<p>打包工具：webpack、grunt、gulp</p>
<p>高阶：css预编译SCSS、 LESS；js的超集TypeScript；Vuex；Ajax优化异步处理… HTTP协议…uni-app… </p>
<p>Flutter的ui…</p>
<br>

]]></content>
  </entry>
  <entry>
    <title>协同过滤</title>
    <url>/2021/04/05/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/</url>
    <content><![CDATA[<ul>
<li><p>user based</p>
<span id="more"></span>
<ul>
<li>有一个行为user，列为item的打分矩阵，且得到每个用户的平均分</li>
<li>对于矩阵中的空值：<br>  通过计算用户a和用户b\c\d等(对itemb有分数的)相似度(皮尔逊相关系数)，然后<br>  根据均分，计算出用户a对itemb的分数</li>
</ul>
</li>
<li><p>item based</p>
<ul>
<li>由一条条访问记录得到一个个user*item的矩阵</li>
<li>上述所有矩阵加和，得到item*item的共现矩阵</li>
<li>根据共现矩阵，计算item和item的相似度，得到相似度的item*item矩阵</li>
<li>item*item的相似度共现矩阵和某用户的浏览序列相乘，得到该用户的打分向量</li>
</ul>
</li>
</ul>
<ul>
<li><a href="https://www.jianshu.com/p/5463ab162a58">协同过滤</a></li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>ml</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>协程</title>
    <url>/2021/04/05/%E5%8D%8F%E7%A8%8B/</url>
    <content><![CDATA[<p>协程、多核与cpu</p>
<span id="more"></span>

<ul>
<li>协程<ul>
<li>从隔离、通信、调度、切换开销角度分析</li>
<li>进程<ul>
<li>资源完全隔离、进程挂了不影响其他进程，通信需要Inter-Process Communication，有七中状态，os进行调度</li>
</ul>
</li>
<li>线程<ul>
<li>每个进程可以有多个线程，共享进程资源、共享全局变量，但会引起多线程并发问题，需要锁机制，通信需要wait signal</li>
<li>切换<ul>
<li>进程切换与线程切换的一个最主要区别就在于进程切换涉及到虚拟地址空间的切换而线程切换则不会。线程共享虚拟地址空间。</li>
<li>和进程一样涉及到内核状态、硬件上下文的切换</li>
</ul>
</li>
<li>线程池里一个线程挂了<ul>
<li>当执行方式是execute时,可以看到堆栈异常的输出。</li>
<li>当执行方式是submit时,堆栈异常没有输出。但是调用Future.get()方法时，可以捕获到异常。</li>
<li>不会影响线程池里面其他线程的正常执行。</li>
<li>线程池会把这个线程移除掉，并创建一个新的线程放到线程池中。</li>
</ul>
</li>
</ul>
</li>
<li>协程<ul>
<li>协程是属于线程的。协程程序是在线程里面跑的，因此协程又称微线程和纤程等.协没有线程的上下文切换消耗</li>
<li>不涉及内核，完全在用户态进行，不用锁</li>
<li>如python中的yield</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>多核的数据传递、总线</p>
<ul>
<li>不管别的CPU私有Cache是否缓存相同的数据，都需要发出一次广播事件。这在一定程度上加重了总线负载，也增加了读写延迟。<br>针对该问题，提出了一种状态机机制降低带宽压力。这就是MESI protocol（协议）。</li>
<li>cache line具有4中状态，分别是Modified、Exclusive、Shared和Invalid。<br>当cache line状态是Modified或者Exclusive状态时，修改其数据不需要发送消息给其他CPU</li>
<li>多核Cache一致性由硬件保证，对软件来说是透明</li>
<li>MOESI Protocol。多一种Owned状态。多出来的状态也是为了更好的优化性能。</li>
</ul>
</li>
<li><p>多cpu和单cpu多核</p>
<ul>
<li><a href="https://www.zhihu.com/question/20998226">cpu</a></li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title>历史中国宪制</title>
    <url>/2021/10/06/%E5%8E%86%E5%8F%B2%E4%B8%AD%E5%9B%BD%E5%AE%AA%E5%88%B6/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>回溯</title>
    <url>/2021/04/13/%E5%9B%9E%E6%BA%AF/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>图算法</title>
    <url>/2021/10/06/%E5%9B%BE%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h5 id="问题与概念域："><a href="#问题与概念域：" class="headerlink" title="问题与概念域："></a>问题与概念域<span id="more"></span>：</h5><p>{</p>
<ul>
<li><p>重叠社区，非重叠社区，nested</p>
</li>
<li><p>nodes 属于多个 communities</p>
</li>
<li><p>如何生成 graph(network)？ 即有了nodes的set，如何定义edge？</p>
</li>
<li><p>有graph如何生成能够generate这个network的 model？</p>
</li>
</ul>
<p>}</p>
<h5 id="Community-Affiliation-Graph-AGM"><a href="#Community-Affiliation-Graph-AGM" class="headerlink" title="Community-Affiliation Graph: AGM"></a>Community-Affiliation Graph: AGM</h5><p>{</p>
<ul>
<li>community c</li>
<li>probability p ( each community has a single p)</li>
<li>memberships m</li>
<li>nodes v<br>=&gt;how to generate network？</li>
</ul>
<ol>
<li>对每个在community A中的nodes对，我们将两者以probability Pa 连接。(只要nodes对有相同的community那么两者就会以一定概率连接)</li>
<li>计算edge probability</li>
</ol>
<p>}</p>
<h5 id="BIGCLAM："><a href="#BIGCLAM：" class="headerlink" title="BIGCLAM："></a>BIGCLAM：</h5><p>{</p>
<ul>
<li><p>memberships have strength，越大则代表越活跃，0则非member</p>
</li>
<li><p>community membership strength matrix F：<br>行为nodes，列为communities</p>
</li>
<li><p>只有一个共同community，计算同一个community中nodes对连接的概率：1-exp(-F(nodea)*F(nodeb))；share多个communities，计算则从反面考虑1-(1-p1)(1-p2)…(即计算至少有一个community将nodes对连接的概率)</p>
</li>
<li><p>给定一个network G(V,E) 如何找到F，使得network的极大似然最大(极大似然的计算公式：有边的概率和无边的概率连×)？即 l(F) = log P(G|F),给定F条件下G的极大似然函数求对数形式。1.计算gradient 2.梯度下降，对每一行计算l对该行F的梯度，以学习步长更新该行F，迭代 3.如果该行F&lt;0，则令其为0</p>
</li>
<li><p>但是计算每行F的偏导是linear time，非常慢。<br>对属于node u的邻节点，和不属于u的邻节点的F都出现在公式中，每次计算偏导都需要对所有node n个 进行遍历。</p>
</li>
<li><p>改进：将所有node的F的和 cache，那么非邻节点部分的计算则转化为和减去邻节点，这样只需要遍历u的邻节点的F 即可计算偏导再进行迭代，加速明显。</p>
</li>
<li><p>数据：BIGCLAM 5min处理300k节点的nets，其他需要10days，能够处理100M条边的连接。</p>
</li>
</ul>
<p>}</p>
<h5 id="Detecting-cluster："><a href="#Detecting-cluster：" class="headerlink" title="Detecting cluster："></a>Detecting cluster：</h5><p>{</p>
<ul>
<li><p>Goal: given network, find densely linked clusters(output: set of nodes in same cluster)</p>
</li>
<li><p>应用场景：<br>query - to - adveritser （许多广告都属于类似的几个query）<br>actor - to -movie （subset of actors belong to a subset of movies）<br>social circles，circles of trust （找到set of my family ，set of my classmates等等network的子集）</p>
</li>
<li><p>how to define a good cluster？<br>最大化cluster内部的连接数量<br>最小化cluster之间的链接数量<br>即为 关于edges的cut函数</p>
</li>
<li><p>cut：(对于有权无权图来说都可用，无权则设所有权都为1) 是set of edges with only one node in the cluster  cut(S) = 所有一个节点属于S另一个指向另一cluster的edge的weight和<br>=&gt;最小cut？最优cut？</p>
</li>
<li><p>引入Conductance–Graph Partitioning Criteria：</p>
</li>
</ul>
<ol>
<li>给定set A ，Conductance = cut(A) / min(vol(A) , 2m-vol(A))，其中<br>vol(A) = di的和 (i是A的node，di : degree of node i)，m是graph的edges总数。<br>注意这里的degree会对edges重复计算，因为是基于nodes的degree。</li>
<li>min意味着，选择的cluster A的所有degree之和应当小于m/2，即总edges一半。否则如果A过大，那对这个比直接过影响太大。即相对于group自身的density来说group和其余network部分的连接。<br>这个比值越低，意味着cut越好。</li>
</ol>
<ul>
<li>为什么使用这个criteria？<br>实践证明能够产生更多balanced partitions<br>但是计算最优cut是np问题。</li>
</ul>
<p>}</p>
<h5 id="Spectral-Graph-Partitioning："><a href="#Spectral-Graph-Partitioning：" class="headerlink" title="Spectral Graph Partitioning："></a>Spectral Graph Partitioning：</h5><p>{</p>
<ul>
<li><p>从邻接矩阵来看，cluster会是什么样子？一块数字比较大，一块几乎是0。数字较大表明nodes之间关联强，属于同一个cluster。行列皆为nodes。</p>
</li>
<li><p>spectrum是邻接矩阵特征值的集合。那么和graph有什么关系？(考虑最简单的情况，graph有两个component，每个component中每个node的degree都为d)<br>intuition：如果一个特征值有两个特征向量，则这两个子图分离；如果两个特征值相近，则两个字图之间有连接但非常弱。</p>
</li>
<li><p>邻接矩阵重要性质：对称，特征值为实数且正交</p>
</li>
<li><p>Degree matrix：D= [dii] dii是node i的degree<br>Laplacian matrix： L = D - A。<br>性质：L *x =0 ；每行每列和为0 ；特征值为0</p>
</li>
<li><p>对一个对称矩阵M第二小的特征值公式：math证明略。化简后问题转为找到最小化 (xi - xj)^2之和的 x ，i、j为一个cluster中的node。</p>
</li>
<li><p>将partition(A,B)表达为一个向量，即属于A时，y为+1，属于B为-1。将 最小化 (xi - xj)^2之和 转为找x使得 (yi-yj)^2之和 最小。yi只能在-1，+1中取值。=&gt;不能准确解决，将y松弛。恰好即为第二小的特征值。</p>
</li>
<li><p>Rayleigh Theorem<br>总结：最优化cut的问题，即时最小化第二小特征值的问题，即时寻找参数 x的问题，而x又是第二小特征值的特征向量。x被称作Fiedler vector</p>
</li>
<li><p>最后，根据L计算出的第二小特征值及特征向量进行分组，如何cluster？即如何选择splitting point的问题。排序，split at 0，分成正负两类。</p>
</li>
<li><p>有降维的意思。x就能够涵盖cluster的信息即graph中所有nodes的信息，比L小得多。</p>
</li>
<li><p>多个cluster 如何？<br>k-way spectral  clustering:</p>
</li>
</ul>
<ol>
<li>迭代bi-partitioning</li>
<li>使用多个特征向量，构成一个矩阵，使用一种聚类方式(kmeans等)进行对这个特征向量构成的矩阵聚类(每个node被特征向量中对应的值表示 i：（x1i, x2i ,x3i…）x1,x2…为特征向量)（实践效果很好）</li>
</ol>
<p>}</p>
<h5 id="Trawling："><a href="#Trawling：" class="headerlink" title="Trawling："></a>Trawling：</h5><p>{</p>
<ul>
<li><p>找到graph中的小的community？</p>
</li>
<li><p>定义task：<br>given：left 有 s个nodes，每个links all nodes on the right (fully connected)；right 有 t个nodes。identify all  the 完全二部子图(complete bipartite subgraphs)</p>
</li>
<li><p>frequent itemsets：<br>找到所有subsets T of at least f  times bought  together by users。这个问题和完全二部子图的关联？=&gt;  frequent itemsets = complete bipartite subgraphs</p>
</li>
<li><p>可以把一个graph转成itemsets，graph中出边指向的node即为set中的items，起始点即可视为为user的basket。完全二部子图是整个graph的一部分，可用频繁物品集表示。</p>
</li>
</ul>
<p>}</p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>学习模型的模式</title>
    <url>/2021/02/21/%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<p>伪代码的算法流程 + 数学逻辑 –&gt;才能真的掌握住这个模型的本质。<br>而其他都是表象。所以数学肯定是要啃的。矩阵论和概率统计，微积分，必然是需要的。<br>更多的应用和实践，会深化对于模型的认知。</p>
<span id="more"></span>

<p>基本要求：<br>1、历史背景及演进：解决什么问题而生<br>2、伪代码表述流程<br>3、前提、局限、适用场景<br>4、演化和可优化之处、优化思路<br>5、和其他模型对比</p>
<p>进阶要求：<br>1、根据伪代码思路，看具体实现<br>2、为什么会有这样的前提和局限–&gt;数学证明：唯一性证明、误差上界证明、loss函数证明、模型参数的迭代公式证明</p>
<p>炉火纯青：<br>1、当我问你某个公式，你能够给出，并给出推导过程<br>2、和其他模型相似问题下的解决方案(公式)对比</p>
]]></content>
      <categories>
        <category>模型方法论</category>
      </categories>
      <tags>
        <tag>方法论</tag>
      </tags>
  </entry>
  <entry>
    <title>对偶</title>
    <url>/2021/03/11/%E5%AF%B9%E5%81%B6/</url>
    <content><![CDATA[<br>


<ul>
<li>对于凸形<br>结论1：凸形可行域只有1个峰，只要达到那个峰，我们就达到了最优，是全局最优。<br>结论2：至少有一个顶点是峰。</li>
</ul>
<span id="more"></span>
<ul>
<li><p>单纯形法：</p>
<ol>
<li>单纯形是什么？数学上可以写成一堆线性不等式限制出来的区域。</li>
<li>单纯形法，仅适用于求解线性规划，线性规划又是凸优化的一种。因为线性规划的定义域是单纯形，单纯形是凸的，即线性规划是定义域为凸、目标函数为线性的问题。</li>
<li>先找到一个顶点，然后从这个顶点，沿着某条边线，走到下一个顶点，直到最优。方向的选择可以有很多种，最多使用的是比较短视的方法：沿着最陡峭的那一条，追求当前步上升最快。</li>
<li>单纯形法寻找路线优化目标函数，直至达到一个峰，而该峰就是全局最优。</li>
</ol>
</li>
<li><p>Slater’s condition 根据wiki， </p>
<blockquote>
<p>Slater’s condition (or Slater condition) is a sufficient condition for strong duality to hold for a convex optimization problem, named after Morton L. Slater. Informally, Slater’s condition states that the <strong>feasible region must have an interior point</strong>.<br>即在对偶问题中gap为0要满足的条件，即可行域中必须有内点的条件。</p>
</blockquote>
</li>
<li><p>线性规划的对偶理论没出现的时候，线性规划是不知道能不能解的。也就是说，对偶理论能够证明一个线性规划问题<strong>不</strong>存在解。思路是找到一个跟原问题的对偶问题密切相关的问题，如果这个问题有解，原问题就没解。</p>
</li>
<li><p><strong>那么证明便归为两个主要部分，1 如何转化为对偶问题 2 为什么两个问题的解相关？</strong></p>
</li>
<li><p>首先[问题要满足是凸优化]。对于凸优化来说，在满足constraint qualifications(如上文的slater condition为其中一种，满足该条件，这里涉及到仿射函数即可表示为f=A*w+b 。仿射函数其实就是线性变换liner。)情况下，gap=0，为强对偶。</p>
</li>
<li><p>其次[在求解凸优化时引入乘子以及最优解需要满足的条件]。在凸优化中的非线性优化问题下，该问题满足一些constraint qualifications，当存在不等式约束时(只有等式约束时，即拉格朗日function求偏导(也就是KKT turns into the Lagrange conditions))，我们用KKT引进 <a href="https://en.wikipedia.org/wiki/Lagrange_multipliers" title="Lagrange multipliers">Lagrange multipliers</a>，这些乘子需要满足KKT 的四个条件。注意到这里的目标函数与约束函数一定是<a href="https://en.wikipedia.org/wiki/Smooth_function" title="Smooth function">continuously differentiable</a> at a point x(local optimum点)，如果functions are non-differentiable,<a href="https://en.wikipedia.org/wiki/Subderivative" title="Subderivative">subdifferential</a><br>versions of Karush–Kuhn–Tucker (KKT) conditions are available.</p>
</li>
<li><p>对偶问题， 这里代表Lagrangian dual problem，还有其他对偶问题 <a href="https://en.wikipedia.org/wiki/Wolfe_dual_problem" title="Wolfe dual problem">Wolfe dual problem</a> and the <a href="https://en.wikipedia.org/wiki/Fenchel%27s_duality_theorem" title="Fenchel&#39;s duality theorem">Fenchel dual problem</a>。</p>
</li>
<li><p>Lagrangian dual problem 需要先形成一个L函数，这个原问题的解是，</p>
</li>
</ul>
<ul>
<li>todo</li>
</ul>
<ul>
<li> 用数学表达：</li>
</ul>
<p>凸优化问题：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-0c145b1194ca5146.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="standard form for convex minimization problem.jpg"></p>
<p>reference:<br>[1]<a href="https://cowles.yale.edu/sites/default/files/files/pub/d00/d0080.pdf">https://cowles.yale.edu/sites/default/files/files/pub/d00/d0080.pdf</a><br>[2]<a href="https://en.wikipedia.org/wiki/Convex_optimization">https://en.wikipedia.org/wiki/Convex_optimization</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>工作问题repo</title>
    <url>/2021/06/11/%E5%B7%A5%E4%BD%9C%E9%97%AE%E9%A2%98repo/</url>
    <content><![CDATA[<p><br> <br></p>
<h4 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h4><span id="more"></span>

<p>1、AUC与点击率的关系，解释为什么rerank会使AUC上升但是点击率下降的问题</p>
<p>模型a的线上效果auc降低还是提高于离线</p>
<p>模型a对模型b的线上auc高于模型b自身的线上auc吗</p>
<p>gauc呢</p>
<p>ctr和auc gauc关系</p>
<p>gauc直接用click次数作为权重是否存在偏差和问题</p>
<br>

<p>实验流量的auc和基线流量的auc。孰高孰低</p>
<p><br> <br></p>
<p>2、id的emb含义，包含了所有特征？比属性特征的本质区别？</p>
<p><br> <br></p>
<p>3、为什么统计上的一些头部推荐 auc高于模型。或者 ，模型没有显著高</p>
<p><br> <br></p>
<p>4、特征一点点地加进去，auc变化的解释</p>
<p><br> <br></p>
<p>5、矩阵分解的好处和弊端，对比于deep。</p>
<p>什么情况下用矩阵分解是有效的，效果是否和先验的加入以及数据的分布问题相关。</p>
<p>根据id共现来推荐的思路，有什么问题。</p>
<p>个性化推荐里，用户会购买的商品到底是不是估出来分数高的</p>
<p><br> <br></p>
<p>6、加入先验约束后的影响</p>
<p><br> <br></p>
<p>7、deep侧和wide侧，加入什么去强化，什么样的特征放在哪里，为什么wide起了效果</p>
<p><br> <br></p>
<p>8、bert</p>
<p>1、bert层数、参数、如何微调</p>
<p>2、bert base还是…</p>
<p>3、两两计算的简化</p>
<p>4、关键词检索…？知识图谱（问题是属性还是label）</p>
<p><br> <br></p>
<p>9、itemid的编码来加和成用户向量，存在的问题</p>
<p><br> <br></p>
<p>10、漏斗越到下面，样本、模型、特征都有怎样的适配更好</p>
<p><br> <br></p>
<p>11、负采样里通过哈夫曼树，弊端。</p>
<p>以频次进行负采样的好处和优化点。</p>
<p><br> <br></p>
<p>12、loss变化的解释</p>
<p><br> <br></p>
<p>13、batch size的选择</p>
<p><br> <br></p>
<p>14、为什么 parallel_interleave来并发from_generator并不比最先版本单进程单队列性能高。</p>
<p><br> <br></p>
<p>15、</p>
<p><br> <br></p>
<p>key tips：多种badcase</p>
]]></content>
      <categories>
        <category>破站</category>
      </categories>
      <tags>
        <tag>破站</tag>
      </tags>
  </entry>
  <entry>
    <title>常见算法复杂度</title>
    <url>/2021/04/05/%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6/</url>
    <content><![CDATA[<br>

<ul>
<li><p>复杂度</p>
<span id="more"></span>
<ul>
<li><p>排序</p>
<ul>
<li>冒泡、直选、直插 n^2 – 对每个元素找位置、两个for循环嵌套</li>
<li>快排、堆、希尔、归并 nlogn – 二分思想(树)</li>
<li>计数 n+max – 先遍历放到max长度数组，再遍历数组取数</li>
<li>基数 n*位数 – 每位都来来一遍分配</li>
<li>不稳定算法： “快希选堆”（快牺牲稳定性） </li>
</ul>
</li>
<li><p>查找</p>
<ul>
<li>分块、二分、二叉排序查找  logn</li>
<li>顺序查找 n</li>
<li>哈希  O(1)</li>
</ul>
</li>
<li><p>遍历</p>
<ul>
<li>树遍历：时间复杂度为O(n)，同样空间复杂度也为O(n)，n为结点数。</li>
<li>图遍历： 邻接表O(V+E) 邻接矩阵O(V^2)</li>
</ul>
</li>
<li><p>红黑</p>
<ul>
<li>插入、删除、查找的最坏时间复杂度都为 O(logn)。</li>
</ul>
</li>
<li><p>b树</p>
<ul>
<li>平衡二叉树没能充分利用磁盘预读功能<br>B树是为了充分利用磁盘预读功能来而创建的一种数据结构<br>专门做索引而发明</li>
</ul>
</li>
<li><p>b+</p>
<ul>
<li>查找，(m/2) * log(m)n</li>
</ul>
</li>
<li><p>b b+区别</p>
<ul>
<li>叶子才有数据-&gt;减少磁盘io</li>
<li>利于范围查询</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>庄家</title>
    <url>/2021/02/22/%E5%BA%84%E5%AE%B6/</url>
    <content><![CDATA[<br>
<br>

<p>Q1：庄家坐庄步骤？<br>1吸货 2拉升 3洗盘 4出货✅<br>1吸货 2洗盘 3拉升 4出货❎</p>
<span id="more"></span>
<p>需要先拉升，在于底部吸货不足，卖家少。<br>拉升2倍左右，开始洗盘，洗盘过程中获利盘出货，给庄家吸货机会。这同时也抬高了吸货成本。<br>洗盘时间视情况而定，有时一个月左右。</p>
<p>Q2：庄家吸货方法？<br>跌停板吸货 （亿安科技）<br>熊市顺势低吸（底部但是爆量）<br>抬升吸货 （洗盘但是无量）</p>
<p>Q3：洗盘？<br>洗盘的目的：洗掉大户和集中的持股人。避免在高位卖出砸盘。<br>大户：几万、几十万都不是大户，几百万、上千万股的”人“。这样的人，属于对手。（主力和老庄家(被套的)–对手）</p>
<p>Q4：主力和庄家？</p>
<p>Q5：出货？<br>拉高出货，以变现锁定的获利。<br>成本区在之前的拉升和吸货之间的位置。<br>填权再拉。<br>然后高位震荡也能出货。<br>之前的拉升，净吃入。拉高出货不能吃，要卖。拉两三天，做个平台，拉两三天再做个平台，别人就会认为每次到平台就突破，就会买入。如果大盘面差，要跑的时候，向下钓鱼出货法–开始砸、跌停板打下来，然后巨大买盘，买盘巨大，不停再往上推，就会有买盘跟进，卖盘都是庄的，择机再砸掉买盘，又往上推，再砸，再推。1天可以卖掉好几千万资金。利用追涨杀跌。</p>
<p>Q6：大波段操作<br>胆大心细。有胆量入场，指定止损–买入后预计涨结果跌了、或者预计强势结果未强势–&gt;改错择机出来。<br>买错即卖。<br>国家政策–&gt;板块和强势股(股票强度、涨停板)–&gt;龙头、二三龙头、补涨股等<br>–&gt;F10基本面符合–&gt;看技术面，平台突破点找买点</p>
<p>Q7：二龙头补涨<br>二龙头的突破(某一天放量、十几分钟拉了3%-5%、分时线k线高于原来的平台)时加仓。补涨等不到回调机会。<br>卖出点只和技术面有关。<br>跌破五日线，先卖1/3。五日线十日线高位死叉卖1/3。死叉后又跌则全出。</p>
<p><img src="/2021/02/22/%E5%BA%84%E5%AE%B6/yakj.png" alt="yakj"></p>
]]></content>
      <categories>
        <category>股市理论</category>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>股市理论</tag>
      </tags>
  </entry>
  <entry>
    <title>彩妆</title>
    <url>/2021/07/28/%E5%BD%A9%E5%A6%86/</url>
    <content><![CDATA[<br>

<h4 id="彩妆"><a href="#彩妆" class="headerlink" title="彩妆"></a>彩妆</h4><ol>
<li>Kevyn Aucoin的修容<span id="more"></span></li>
<li>硅E乳 北京协和</li>
<li>Free plus 洁面 才木始洁面 至本洁面 … 及爽肤水</li>
<li>华熙生物？山东福瑞达？</li>
<li>菜鸟和配方师姜黄素精华液</li>
<li>snowberry超胜肽抗老精华</li>
<li>Charlotte tilbury唇线笔 pillow talk 3 intense</li>
<li>Pat mcgrath shade</li>
<li>essence高光 20</li>
<li>kiko遮瑕 02色号</li>
<li>kiko修容棒 201</li>
<li>漫野眉笔</li>
<li>MAC omega</li>
<li>ABH的Norvina</li>
<li>dw粉底液，3c2</li>
<li>LAGirl</li>
<li>e.l.f</li>
<li>wet n wild</li>
<li>huda beauty</li>
<li>fenty beauty 双色高光</li>
<li>zoeba</li>
<li>LB奶油眼线笔</li>
<li>兰芝唇膜</li>
<li>安的玫瑰庄园</li>
<li>橘朵香水</li>
<li>无敏氏神经酰胺修复原液+修红乳</li>
<li>Kiss自动卷发棒</li>
<li>TT梳</li>
<li>摩洛哥护发精油</li>
<li>Skin boutique鱼子酱发膜</li>
<li>美白：olay光感小白瓶，olay亮洁皙颜祛斑精华液</li>
<li>面霜：薇诺娜，cpb，娇韵诗，雨泽，珂润，资生堂百优，兰蔻菁纯</li>
<li>芦荟胶：寻荟记，爱露玛德</li>
<li>防晒：薇诺娜，碧柔 防晒放水：芳珂，TIZ02</li>
<li>身体乳：标婷维E，薇诺娜，适乐肤c乳，茱莉蔻</li>
<li>卸妆：花印，碧柔，芙丽芳丝</li>
<li>棉柔巾：棉森、babycare、名创优品、屈臣氏</li>
<li>洗发水：紫吕、施华蔻、植观</li>
<li>防晒伞：蕉下五折，天堂三折，安晴五折</li>
<li>防晒衣：网页严选，优衣库</li>
<li>加湿器：戴森，智米，小熊，德尔玛，美菱，格力，斯泰得乐</li>
<li>维多利亚的秘密，身体乳</li>
<li>benefit 5色修容腮红盘</li>
<li>KALEIDOS小三角唇釉 TM03</li>
<li>Anastiasia眉膏 EBONY</li>
<li>dolly wink、eye love magic 66/1、dup 807、luminous change05</li>
<li>OLAY烟酰胺身体乳</li>
<li>korres野玫瑰</li>
<li>MIUSI假睫毛 A002</li>
<li>猫家的DL缓慢生长底液、top coat</li>
<li>纳米美白牙贴面</li>
<li>思婉妮可可的积雪草软膏</li>
<li>mimitao无肩带内衣</li>
<li>susisu小金柱唇釉</li>
<li>girlcult睫毛膏 单极红赤</li>
<li>canmake眼线胶笔</li>
<li>爱丽小屋彩妆笔08</li>
<li>STRIDEX水杨酸棉片</li>
<li>bobbi brown眼线膏、妆前柔润底霜</li>
<li>Kevyn Aucoin修容</li>
<li>欧莱雅逆时精华</li>
<li>皮宝</li>
<li>943</li>
<li>玉泽屏障修复精华</li>
<li>Eucerin优色林舒安系列</li>
<li>欧莱雅恒放溢彩粉底液</li>
<li>LIERAC VC小炮弹精华</li>
<li>霏丝佳AI乳 PHYSIOGEL </li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>微信的GraphTR模型</title>
    <url>/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h4 id="提出背景"><a href="#提出背景" class="headerlink" title="提出背景"></a>提出背景</h4><p>“迁移学习+多任务”的场景<br>通过将不同领域的节点、关系都建模在一幅图中，通过图卷积，完成知识从数据丰富的领域向数据稀疏领域的迁移，并兼顾两个领域的指标。</p>
<span id="more"></span>

<p>多 域信息的异构图上完成图卷积，每个节点要聚合来自多个领域的异构消息。之前传统的聚合方式，如mean/max pooling，矩阵相乘，可能带来异构消息相互抵销而引入信息损失。</p>
<p>为此微信团队采用了GraphSAGE+FM+Transformer多种手段，从不同粒度来交叉、聚合消息，极大提升了模型的表示能力，这种新的消息聚合方式值得借鉴。</p>
<h4 id="场景及难点"><a href="#场景及难点" class="headerlink" title="场景及难点"></a>场景及难点</h4><p>1、微信团队面临的场景是：</p>
<p>每个视频都打有若干tag（人工标注或由内容理解算法打上的）<br>用户观看视频时，需要有算法从这个视频自带的tag中挑选出与当前用户最相关的若干个tag，展示在视频的下方。<br>用户点击某个tag，会进入一个沉浸式频道，其中展现的全部是与该tag相关的视频</p>
<p>2、难点在于：用户点击视频的行为比较丰富，但是用户点击tag的行为比较稀疏，训练数据不足。</p>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>方案一：<br>训练一个模型，输入视频的多模态信息(标题、封面图、关键帧)，输出是与这个视频最match的tag。训练时，拿人工打标的结果作为label。线上serving时，将预测出来的top-K个标签，展示在视频的下方。</p>
<p>这个方案可行，但是其只利用了视频的静态属性，没有用户的信息，所以推荐出来的tag只有与视频在语义上的相关性，完全没有针对当前用户的个性化，不满足业务需求。</p>
<p>方案二：<br>1、tag embedding用tag的word embedding。<br>2、用户的embedding是其过去有过”正交互”的tag embedding的pooling<br>所谓“正交互”，可以是用户过去一段时间内点击过的tag<br>但是考虑到user-tag的交互太稀疏，因此可以选用户过去点击过视频所携带的tag<br>pooling时，也可以考虑进播放完成度、时间衰减等因素，进行加权平均。<br>3、线上serving时，拿user embedding在当前视频所携带的tag embedding中寻找Top-K近邻，展示在视频下方。</p>
<p>怎么评价这一方案：</p>
<p>1、该方案，考虑了用户的历史，有更强的个性化。<br>2、但是拿word embedding做tag embedding，仍然只考虑了tag的语义信息。用户行为蕴含的信息，要比语义信息更加重要。<br>3、用户与tag的交互行为太少了，很难在“用户点击tag的序列”上套用word2vec来学习到tag embedding</p>
<p>方案三：微信的GraphTR模型</p>
<hr>
<h4 id="优化点"><a href="#优化点" class="headerlink" title="优化点"></a>优化点</h4><p>GraphTR是为了要<em>学习优质tag embedding</em>，为此要注重利用用户的行为信息<br>但是由于user-tag的行为太稀疏，因此GraphTR需要<em>通过user-video的行为学习到tag embedding</em><br>要达成以上目标，也有多种作法。而GraphTR的做法是：</p>
<p>1、将user, video, tag（还加上video的来源media）都放入一个大的异构图<br>通过图卷积，学习到video embedding</p>
<p>2、再建模video与video之间的相关性（比如在同一个session中播放过）</p>
<p>3、因为video embedding融合了tag embedding，因此在优化目标达成之后，一个优质的副产品就是得到tag embedding</p>
<h4 id="GraphTR是如何构建这个异构图的？"><a href="#GraphTR是如何构建这个异构图的？" class="headerlink" title="GraphTR是如何构建这个异构图的？"></a>GraphTR是如何构建这个异构图的？</h4><p>1、node：</p>
<p>图上要包括：user, video, tag, media (视频来源)这 4类节点。<br>因为用户数目太多，而每个用户的行为相对稀疏，GraphTR将用户按照gender-age-location分成84000组，用user group替代user，在图中建模。</p>
<p>2、edge：<br>video-video：同属一个观看session中的两video之间有边<br>user-video：某视频被某user group一周观看超过3次<br>video-tag：video和其携带的tag<br>video-media：video和其来源<br>tag-tag：两个 tag属于同一个视频</p>
<h4 id="如何传递、融合图上异构节点的信息？"><a href="#如何传递、融合图上异构节点的信息？" class="headerlink" title="如何传递、融合图上异构节点的信息？"></a>如何传递、融合图上异构节点的信息？</h4><p>1、为了完成user, video, tag, media这四类节点的信息融合，GraphTR设计了3层卷积结构，称为Heterogeneous field interaction network (HFIN)。</p>
<p>2、最底层Heterogeneous Feature Layer：<br>3-hop的embdding是lookup获得的，分别有四个域(user/video/tag/media域的特征)，相加得到2-hop邻居的embedding。</p>
<p>3、中间层：Multi-field Interaction Layer：<br>这一层的任务是由2-hop邻居的embedding，聚合生成1-hop邻居的embedding。<br>而HFIN采用了GraphSAGE+FM+Transformer三种方式，粒度上从由粗到细，完成聚合。</p>
<h4 id="三种聚合方式"><a href="#三种聚合方式" class="headerlink" title="三种聚合方式"></a>三种聚合方式</h4><p>1、GraphSAGE聚合<br>graphsage聚合<br><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/graphsage%E8%81%9A%E5%90%88.png" alt="graphsage聚合"></p>
<p>这里的hGraph就是1-hop的最终embding。</p>
<p>2、FM 聚合<br>FM聚合，区分各域，因此粒度更细一些。<br><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/fm%E8%81%9A%E5%90%88.png" alt="fm聚合"></p>
<p>hFM2就是1-hop的最终embding。</p>
<p>3、Multi-field transformer aggregator</p>
<p>GraphTR觉得FM聚合时，各域节点（即各域特征）交叉得还不够：1:FM聚合，只有在第2步才做域与域之间的交叉。2:在一个域内部，这n+1个特征之间，只有简单pooling，不存在交叉。3:FM聚合的第1步，每个域average pooling的是，这1+n个节点的原始特征。</p>
<p>Transformer聚合，希望增强各域节点（即各域特征）的交叉。步骤如下：</p>
<p>S1:Transformer决定在第1步引入交叉。具体方式就是，在一个域的1+n个节点之间进行Transformer变换，重新生成1+n个向量，每个新向量是老向量的加权平均，权重是当前老向量相对于其他老向量的attention score。(一套attention恐怕没有代表性，还引入多头机制)</p>
<p><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/transformer%E8%81%9A%E5%90%88.png" alt="transformer聚合"></p>
<p>S2:再拿生成的1+n个新向量，做average pooling。</p>
<p>S3:最后将“域间交叉结果”与”域内交叉结果”拼接在一起返回，作为由Transformer聚合得到的1-hop邻居的embedding。论文的实验结果证明，这个最复杂、最细粒度的聚合，对于模型性能的提升也最大。</p>
<p>4、通过三种聚合方式，我们就可以得到1-hop邻居的最终embedding，是这三种聚合结果的concat。</p>
<p>5、最上层：The Second Aggregation Layer<br>这一层负责由1-hop邻居节点（1个target node自身，m个邻居节点，一共1+m个）的embedding(下边公式中的矩阵H)，生成target node上的embedding。聚合方式也是基于Transformer的。</p>
<p>根据1+m个原向量，生成1+m个新向量，每个新向量是所有老向量的加权平均，权重是当前原向量与其他原向量的attention score<br>再拿这1+m个新向量，取平均，得到target node上的最终向量表示。</p>
<h4 id="如何定义loss"><a href="#如何定义loss" class="headerlink" title="如何定义loss?"></a>如何定义loss?</h4><p>通过以上三层卷积，就能够给图上所有类型的所有节点，都产生一个embedding。接下来的问题就是，如何定义优化目标，使这些节点的embedding得到优化？</p>
<p>这一部分的解决方案比较常规，无非就是建模节点之间的相关性，可以有选择是:</p>
<p>建模user-tag之间的相关性，user与点击过的tag之间的距离要尽可能小。但是user-tag之间交互的数据太少；建模user-video之间的相关性，user与点击过的视频之间，距离应该较近。但是图上建模的不是单个user而是user group，一个user group包含的用户兴趣太复杂，拿user-goup与video训练，可能噪声比较大；建模video-video之间的相关性，在同一个session被观看的视频之间，距离要尽可能小。因为video的点击行为比较多，这方面的数据比较丰富，文中采用的是这种方案。</p>
<p><img src="/2021/03/08/%E5%BE%AE%E4%BF%A1%E7%9A%84GraphTR%E6%A8%A1%E5%9E%8B/loss.png" alt="loss"></p>
<h4 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h4><p>将这些tag emedding代入上文的”第二个方案”，即拿用户观看过视频携带的tag的embedding加权平均得到user embedding，再拿这个user embedding在当前视频所携带的tag的embedding中寻找出距离最近的top-k个tag，作为推荐结果显示在视频的下方。因为这些tag embedding蕴含了丰富的user-video行为信息，不仅有助于提升用户对tag的点击率，也有助于提升进入沉浸式tag频道后的观看时长。</p>
<h4 id="借鉴"><a href="#借鉴" class="headerlink" title="借鉴"></a>借鉴</h4><p>1、数据少的领域如何借力于数据多的领域，同时要兼顾两个领域的优化目标：<br>通过将不同领域的不同节点、关系建立在一张异构图上，通过图卷积，使得每个节点的embedding都浓缩了多个领域的知识，达成了“知识迁移+目标兼顾”。</p>
<p>2、GraphTR采用了GraphSAGE+FM+Transformer多种手段，粒度上从粗到细，交叉、聚合来自不同领域的异构消息，相比于mean/max pooling、浅层FC等传统聚合方式，极大提升了模型的表达能力。</p>
]]></content>
      <categories>
        <category>模型框架</category>
      </categories>
      <tags>
        <tag>dl</tag>
        <tag>模型框架</tag>
      </tags>
  </entry>
  <entry>
    <title>思想深度</title>
    <url>/2021/02/22/%E6%80%9D%E6%83%B3%E6%B7%B1%E5%BA%A6/</url>
    <content><![CDATA[<br>
<br>

<h4 id="观点"><a href="#观点" class="headerlink" title="观点"></a>观点</h4><ul>
<li><p>计划经济是狂妄、愚蠢。</p>
<span id="more"></span></li>
<li><p>小波动基本是没意义的，一个现象进入分析的视野，已经承认这现象不属于小的波动，这点是必须明确的。</p>
</li>
<li><p>有领涨的并不一定都能最终走出行情，即使是上涨的趋势，行情还可以分大、中、小。</p>
</li>
<li><p>一个股票中的常见现象，就是当前一段领涨个股和大盘走势相背离的时候，往往意味着一个结构性震荡的来临，这一点至少在股票中是通用的，而76年的改变其实也是很符合这一点的。 </p>
</li>
<li><p>汉奸，当然可以有很高的的艺术造诣。所谓坏人就不能有优点了？无所谓好坏吧。</p>
</li>
<li><p>真正的你又何曾生死，生死又与你何干？。而你的心究竟又是哪个心？</p>
</li>
<li><p>所有的现代战争，从根本意义上就是货币战争。</p>
</li>
<li><p>和庄股一样，目前的关键是不能让资本大量逃离，否则就会连续崩盘跳水。</p>
</li>
</ul>
<br>

<h4 id="民族复兴周期-与-世界经济周期-历史性共振下的-国家地缘与货币战略"><a href="#民族复兴周期-与-世界经济周期-历史性共振下的-国家地缘与货币战略" class="headerlink" title="民族复兴周期 与 世界经济周期 历史性共振下的 国家地缘与货币战略"></a>民族复兴周期 与 世界经济周期 历史性共振下的 国家地缘与货币战略</h4><p>框架：</p>
<p>1、从历史大现象规律出发，得到–&gt;当前为强盛时期。</p>
<p>2、由马克思的”五阶段论”，得到–&gt;列宁的社会主义本质是公有制，斯大林的社会主义是资本主义–因此苏东突变后权力或权力资本迅速转为资本主义(本来就同源)，毛泽东的文革，重点放在了人与人的关系而非人与自然，而不能真的实现反资。邓小平的中国特色社会主义和社会主义初级阶段策略同样也应该是一个民族主义的策略。</p>
<blockquote>
<p>社会中一部分人对另一部分人不再存在依附关系，而是全社会的人都毫不例外地依附于一个非自然的身外之物：资本，就叫做资本主义社会。<br>因此人与自然的关系被打破，不再依附自然。</p>
</blockquote>
<p>3、文革的必然失败–&gt;使得面对资本全球化成了无可逃避的现实</p>
<p>4、1得到中国处于强盛期 + 23得到世界资本全球化–&gt;机遇<br>为什么是力量在中国？：从霸业的人口上得到的。大不列颠王国以5000万，美国和苏联在2亿5千万，下一个12亿5千万。<br>时间推定：1929英德老的5千万级别主导循环结束；美苏90年的循环在一半1974年形成了石油危机的中型调整，美苏这两个不同类型的资本主义之间的同级别竞争以美国的胜利结束；然后到2000年美国出现高点，开始调整；2019年中国开始。</p>
<p>5、地缘战略：以环渤海湾地区、珠江三角洲地区、秦川地区建构大的战略三角，成为亚洲之王，其领土（或附庸性质的影响）应该从乌拉尔山往东直到大海与美洲对望，从北冰洋直到太平洋俯视澳洲，形成世界的中轴，让欧洲和美洲成为其两翼</p>
<p>6、货币：<br>最有竞争力的货币将是美元、欧元、卢比和人民币。<br>在目前阶段一定要坚持对美元采取一种不挂钩的挂钩政策，坚决长期地维持人民币对美元的币值稳定。<br>逐步扩大对亚洲区的影响，取代日元的地位，逐步成为实质亚洲货币。<br>利用第一个阶段形成的对美元的极大落差，配合世界经济大循环周期，选择时机释放，将美元在一次精心策划的战役中一次性击毁。最后在一个长期反复、拉锯的过程中，利用新的12亿5千万级别世界经济大循环周期确立中国对美国的领先地位。</p>
<p>7、分析美国：<br>美国操纵汇率–&gt;为了其总体利益服务的。<br>美国危机–&gt;泡沫化，0以下的储蓄率<br>2000年的下跌速度极快–&gt;大规模的资本逃离还没有出现<br>目前的大级别反弹–&gt;构成资本逃离的机会，一旦反弹到位，出现大资本逃离。<br>为了避免大资本逃离–&gt; 大反弹到位前把货币贬值到一个相应的地位，这样才使得美圆资本套现后不能以一个较高的汇率出逃</p>
<p>但，如果有一个容量极大的货币紧贴美圆，则美圆贬值的所有如意算盘将打不响，而人民币正好就是这种货币。人民币与美圆的挂钩使得美圆资产变现以后有了一个顺畅的逃跑渠道。</p>
<p>这也是以前帖子里面预测2019年90年大周期世界经济大危机的现实基础，正确的人民币战略将加快、加深这个进程。</p>
<hr>
<p>综上，</p>
<p>1、够有想象力，三角洲和攻击美元的策略也能想得出，而且还自圆其说<br>2、对社与资(列林斯大林)的理解，我闻所未闻<br>3、历史的宏观视角，让我有大历史之感，得到一些规律，证明论点<br>4、美国现象到本质的精辟概括</p>
<h4 id="我的观点"><a href="#我的观点" class="headerlink" title="我的观点"></a>我的观点</h4><p>1、出得了方案<br>2、给得了完备解释<br>3、理解深刻性或者说独到处，一语成谶<br>4、精辟</p>
<p>=&gt;想象力、知识沉淀、思考本质 =&gt;精辟且自洽</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>投资大佬</title>
    <url>/2021/07/28/%E6%8A%95%E8%B5%84%E5%A4%A7%E4%BD%AC/</url>
    <content><![CDATA[<br>

<br>



<h3 id="网站、资源"><a href="#网站、资源" class="headerlink" title="网站、资源"></a>网站、资源</h3><ol>
<li>reuters</li>
<li>中国基金报</li>
<li></li>
</ol>
<br>

<h3 id="人物和组织"><a href="#人物和组织" class="headerlink" title="人物和组织"></a>人物和组织</h3><br>

<span id="more"></span>

<ul>
<li>television business news program</li>
<li><ol>
<li>asia-squawk-box</li>
</ol>
</li>
</ul>
<ul>
<li>Senior officials</li>
<li><ol>
<li>State Department spokesman Ned Price</li>
<li>Secretary of State Antony Blinken</li>
<li>China’s Vice Premier Liu He</li>
<li>U.S. Treasury Secretary Janet Yellen</li>
<li> Chinese Foreign Minister Wang Yi, Vice Foreign Minister Xie Feng</li>
<li>U.S. Defense Secretary Lloyd Austin </li>
<li> Secretary of State Antony Blinken</li>
<li><strong>Zoltan Pozsar</strong></li>
</ol>
</li>
</ul>
<ul>
<li>investment group</li>
<li><ol>
<li>Bespoke Investment Group </li>
<li>Oanda Asia Pacific Pte.</li>
<li><a href="https://research-doc.credit-suisse.com/docView?language=ENG&amp;format=PDF&amp;sourceid=em&amp;document_id=1083870621&amp;serialid=7Y4SC5R3JwuWssYSE1+HK0YN0zn1kYEHLQF0NHLEWx0=&amp;cspId=null">https://research-doc.credit-suisse.com/docView?language=ENG&amp;format=PDF&amp;sourceid=em&amp;document_id=1083870621&amp;serialid=7Y4SC5R3JwuWssYSE1%2BHK0YN0zn1kYEHLQF0NHLEWx0%3D&amp;cspId=null</a>  <strong>credit suisse</strong></li>
</ol>
</li>
</ul>
<ul>
<li>famous investors</li>
<li><ol>
<li>Kelvin Tay of UBS Global Wealth Management</li>
<li>Lorraine Tan , Morningstar’s director of equity research for Asia</li>
<li>Du Kejun,  fund manager at Beijing Gelei Asset Management Center LP</li>
<li>Dai Ming, a Shanghai-based fund manager at Huichen Asset Management</li>
<li><strong>桥水的达里奥</strong></li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统</title>
    <url>/2021/04/05/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<br>

<p>推荐系统</p>
<span id="more"></span>

<ul>
<li>冷启动<ul>
<li>用户冷启动<ul>
<li>热门</li>
<li>内容品类 多样性探索</li>
<li>用户画像</li>
<li>快速探索(各个筛选的选择tab)</li>
<li>主产品的用户迁移</li>
</ul>
</li>
<li>作品冷启动<ul>
<li>标的物跟用户行为的相似性</li>
<li>利用标的物跟标的物的相似性</li>
</ul>
</li>
<li>用户自己搜索、自己筛选、选择主题等</li>
</ul>
</li>
</ul>
<ul>
<li><p>es向量召回</p>
<ul>
<li><p>订单规则太多，导致召回排序后没多少曝光    </p>
<ul>
<li>走es召回。先过滤再取topN</li>
</ul>
</li>
<li><p>订单主页的日活100w 直播间dau30w 聊天室10w</p>
</li>
<li><p>冷门无机会曝光</p>
<ul>
<li>具体哪一个阶段导致，然后加入策略</li>
</ul>
</li>
<li><p>item侧的embding隔天变化。user的emb实时更新<br>  请求来了去es的item里找top</p>
</li>
<li><p>es比fassi的好处</p>
</li>
<li><p>性能</p>
<ul>
<li>qps100左右，128的emb，召回压力不大</li>
</ul>
</li>
</ul>
</li>
<li><p>向量距离方法</p>
<ul>
<li>fassi</li>
<li>kdtree</li>
</ul>
</li>
</ul>
<ul>
<li><p>推荐系统召回层打压热门item</p>
<ol>
<li>首先，大方向分为统计和算法两类模型。</li>
<li>召回阶段区别于排序，在于<ol>
<li>数量级千万级，一般为双塔。以便于**单独生成user embedding和item embedding，喂入的特征禁止含有user/item之间的交叉特征 **user特征喂入user tower得到user embedding，item特征喂入item tower得到item embedding。离线时，先将item embedding喂入FAISS建立索引，线上召回时，拿user embedding去FAISS里进行top-k近邻搜索，找到与其最接受的item embedding。</li>
<li>样本选择上， 正样本没有太多的争议，以内容推荐为例，选“用户点击”的item为正样本。最多考虑一下用户停留时长，将“用户误点击”排除在外。负样本的选择比较有讲究。如果说排序是特征的艺术，那么召回就是（负）样本的艺术。 </li>
<li>原则之一就是不能（只）拿“曝光未点击”做负样本，负样本的绝大部分应该由“<strong>随机负采样</strong>”生成。具体原因见我的另一篇文章《**<a href="https://zhuanlan.zhihu.com/p/165064102">负样本为王</a>**》。原则之二就是要打压热门item</li>
<li>因为绝大多数负样本是通过随机采样生成的，含有一定的噪声，因此召回不适合采用CTR预估常用的pointwise cross-entropy loss，而经常采用pairwise loss，比如margin-based bpr loss或hinge loss。（具体原因见我的另一篇文章《**<a href="https://www.zhihu.com/question/341529083/answer/1616964921">CTR和推荐算法有什么本质区别？</a>**》） </li>
<li>因此喂入模型的样本，区别于排序中常见的&lt;user, item, label&gt;，而是三元组&lt;user, item+, item-&gt;，预测的目标是MatchScore(user, item正)要远高于MatchScore(user, item负)</li>
</ol>
</li>
<li>20%的热门item占据了80%的曝光量或点击量。</li>
<li>为什么要打压<ol>
<li>训练时，为了降低loss，<strong>算法会使每个user embedding尽可能接近少数热门item embedding</strong></li>
<li>预测时，每个user embedding从FAISS检索出来的邻居都是那少数几个热门item embedding，<strong>完全失去了个性化</strong></li>
</ol>
</li>
<li>如何打压：<ol>
<li>item越热门，其成为item+的概率就应该越低。公式略。图形<img src="https://pic3.zhimg.com/80/v2-556679fa3e11b0cb0ba11e533bfc3793_1440w.jpg?source=1940ef5c" alt="img"></li>
<li><strong>提升热门item成为item-的概率</strong>。在随机采样负样本时，一方面需要采集到的item-能够尽可能广泛地覆盖所有候选item，另一方面又需要使采集到的item-尽量集中于高热item。</li>
</ol>
</li>
</ol>
  <br>

  <br>

<hr>
  <br>

<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ol>
<li>随机负采样时，ctr的loss和推荐召回的loss各自适合什么？</li>
</ol>
<p>  召回不适合采用CTR预估常用的pointwise cross-entropy loss，而经常采用pairwise loss，比如margin-based bpr loss或hinge loss</p>
<ol start="2">
<li>ctr和推荐算法本质区别</li>
<li>负样本为什么用随机采样更好</li>
<li></li>
</ol>
</li>
</ul>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>提升方法</title>
    <url>/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<br>

<h4 id="bagging和boosting"><a href="#bagging和boosting" class="headerlink" title="bagging和boosting"></a>bagging和boosting</h4><p>1、区别？<br>采样：随机采样(Bootstrap sampling–有放回)、加大错误样本权重<br>特征的采样</p>
<span id="more"></span>
<p>并行计算上<br>弱学习器的权重</p>
<p>2、从偏差和方差的角度解释bagging和boosting的原理？<br>bagging：<br>重采样、权重也相同–模型的区别性不大，bias小。<br>但如果假设各个子模型独立，则显著降低variance。如果完全相同的子模型，则var和单个模型一样。bagging属于两者之间，一定程度降低了var。RF特征上随机选择，进一步降低了模型的相关性，从而进一步降低了var。</p>
<p>boosting:<br>前向分步学习算法，是sequencial地减少损失函数，loss是逐步地下降的，bias也随之逐步下降。但由于是这种sequence、adaptive地，模型相关性较高，不能显著减少var。</p>
<h4 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h4><p>1平均法；2投票法：多数投票、绝对多数投票、加权投票；3学习法。<br>学习法，代表方法是stacking。stacking是再加上一层学习器，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。</p>
<h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><p>1、Adaboost是模型为加法模型，学习算法为前向分步学习算法，损失函数为指数函数的分类问题。</p>
<p><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E5%8A%A0%E6%B3%95%E5%89%8D%E5%90%91.png" alt="加法前向"></p>
<p><strong>为什么loss是指数函数？证明如下：</strong></p>
<p>————————————————————————————————————————————————————————————<br><br><br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/exp%E8%AF%81%E6%98%8E1.png" alt="exp证明1"><br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/exp%E8%AF%81%E6%98%8E2.png" alt="exp证明2"><br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/exp%E8%AF%81%E6%98%8E3.png" alt="exp证明3"></p>
<!-- <img src=exp证明1.png width=80% height=450>
<img src=exp证明2.png width=80% height=700>
<img src=exp证明3.png width=80% height=300> -->
<br>
——————————————————————————————————————————————————————————--

<p>2、分类器结合时的权重？<br>由于Adaboost中若干个分类器的关系是第N个分类器更可能分对第N-1个分类器没分对的数据，而不能保证以前分对的数据也能同时分对。所以在Adaboost中，每个弱分类器都有各自最关注的点，每个弱分类器都只关注整个数据集的中一部分数据，所以它们必然是共同组合在一起才能发挥出作用。所以最终投票表决时，需要根据弱分类器的权重来进行加权投票，权重大小是根据弱分类器的分类错误率计算得出的，总的规律就是弱分类器错误率越低，其权重就越高。</p>
<p>计算公式：</p>
<p>1 误差<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E8%AF%AF%E5%B7%AE.png" alt="误差"></p>
<!-- <img src=误差.png width=60% height=150>
 -->
<p>2 权重系数<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E5%BC%B1%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%83%E9%87%8D%E7%B3%BB%E6%95%B0.png" alt="弱分类器权重系数"></p>
<!-- <img src=弱分类器权重系数.png width=60% height=100> -->

<p>3 样本权重<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E6%A0%B7%E6%9C%AC%E6%9D%83%E9%87%8D.png" alt="样本权重"></p>
<!-- <img src=样本权重.png width=80% height=300> -->

<p>4 分类器结合及最终分类器<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E5%88%86%E7%B1%BB%E5%99%A8%E7%BB%93%E5%90%88.png" alt="分类器结合"></p>
<!-- <img src=分类器结合.png width=80% height=230> -->


<p>由上面的公式，可得到：<br>每次新增加一个弱分类器的时候，前面的弱分类器分错的样本的权重占总样本权重的0.5，前面弱分类器分对的样本等权重也占总样本权重的0.5。</p>
<p>3、正则化</p>
<p>fn = fn-1 + θ * a * G<br>θ 为正则化项</p>
<p>4、评价</p>
<p>可解释性<br>参数个数<br>performance<br>异常点敏感<br>弱分类器选择<br>可用于特征选择</p>
<h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p>1、区别于adaboost</p>
<p>Adaboost是通过提高错分样本的权重来定位模型的不足，GBDT是通过负梯度来定位模型的不足，因此GBDT可以使用更多种类的损失函数。由于loss可以选择更鲁棒的，对于adaboost存在异常点敏感的问题,gbdt更健壮。</p>
<p>可以灵活处理离散和连续值。</p>
<p>分类的GBDT：是用指数损失函数，此时GBDT退化为Adaboost算法。<br>另一种方法是用类似于逻辑回归的对数似然损失函数的方法。</p>
<p>使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。有很多人对GBDT算法进行了开源代码的开发，比较火的是陈天奇的XGBoost和微软的LightGBM。</p>
<p>2、为什么只能分类树</p>
<p>GBDT的核心在于累加所有树的结果作为最终结果，而分类树的结果显然是没办法累加的，所以GBDT中的树都是回归树，不是分类树。</p>
<p>3、损失函数有哪些？</p>
<p>1指数损失；2对数损失；3均方差(如果我们选择平方损失函数，那么这个差值其实就是我们平常所说的残差。)；4绝对损失；5Huber损失(它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。)；6分位数损失。56主要用于健壮回归，也就是减少异常点对损失函数的影响。</p>
<p>4、例子</p>
<p><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/gbdt%E4%BE%8B%E5%AD%90.png" alt="gbdt例子"></p>
<p>5、SGBDT</p>
<p>子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。</p>
<p>6、构造特征输入LR<br>如果我们想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力。Facebook 在2014年 发表的一篇论文便是这种尝试下的产物，利用gbdt去产生有效的特征组合，以便用于逻辑回归的训练，提升模型最终的效果。如我们 使用 GBDT 生成了两棵树，两颗树一共有五个叶子节点。我们将样本 X 输入到两颗树当中去，样本X 落在了第一棵树的第二个叶子节点，第二颗树的第一个叶子节点，于是我们便可以依次构建一个五纬的特征向量，每一个纬度代表了一个叶子节点，样本落在这个叶子节点上面的话那么值为1，没有落在该叶子节点的话，那么值为 0。于是对于该样本，我们可以得到一个向量[0,1,0,1,0] 作为该样本的组合特征，和原来的特征一起输入到逻辑回归当中进行训练。实验证明这样会得到比较显著的效果提升。</p>
<p>7、CART分类树过程<br><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/cart%E5%88%86%E7%B1%BB%E8%BF%87%E7%A8%8B.png" alt="cart分类过程"></p>
<p>8、相比于传统的LR，SVM效果为什么好一些</p>
<ul>
<li><p>GBDT基于树模型，继承了树模型的优点 [对异常点鲁棒、不相关的特征干扰性低（LR需要加正则）、可以很好地处理缺失值、受噪音的干扰小]</p>
</li>
<li><p>处理 missing feature</p>
</li>
<li><p>数据规模影响不大，因为我们对弱分类器的要求不高，作为弱分类器的决策树的深 度一般设的比较小，即使是大数据量，也可以方便处理。像 SVM 这种数据规模大的时候训练会比较麻烦。</p>
</li>
<li><p> 通常在给定的不带噪音的问题上，他能达到的最佳分类效果还是不如 SVM，逻辑回归之类的。<br>实际问题中，往往有很大的噪音，使得 Decision Tree 这个弱势就不那么明显了。</p>
</li>
</ul>
<p>9、加速训练？<br>是否预排序,预排序可以加速查找最佳分裂点（不确定）.在样本规模上的并行计算。</p>
<p>10、参数</p>
<ul>
<li><p>第一类Miscellaneous Parameters </p>
</li>
<li><p>第二类：Boosting Parameters:<br>n_estimators 最大弱学习器的个数，太小欠拟合，太大过拟合<br>learning_rate 学习率，太大过拟合，一般很小0.1，和n_estimators一起调<br>subsample 子采样，防止过拟合，太小欠拟合。GBDT中是不放回采样</p>
</li>
<li><p>第三类：Tree-Specific Parameters<br>max_features 最大特征数<br>max_depth 最大树深，太大过拟合<br>min_samples_split 内部节点再划分所需最小样本数，越大越防过拟合<br>min_weight_fraction_leaf 叶子节点最小的样本权重和。如果存在较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。越大越防过拟合<br>max_leaf_nodes:最大叶子节点数 ，太大过拟合<br>min_impurity_split:节点划分最小不纯度<br>presort:是否对数据进行预分类，以加快拟合中最佳分裂点的发现。默认False。非稀疏数据则预排序，若稀疏数据则不预排序。小规模数据预排序。</p>
</li>
</ul>
<p>11、调参思路</p>
<p>1、首先使用默认的参数，进行数据拟合；<br>2、从步长(learning rate)和迭代次数(n_estimators)入手；一般来说,开始选择一个较小的步长来网格搜索最好的迭代次数。这里，可以将步长初始值设置为0.1。对于迭代次数进行网格搜索；<br>3、接下来对决策树的参数进行寻优<br>4、首先我们对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索。【min_samples_split暂时不能一起定下来，因为这个还和决策树其他的参数存在关联】<br>5、接着再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参；做到这里，min_samples_split要做两次网格寻优，一次是树的最大深度max_depth，一次是叶子节点最少样本数min_samples_leaf。<br>【具体观察min_samples_split的值是否落在边界上，如果是可以进一步寻优】<br>6、继续对最大特征数max_features进行网格搜索。做完这一步可以看看寻找出的最优参数组合给出的分类器的效果。<br>7、可以进一步考虑对子采样的比例进行网格搜索，得到subsample的寻优参数<br>8、回归到第2步调整设定的步长(learning rate)和迭代次数(n_estimators)，注意两者的乘积保持不变，这里可以分析得到：通过减小步长可以提高泛化能力，但是步长设定过小，也会导致拟合效果反而变差，也就是说，步长不能设置的过小。</p>
<hr>
<p>mind:</p>
<p><img src="/2021/02/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/%E6%8F%90%E5%8D%87.png" alt="提升"></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>操作系统</title>
    <url>/2021/07/20/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<br>

<br>

<h4 id="要刷的几个题目"><a href="#要刷的几个题目" class="headerlink" title="要刷的几个题目"></a>要刷的几个题目</h4><span id="more"></span>

<br>

<br>



<h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><ol>
<li><p>进程实体：PCB 程序段 数据段。进程：进程实体运行的过程，动态的，系统资源分配和调度的单位。</p>
</li>
<li><p>PCB：进程描述信息：pid(进程标识符)，uid；进程管理控制信息：进程优先级，当前状态；资源分配清单：程序段、数据段指针，键盘、鼠标等资源；处理机cpu相关：寄存器值。</p>
</li>
<li><p>进程的组织：1链接方式–PCB分多个队列、os有队列的指针；2索引方式–建立几个索引表(索引表的每一项是属于该表的各个PCB的指针)、os有索引表指针</p>
</li>
<li><p>进程特性：动态性、并发性、独立性–独立获得资源、异步性、结构性</p>
</li>
<li><p>三种基本状态：运行、就绪、阻塞。还有两种：创建态、终止态(撤销pcb、回收系统资源等)。阻塞原语：等待资源；等待其他进程。唤醒原语：等待的事件发生。挂起状态，比如就绪挂起，那么<strong>创建态、阻塞挂起、运行态、就绪态</strong>的进程都有可能被调入到就绪挂起里。挂起的进程，进程除了pcb会被调入到外存中。</p>
</li>
<li><p>进程控制：对进程各个状态中切换进行控制。os提供了几种原语：功能–更新PCB、插入PCB到合适的队列、分配回收资源</p>
</li>
<li><p>原语：采用关中断和开中断实现，过程中不能被中断，这样的操作也是 原子操作。在 核心态执行。</p>
</li>
<li><p>进程通信：由于进程有自己的内存空间，所以之间的通信需要一定的共享机制。1共享存储，分为数据结构的共享(queue)和存储区域的共享(更快、高级)。2管道，即内存的一块缓冲区，连接着读写，半双工的通信。读写之间是阻塞的，没有写满不允许读，没有读空不允许写。3消息传递，分为直接和间接两种方式。直接就是每个进程有自己的消息队列，其他进程往这个队列里去发消息，间接则有个中间邮箱暂存消息，大家都往一个地方写，系统管理着信箱。</p>
</li>
<li><p>多线程及属性：来提高系统并发度。1进程就只作为资源分配的单元，而调度的基本单位是线程。2切换线程的系统开销很低，线程几乎没有系统资源，而是共享一个进程的资源，线程间的通信只要通过进程而不用系统干预。3多cpu ，各个线程可以占用不同的cpu。4线程有threadid，以及TCB。</p>
</li>
<li><p>线程实现方式：</p>
<ol>
<li><p>用户级线程：用户线程由应用程序通过线程库(pool)实现，切换也是用户态完成。os意识不到线程存在，对os是透明的，是多对一。(多cpu 的，我开了多个线程，如果是用户级的，能不能充分利用多核？–是不能的。) –&gt;切换效率高，但并发度就很低，一个线程被阻塞，整个进程都会被阻塞，多线程不能在多核上并行。</p>
</li>
<li><p>内核级线程：线程管理是os内核负责的，内核线程切换要核心态完成。用户的线程和核心线程是一一对应的。而内核级线程才是os分配cpu的单位。–&gt;并发能力强，管理成本高。</p>
</li>
<li><p>1和2的组合，n个用户的线程映射到m个内核线程上。n&gt;=m。</p>
</li>
<li><p>上面的总结，可以理解为是多线程模型。多对一。一对一。多对多。</p>
</li>
</ol>
</li>
<li><p>cpu调度：分为高级调度、中级和低级。高级(面向作业)：后备的作业队列中选择一个或多个作业，调入内存并分配资源，建立PCB，然后就可以去竞争cpu。中级：引入虚拟存储技术后，os会将空闲的进程调出到外存，即为挂起状态(有就绪挂起和阻塞挂起)，(PCB常驻内存)被os放在挂起队列里；中级调度是决定哪个挂起状态的进程重新调入内存。低级：即进程调度，从就绪队列里选择进程进入运行(是内存和cpu之间的，其他为内存和外存之间)。三种名称又是，作业调度、内存调度、进程调度。</p>
</li>
<li><p>进程调度：时机、方式、与进程切换。</p>
<ol>
<li><p>时机</p>
<ol>
<li>主动放弃cpu以及被动。主动：完成、异常、等待io。</li>
<li>被动：时间片到了、优先级高的抢了、有其他需要先处理如io中断。</li>
<li>不能调度：处理中断过程、原子操作过程、os内核程序临界区(临界区是指访问临界资源的那段代码，临界资源是必须要互斥地被访问的资源)中(某个进程正在访问内核的数据结构如PCB队列，此时会将此结构上锁，那么如果在此时进行调度，调度必然要访问这个PCB队列，而已被锁住，这就死锁了，此时要等进程对内核临界区访问结束然后再调度。如果是一般的非内核临界区，则可以直接剥夺进行调度。不会有系统隐患。)。</li>
</ol>
</li>
<li><p>方式</p>
<ol>
<li><p>抢占式，分时操作和实时操作系统</p>
</li>
<li><p>非抢占，早起的批处理</p>
</li>
</ol>
</li>
<li><p>进程切换：</p>
<ol>
<li>调度选中的进程可能是刚刚暂停的进程，切换是进程让出cpu而另一个进程占用cpu。切换需要保存，pc、程序状态字、寄存器现场等</li>
<li>广义的进程调度包括，选择一个进程以及进程切换</li>
</ol>
</li>
</ol>
</li>
<li><p>调度算法评价指标</p>
<ol>
<li>cpu利用率 =  忙碌时间/总时间</li>
<li>系统吞吐量 = 总共完成的作业/总共的时间</li>
<li>周转时间 = 完成时间 - 作业提交时间 ，os更加关注平均周转时间</li>
<li>带权周转时间 = 周转时间/作业实际运行时间</li>
<li>等待时间 = 所有等待时间之和</li>
<li>响应时间 = 首次响应时间 - 提出请求时间</li>
</ol>
</li>
<li><p>调度算法</p>
<ol>
<li><p>FCFS</p>
<ol>
<li>等待时间越长越先，可以用于作业调度也可以用于进程调度，非抢占式，公平、简单</li>
<li>缺点：带权周转时间比较大，不利于短作业</li>
<li>不会饥饿</li>
</ol>
</li>
<li><p>SJF</p>
<ol>
<li>进程调度时候是，短进程优先。有抢占版本：最短剩余时间优先算法。默认做题是非抢占</li>
<li>在所有的进程同时可运行时/几乎同时到达，SJF的平均等待和平均周转时间是最少的。但相比于FCFS，是要少的平均等待和平均周转</li>
<li>不公平，利于短作业，会饥饿，且作业时间是用户可设置的</li>
</ol>
</li>
<li><p>HRRN高响应比算法</p>
<ol>
<li>响应比= (等待时间+作业时间)/作业时间，非抢占</li>
<li>不会饥饿</li>
</ol>
</li>
<li><p>总结</p>
<ol>
<li><p>FCFS考虑了等待时间但不考虑作业时间，所以短作业不友好，SJF考虑了作业时间但是没考虑等待，所以不公平。=&gt;来了HRRN。</p>
</li>
<li><p>这三种，考虑了公平、系统性能，但是没有考虑响应时间及优先级，适用于批处理系统，但是不适用交互式。</p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>调度算法二</p>
<ol>
<li>时间片轮转RR<br>1. </li>
<li>优先级调度</li>
<li>多级反馈队列</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>整体思维</title>
    <url>/2021/04/15/%E6%95%B4%E4%BD%93%E6%80%9D%E7%BB%B4/</url>
    <content><![CDATA[<br>

<p>人生怎么走，成为什么人</p>
<span id="more"></span>



<p>时间，大多被浪费。各种事情打散了力量。</p>
<p>能量如果不凝聚起来，和热量有什么大区别？</p>
<p>深度。体系地思考决定深度。而小聪明，片段化地推理，没有温故和反复的逻辑，无法体现深度。</p>
<p>体系，需要书籍、积累、重复构建。需要阅历、经验和磨炼。有质量地思考。</p>
<p>力求成为什么样的人，就去照着那样行动。不要太过顾忌，在意别人的想法。甚至有时候你觉得，这样是不是太故作姿态了。但是也要做。不去模仿，没有可能性成为。</p>
<p>理性和思考，让你自信。大家都能想到的思路和方案，不是你的特殊性。要在你自己的特殊性上发力，要进一步去思考和抽象。而不是停留在表象，或者前滩。其实这样的随笔也不过是警戒。不必要写太多，起到作用就行。更多还是要自己去创造。走到最前方。关注着有思想的人。</p>
<p>去解决问题。</p>
<p>研究生博士生上不了，你能够有创造出产品的能力吗。研究生博士生最终还是为了创造和能力的提升。</p>
<p>你能到达的优秀必须有厚重的基底。否则还是走不远。</p>
<p>我需要去读。因为我需要时间再去沉淀。我看得到自己的智慧，我希望能够发光。而不是在局部里挣扎。</p>
<p>不过始终还是为了站得更高。</p>
<p>语言、倾听、拒绝、让人信赖……</p>
<p>都是要你去反思和提升深度广度。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>方便的工具</title>
    <url>/2021/08/02/%E6%96%B9%E4%BE%BF%E7%9A%84%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<br>

<h4 id="平板"><a href="#平板" class="headerlink" title="平板"></a>平板<span id="more"></span></h4><ol>
<li>画图：sketch 、krita、 ArtFlow</li>
</ol>
<br>

<h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><ol>
<li>CAMBLY英语口语 aze2021</li>
</ol>
 <br>



<h4 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h4><ol>
<li>process on</li>
</ol>
<h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><ol>
<li>虫部落 – 英文电子书</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>机器</title>
    <url>/2021/04/05/%E6%9C%BA%E5%99%A8/</url>
    <content><![CDATA[<p>物理cpu、逻辑cpu、内存、硬盘、gpu</p>
<span id="more"></span>

<p>命令：<br>cat /proc/cpuinfo | grep “physical id” | uniq | wc -l<br>cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c<br>cat /proc/cpuinfo | grep physical | uniq -c<br>grep MemTotal /proc/meminfo<br>df -h<br>nvidia-smi<br>lspci | grep -i nvidia</p>
<p>1    prodbigdata-aitm-distributed-job001 10.111.20.180    [DATACOMMAND]           1<em>4c  8g 200g                                                                                 <br>2    prodbigdata-aitm-distributed-job002 10.111.20.181    [DATACOMMAND]              1</em>4c  8g 200g                                                                                 <br>3    prodbigdata-algo001                 10.111.4.208     [DATACOMMAND]                      1<em>16c  16g 200g                                                                         <br>4    prodbigdata-algo002                 10.111.51.113    [DATACOMMAND]                       1</em>8c  16g 256g                                                                      <br>5    prodbigdata-algo004                 10.111.4.204     [DATACOMMAND]                          1<em>16c  16g 200g                                                                      <br>6    prodbigdata-algo005                 10.111.4.205     [DATACOMMAND]                          1</em>16c  16g 200g                                                                      <br>7    prodbigdata-rec-algo-service001     10.111.106.61    [DATACOMMAND]                                                                                            <br>8    prodbigdata-rec-algo-train001       10.111.106.62    [DATACOMMAND]                       1<em>54c 500g   PB级别     8gpu 每个显存16g                                                              <br>9    prodbigdata-youtube-dnn001          10.111.22.113    [DATACOMMAND]                      1</em>16c 120g  1T             2gpu 每个显存16g                                                   <br>10   prodbigdata-ypp-keras-frcnn         10.111.3.220     [DATACOMMAND]                        1*16c 120g  1T             2gpu 每个显存16g                                                                          <br>11   testbigdata-kafka-streaming-job001  10.111.51.50     [DATACOMMAND]            </p>
<p>Mon Mar 22 12:53:07 2021       <br>+—————————————————————————–+<br>| NVIDIA-SMI 430.40       Driver Version: 430.40       CUDA Version: 10.1     |<br>|——————————-+———————-+———————-+<br>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>|===============================+======================+======================|<br>|   0  Tesla P100-PCIE…  Off  | 00000000:00:09.0 Off |                    0 |<br>| N/A   29C    P0    32W / 250W |   1419MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   1  Tesla P100-PCIE…  Off  | 00000000:00:0A.0 Off |                    0 |<br>| N/A   28C    P0    25W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   2  Tesla P100-PCIE…  Off  | 00000000:00:0B.0 Off |                    0 |<br>| N/A   29C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   3  Tesla P100-PCIE…  Off  | 00000000:00:0C.0 Off |                    0 |<br>| N/A   34C    P0    27W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   4  Tesla P100-PCIE…  Off  | 00000000:00:0D.0 Off |                    0 |<br>| N/A   32C    P0    31W / 250W |   2384MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   5  Tesla P100-PCIE…  Off  | 00000000:00:0E.0 Off |                    0 |<br>| N/A   35C    P0    34W / 250W |    263MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   6  Tesla P100-PCIE…  Off  | 00000000:00:0F.0 Off |                    0 |<br>| N/A   27C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+<br>|   7  Tesla P100-PCIE…  Off  | 00000000:00:10.0 Off |                    0 |<br>| N/A   29C    P0    31W / 250W |   1301MiB / 16280MiB |      0%      Default |<br>+——————————-+———————-+———————-+</p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>样本</title>
    <url>/2021/04/05/%E6%A0%B7%E6%9C%AC/</url>
    <content><![CDATA[<ul>
<li><p>正样本稀疏</p>
<span id="more"></span>
<ul>
<li>欠采样、过采样</li>
<li>集成<ul>
<li>负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据<br>  然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。</li>
<li>这种解决问题的思路类似于随机森林</li>
</ul>
</li>
<li>权重<ul>
<li>不同样本数量的类别分别赋予不同的权重</li>
</ul>
</li>
<li>一分类<ul>
<li>把它看做一分类（one class learning） 或异常检测问题，这类方法的重点不在于捕捉类间的差别，而是为其中一类进行建模，比较有代表性的是 one-class-SVM。</li>
<li>符合这些图像特征的就属于人脸，反之则不是。对比二分类，显著的区别就是，二分类不但能的出来这个图片不是人脸，他还能告诉你这个图片是猪脸。</li>
</ul>
</li>
</ul>
</li>
<li><p>推荐之样本</p>
<ul>
<li><p>避免高度活跃用户对loss的影响</p>
<ul>
<li>训练集中对每个用户提取相同数量的训练样本</li>
</ul>
</li>
<li><p>根据用户最后一次点击行为的位置，过滤掉最后一次点击之后的展示，可以人为认为用户没有看到。</p>
</li>
<li><p>一个用户对同一个内容点击与不点击并存的情况，如果多次曝光的间隙非常短，考虑只使用其中的一次曝光数据。</p>
</li>
<li><p>考虑去除只有曝光但没有点击操作的用户的样本（也就是说有的用户只有负样本，没有正样本），不过去除的话，那模型就只能够学习到活跃用户或者有意向用户的行为习惯</p>
</li>
<li><p>要求当线上模型在预测时，需要将喂给模型的特征做一次落地，比如传到kafka，后续再由相应程序进行解析生成之后的的训练样本。</p>
</li>
<li><p>同一个request中，如果收到某样本后面样本的展示或者点击事件，5min后还没有收到该样本的点击事件，则作为负样本进行训练；如果在作为负样本训练之后，在一段时间之后又收到该样本的正例行为，则需要作出补偿。</p>
</li>
<li><p>专家样本</p>
</li>
<li><p>坏样本</p>
<ul>
<li>样本偏差、模型敏感、无法代表全体、</li>
</ul>
</li>
<li><p>在信用卡欺诈模型中，对于召回率的要求比较高（不希望漏掉一个欺诈用户），并且预测出来的数据还会经过人工审核，相对的对于准确率要求就低一些</p>
</li>
<li><p>但是在我们的原始数据中，正样本的比例本身就占比非常小了，或者正样本本身就是正太分布部分，但是在预测的时候，连长尾分布的部分也不能放过，（尽量的把所有欺诈用户召回），比如信用卡欺诈里有的超级用户虽然数量小，但是一次违约就是几十万，比几百个普通用户还严重，这种时候是否要用权值设置或者复制正样本的方式，来做识别增强。</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度</title>
    <url>/2021/02/04/%E6%A2%AF%E5%BA%A6/</url>
    <content><![CDATA[<h2 id="梯度消失和爆炸"><a href="#梯度消失和爆炸" class="headerlink" title="梯度消失和爆炸"></a>梯度消失和爆炸</h2><ul>
<li><p>deep后带来的信息传递/梯度传递问题</p>
<ul>
<li><p>层数过多导致？sigmoid和tanh为什么会导致梯度消失？</p>
<ol>
<li>直观解释：从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多。</li>
<li>反向传播角度解释：由于反向传播过程中，前面网络权重的偏导数的计算是逐渐从后往前累乘的，如果使用激活函数如sigmoid，导数小于一，因此累乘会逐渐变小，导致梯度消失，前面的网络层权重更新变慢；如果权重 本身比较大，累乘会导致前面网络的参数偏导数变大，产生数值上溢。<br>    </li>
</ol>
</li>
<li><p>梯度消失</p>
<ol>
<li>原因：层数过多，学习率的大小，网络参数的初始化，激活函数的边缘效应</li>
<li>在深层神经网络中，每一个神经元计算得到的梯度都会传递给前一层，较浅层的神经元接收到的梯度受到之前所有层梯度的影响。如果计算得到的梯度值非常小，随着层数增多，求出的梯度更新信息将会以指数形式衰减，就会发生梯度消失。<br>
<span id="more"></span></li>
</ol>
</li>
<li><p>梯度爆炸</p>
<ol>
<li>原因：1）隐藏层的层数过多；2）<strong>权重的初始化值过大</strong></li>
<li>在深度网络或循环神经网络（Recurrent Neural Network, RNN）等网络结构中，梯度可在网络更新的过程中不断累积，变成非常大的梯度，导致网络权重值的大幅更新，使得网络不稳定；在极端情况下，权重值甚至会溢出，变为$NaN$值，再也无法更新。</li>
<li>解决：1）用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。（2）用Batch Normalization。（3）<strong>LSTM的结构设计也可以改善RNN中的梯度消失问题。</strong>（4）进行梯度裁剪(clip), 如果梯度值大于某个阈值，我们就进行梯度裁剪，限制在一个范围内.（5）使用正则化，这样会限制参数 的大小，从而防止梯度爆炸。（6）设计网络层数更少的网络进行模型训练</li>
</ol>
</li>
</ul>
</li>
</ul>
<h4 id="LSTM为什么有助于解决梯度消失和爆炸问题？"><a href="#LSTM为什么有助于解决梯度消失和爆炸问题？" class="headerlink" title="LSTM为什么有助于解决梯度消失和爆炸问题？"></a>LSTM为什么有助于解决梯度消失和爆炸问题？</h4><p><a href="https://www.zhihu.com/question/34878706">https://www.zhihu.com/question/34878706</a></p>
<p>RNN 中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的梯度不会消失，所有梯度之和便不会消失。RNN 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。</p>
<p>其一是遗忘门接近 1（例如模型初始化时会把 forget bias 设置成较大的正数，让遗忘门饱和），这时候远距离梯度不消失；其二是遗忘门接近 0，但这时模型是故意阻断梯度流的，这不是 bug 而是 feature（例如情感分析任务中有一条样本 “A，但是 B”，模型读到“但是”后选择把遗忘门设置成 0，遗忘掉内容 A，这是合理的）。当然，常常也存在 f 介于 [0, 1] 之间的情况，在这种情况下只能说 LSTM 改善（而非解决）了梯度消失的状况。</p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>1、优秀激活函数的性质<br>非线性–多层后能够逼近所有函数<br>可导–优化器大多用梯度下降更新参数<br>单调–能够保证单层网络的损失函数是凸函数<br>近似恒等性– f(x)近似=x，参数初始化为随机小值时，神经网络更稳定<br>输出均值为0 – 能够加速收敛</p>
<p>2、激活函数输出范围<br>有限 – 基于梯度更新参数更稳定<br>无限 – 调小学习率</p>
<p>3、sigmoid<br>导数范围在0-1/4，梯度消失问题<br>输出均值不为0，收敛慢<br>幂运算耗时高</p>
<p>4、tanh<br>梯度消失<br>幂运算耗时高</p>
<p>5、 relu<br>缺点：<br>输出均值非0<br>dead relu – 某些神经元永远不被激活，导致相应的参数无法更新</p>
<p>6、dead relu改进<br>负数输入过多导致，改变参数初始化避免过多的负数特征送入relu，设置更小的学习率，避免参数分布巨大变化<br>leaky – 虽然能解决dead，但实际效果中没有证明出比relu更好</p>
<h4 id="初始化建议"><a href="#初始化建议" class="headerlink" title="初始化建议"></a>初始化建议</h4><p>均值为0<br>标准差为 sqrt(2/当前层输入特征个数)的正太分布</p>
<h4 id="指数加权平均（Exponentially-weighted-average）"><a href="#指数加权平均（Exponentially-weighted-average）" class="headerlink" title="指数加权平均（Exponentially weighted average）"></a>指数加权平均（Exponentially weighted average）</h4><p>V_t=β*V_t−1+(1−β)*θ_t</p>
<p>1、当 β 较大时（β = 0.98 相当于每一点前50天的平均气温)。曲线波动相对较小更加平滑，因为对很多天的气温做了平均处理，正因为如此，曲线还会右移。</p>
<p>较小，0.5时，曲线波动相对激烈，但是它可以更快的适应温度的变化。</p>
<p>2、当 β = 0.9时，我们可以近似的认为当前的数值是过去10天的平均值，但是显然如果我们直接计算过去10天的平均值，要比用指数加权平均来的更加准确。但是如果直接计算过去10天的平均值，我们要存储过去10天的数值，而加权平均只要存储V_t−1</p>
<p>3、指数加权平均 不能很好地拟合前几天的数据，因此需要 偏差修正<br>在机器学习中，多数的指数加权平均运算并不会使用偏差修正。因为大多数人更愿意在初始阶段，用一个捎带偏差的值进行运算。不过，如果在初试阶段就开始考虑偏差，指数加权移动均值仍处于预热阶段，偏差修正可以做出更好的估计。<br>​<br>V_t = V_t / (1 - β_t)</p>
<h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><p>GD 到 BGD 到 SGD–即minibatch的SGD(现在说SGD一般都指MBGD)，<br>BGD即每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型参数。这样就会大大加快训练速度。<br>SGD即每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。</p>
<p>步骤：<br>1、计算 t时刻损失函数关于当前参数的梯度 g_t = ▽loss<br>2、计算 t时刻的一阶动量m_t和二阶动量 v_t<br>3、计算 t时刻下降梯度 n_t = lr * m_t/sqrt(v_t)<br>4、计算t+1时刻的梯度即 w_t+1 =  w_t - n_t</p>
<p>其中一阶动量是，梯度相关的函数<br>二阶动量是，梯度平方相关的函数</p>
<p>1、SGD<br>m_t = g_t<br>v_t = 1</p>
<p>SGD每次都会在当前位置上沿着负梯度方向更新（下降，沿着正梯度则为上升），并不考虑之前的方向梯度大小等。<br>动量（moment）通过引入新的变量去积累之前的梯度（通过指数衰减平均得到），得到加速学习过程的目的。</p>
<p>若当前的梯度方向与累积的历史梯度方向一致，则当前的梯度会被加强，从而这一步下降的幅度更大。若当前的梯度方向与累积的梯度方向不一致，则会减弱当前下降的梯度幅度。如下图</p>
<p>2、SGDM 含有momentum的SGD</p>
<p>m_t = θ * m_t-1 + (1 - θ) * g_t   – 指数加权平均<br>v_t = 1 </p>
<p>初始化，m_t = 0</p>
<p><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/momentum.png" alt="momentum"></p>
<p>3、Ada 引入了二阶动量<br>m_t = g_t<br>v_t = ∑ g_t^2</p>
<p>优点：对于梯度较大的参数，意味着学习率会变得较小。而对于梯度较小的参数，则效果相反。这样就可以使得参数在平缓的地方下降的稍微快些，不至于徘徊不前。<br>缺点：由于是累积梯度的平方，到后面累积的比较大，会导致梯度消失。</p>
<p>在凸优化中，AdaGrad算法具有一些令人满意的理论性质。但是，在实际使用中已经发现，对于训练深度神经网络模型而言，从训练开始时累积梯度平方会导致学习率过早过量的减少。AdaGrad算法在某些深度学习模型上效果不错，但不是全部。</p>
<p>Adadelta<br>Adadelta是对Adagrad的改进，主要是为了克服Adagrad的两个缺点（摘自Adadelta论文《AdaDelta: An Adaptive Learning Rate Method》）：<br>the continual decay of learning rates throughout training<br>the need for a manually selected global learning rate</p>
<p>4、RMSProp<br>m_t = g_t<br>v_t = θ * v_t-1 + (1 - θ) * g_t^2</p>
<p>RMSprop也是对Adagrad的扩展，以在非凸的情况下效果更好。和Adadelta一样，RMSprop使用指数加权平均（指数衰减平均）只保留过去给定窗口大小的梯度，使其能够在找到凸碗状结构后快速收敛。</p>
<p>在实际使用过程中，RMSprop已被证明是一种有效且实用的深度神经网络优化算法。目前它是深度学习人员经常采用的优化算法之一。keras文档中关于RMSprop写到：This optimizer is usually a good choice for recurrent neural networks.</p>
<p><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/rmsprop.png" alt="rmsprop"></p>
<p>5、Adam</p>
<p>Adam实际上是把momentum和RMSprop结合起来的一种算法<br><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/adam.png" alt="adam"><br><img src="/2021/02/04/%E6%A2%AF%E5%BA%A6/adamformula.png" alt="adamformula"></p>
<p>reference:</p>
<p><a href="https://ruder.io/optimizing-gradient-descent/index.html">https://ruder.io/optimizing-gradient-descent/index.html</a></p>
]]></content>
      <categories>
        <category>dl</category>
      </categories>
      <tags>
        <tag>dl</tag>
      </tags>
  </entry>
  <entry>
    <title>母仪天下</title>
    <url>/2021/05/25/%E6%AF%8D%E4%BB%AA%E5%A4%A9%E4%B8%8B/</url>
    <content><![CDATA[<br>

<br>

<p>1.</p>
<p>看了5个小时左右，总要写写感想。</p>
<p>女人对感情多多少少的不满足。萧育那样的男人，大多女人都会受诱惑。才华，浪漫，个性，有所思想。</p>
<span id="more"></span>

<p>他和王政君的单独相处，都还会让我心悸。而我以为自己，早已不会陷入这样的”调情“气氛了。</p>
<p>讲到底，调情，讲究魅惑，讲究女人的技术，也讲究两人之间感情的厚度。没有厚度，不过也是一场无聊的戏。</p>
<p>女人还是渴望一个骑士样的男人吧。一生钟情。即使不可得。但是萧育开始却是个风花雪月的男人。而此时，是没有厚度可言的，却是厚度的种子。因为从开始，王政君就是有原则的。哪怕为他所动。</p>
<p>富二代、公子哥、英俊风流，追逐大哥的女人。不能得却一生守护的剧情。比一般的网剧略有质量的，就是加了些历史作料，对白略文雅。但也是白话。但表达至少是好些的。敢于表达，有真有假。但不会感觉太虚伪。</p>
<p>加上演员还是够撑得住的。才能让观众入得了情。</p>
<p>历史上到底有没有这一说，实在不重要。这段情感塑造，满足了女人对爱情的想象。不过也终是想象，现实中追求这样的爱情无可厚非，只是要清醒。遇到了万幸。大部分情况，只是一厢情愿，伤害了自己后认清现实。本就没有真相，只有想象和想象的破灭。</p>
<p>而空想剥离了当下，是不如想象来得有约束感的。想象之所以有魅力，里面裹挟着人们的判断、信念、偏执、努力、期待… </p>
<p>女人太过专注于感情，就浪费了很多人间乐趣。女人亦可风花雪月，只要她喜欢。到了一定阶段，也就认为这个没什么意思了，因为也激发不了她感官的刺激、满足。所有外在形式虽然能过把瘾，他也不再追逐此风流。人是摇摆的。有时候觉得小三美轮美奂。有时候觉得正妻与你共柴米油盐才难得。</p>
<p>两者不可兼得。兼得了也不过一种过渡形式而已。以此为乐则受其所困。不以为乐，也会百无聊赖，也会有所坚持。但是坚持是反感官的。</p>
<br>

<p>2.</p>
<p>王政君让我看到一种不精明，却远比精明讨人喜。或许只是编剧的感性，有先验的倾向性。把她塑造在”母仪天下“的荣耀里。为了此，做出很多让步。</p>
<p>她很慢。事情来了，没有清楚之前，是无言耐性的状态。她可以为了爱的人，有条件地忍下一些原则底线的试探突破。</p>
<p>灾难面前的无畏和冷静。心中有原则而沉稳。有希望和信念就不畏。并且能够吃苦而承受住生理的疼痛折磨。</p>
<p>寂寞于她，也是苦楚。但她不诉。她有泪水，而节制。因为坚强。忍住了寂寞，让那份感情更加满足想象。</p>
<p>过于坚强，秉持”母仪天下“的权力信念，使得王政君更加让人无法共情。</p>
<p>和飞燕合德而比，是无风情、无趣之人。她亲近的男人都无法给予她爱。能认识到这种女人的美，是很少的，因为反感官。女人更多被认可的美德，还是在于魅惑和漂亮。端庄大气，智慧坚毅也是要的，但是最好不要摆在软弱而沉溺情感、耽于享乐的男人面前。武则天就更能变通了。</p>
<p>人是很难做到灵活的。原则最大的坏处就是僵硬。让渡一些获得灵活性。</p>
<p>有趣的事情，大多掺杂着情愫和欲望。理性如果被标榜起来，大多是启人心智，而感性被抬高，就不免都希望能有意思、满足才行。理性会制约你的满足，让你忧道不忧贫。而感性的倾斜，让你更能被理解，使人共情。</p>
<p>嫉妒的人，不讨人喜，加上恶毒，简直就是所谓坏人了。现实中，坏人要比傅瑶更加高明。不精明反而更有手段。这个没有被揭示出来，降低了剧的质量。太简单粗暴。细腻的就少了。虽然，说不定受众会更热捧。看编剧境界吧。</p>
<br>

<p>3.</p>
<p>班婕妤向汉成帝证明自己没有参与许皇后巫蛊事件：</p>
<p>妾闻生死有命，富贵在天，修正尚未得福，为邪还有何望？若使鬼神有知，岂肯听信谗说？万一无知，咒诅何益，妾非但不敢为，也是不屑为！</p>
<br>

<p>4.</p>
<p>无法挖掘什么更体系、深刻的智慧或者逻辑性。文字尚且可以忽悠，更别说这种影视作品。细节里隐藏所有可能的深刻性。本质也展露在微妙之中。体悟之中。那些沉默和忍耐的画面，比一句句的对白，更能看到背后的舍弃，和对一个人物、对人性的考量。</p>
<p>如果记不住一些精妙的细节，过程中不带着审视的眼光，只是享受了被动的感情带入而已。</p>
<p>不过我们成为很难不被带入的动物。不被带入的剥离感，悬空感，需要破除身在此山的迷惑。</p>
]]></content>
  </entry>
  <entry>
    <title>汇率</title>
    <url>/2021/05/23/%E6%B1%87%E7%8E%87/</url>
    <content><![CDATA[<br>



<h3 id="汇率决定理论"><a href="#汇率决定理论" class="headerlink" title="汇率决定理论"></a>汇率决定理论</h3><p>1、流量</p>
<p>1】购买力平价</p>
<span id="more"></span>



<br>

<p>2】利率平价</p>
<br>

<p>3】国际收支说</p>
<br>

<br>

<p>2、存量</p>
<p>1】弹性</p>
<br>

<p>2】粘性</p>
<br>

<p>3】资产组合分析法</p>
<br>



<h3 id="影响汇率的一般因素"><a href="#影响汇率的一般因素" class="headerlink" title="影响汇率的一般因素"></a>影响汇率的一般因素</h3><p>两个角度，经济基本面 以及 政策因素。</p>
<br>

<br>

<p>基本面：</p>
<p>1、GDP</p>
<p>国际收支说，GDP ↑ - 国内进口↑ - 逆差 ↑ CA利空 - 贬值 e↑ (短期)</p>
<p><strong>弹性分析法，GDP ↑ - Md ↑ KA利好 - Ms → - 升值 e↓ (短期)</strong></p>
<p>实践： <strong>购买力平价，GDP↑ - BS效应 e↓ (长期)</strong></p>
<br>

<p>2、通胀</p>
<p>购买力平价，通胀↑ - 价格↑ - e ↑</p>
<p>国际收支说，通胀↑ - 出口↓ - CA 利空 逆差 - e ↑</p>
<p>实践：高通胀带来贬值压力</p>
<br>

<p>3、国际收支</p>
<p>顺差 - 升值压力</p>
<p>逆差 - 贬值</p>
<br>

<p>4、利率</p>
<p>利率平价，i ↑ - e ↓</p>
<p>国际收支，i ↑ - 利好KA - OB↑ - e↓</p>
<p>**弹性货币分析，i ↑ - Md↑ - Ms↑ - 价格↑ - e↑ **</p>
<p>实践：i ↑ - 利好KA - OB↑ - e↓</p>
<br>

<br>

<p>政策：</p>
<p>1、央行的直接、间接干预</p>
<p>2、预期的自我实现机制</p>
<br>

<br>

<h3 id="受汇率影响的经济变量"><a href="#受汇率影响的经济变量" class="headerlink" title="受汇率影响的经济变量"></a>受汇率影响的经济变量</h3><p>1、GDP</p>
<p>短期，</p>
<p>e ↑ - NX↑ -Y↑ </p>
<p><strong>e ↑ - 贬值税效应↑ - 消费↓ -Y↓</strong></p>
<p><strong>e ↑ - 外债负担↑ - 投资↓ -Y↓</strong></p>
<p>长期，看，供给侧 劳动生产率和产业结构、民族工业等</p>
<br>

<p>2、价格</p>
<p>生产成本 – 进口成本推动型通胀，<strong>e ↑ - 进口原材料成本↑ - 进口产品价格↑ -价格↑</strong></p>
<p>货币工资 – ，e ↑  - 进口消费品成本↑ - 工资购买力↓ wage↓ - 价格↑</p>
<br>

<p>3、国际收支</p>
<p>CA:</p>
<p>弹性论，满足ML条件时 e ↑ - CA↑ - TB↑</p>
<p><strong>吸收论，e ↑ - Y↑ and A(国内吸收)↓ 出口↑  - TB↑</strong>  ，这里Y和e相关体现在NX上</p>
<br>

<p>KA:</p>
<p><strong>回归性</strong> ：  e ↑ - 升值预期、Md↑ - KA↑</p>
<p><strong>外推性</strong> ： e ↑ - 贬值、Md↓ - KA↓ </p>
<br>

<br>

<h3 id="人民币汇率形成机制"><a href="#人民币汇率形成机制" class="headerlink" title="人民币汇率形成机制"></a>人民币汇率形成机制</h3><p>市场供求为基础，参考一篮子货币进行调节，有管理的浮动汇率制</p>
<p>中间价 = 上个交易日收盘价 + 保持人民币对一篮子货币夜间汇率稳定所需的汇率变化 + 逆周期调节因子(核心：抑制因预期导致的汇率顺周期波动，让汇率更多反映基本面，少反应预期)</p>
<br>

<br>

<h4 id="汇率市场化"><a href="#汇率市场化" class="headerlink" title="汇率市场化"></a>汇率市场化</h4><p>1、汇率管制工具</p>
<p>中间价、波动幅度、外汇市场常态式干预</p>
<br>

<p>2、好处</p>
<p>1 减少外部失衡对内部的冲击，浮动汇率起着国际收支自动稳定器的作用</p>
<p>2 固定汇率下跨进资金危害金融体系稳定性。浮动汇率灵活调整市场预期，平衡资本流动</p>
<p>3 三元悖论。KA的开放程度越高，为了保证货币政策独立性，越需要放开汇率</p>
<br>

<p>3、政策建议</p>
<p>1 外汇市场交易品种多样化</p>
<p>2 增强企业对冲汇率波动风险能力，进行汇率风险对冲保值</p>
<p>3 由于 羊群效应 汇率超调的存在，央行需要对外汇市场保留干预能力</p>
<br>

<br>



<h4 id="人民币贬值"><a href="#人民币贬值" class="headerlink" title="人民币贬值"></a>人民币贬值</h4><p>1、原因</p>
<p>1 报价机制中，一篮子货币里其他非美元货币对美元贬值 ： 1欧元区 英国等经济体经济弱于美国 2不确定性上升使得美元成为避险资产</p>
<p>2 市场供求导致 – 具体结合当下政策，展开分析经济变量近况，以及分析对e的影响</p>
<p>3 央行退出外汇市场的常态式干预导致人民币近期贬值</p>
<br>

<p>2、影响</p>
<p>结合模型分析对其他变量的影响</p>
<p>对贸易摩擦 影响 : 企业外债负担、结构性关税(总量型贬值税)、打击进口、激化摩擦… </p>
<p>对人民币国际化</p>
<p>金融市场危机防范等… </p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>海量数据</title>
    <url>/2021/10/06/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul>
<li>存储<span id="more"></span>、运算(排序等)</li>
</ul>
<h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><ul>
<li><p>stl 容器</p>
<ul>
<li>序列式</li>
<li>关联式 <ul>
<li>类比 关联式数据库</li>
<li>非关联数据库，document里也是kv</li>
<li>底层<ul>
<li>红黑树<ul>
<li>set/map/multiset/multimap<ul>
<li>multi即允许重复</li>
<li>insert_equal()而非insert_unique()</li>
</ul>
</li>
</ul>
</li>
<li>hashtable <ul>
<li>hash_set/hash_map/hash_multiset/hash_multimap</li>
<li>HashSet实现了Set接口，HashMap实现了Map接口，HashSet扩展了HashMap，底层存储相同</li>
<li>hashtable和hashmap ： 前者继承Dictionary类、synchronized、不允许键或值为 null、Enumerator是fail-safe</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>​    </p>
<ul>
<li>hashcode<ul>
<li>equals为true，hashcode一定相等，反之不成立</li>
<li>(key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)   </li>
</ul>
</li>
</ul>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><ul>
<li><p>分而治之/hash映射 + hashmap统计 + 堆/快速/归并排序；</p>
<ul>
<li><p>负载均衡算法</p>
<ul>
<li><p>轮循算法（Round Robin）、哈希算法（HASH）、最少连接算法（Least Connection）、响应速度算法（Response Time）、加权法（Weighted）</p>
</li>
<li><p>hash </p>
<ul>
<li><p>取余</p>
</li>
<li><p>一致性</p>
<ul>
<li>分布式系统负载均衡的首选算法</li>
<li>Hash 算法的一个衡量指标是单调性（ Monotonicity ）： 新增的key被hash到新的机子上</li>
<li>圆圈</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>topk</p>
<ul>
<li>堆排序<ul>
<li>N个无重值topk的复杂度，N*O(logK)</li>
<li>TOP10小，用最大堆，TOP10大，用最小堆</li>
</ul>
</li>
<li>trie tree</li>
</ul>
</li>
<li><p>同一个key在两台机子</p>
<ul>
<li>全hash，同一个key到一台。然后分布式架构处理，如MapReduce，最后合并</li>
<li>暴力，每台都算出来，再加和</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li>双层桶划分</li>
<li>Bloom filter/Bitmap</li>
<li>Trie树/数据库/倒排索引</li>
<li>外排序</li>
<li>分布式处理之Hadoop/Mapreduce</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>牛猫2108</title>
    <url>/2021/08/05/%E7%89%9B%E7%8C%AB2108/</url>
    <content><![CDATA[<h4 id="日子"><a href="#日子" class="headerlink" title="日子"></a>日子<span id="more"></span></h4><br>

<br>



<br>

<h5 id="0801"><a href="#0801" class="headerlink" title="0801"></a>0801</h5><ol>
<li>白马：<strong>1死扛，2回避，3选择性参与。</strong></li>
<li>无论行情风格怎么转换，都不应该切回到炒垃圾股、高频投机的那条烂路上去</li>
<li>**茅台半年报，营收+11.68%，归母利润+12.5%**。–回避</li>
<li><strong>格力电器回购95亿，股份比例达到3%。</strong>股价将将守在47附近，高瓴400多亿被套里面，虽然收购时的成本是46，但资金很多都是找银行借的，这么多年利息算下来肯定浮亏。格力就是我说的第一类股票，目前实在没有割肉的必要，做好准备蹲一个周期。</li>
<li><strong>本周北向资金对中国平安加仓最多，对茅台的减仓最多。</strong></li>
<li>清了智飞生物、韦尔，都是觉得它里面有基金在出货，还不如买芯片ETF更划算。结果星期五一天，芯片ETF就跑赢韦尔5%。除非已经跌透，否则那些持续一段时间跑输行业均值的基金重仓股，我都不想碰了。 ==&gt;虽然是下跌通道中，但是清的时间点不是最低</li>
</ol>
<br>

<h5 id="0802"><a href="#0802" class="headerlink" title="0802"></a>0802</h5><ol>
<li><strong>成交量爆炸，达到了1.5万亿的数量级。除了极少数强势板块回调外，小盘股、大盘股不再像以前那样玩跷跷板，</strong>绝大多数板块都是均衡普涨</li>
<li><strong>二五重工今天涨停</strong>，变成了二七重工。另外像恒立液压、中联重科等股票今天也都强势涨停，<strong>工程器械板块今天整体+8%，居两市涨幅榜榜首。</strong>海螺水泥也几乎涨停</li>
<li>原因：1政治局会议指出，积极的财政政策，兜牢基层“三保底线”，合理把握预算内投资和地方政府债券发行进度 == 搞下基建？ 2水泥价格反弹，需求端回暖 ==&gt;这套分析也是典型的先射箭，后画靶。事实上工程机械今早一开盘还是跌的，而海螺水泥的股价更是到10:30才勉强翻红。 3 内因：无非就是跌够了，跌到位了，正好今天锂电、新能源、芯片出现了集体回调，<strong>资金从焦点板块流出，倒灌回这些坑里面的票，于是就形成了今天“杀富济贫”的主线。</strong></li>
<li><strong>花旗：下调美团目标价19%，从440降低至357，维持买进评级。</strong>原因是根据新政策，外卖送餐员的社保将增加运营成本，不过看好美团度过这次挑战，成为本地服务门户公司。我依然持有美团，因为我觉得未来还有更好的时候。</li>
<li><strong>美的集团累计回购27.5亿元</strong>，昨天是格力累计回购快100亿了，这两难兄难弟，回购再多也抵不过想跑的机构资金</li>
<li><strong>创业板的新股，叫义翘神州，发行价292.92元，一个签的资金高达14.65万</strong>。主营的概念还不错。单价虽然贵，但估值其实还可以，这个谁要是中一签盈利区间大概是15-20万。</li>
<li><strong>财富公布了中国500强，其中还披露了一个净资产收益率前40强</strong>(统计的是去年的净利润)。<strong>净资产收益率是衡量一家优质公司的重要指标</strong>。 <img src="/2021/08/05/%E7%89%9B%E7%8C%AB2108/ROE40%E5%BC%BA" alt="ROE40强"></li>
<li>后面再买的话会加仓IC，也就是中证500指数基金</li>
</ol>
<br>

<h5 id="0803"><a href="#0803" class="headerlink" title="0803"></a>0803</h5><ol>
<li>经济参考登了批游戏的，中午又撤了，下午开盘后游戏板块强烈反弹，差不多把上午跌掉的一半给收了回来，但相关上市公司还是损失了5000亿左右的市值</li>
<li><strong>人吃苦是为了提升自我，不是为了吃一辈子的苦。暂时克制娱乐，是为了以后更好的娱乐，不是彻底告别娱乐。</strong></li>
<li><strong>阿里巴巴第一财季利润434亿</strong>，超过市场预期293亿，去年同期475亿。计划回购股票从100亿美元提升至150亿美元，这是中国企业史上最大金额的回购</li>
</ol>
<br>

<h5 id="0804"><a href="#0804" class="headerlink" title="0804"></a>0804</h5><ol>
<li><strong>段永平在雪球上发帖，说自己抄底腾讯了</strong>，如果后面跌，他再买一点。大佬说话还是很有号召力的</li>
<li><strong>工信部开会，千方百计增加平板玻璃生产</strong>，严惩哄抬物价行为。相关板块上来领板子。</li>
<li><strong>市场监管总局对涉嫌哄抬价格的化肥企业立案调查</strong>，这些现在都是风向标。</li>
<li>新华社：警惕<strong>电子烟</strong>流向未成年。</li>
<li><strong>保利地产高管、大股东宣布增持</strong>，就一个标题，正文数额我懒得看了，因为增再多也没格力美的回购的多，决定你跌不跌的不是你的回购，是那些叛徒还有多少要卖。</li>
<li>深圳学位房采取大学区制，细节我没看，估计和北京差不多，都是全区随机摊派。之前北京西城就有家长去喂犬闹过，牵头的中介还被抓了。现在还炒学区房的都是蠢材，政府连万亿市场的教培行业都是直接咔嚓，决心之大清晰可见，至于学区房那点既得利益者，给你个狗胆你去折腾。</li>
</ol>
<br>

]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>理财基础</title>
    <url>/2021/08/30/%E7%90%86%E8%B4%A2%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<p>1、 分红</p>
<span id="more"></span>

<br>

<p>1、 分红</p>
<p>分现金，或者分股票。</p>
<p>A股通常有2次分红，分别在半年报和年报，主要都集中在每年1-4月披露的年报，半年报只有少数骚包的公司才分红。</p>
<p>分红是要交税的。比如现金分红，持股时间越长交税越低。持股一年以上，免税。</p>
<p>股价要除权，红利要交税，对于短线交易者来说的确不利。所以当现金分红特别多时，会有人提前一晚卖出来避税。</p>
<p>分红的术语有3种，比如10转5送4派3，这里的派就是给现金，意思是每10股给你3元现金；转和送都是给你股票，5+4等于一共给你9股，唯一的区别是转增的股票不交税，送增的股票要按每股1元的面额交税。</p>
<p><a href="http://t.cn/zOcw6Ft%E3%80%82%E5%88%86%E7%BA%A2%E8%BD%AC%E5%A2%9E%E9%99%A4%E6%9D%83%E7%99%BB%E8%AE%B0%E6%97%A5%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%E6%9C%80%E8%BF%91%E6%9C%89%E5%93%AA%E4%BA%9B%E8%82%A1%E8%A6%81%E6%89%A7%E8%A1%8C%E5%88%86%E7%BA%A2%E3%80%82">http://t.cn/zOcw6Ft。分红转增除权登记日，可以看到最近有哪些股要执行分红。</a></p>
<br>

<hr>
<p>2、资金流入流出</p>
<p>下跌的时间里的成交量加起来，就是资金流出量；把上涨时间里的成交量加起来，就是资金流入量。不涨不跌的时间不要。把资金流出量和流入量互相减一下，谁数额大就代表当天的资金流出或流入。</p>
<p>资金流入股价下跌，说明盘中有资金想抬升股价做多，但到收盘的时候失败被套，这就是指标传递出来的信息。</p>
<br>

<hr>
<p>3、etf</p>
<p>沪深300，中证50，中证500，创业板指，中证科技，证券指数，中证军工，芯片指数。这些指数都有对应的etf。</p>
<p>交易ETF基金和交易股票<strong>唯一</strong>的区别，就是不用交印花税，其他都一样。</p>
<p>7月22看到的文章里，推荐了芯片etf159995，新能源车etf515030、新材料etf 516360。可以踩到均线的时候择机上车。或者宽基的etf，创业板、中证500。</p>
<br>

<hr>
<p>4、分级基金</p>
<p>母基金的💰一部分给A份额，一部分B份额。分级基金分为A类基金和B类基金。A类固定收益，B类借钱(AB一共的钱)炒股收益，固定给A份额的钱利息。</p>
<p>为了保障A的本金，有向下折算条款。一旦资金低于某个值，A可以先取回部分本金。</p>
<p>分级基金的净值和二级市场上的股价有着巨大的偏差，通常是A基有折价，B基有溢价。在下折的时候由于杠杆倍数暴降，B基的溢价会被疯狂挤压，从而出现30-40%的暴跌。</p>
<p>150<strong><strong>，</strong></strong>AorB，一般是市场的分级基金组合。</p>
<br>

<hr>
<p>5、理财</p>
<p>无本金风险：</p>
<p>1.各个银行的t+0理财：一般小银行的比大银行收益高。上海银行的易精灵、平安银行的日添利，2.73%年化。</p>
<p>2.证券账户放逆回购：上海的204001，深圳的131810，卖了一个131810的5.03，金额是5000，意思就是你把5000元以年化5.03%的利息借给他人。204002、131800等不同天数的出借。9：40和14：45左右容易出现日内高点，临收盘前通常会暴跌，逆回购次日回来的钱可用不可取，所以周四以及长假前2个交易日利率偏高。</p>
<p>3.证券账户放货币基金：比如添富快，华宝添益、银华日利、华夏保证金等，问券商是否有手续费。</p>
<p>4.理财产品：理财产品的💰用于包含了打新、公司债，风险只存在于理论上，仅高于存银行。6%以内的即便对方说不保本基本问题不大。目前的行情是5%左右，只适合没时间的业余理财人士。国家印钞税约10%。</p>
<p>有风险：</p>
<p>1.信托：自然人购买信托产品最低100万，主要分3类。第1类是地方政府公共建设项目，第2类是股权质押，信托公司都是以3-5折质押，上市不到一年的次新股质押需警惕，这类信托风险和收益都略高。目前信托的行情是年化收益9%，比前几年低，只有部分长年限+股权质押的才有可能10%+收益。信托产品到目前为止都是刚性保本兑付。</p>
<p>第3类是阳光私募信托，即依法的代理炒股，你若把股票账号给我，5分钟就能取走里面90%的钱，只是这么做需要一些专业知识。</p>
<p>2.p2p理财</p>
<p>陆金所放贷是合法的，而且背后有中国平安支撑兜底，在陆金所放贷收不回本金的概率和中国平安破产的概率很接近。正因为风险小，收益也偏低，年化8%的两年期还经常要抢，好在门槛低，也就几万块钱。</p>
<p>开鑫贷。背景是江苏国开金融子公司，上面借钱的几乎都是江苏省内小贷公司，小贷公司是有合法牌照并受监管的，所以风险虽然比中国平安大，但有限。在上面放贷10万起步，可以拿到10-12%的年息，主页君也在上面理过财。补充：开鑫贷投资需开通江苏银行账户。</p>
<p>宜信。宜信不是网站，是中国最大的p2p贷款公司，有一定的政策风险，但以宜信今日之规模，你可以把它视为1998年那会的中国平安。</p>
<p>3.打新</p>
<p>6万块钱，参与人民网打新，2个签号就中了1000股，几天后赚了1万多。打新一年的收益率在6-14%区间波动，当你投入资金超过100万时，基本上就稳定在8-10%。</p>
<p>4.股票型基金。股基有60%的持仓下限，如果你喜欢定投，建议多买一些指数基金，这样连基金经理的道德风险也规避了。</p>
<p>5.黄金</p>
<p>买纸黄金，买期货，买实物金条都算投资黄金，但买黄金首饰绝对不算，这里面有巨大的差价。如果美元对人民币一直在贬值，对于美元计价的黄金这不是什么好事。</p>
<p>6.房子</p>
<p>决定商品房价格的第一主因是以房子为圆心，半径10公里内所有人的现金收入。</p>
<p>其次看持有价值。95%以上的A股没有持有价值，只有交易价值，但房子是有持有价值的。无论是住是租它都比握在手里的股票强多了，外地我不清楚，北京的房租收益在2-3%一年，很显然如果按持有价值算的话，30倍pe的房价是略高的。</p>
<p>再次看供求。根据人口普查的数据，中国出生人口高峰是1990年，之后的23年一直在递减。我们假设28岁是适婚年龄的话，2018年以后刚需会开始下降，至少到2041年都在下降。另外<strong>中国从2020年开始进入老龄化</strong>，一直到2040年，60岁以上人口持续增长，60岁以下人口持续下降。最后看货币。领导人说了，十年收入翻番。</p>
<p>目前房价按持有价值算有溢价，主要是包含了对未来上涨的预期；2018-2020左右需求开始下降，之后20年将迎来漫长的衰退期，但在2018年之前需求持续旺盛；货币政策持续提供支持。</p>
<br>

<hr>
<br>

<p>6、缺口理论</p>
<p>缺口，通常又称为跳空，是指证券价格在快速大幅波动中没有留下任何交易的一段真空区域。缺口的出现往往伴随着向某个方向运动的一种较强动力。缺口的宽度表明这种运动的强弱。不论向何种方向运动所形成的缺口，都将成为日后较强的支撑或阻力区域，不过这种支撑或阻力效能依不同形态的缺口而定。</p>
<p>缺口划分为普通缺口、突破缺口、持续性缺口和消耗性缺口四种形态。</p>
<p>（A股整体缺口回补率在97.3%以上，其中68.3%的缺口会在10个交易日内回补）</p>
<p>普通缺口经常出现在股价震荡整理形态中，因此不影响股价短期内的走势，支撑或阻力效能一般较弱，成交量很小，很少有主动的参与者，一般会在3日内回补。回补带来了操作机会，但是这种操作的前提是必须判明缺口是否为普通缺口，且证券价格的涨跌是否达到一定的幅度。</p>
<p>（趋势发动缺口）突破缺口是证券价格向某一方向急速运动，跳出原有形态所形成的缺口。突破缺口的形成在很大程度上取决于成交量的变化情况，特别是向上的突破缺口。若突破时成交量明显增大，且缺口未被封闭（至少未完全封闭），则这种突破形成的缺口是真突破缺口。若突破时成交量未明显增大，或成交量虽大，但<strong>缺口短期内很快就被封闭</strong>，则这缺口很可能是<strong>假突破缺口</strong>。<strong>如何鉴定突破性缺口呢？</strong>关键在于出现缺口前的走势，大都是横向震荡的一个平台。连续上涨或下跌一段时间后，再出现的缺口，一定不是突破性缺口。</p>
<p>一般来说，突破缺口形态确认以后，无论价位（指数）的升跌情况如何，投资者都必须立即做出买入或卖出的指令，即向上突破缺口被确认立即买入：向下突破缺口被确认立即卖出，因为突破缺口一旦形成，行情走势必将向突破方向纵深发展。</p>
<p>持续性缺口是在证券价格向某一方向有效突破之后，由于急速运动而在途中出现的缺口，它是一个趋势的持续信号。在缺口产生的时候，交易量可能不会增加，但如果增加的话，则通常表明一个强烈的趋势。持续性缺口的市场含义非常明显，<strong>它表明证券价格的变动将沿着既定的方向发展变化，并且这种变动距离大致等于突破缺口至持续性缺口之间的距离，即缺口的测量功能</strong>(通俗一点解释，就是往下还要再跌一倍。)。</p>
<p>（趋势衰竭缺口）消耗性缺口一般发生在行情趋势的末端，表明股价变动的结束。若一轮行情走势中已出现突破缺口与持续性缺口，那么随后出现的缺口就很可能是消耗性缺口。<strong>判断消耗性缺口最简单的方法就是考察缺口是否会在短期内封闭。</strong>若缺口封闭，则消耗性缺口形态可以确立。消耗性缺口容易与持续性缺口混淆，它们的最大区别是：消耗性缺口出现在<strong>行情趋势的末端，而且伴随着大的成交量</strong>。</p>
<p>经典3缺口下跌浪：<img src="/2021/08/30/%E7%90%86%E8%B4%A2%E5%9F%BA%E7%A1%80/3%E7%BC%BA%E5%8F%A3%E4%B8%8B%E8%B7%8C%E6%B5%AA.png" alt="3缺口下跌浪"></p>
<p>缺口适用于个股吗？可以使用，但是个股偶然性因素大，所以回补概率不像指数那么稳定。<strong>市值越大的标的物，回补概率越稳定</strong>，指数更适用。像中石油、中石化、工行这样的股票形成的缺口，回补率也比较稳定。</p>
<p>有三类缺口不算：1涨停、跌停次日的缺口不算2长期停牌复牌后的缺口不算3中国特色的7天长假后的缺口不算。</p>
<p>缺口理论在所有市场都有效吗？<strong>自主定价市场是很有效的。</strong>就是价格自己说了算。比如A股就是典型的自主定价市场，香港不是，香港受大陆和美股的多重影响，美股也是自主定价市场，所以缺口在美股的回补率也很高。焦碳、螺纹钢这些我们有定价权的品种的缺口少，回补率高。而像黄金、白银，都是老外定价说了算，我们睡觉的时候老外黄金涨了3%，第二天我们被迫跟着高开出一个大缺口，这个不一定能补掉，因为我们的价格是人家的影子，影子没有补缺的权利。</p>
<p>小作业：计算107个沪深指数的缺口及回补率。</p>
<br>

<hr>
<p>7、模型及策略及指标</p>
<p>波动率，就是统计某个指数日震幅的均值，日震幅=日内最高点/日内最低点。波动率越高的指数越活跃，就越有可能走出大涨大跌的行情。</p>
<p>再好的量化模型，具体到单个股票的随机性很大。</p>
<br>

<hr>
<p>8、投资观和心理建设</p>
<p>真正的优势是拥有一个正确的交易习惯，涨的时候拿的住，跌的时候不会心存侥幸</p>
<p>因为随机，所以不要试图在随机里去赌对错。</p>
<p>忘记成本。不要让成本干扰交易。买卖的价格一旦交易后要忘掉。因为后一笔交易和前面的完全不相关。预计超出止损，则卖。</p>
<p>低胜率+高盈亏比，这是趋势交易最典型的特征。</p>
<p>当投机的人多了，业绩就不再是王道，“价值投资”不再只是捡烟头的概念。盘子小，涨得快才是王道。 – 难道美股投机少吗，大家一致只拥护价值？中国的公募要业绩，被迫博傻，难道Cathie Wood就不要么？关键还是政策影响力。美股影响力最大的是价值，而A是党中央。</p>
<p>价格投机是投资队友，如果买入的队友是狼性的，是逐利的资金。而另一只股票，却是怕死的安全的资金在里面。</p>
<br>

<hr>
<p>9、</p>
]]></content>
  </entry>
  <entry>
    <title>积分</title>
    <url>/2021/08/04/%E7%A7%AF%E5%88%86/</url>
    <content><![CDATA[<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结<span id="more"></span></h3><br>

<h4 id="二重积分"><a href="#二重积分" class="headerlink" title="二重积分"></a>二重积分</h4><br>

<ol>
<li>性质上的、换坐标系的</li>
<li>形心简化</li>
<li>被积区间的拆分、补全、放缩，以及根据区间来触发思路</li>
<li>不等式相关，略难。 – 被积函数的放缩(泰勒、对称性的不等式、积分中值定理)、区间</li>
<li>能看出来凑微分 （定积分）</li>
<li>换元</li>
<li>应用：体积(h*dσ)</li>
<li>把对称性、不等关系(稍微变化以下，但是还是平方不等式)、提取公因式，整合到一个题目的不等式证明里，本身就有些难想到了。再加上形式很隐蔽，不够容易联想。</li>
<li></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>统计角度看ml</title>
    <url>/2021/03/11/%E7%BB%9F%E8%AE%A1%E8%A7%92%E5%BA%A6%E7%9C%8Bml/</url>
    <content><![CDATA[<br>

<p>一些思考，从整体上看模型的思路，进行比较。</p>
<span id="more"></span>
<p>极大似然？就是后验、大量样本的整体出现概率值最大。样本之间独立。可应用乘法原理。</p>
<p>条件概率，即某(些)条件下某(些)事件出现的概率。</p>
<p>决策树则是求其极大值，局部选择当前条件概率最大。条件概率越大，不确定性越低，条件熵越小。整体熵未必减小。考虑整体熵值的是最大熵模型和logistics模型。logistics是可以通过假设先验概率，转化为这样的问题：推导出满足极大似然极值而得到的w参数。都可以通过拉格朗日将问题转化为无约束求极值，偏导为0，算法里有迭代尺度、梯度下降、牛顿插值。牛顿插值迭代收敛快些。</p>
<p>特征选择无论是熵、增益(比)还是基尼系数都体现出不确定性的思想。增益大则说明特征对结果的影响力大，因为说明条件熵越小，即条件概率越大，该条件下不确定性越低。增益或比越大，而基尼越小越好。因为前者都有-号，类似相反数的关系。本质还条件概率的问题。</p>
<p>决策生成树可以用动归提高算法效率。</p>
<p>感知器从loss func出发，求最小。距离如何定义还要看具体应用场景。主要算法里有<a href="https://blog.csdn.net/greenyang5277/article/details/104270803">Gram矩阵</a>，对偶算法。</p>
<p>k邻很简单，经验风险最小，就是多数为胜，即最简单的频率派概率最大思路，k不一样结果可能瞬间不同了，参数也少。</p>
<p>朴素贝叶斯主要是独立性假设，在类确定下特征条件独立，才能将公式分子简化，否则不那么容易求最大值，后验概率最大，还是极大似然思路，后验概率这是可用简单的乘法原理表达。如果有0，这里提出可以平滑的思想。</p>
<p>无论是熵、条件熵、基尼系数，还是贝叶斯、条件概率，还是感知器的loss func，还是c4.5里的loss(在剪枝时通过熵建立的loss，加入了模型复杂度因子，通过比较剪枝前后大小来判断是否剪枝)，还是logistics(初始分布进行求对数几率，求极大似然最大的参数)、最大熵(公式也化为求极大似然最大)两个对数线性模型的经验分布推导出来的无约束最优化公式，都是对初始概率进行包装，要么转为极大似然问题或者说转为条件概率问题，要么转为loss最小问题。概率问题则为监督学习中的两种，生成和判别。非概率则自定义的一些代数loss。</p>
<p>对给定输入判别输出，判别要么是f(x),要么是P(Y|X)。前者会出现loss func，定义距离如感知器、LDA，后者则决策树、k邻、贝叶斯、GMM。对数线性模型也是P(Y|X)概率分布公式表达的分类模型。都可以说是在对这两种函数求最优的问题。但如决策树、k邻，没有什么公式，也就没有什么参数需要调整，大多重点在算法，如决策树里主要是三个算法里剪枝过程应用到熵之类问题，而kmeans主要kd树解决高维搜索问题。理论框架虽然可以和统计通过加条件等方式相关联，但是更多是另一种思路。</p>
<p>生成模型，由学习数据得到原始分布，再来求P(Y|X)，如贝叶斯。判别模型，学习数据不从分布入手，而直接对条件概率进行假设，如LR。</p>
<p>具体细节，如收敛性证明，拉格朗日转化，最大熵的约束公式，包括泛化误差利用切比雪夫求上界的前提条件，感知器中正负，logistics里对数几率特征空间是n+1维，都需落在数学上去一步步转化推证。</p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>编译tensorflow</title>
    <url>/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/</url>
    <content><![CDATA[<br> 

<p>记录编译过程</p>
<span id="more"></span>

<br> 

<h4 id="前序安装"><a href="#前序安装" class="headerlink" title="前序安装"></a>前序安装</h4><p>1、bazel 2.0.0 通过sh安装，需要chmod，然后export环境变量。我的路径在/var/root/bin，添加到source ~/.bash_profile。把这句source添加到 vi .zshrc ，然后source ~/.zshrc ，重启也有bazel了。</p>
<p>2、tensorflow v2.2.0 </p>
<p>3、conda环境，选择了python2.7，然后安装requirements (tensorflow-2.2.0/tensorflow/tools/pip_package/setup.py )如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">future</span><br><span class="line">absl-py &gt;&#x3D; 0.7.0</span><br><span class="line">astunparse &#x3D;&#x3D; 1.6.3</span><br><span class="line">backports.weakref &gt;&#x3D; 1.0rc1</span><br><span class="line">enum34 &gt;&#x3D; 1.1.6</span><br><span class="line">gast &#x3D;&#x3D; 0.3.3</span><br><span class="line">google_pasta &gt;&#x3D; 0.1.8</span><br><span class="line">h5py &#x3D;&#x3D; 2.10.0</span><br><span class="line">keras_preprocessing &gt;&#x3D; 1.1.0</span><br><span class="line">numpy &#x3D;&#x3D; 1.16.0</span><br><span class="line">opt_einsum &gt;&#x3D; 2.3.2</span><br><span class="line">protobuf &gt;&#x3D; 3.8.0</span><br><span class="line">tensorboard &#x3D;&#x3D; 2.1.0</span><br><span class="line">tensorflow_estimator &#x3D;&#x3D; 2.2.0</span><br><span class="line">termcolor &gt;&#x3D; 1.1.0</span><br><span class="line">wrapt &gt;&#x3D; 1.11.1</span><br><span class="line"></span><br><span class="line"># python3 requires wheel 0.26</span><br><span class="line">wheel &gt;&#x3D; 0.26</span><br><span class="line"></span><br><span class="line"># mock comes with unittest.mock for python3 need to install for python2</span><br><span class="line">mock &gt;&#x3D; 2.0.0</span><br><span class="line"></span><br><span class="line"># functools comes with python3 need to install the backport for python2</span><br><span class="line">functools32 &gt;&#x3D; 3.2.3</span><br><span class="line">six &gt;&#x3D; 1.12.0</span><br><span class="line"></span><br><span class="line"># scipy &lt; 1.4.1 causes segfaults due to pybind11</span><br><span class="line"># Latest scipy pip for py2 is scipy&#x3D;&#x3D;1.2.2</span><br><span class="line">scipy &#x3D;&#x3D; 1.2.2</span><br></pre></td></tr></table></figure>



<p>4、如果需要gpu，则还要安装cuda 10.1 和 cudnn 7.6.5</p>
<br> 

<h4 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h4><p>1、配置基本选择N，需要gpu则在CUDAsupport选择y</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd tensorflow-2.2.0</span><br><span class="line">.&#x2F;configure</span><br></pre></td></tr></table></figure>

<p>2、bazel</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bazel build --noincompatible_do_not_split_linking_cmdline --local_ram_resources&#x3D;2048  &#x2F;&#x2F;tensorflow&#x2F;tools&#x2F;pip_package:build_pip_package</span><br><span class="line"></span><br><span class="line">bazel build --noincompatible_do_not_split_linking_cmdline --local_ram_resources&#x3D;2048 &#x2F;&#x2F;tensorflow:libtensorflow_cc.so</span><br></pre></td></tr></table></figure>



<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/inprogress.jpg" alt="过程"></p>
<br>



<p>编译成功，花了12个小时多。</p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/success.jpg" alt="success"></p>
<br>

<h4 id="生成whl"><a href="#生成whl" class="headerlink" title="生成whl"></a>生成whl</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo .&#x2F;tensorflow&#x2F;tools&#x2F;pip_package&#x2F;build_pip_package.sh &#x2F;tmp&#x2F;tensorflow_pkg</span><br><span class="line">#接着我切了conda环境</span><br><span class="line">pip install --upgrade pip</span><br><span class="line">pip uninstall tensorflow</span><br><span class="line">pip uninstall tensorboard</span><br><span class="line">pip uninstall tensorflow-tensorboard</span><br><span class="line">#pip3 install tensorboard&#x3D;&#x3D;2.2.1  #有个巨坑。pip3才能安装2.2.1，但是我们环境用的都是python2.7</span><br><span class="line">#要么把之前的切到3，要么下载下来site-package 装到目录里。我是选择粘贴到目录 #&#x2F;Users&#x2F;wyq&#x2F;anaconda2&#x2F;lib&#x2F;python2.7&#x2F;site-packages&#x2F;tensorboard</span><br><span class="line">pip install -U --ignore-installed wrapt  </span><br><span class="line">#又是坑。wrapt：ERROR: Cannot uninstall &#39;wrapt&#39;. It is a distutils installed project and thus #we cannot accurately determine which files belong to it which would lead to only a partial #uninstall.</span><br><span class="line">pip3 install &#x2F;tmp&#x2F;tensorflow_pkg&#x2F;tensorflow-2.2.0-cp27-cp27m-macosx_10_14_x86_64.whl</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/whl0.jpg" alt="whl0"></p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/whl.jpg" alt="whl"></p>
<h4 id="python验证"><a href="#python验证" class="headerlink" title="python验证"></a><br>python验证</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#打开ipython </span><br><span class="line">#如果遇到这个问题 ImportError: cannot import name pywrap_tensorflow</span><br><span class="line">#看这个issue </span><br><span class="line"># https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;35953210&#x2F;error-running-basic-tensorflow-example</span><br><span class="line">import tensorflow as tf</span><br><span class="line">tf.__version__</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></table></figure>

<br>

<p>终于成功了</p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/whlsuccess.jpg" alt="whlsuccess"></p>
<br>

<h4 id="c-验证"><a href="#c-验证" class="headerlink" title="c++验证"></a>c++验证</h4><p>选择了vscode，下次clion可以也试试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#在tf根目录下运行</span><br><span class="line">bazel build &#x2F;&#x2F;tensorflow&#x2F;cc:tutorials_example_trainer</span><br><span class="line">bazel build &#x2F;&#x2F;tensorflow&#x2F;cc:client_client_session_test￼</span><br><span class="line"></span><br><span class="line">#然后运行程序</span><br><span class="line">.&#x2F;bazel-bin&#x2F;tensorflow&#x2F;cc&#x2F;client_client_session_test￼</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/csuccess.jpg" alt="csuccess"></p>
<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/clienttest.jpg" alt="demo"></p>
<br>

<h4 id><a href="#" class="headerlink" title></a></h4><h4 id="c-demo"><a href="#c-demo" class="headerlink" title="c++demo"></a>c++demo</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;main.cpp</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &quot;tensorflow&#x2F;cc&#x2F;client&#x2F;client_session.h&quot;</span><br><span class="line">#include &quot;tensorflow&#x2F;cc&#x2F;ops&#x2F;standard_ops.h&quot;</span><br><span class="line">#include &quot;tensorflow&#x2F;core&#x2F;framework&#x2F;tensor.h&quot;</span><br><span class="line"></span><br><span class="line">using namespace tensorflow;</span><br><span class="line">using namespace tensorflow::ops;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    Scope root &#x3D; Scope::NewRootScope();</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Matrix A &#x3D; [3 2; -1 0]</span><br><span class="line">    auto A &#x3D; Const(root, &#123; &#123;3.f, 2.f&#125;, &#123;-1.f, 0.f&#125; &#125;);</span><br><span class="line">    &#x2F;&#x2F; Vector b &#x3D; [3 5]</span><br><span class="line">    auto b &#x3D; Const(root, &#123; &#123;3.f, 5.f&#125; &#125;);</span><br><span class="line">    &#x2F;&#x2F; v &#x3D; Ab^T</span><br><span class="line">    auto v &#x3D; MatMul(root.WithOpName(&quot;v&quot;), A, b, MatMul::TransposeB(true));</span><br><span class="line"></span><br><span class="line">    std::vector&lt;Tensor&gt; outputs;</span><br><span class="line">    ClientSession session(root);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Run and fetch v</span><br><span class="line">    TF_CHECK_OK(session.Run(&#123;v&#125;, &amp;outputs));</span><br><span class="line">    std::cout &lt;&lt; &quot;tensorflow session run ok&quot; &lt;&lt; std::endl;</span><br><span class="line">    &#x2F;&#x2F; Expect outputs[0] &#x3D;&#x3D; [19; -3]</span><br><span class="line">    std::cout &lt;&lt; outputs[0].matrix&lt;float&gt;();</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;BUILD file</span><br><span class="line"></span><br><span class="line">load(&quot;&#x2F;&#x2F;tensorflow:tensorflow.bzl&quot;, &quot;tf_cc_binary&quot;)</span><br><span class="line">tf_cc_binary(</span><br><span class="line">     name &#x3D; &quot;main&quot;, #目标文件名</span><br><span class="line">     srcs &#x3D; [&quot;main.cpp&quot;], #源代码文件名</span><br><span class="line">     deps &#x3D; [</span><br><span class="line">        &quot;&#x2F;&#x2F;tensorflow&#x2F;cc:cc_ops&quot;,</span><br><span class="line">         &quot;&#x2F;&#x2F;tensorflow&#x2F;cc:client_session&quot;,</span><br><span class="line">         &quot;&#x2F;&#x2F;tensorflow&#x2F;core:tensorflow&quot;</span><br><span class="line">         ],</span><br><span class="line"> )</span><br><span class="line"> </span><br><span class="line"> &#x2F;&#x2F;执行</span><br><span class="line"> bazel build &#x2F;&#x2F;tensorflow&#x2F;demo:main</span><br></pre></td></tr></table></figure>

<p><img src="/2021/04/12/%E7%BC%96%E8%AF%91tensorflow/demo.jpg" alt="demo"></p>
<br>

<p>来bili 看跳转~</p>
<p><a href="https://www.bilibili.com/video/BV1Q54y1b74n">https://www.bilibili.com/video/BV1Q54y1b74n</a></p>
<h4 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h4><p>相关编译、过程中error issue、cmake、bazel以及用c++ api构建线上服务</p>
<p><a href="https://github.com/tensorflow/tensorflow/issues/43654">https://github.com/tensorflow/tensorflow/issues/43654</a></p>
<p><a href="https://xugaoxiang.com/2020/05/22/compile-tensorflow2-with-gpu/">https://xugaoxiang.com/2020/05/22/compile-tensorflow2-with-gpu/</a></p>
<p><a href="https://www.tensorflow.org/install/errors">https://www.tensorflow.org/install/errors</a></p>
<p><a href="https://www.javatt.com/p/43568">https://www.javatt.com/p/43568</a></p>
<p><a href="https://www.jianshu.com/p/72b228223804">https://www.jianshu.com/p/72b228223804</a></p>
<p><a href="http://sixerwang.github.io/2018/12/24/cpp-call-tf/">http://sixerwang.github.io/2018/12/24/cpp-call-tf/</a></p>
<p><a href="https://github.com/hemajun815/tutorial/blob/master/tensorflow/compilling-tensorflow-source-code-into-C%2B%2B-library-file.md">https://github.com/hemajun815/tutorial/blob/master/tensorflow/compilling-tensorflow-source-code-into-C%2B%2B-library-file.md</a></p>
<p><a href="https://cmake.org/cmake/help/latest/guide/tutorial/index.html">https://cmake.org/cmake/help/latest/guide/tutorial/index.html</a></p>
]]></content>
      <categories>
        <category>tensorflow源码</category>
      </categories>
      <tags>
        <tag>tensorflow源码</tag>
      </tags>
  </entry>
  <entry>
    <title>编辑距离</title>
    <url>/2021/03/11/%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/</url>
    <content><![CDATA[<br>
<br>


<h2 id="编辑距离"><a href="#编辑距离" class="headerlink" title="编辑距离"></a>编辑距离</h2><span id="more"></span>

<br>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">static int editDist(String str1 , String str2 , int m ,int n)&#123;</span><br><span class="line"></span><br><span class="line">    if (str1.charAt(m-1) &#x3D;&#x3D; str2.charAt(n-1))</span><br><span class="line">        return editDist(str1, str2, m-1, n-1);</span><br><span class="line"></span><br><span class="line">    return 1 + min ( editDist(str1,  str2, m, n-1),    &#x2F;&#x2F; Insert</span><br><span class="line">                     editDist(str1,  str2, m-1, n),   &#x2F;&#x2F; Remove</span><br><span class="line">                     editDist(str1,  str2, m-1, n-1) &#x2F;&#x2F; Replace                     </span><br><span class="line">                   ); &#x2F;&#x2F;三个子问题</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;s: beforeediting,length&#x3D;n  t: afterediting,length&#x3D;m</span><br><span class="line">&#x2F;&#x2F;d: recording the distance</span><br><span class="line">        d &#x3D; new int[n + 1][m + 1];  </span><br><span class="line">              </span><br><span class="line">        for (i &#x3D; 0; i &lt;&#x3D; n; i++) &#123;  </span><br><span class="line">            d[i][0] &#x3D; i;  </span><br><span class="line">        &#125;  </span><br><span class="line">        for (j &#x3D; 0; j &lt;&#x3D; m; j++) &#123;  </span><br><span class="line">            d[0][j] &#x3D; j;  </span><br><span class="line">        &#125;  </span><br><span class="line">          </span><br><span class="line">         </span><br><span class="line">        for (i &#x3D; 1; i &lt;&#x3D; n; i++) &#123;  </span><br><span class="line">            s_i &#x3D; s.charAt(i - 1);  </span><br><span class="line">              </span><br><span class="line">            for (j &#x3D; 1; j &lt;&#x3D; m; j++) &#123;  </span><br><span class="line">                t_j &#x3D; t.charAt(j - 1);  </span><br><span class="line">                 </span><br><span class="line">                cost &#x3D; (s_i &#x3D;&#x3D; t_j) ? 0 : 1;  </span><br><span class="line">                 </span><br><span class="line">                d[i][j] &#x3D; min(d[i - 1][j] + 1, d[i][j - 1] + 1,  </span><br><span class="line">                        d[i - 1][j - 1] + cost);  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if x &#x3D;&#x3D; y, then d[i][j] &#x3D;&#x3D; d[i-1][j-1]</span><br><span class="line">if x !&#x3D; y, 插入y则 d[i][j] &#x3D; d[i][j-1] + 1</span><br><span class="line">if x !&#x3D; y, 删除x则 d[i][j] &#x3D; d[i-1][j] + 1</span><br><span class="line">if x !&#x3D; y, x改变为y则 d[i][j] &#x3D; d[i-1][j-1] + 1</span><br><span class="line">When x!&#x3D;y, d[i][j] 取三中编辑方式最小代价。</span><br><span class="line">初始化条件 ： d[i][0] &#x3D; i, d[0][j] &#x3D; j</span><br></pre></td></tr></table></figure>
<ul>
<li><p>为什么不同操作就是对应与d的左移上移或左上移？<br>这个问题递归角度较易理解。<br>DP角度，d记录的是目前最小编辑距离。左、上、左上为子问题，即儿子。若为插入，则j需+1得到t的下一个字符，而x继续与此字符比较，i不变。</p>
</li>
<li><p>由递归到DP，实质就是找到递归中子问题。<br>找到后，将子问题结果记录在数组即可。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>缠</title>
    <url>/2021/08/31/%E7%BC%A0/</url>
    <content><![CDATA[<br>

<br>

<h4 id="3月"><a href="#3月" class="headerlink" title="3月"></a>3月</h4><br>

<span id="more"></span>

<p>3/17 前面印多了，后面社融会降；房地产复合增速7.6%，太火要灭(投资消费挤压、房住不炒、深房理)否则破裂则惨</p>
<p>3/18中央明确–&gt;碳达峰、碳中和涨停，大盘风险大</p>
<p>19十年期美债收益率达到2％，中国股市的“公允价值”将下降10%，勿侥幸；大盘是日线一笔向下中</p>
<p>22易会满–&gt;不应过于关注美债收益率等外部因素，全力挖掘内部潜力，等待完善机制后再放开IPO。市场暂时转向炒作次新中小盘题材股</p>
<p>23美债收益率就是公认的全球资产定价之锚；美国金融调控能力、科技实力首屈一指；中国实际货币发行量或许比美国更多，比如2009年的4万亿刺激计划，相当于10次QE都不止。大A股现在只有碳中和一个明确的主线，但处于概念阶段</p>
<p>24周一“一夜情”式反弹后，市场心态如此弱不禁风，中国外交部长王毅从3月24日起对包括沙特、伊朗在内的中东地区主要产油国进行正式访问。相信美国那边应该看得明白中国的潜台词，毕竟原油是美元霸权体系之锚，美国掂量着办！–&gt;这几天中国与欧美政治关系全面恶化。–&gt;外部扰动带来的跌，但周四、周五反弹的概率相当之大</p>
<p>25至少有两个深层原因：一是担心全球宽松货币潮会很快结束；二是近日来欧美抱团攻击中国，国内市场情绪有些担忧。美国国债目前高达30万亿美元 – 不加息美元信用面临危机，加息的话且不说对美股有多大压力，单就国债利息一项就够美国喝一壳的！所以，对美国来说最希望找到转嫁对象（即中国）。– 军工、科技这类题材。外部大环境恶劣情况下，支持这类板块的基本面逻辑相对充分 – <strong>担忧全球宽松潮提前结束是不必要的，除非2021年、2022年中国经济非常差，美国会想到通过提前加息给中国经济致命一击！</strong></p>
<p>26清明旅游</p>
<p>29周一，大盘走势相当纠结。地缘局势：周末中国与伊朗–未来将以人民币结算数以千亿美元计的原油交易；李克强总理最近特别指出“大宗商品价格上涨快”。同时，货币政策有偏紧迹象，比如“不急转弯”的措施变成了“灵活精准”。7月份若能启动像样的全面上涨，就算比较乐观了。因此，短线只能是关注结构性板块行情。比如，进一步挖掘<strong>碳中和题材，时刻关注军工、科技股</strong>的反弹机会，外部环境对后两个板块构成明显利好。</p>
<p>30  低开翻红，个股普跌。午后，两市维持窄幅震荡态势。截至收盘，沪指涨0.62%，深证成指涨0.85%，创业板指涨1.37%。盘面上，新疆板块、稀土永磁、光伏等板块涨幅居前；碳中和概念、数字货币等跌幅居前。黄白线走势背离，个股表现两极分化，权重股集体回暖，中小市值股大幅杀跌，两市近3000个股下跌。<strong>市场阶段性的调整基本告一段落</strong>。上证指数是在30分钟第二个中枢震荡中。</p>
<p>31继续关注新疆板块，因为中国与伊朗签订长期合作协议，表明中国坚定推进一带一路进行战略突围的意志相当明确</p>
<br>

<hr>
<p>关键词：碳中和、资源、科技、新疆、军工。总体乐观谨慎。主要关注，央行两会、(外交)中美关系、美元美债。</p>
<hr>
<p>问题：</p>
<p>1、高标、大而美、资源股、顺周期股、分支碳交易？</p>
<p>2、为什么十年美债利率高企，为什么没人要国债，除了通胀和美元，以及政府赤字。为什么卖国债，逆回购什么原因</p>
<p>3、有色背后的逻辑是什么，战略地位是</p>
<p>4、expma</p>
<p>5、基金挤兑潮，原因。发行日期规则</p>
<p>6、ipo的问题，易的关注点</p>
<p>7、原油的战略地位</p>
<hr>
<br>

<br>

<h4 id="4月"><a href="#4月" class="headerlink" title="4月"></a>4月</h4><p>1拜登政府的2万亿美元基建计划；压着不让涨是为了今后慢慢有计划地上涨。国家要的是未来十年、二十年的慢牛。预计上证大盘至少还会振荡调整两个月以上，这期间一些优秀股票可能出现明显调整，很可能就包括科技股、新能源、碳中和这些板块。这将是难得的参与机会</p>
<p>2新能源车板块，美国刺激计划中有对电动车的资助计划，这对A股新能源车板块肯定是利好</p>
<p>3一大堆利好支撑的芯片板块却没有动静，这有些反常。美国在想尽一切办法对中国进行半导体技术上的封锁。比如日本已宣布与美国共同打造半导体生态链。美国国内已经有人提议停止与中国的贸易战，而透过科技封锁对中国进行精准打击。拜登上台后很快决定对美国国内半导体产业链进行管控，也是为了进一步防止技术外流中国。这还不够，美国还在说服韩国加入对华半导体封锁行动。总之，形势对中国相当严峻，如果美国科技封锁成功，中国经济升级之路将被打断，严重后果是战略性的。</p>
<p>4周三大盘柔弱依旧，但论到赚钱效应还是比较爽的。本来早间大盘指数一度下滑明显，后在航运、钢铁、碳中和等概念板块的依次推动下，总算相安无恙。综合这段时间看，题材炒作是越来越热闹了，但管理层非常罕见地保持了克制，最大妖股也是在连拉21个板之后才被停牌。</p>
<p>航运板块大爆发，是由中远海控一季度业绩同比暴增50倍引发的，大家可以选择基本面相对优秀的航运个股进行中线布局。</p>
<p><strong>毕竟碳中和的概念太具有战略高度了，对内而言甚至关乎到中华民族崛起大业，对外有利改善中国与欧盟的关系。因为欧洲最重视环境保护问题，那样至少让欧洲不至于完全彻底倒向美国。所以说，碳中和这个板块，建议大家选择优秀标的进行中线甚至长线布局，去赚踏实的钱。</strong></p>
<p>5前两年那些机构抱团股过于透支政策和业绩空间了，估值到现在也还是太强，酒类都高达好几十倍，新能源都上百倍，这哪是几个月甚至今年之内能挤完泡沫的。管理层现在既不希望指数大涨，也不希望股市整个崩盘，所以题材个股仍将延续相对火爆局面，只要市场主力别太过分，就不会有大的变化</p>
<p>9如果在管理层将顺控发展关小黑屋之后，题材炒作还不收手，就会逼管理层下重手，那样各方脸上都挂不住。</p>
<p>金融委会议曾表示保持政策连续性，相当于重申政策不会急转弯。但市场资金主力并没有“上当”，周五保持了几分平静，指数跌得明显，但个股情况比昨天好多了。这种情况下，预计管理层更会考虑对市场稳定的维系，毕竟后面科技企业还需要大量上市融资啊！</p>
<p>从盘面上看，今天海南板块、旅游板块表现比较强劲，但这两个板块短线技术形态见顶比较明显，建议还是审慎参与。今天钢铁板块有冲高回落，但后市仍有上涨的逻辑支撑，这个行业还重组预期，一旦得到落实，钢铁行业仍有继续上涨空间，将反弹变成反转也正常。</p>
<p>还有，本周芯片半导体一直很冷清，这很不正常。一是因为行业一直存在芯片荒，二是美国一直对中国进行科技封锁。总之，基本面利好因素一直存在，就看市场主力选择什么时机启动了。</p>
<p>12国内通胀有抬头迹象，加上金融委会议和企业家座谈会都谈到了大宗商品涨价问题，预计管理层会适当干预。说实话，原材料价格上涨明显是老美给埋的雷。试想，那边天量放水，美元又是国际货币，中国是无法躲开输入型通胀的。</p>
<p>13惯性的力量真的很强，平时每年4月中下旬行情多数是不理想的，今年更是如此。前期基本形成主线的“顺周期+碳中和”周二出现崩塌，电力、煤炭等跌幅居前。顺周期板块因为高层关注原材料问题而回调</p>
<p>近来较为活跃的海南板块周二也是吃面了！盘面上涨幅居前的是养殖业、农业、数字货币等板块，要么是防守品种，要么是突发行情，比如养殖板块。</p>
<p>   更让大家难以接受的是，一些业绩同比预增几十倍的绩优股竟然大幅下跌，而且这种业绩见光死不是个别现象。现在是无论业绩好与坏，都难逃一个跌字！究其原因，还是宏观大环境太过严峻了。记得上周管理层召开的两个都提到了关注原材料涨价问题，这种外来输入型通胀对宏观经济是利空。再加上国家重罚阿里巴巴，对网络科技网形成了持续利空。关于中美关系，博弈是越来越严重了，除了没有发生战争，一切领域都展开较量了。</p>
<p>   <strong>一句话，顺周期行情是中长期的大戏，只是短线分化后需要我们花大力气去寻找新的龙头标的。</strong></p>
<p>14钢铁大年，应该说有一定道理。</p>
<p>15行情可持续性太差了。目前国内国外都出现收紧宽松货币政策的苗头。更为微妙的是，国内国外现在还都是“只做不说”，但市场又切实能够感觉到这种收紧。</p>
<p>有色上涨快一年了，短线调整还没有结束，谨慎参与。</p>
<p>建议大家重点关注医美概念板块，从整个板块指数的技术形态看，今年以来一直处于上涨通道，有可能成为全年走牛板块，建议大家挑选龙头股票给予持续关注。</p>
<p>16</p>
]]></content>
  </entry>
  <entry>
    <title>缠0811-0914分析</title>
    <url>/2021/09/14/%E7%BC%A00811-0914%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>我想理清楚他的思路<span id="more"></span>。尤其从8月11到现在的节点，为什么他选择这些股票。</p>
<p>他对宏观的把握以及对于行业到个股的选择。再到对k线的分析。</p>
<br>



<h4 id="8月"><a href="#8月" class="headerlink" title="8月"></a>8月</h4><br>

<br>

<p>8/2：两市除钢铁、煤炭、有色以外，基本呈现普涨行情。景气度高的<strong>稀缺资源类个股</strong>和<strong>困境反转类</strong>个股。</p>
<p>8/3： 今日沪深三大指数全线低开，随后指数宽幅震荡，创指一度跌逾1%，前期热点板块大幅回调，医药板块今天表现不错。盘面上：生物制品、新冠检测、医疗器械服务等板块涨幅居前；盐湖提锂、光刻胶、稀土等板块跌幅居前。</p>
<br>

<br>

<h4 id="9月"><a href="#9月" class="headerlink" title="9月"></a>9月</h4><p>9/14：鸿蒙概念个股集体高涨，石油、CRO概念板块拉升走强；有色、钢铁、煤炭板块集体回调。                                      </p>
<br>

<br>]]></content>
  </entry>
  <entry>
    <title>缠解读后感</title>
    <url>/2021/06/01/%E7%BC%A0%E8%A7%A3%E8%AF%BB%E5%90%8E%E6%84%9F/</url>
    <content><![CDATA[<br>

<br>

<h4 id="练脑子"><a href="#练脑子" class="headerlink" title="练脑子"></a>练脑子</h4><span id="more"></span>

<p>还是得读个自圆其说。如能完满不矛盾地解释，并且释义能够高于一般理解。已然是高一层的解读了。没有思考，便自相矛盾。过段时间再看，又会更透彻些。其中缺陷也会自然显露。于是乎，记下当即的想法。</p>
<hr>
<br>

<p>整体理解是，”当下“、”现实“为最鲜活有生命之物，所有”君子“皆于当下成就。那么，当下又何以把握？即”行“、”一以贯之“。”一“是现实，”之“是”圣人之道“。再白话，就是在当下、现实之中，能够时刻地闻见学行君子之道，能够躬身力行。重点落在行上。如”性相与习相“。孔子要求是改变世界。</p>
<p>比如，按照对颜回的赞赏，能够”食不求饱“，”居不求安“，能”安贫乐道“，陋巷不陋。此为贤也。但是颜回所为，改变世界了么？当然，因其能不受相所困，已然是种作为。相还有就是受自己好恶和知识所困，而不察。而回，”不迁怒，不贰过“，回的理解和言行，都没有偏差，没有那些个小人儒，假儒，也没有乌托邦。</p>
<p>这和”道德“到底有什么相干。孔子不尊崇道德，不尊崇忠孝。尊崇”当下“、”行“、”乐“和”说“、”不相“，”好学“。尊崇富而好礼，贫而得乐。不是勤奋，不是说要遵守纪律，要守“礼”。而是说，要“不相” “当下” 之相。不受当下所困，而能超拔，有自己所乐。总体是自由的，而不是伦理条条框框。小人不过就是选择了小的视野，汲汲于利，忘乎自己是“人”，而天地人之心的人，岂能为利字所困？而不得得乐呢。岂不是“小”了太多。</p>
<p>进一步，也不能将现实作为本位、先验。因为现实是可改变的。改变现实，与天其时的君子，才是闻见学行对照、调整的对象。”见机而作、明时通变。“</p>
<p>因为当下去实践，才能够保持道的鲜活和生命力。</p>
<hr>
<br>

<p>文中，勾连着马克思、海德格尔、康德，易经。大体结论是，孔子和马克思遥相呼应，不同于康德（列宁是康德的延伸，因其都是必然如何走向自由，而与孔子无关，孔子并不假设从必然到自由有一条路，孔子认为人本是自由，他更强调自由意志在现实中的实践，孔子认为实践才是皈依 ），高明于海德格尔(因为海纠结于无为什么不存在，而孔子直接认为，有因无才成为有，”不患，无位”。无位次不意味”不存在“。也即存在与否不重要，重要的是无位次与位次)，符合易经(如艮卦)。</p>
<p>无位次与位次：就像无味、甜、咸。有了无味，才能区别出甜。有了震荡，才能区别出来趋势。</p>
<p>人“原罪”，要上帝or理性来“救赎”。但是，没有上帝，也不是理智的光辉给你救赎，也不是科学唯上。关键在于”人“，若握住理性，则为我所用，展现我的言行，若”无知”，也未尝不可，但也要”无愠“才得道。也不是”人本“，主义若是本源、本位，则又丧失了同样的 – 人。人本不过是种秉持的主义。有了人本主义为本位，人就不在本位。</p>
<p>而人，即君子。为天地之心。天地为何要有心？</p>
<p>你只有像“人”一样地承担，承担你的当下。并且固守如此，而不是朝三暮四。把承担，看成成就之事，若能“乐”于此承担，岂不妙过其他？</p>
<p>人无非两种，”不知“与”闻见学行“。”不知“者，要么小人，要么闻见不彻底、而得过且过，随波逐流，亦为一种不知。更恰当 的是，每个人都有这两种成分。</p>
<p>人容易相，富有富者相，贫有贫者相。善人为邦，要”胜残去杀“。不是杀灭，而是慢慢地不相、不愠、消除。</p>
<p>”庶之、富之、教之“，多样性和差异性，到全面发展、层次性，再到不相、不愠之知。”必世而后仁“也是这个意思，先后有序，先”富庶“再谈”仁“。”仁“”教“即不相，即有教无类。为什么先”世“？</p>
<p>还有一些告诫，如”放于利则多怨“、”使民战，既往不咎“等，要”即戎“。</p>
<p>”患与不患“，”人不知与不己知“，”不在其位不谋其政“等，都是在说无位次与位次的问题，无和有的问题。无是没有位次的，算是先验吧，有因为无的无位次而有位次。就像，有些必然的、其他因其必然而有关联，而产生程度和范围，而有了自己一定的概率，虽然不至于必然的1，但有了概率，有了位次。永远无法改造人，无法达到己知，那只能从不知不愠出发，而达到君子世界。因不在其位，无法达到，有现实的局限和鲜活性，即”知新“，所以应当不谋其政。也是顺应天时。与天其时。</p>
<br>



<hr>
<p>道家要无为，因其立论在有天道，有一，而人因返璞归真，回到自然状态，撇除人为达到无为、无人为。但孔子是有为的，”认识从实践中来，再指导实践，实践是认识的根本来源，从来达到螺旋上升“。和马克思的理论接近。</p>
<br>



<hr>
<p>关于学，”多闻“而”识“。左媒和党媒的天下，不是多闻，难以多闻，而不多闻，如何择善而从呢，如何能识呢。不能识，如何能知。当下入世磨炼，才能多识而有知啊。”犹恐失之“，陷于一些无用的情绪中，就会失去承担。差异多样的学、多闻，要和同一的思相结合，多样性要有、理论性高度要有。</p>
<p>不效仿人，而效仿道，见解，识辨。</p>
<br>



<hr>
<p>关于小人。不能依附也不能违背，”小人难养“。要受其历练，才能成就。”学而优则仕“也是”小人儒”。君子是”周而不比“、”喻于义“的，小人就….不要以”相“，看君子小人。</p>
<br>



<hr>
<p>很多是一意多词：”位次 先验“、”不相“、”同、和“；“承担”、“固守 不舍昼夜”；”闻 识 思 学”…</p>
<p>之间的关系：</p>
<p>以”不知不愠“为目的。要在每一个”当下“”固守“”不相、与天其时天与其时、无有位次、和而不同“，在当下”习相“以保持道的鲜活。</p>
<p>步骤：</p>
<p>按照”学、立、不惑、知天命、耳顺、从心所欲不逾矩“，先多闻多见，学而思，并在现实中多识，明时通变，积累一段时间则能知道事物的位次，客观关系、万事万相等，即看山是山，再然后，由位次上升到无位次，无位次即无相，即看山不是山，在后来，能明白天机天时，能明时，六十，闻见的道理，明白的天时，能落实实践。七十，君子之道的规矩方圆，大体已经融入习惯，游刃有余，能够顺人心同时不违背道。</p>
<hr>
<h4 id="辟谣"><a href="#辟谣" class="headerlink" title="辟谣"></a>辟谣</h4><br>

<p>我一直认为自己的理性和客观规律是最高权威了。然而规律是要”利用“的。人要练。脑子要练。心要练。观念要刷洗。</p>
<p>一直认为思之”邪“要去除，要“破”，然而”不知“如果是以”攻乎异端“的方式来解决，则”斯害也已“，破除的方式本身，就是不符合君子之道的。那如何煮米成饭呢，就要坚持闻见学行。即使无法做到，也当”不愠“。让它放在那里，不愠，不能扰乱主成分。</p>
<p>认为”民主“至少比”不民主“或者”社会主义民主“要好，”市场“要比”政府“要好。然而这样的比较，就是在判断客观性上哪一个主义更优越。然而这样比较意义何在呢？不和当下结合起来衡量，不和现实与人，结合起来，又是把本位给设定前提了而已。</p>
<p>一直以为要掌握规律，通过此来分析问题解决问题。但是，没有任何先验逻辑存在，具体问题具体分析，是从现实问题出发，而不是有一个通用原则。</p>
<p>慎言慎行之类的位次是后的，多闻见，有识有知的慎，才不是人云亦云的。</p>
<p>乱说话多人太多。</p>
<br>



<hr>
<h4 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h4><p>整体也算融汇起来。也是有中心，然后顺着去注解。</p>
<p>大体是怎样的世界，怎样去做的第一原则。然后顺着这个第一原则，展开注解。</p>
<p>一意多词太常见了。</p>
<p>主要观点还是， 位次与不相、明时、现实实践而一贯。目的为不知不愠，皈依为现实实践，标准为不相、明时，要求为固守。</p>
]]></content>
      <categories>
        <category>读后感</category>
      </categories>
      <tags>
        <tag>读后感</tag>
      </tags>
  </entry>
  <entry>
    <title>股市基础</title>
    <url>/2021/06/20/%E8%82%A1%E5%B8%82%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<br>
<br>

<p><strong>委比&amp;委差</strong></p>
<span id="more"></span>

<p>就是委托买入卖出的一个差值，但是很多软件只是显示了5档，所以只是这5挡的一个差异。</p>
<p><strong>量比</strong></p>
<p>当天成交量和最近5日平均成交量的比值</p>
<p><strong>换手率</strong></p>
<p>成交数量和流通股的一个比率，换手率高就代表成交活跃，换手率低代表成交不活跃。</p>
<p><strong>外盘&amp;内盘</strong></p>
<p>外盘是以卖出下单成交的交易，内盘是以买入下单成交的交易</p>
<p>外盘+内盘=成交量</p>
<br>



<p>左侧交易</p>
<p>最大回撤</p>
<p>挂单</p>
<p>老鼠仓</p>
<p>封板</p>
<p>扫货</p>
<p>敲单员</p>
<p>集合竞价</p>
<p>摆单、吃单、补单</p>
<p>2013光大证券 乌龙指</p>
<p>18线家电太次了</p>
<p>某个价位突然地补单</p>
<p>落袋为安和降低持仓成本</p>
<p>资金动向和主力手法</p>
<p>杀某行的多头头寸把原油价格打到负数</p>
<p>盘口 市场深度</p>
<p>资金流入：</p>
<p>主动性买盘 是资金流入；现价买入，计入资金流入</p>
<p>买一到买四的都被击穿。股价下跌</p>
<p>并购换股为什么不稀释股权</p>
<p>预增</p>
<p>中概股</p>
<p>KDJ、MACD、WMS、布林线、RSV、RSI、PSY、OBV、OBOS、乖离率、资金流入流出、ADL……</p>
<hr>
<h3 id="游资逻辑"><a href="#游资逻辑" class="headerlink" title="游资逻辑"></a>游资逻辑</h3><p>1、</p>
<p>开盘价不高。因为开盘价不高，有些人会纪律止盈，然后早上分时就有个下探。下探后是正常的，能不能被拉起来，就得继续观察了。这还得看资金活跃度，还有分时是否稳定。</p>
<p>盘子相对比较大的票，而且关注度很高，成交量很大，分时也相对稳定，这种情况可信度就会高一些。所以就可以追高。但<strong>这个手法的特点是，位置越低越安全</strong>。做一进二的时候成功率会高点。但是下午就被砸了。这个原因就是市场有记忆效应，可能认为小康会重复昨天的走法。然后就会有些赌徒因为错过了昨天的小康，开始手痒，于是就介入了。</p>
<br>

<p>2、</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>能源</title>
    <url>/2021/06/08/%E8%83%BD%E6%BA%90/</url>
    <content><![CDATA[<br>

<p>1、提升非化石能源在一次性能源消费比重 、电能在终端能源消费比重</p>
<span id="more"></span>

<p>2、</p>
<table>
<thead>
<tr>
<th>目标</th>
</tr>
</thead>
<tbody><tr>
<td>60亿吨标准煤</td>
</tr>
<tr>
<td>化石能源20%</td>
</tr>
<tr>
<td>非化石发电50%</td>
</tr>
<tr>
<td>全社会用电7万亿千瓦时</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>自解投资逻辑</title>
    <url>/2021/09/23/%E8%87%AA%E8%A7%A3%E6%8A%95%E8%B5%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<br>

<br>

<h4 id="吻"><a href="#吻" class="headerlink" title="吻"></a>吻<span id="more"></span></h4><p>1、均线之间的关系。</p>
<p>突破短期无法突破长期，则为飞吻。突破长期而反复，则为湿吻。突破长期形成陷阱(空头或者多头趋势)为唇吻。</p>
<p>2、在不同的体位时，分析均线关系。</p>
<p>男上，原趋势为向下，此时湿吻，意味着有一定强度去转折。如果只是飞吻或者唇吻，很可能是中继。</p>
<p>3、转折</p>
<p>要么湿吻缠绕后对原趋势形成陷阱然后回头。要么盘整，时间换空间。</p>
<p>湿吻后陷阱，明显的特征就是背驰。</p>
<br>

<br>

<h4 id="背驰"><a href="#背驰" class="headerlink" title="背驰"></a>背驰</h4><p>1、只有趋势才有背驰。盘整中次级别可能有背驰。本级别盘整不会出现背驰。</p>
<p>盘整如果末端出现离开盘整区间的趋势，则为陷阱 – 不算本级别的背驰。可以是转折。</p>
<p>2、量化：</p>
<p>1面积变小</p>
<p>2面积除以时间变小</p>
<p>3macd柱子面子变小但是有新高或者新低</p>
<br>

<br>



<h4 id="买卖点操作"><a href="#买卖点操作" class="headerlink" title="买卖点操作"></a>买卖点操作</h4><p>程序：</p>
<p>一、首先只选择出现“下跌+盘整+下跌”走势的。二、在该走势的<strong>第二段下跌出现第一类买点时</strong>介入。三、介入后，<strong>一旦出现盘整走势，坚决退出</strong>。注意，这个退出肯定不会亏钱的，因为可以利用<strong>低一级别的第一类卖点</strong>(第一个卖点是女上最后一缠的背驰，第二个卖点是男下第一缠的)退出，是肯定要赢利的</p>
<br>

<br>

<h4 id="中枢"><a href="#中枢" class="headerlink" title="中枢"></a>中枢</h4><p>1、本级别的趋势的延续、盘整的延续，连接的必然都是次级别或以下级别的走势类型。只有从次级别 其走势确定，才能看出是否是本级别的离开还是返回。</p>
<p>2、本级别的中枢破坏，当仅当次级别走势离开此中枢，任何回抽不回到该中枢。</p>
<p>3、</p>
<br>

<br>

<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>1、因为<strong>不想坐庄的大资金的安全建仓在六种走势中只可能在下跌+盘整+上涨这一种</strong>，其他都不适用。至于<strong>坐庄的建仓方法，和这些都不同。</strong></p>
<p>2、</p>
]]></content>
  </entry>
  <entry>
    <title>英文</title>
    <url>/2021/07/18/%E8%8B%B1%E6%96%87/</url>
    <content><![CDATA[<br>

<br>



<h5 id="0718"><a href="#0718" class="headerlink" title="0718"></a>0718</h5><span id="more"></span>

<p>I slept for the whole day yesterday,felt exhausted. I got no interest in doing anything inculding sleeping. Time flys ,but i still don’t have any resolution to start anything.When i find others so exsited and vigorous,i just cannot figure out why.I have to do list which already accumulated a lot, still cannot impel myself to complete any of those things.</p>
<p>I spent almost three hours searching the internet for this pdf called 《Fiasco:the inside of a Wall Street trader》which is recommeded by Charlie Munger .  I googled for the pdf version and alsosearch it on the apps like  PDD ,Amazon, DangDang and TaoBao. I asked so many client services  of the stores in  PDD, and they all only had the Chinese ebook which i barely had interest in. This was  a complete waste of time that they both assured me that they had the English version but after i placed the order they told that they found what they got is Chinese. WTF. </p>
<p>My writing only reached primary level…</p>
<p>I conclude that when i need to search for some resources  the first and foremost i should list who would have the answer or where have i met with the problem before. In line with this idea, i would get the collection of websites used for downloading free pdf ,also the websites once had been favorited by myself should come out since they might be valuable tools i once had found  to navigate to resources.In one word, when  confronted with problem related to locating resources ,instead of searching right away, extract information from your memory or brain’s database to  pick up the train of thought and then carry out the ideas to test and verify.</p>
<br>





<h5 id="0719"><a href="#0719" class="headerlink" title="0719"></a>0719</h5><p>My mood cannot be predicted by any signal.I changed as if i forgot what happened before.I just talked  with Gao about the future with expectation,but yesterday i still had a gloomy  perspective.</p>
<p>I talked a lot and then i ate a substantial lunch and had a break.I felt different from last week  for the less i thought about how  unpleasant the trivialities were and what a meaningless life i was living for the more relaxed i became.So i thought to myself that maybe i was too stressed and it was no good at all .And it occured to me that the only pressure I’m under is the pressure I’ve put on myself.</p>
<p>I was more focus on the solution of problems rather than how hard to figure it out.The more difficulty i put in the progress of finding the answer the less effort i would to try to stimulate my enthusiasm to be more aggresive and adventurous.</p>
<p>I was awake until the 5am playing around staff i even didn’t care. I genuinely had no will to move forward and everything marched very much depended on my mood. Moods are never predicable  even they belongs to yourself.</p>
<p>I  made plans and made a list including what i expected myself to accomplish like daily English news reading and writing ,also daily preparations for the entrance exam. </p>
<br>



<h5 id="0720"><a href="#0720" class="headerlink" title="0720"></a>0720</h5>]]></content>
      <categories>
        <category>英文写作</category>
      </categories>
      <tags>
        <tag>英文写作</tag>
      </tags>
  </entry>
  <entry>
    <title>药</title>
    <url>/2021/07/28/%E8%8D%AF/</url>
    <content><![CDATA[<br>





<h4 id="药"><a href="#药" class="headerlink" title="药"></a>药</h4><ol>
<li>曲安奈德益康挫乳膏<span id="more"></span>，哈西奈德溶液</li>
<li>莫匹罗星软膏</li>
<li>三金片、盐酸左氧氟沙星胶囊</li>
<li>胸腺肽肠溶胶囊康司艾、保妇康栓</li>
<li>氯雷他定片</li>
<li>氨溴特罗口服溶液、小儿肺热咳喘颗粒、</li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>行为分析</title>
    <url>/2021/06/01/%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<br>

<br>

<h4 id="言行之蠢"><a href="#言行之蠢" class="headerlink" title="言行之蠢"></a>言行之蠢</h4><span id="more"></span>

<p>1、”只此一次，以后不再。不用担心***“  –&gt; 自欺欺人，借口</p>
<p>2、”我先去**，就很好了。“ – 环境不行。”再去那里吧，万一被占了呢，嗯不会的。“ –被占了。”太烧钱，不去了，不想回去“ –来回走。 –&gt;“钱” 和 犹豫</p>
<p>3、“我再看会。看这个有什么用呢，做这个没用啊。”  –&gt;消极</p>
<p>4、“唉，不过是总结，又会忘，有什么意义呢。”  –&gt;消极</p>
<p>5、“算了，退掉吧。我担心*** ”  –&gt;犹豫</p>
<p>6、“我就是想舒服下。看看书，想想事，享受美… 能大量消化知识”  –&gt;自欺欺人 ，不计后果，不考虑代价</p>
<p>7、”我可以控制，这样才合理…“ </p>
<p>8、”没意义，一直在浪费时间做没意义的事情“</p>
<p>9、”又耽误了… “</p>
<p>10、“嗯，是的… 嗯嗯”</p>
<p>11、小表情，反驳</p>
<p>12、“明天少点…现在不想控制” –&gt;接口，重复，没有真的要做事，就像”施“</p>
<p>13、“这个是你的代码啊，我不知道recall逻辑，我怎么测。” “那你那个…给我吧，行吧，这个可以吧” “好” –&gt; 安静。 “第一，我是会造数据。现在的问题是，比如，我把出的商品 mock到活动队列，逻辑是什么。“</p>
<p>14、</p>
<br>

<p>会重复一遍又一遍。太阳升起又是一天，就又来一遍。</p>
<hr>
<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>1、找到“疏漏”，精准，而不是东榔头西棒槌地尝试。精准需要背后有思量。</p>
<p>2、找到症药，“ trick”，解决问题。</p>
<p>3、“缺乏信心，才导致去寻求其他欲望满足，而不选择固守。” </p>
<p>4、更多情况，只是想“拥有”外物，而并非真的“需求”，而当你真的想在解决问题时，其他是没有什么多余欲望的。</p>
<p>5、只有不断地突破，反抗自己已认知的。才能慢慢再次形成新的认知。</p>
<br>



<hr>
<h4 id="大嫂"><a href="#大嫂" class="headerlink" title="大嫂"></a>大嫂</h4><br>

<br>

<p>和大嫂聊天的时候，没有勇气反驳，没能马上反应出来问题。</p>
<p>一个是谈的具体东西体味不深刻，二是，自己不够放松。总是限制在她的话语里，而不能足够自信去拆解。</p>
<p>这个可以解决。反驳是不好的，克制反驳是对的，但是不同观点的理性表达是应该的。不要觉得自己的提问愚蠢，也不要认为沉默是糟糕的。即使问题比较愚蠢，但是不同的表达方式会产生不同的效果。</p>
<p>so，表达和提问都是必要的。大可随意些，但是用词需要考虑。考虑到用词的效果后表达，并且尽量让大脑放松，即可。问题的关键是什么，他们陷入什么思维里，边角的问题是什么。</p>
<p>1随意自信些 2表达不同意见，注意用词效果 3提问和思考  4顺着直接感受的表露就是小表情，等待下、放下</p>
<p>其实都是各种经验的交流，以及想法的表达。各自对某一方面的理解。对“打比方”的攻克是什么。我是不是只是想在语言上战胜？</p>
<p>“打比方”的方式，为了抽象，不谈具体而谈模式和思路。概括全貌和思路。</p>
<br>

<p>讨论的话题：大哥这个人、他们的历史-唐伟和他老婆、管理人的问题-小五-(人和思路、模式、技巧)、创业的策划思路、她见过的“人”以及“生意”以及“模式”、物流的问题和她的见解、房子的贷款问题、婚姻(平衡、空间)等等。</p>
<p>我思考过生意？她见过的那些我是否有自己的交叉部分？她的思路是否可行和落地？我对这个是否有自己的想法？大哥对她的影响、她目前的局限和需要？</p>
<p>对这些我没有更进一步的思考。也没有意识到能够如何给我带来什么。没有利用起来。我觉得这个是需要思考和讨论的。并且是真的用心去思考。因为这个是我的现实和我的当下。</p>
<p>可做的事情非常多。只是是否选择去真的在当下里去取舍和判断。怎样的当下。如何能更好利用当下。</p>
<br>



<hr>
<br>

<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>1、行为考虑因素太多，也意味着受制于太多，显得没有生机。比如，去干啥干啥，又会犹豫，觉得耽误，没有意义。然后手里正在做的事情，不过更缺乏意义。并且还是减少能量消耗的一种方式。人应该勇敢果决地选择多去消耗能量。并且努力地去想办法补充能量，从而形成正向的循环。</p>
<p>2、lazy，用时加载。按点满足。欲望，总是延迟也是问题。需要安排适当。才是正道。</p>
<p>3、效率低。没有计划。软绵绵的。周日只看了四课。我不知道这个理论是否有用。以我的智商，完全无法判断。但是我知道，他们都看过。比如谷。那么他们都看过的，我必须看过。但是我效率低很多。是我自己还是没有全身心地放在这里。</p>
<p>4、中午又40min买吃的。大部分时间，不是买东西，就是在聊天。当然我觉得聊天是好的。买东西需要减少。我想减少。包括各方面的需求。从什么时候开始，我越来越多的需求呢。并且还想着要花费更少。如果我突破不了，静不下来，没有办法真的从其中去悟，那么我始终被自己打败。还是我的脑里在逃避问题。逃避去看。我每天给自己的计划，反馈力度不够。</p>
<p>– 受制于太多而选择节约热量而不是去补充和消耗，生命在于消耗、快速反应</p>
<p>–逃避计划中的事情，总是在意于效果和变化，而不把脑子放在问题本身上。缺少反馈和调节，比较僵硬。也没有调动好积极–</p>
<br>

]]></content>
      <categories>
        <category>读后感</category>
      </categories>
      <tags>
        <tag>读后感</tag>
      </tags>
  </entry>
  <entry>
    <title>说服的技巧</title>
    <url>/2021/05/25/%E8%AF%B4%E6%9C%8D%E7%9A%84%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>贝叶斯网络</title>
    <url>/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<br>

<h2 id="概率图"><a href="#概率图" class="headerlink" title="概率图"></a>概率图</h2><ul>
<li><p>概率图模型分为贝叶斯网络（Bayesian Network）和马尔可夫网络（Markov Network）两大类。</p>
<span id="more"></span></li>
<li><p>贝叶斯网络可以用一个有向图结构表示，马尔可夫网络可以表示成一个无向图的网络结构。若随机变量Y构成一个无向图 G=(V,E)表示的马尔科夫随机场（MRF），则条件概率分布P(Y|X)称为条件随机场（Conditional Random Field, 简称CRF。</p>
</li>
<li><p>更详细地说，概率图模型包括了朴素贝叶斯模型、最大熵模型、隐马尔可夫模型、条件随机场、主题模型等，在机器学习的诸多场景中都有着广泛的应用。</p>
</li>
</ul>
<p><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E6%A6%82%E7%8E%87%E5%9B%BE.png" alt="概率图"></p>
<h2 id="频率派与贝叶斯派"><a href="#频率派与贝叶斯派" class="headerlink" title="频率派与贝叶斯派"></a>频率派与贝叶斯派</h2><p>频率派与贝叶斯派各自不同的思考方式：</p>
<p>频率派把需要推断的参数θ看做是固定的未知常数，即概率虽然是未知的，但最起码是确定的一个值，同时，样本X 是随机的，所以频率派重点研究样本空间，大部分的概率计算都是针对样本X 的分布；</p>
<p>而贝叶斯派的观点则截然相反，他们认为参数是随机变量，而样本X 是固定的，由于样本是固定的，所以他们重点研究的是参数的分布。</p>
<p>贝叶斯派既然把概率看做是一个随机变量，所以要计算概率的分布，便得事先知道的无条件分布，即在有样本之前（或观察到X之前），有着怎样的分布呢？这种在实验之前定下的属于基本前提性质的分布称为先验分布，或着无条件分布。</p>
<p>其中，先验信息一般来源于经验跟历史资料。而后验分布π（θ|X）一般也认为是在给定样本X的情况下的θ条件分布，而使π（θ|X）达到最大的值，称为最大后验估计，类似于经典统计学中的极大似然估计。</p>
<h2 id="判别和生成"><a href="#判别和生成" class="headerlink" title="判别和生成"></a>判别和生成</h2><p>常见的概率图模型有朴素贝叶斯、最大熵模型、贝叶斯网络、隐马尔可夫模<br>型、条件随机场、pLSA、LDA等。</p>
<p>朴素贝叶斯、贝叶斯网络、pLSA、LDA等模型都是先对联合概率分布进行建模，然后再通过计算边缘分布得到对变量的预测，所以它们都属于生成式模型；而最大熵模型是直接对条件概率分布进行建模，因此属于判别式模型。隐马尔可夫模型和条件随机场模型是对序列数据进行建模的方法，将在后面的章节中详细介绍，其中隐马尔可夫模型属于生成式模型，条件随机场属于判别式模型。</p>
<p><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E5%88%A4%E5%88%AB%E4%B8%8E%E7%94%9F%E6%88%90.png" alt="判别与生成"></p>
<p>我的视频讲解(<a href="https://www.bilibili.com/video/BV16y4y187pE">https://www.bilibili.com/video/BV16y4y187pE</a>)</p>
<h2 id="解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？"><a href="#解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？" class="headerlink" title="解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？"></a>解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？</h2><p>先验概率：就是因变量（二分法）在数据集中的比例。这是在你没有任何进一步的信息的时候，是对分类能做出的最接近的猜测。<br>似然估计：似然估计是在其他一些变量的给定的情况下，一个观测值被分类为1的概率。例如，“FREE”这个词在以前的垃圾邮件使用的概率就是似然估计。<br>边际似然估计：边际似然估计就是，“FREE”这个词在任何消息中使用的概率。</p>
<h2 id="贝叶斯网络"><a href="#贝叶斯网络" class="headerlink" title="贝叶斯网络"></a>贝叶斯网络</h2><p>贝叶斯网络(Bayesian network)，又称信念网络(Belief Network)，或有向无环图模型(directed acyclic graphical model)</p>
<p>例子：<br><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C.png" alt="贝叶斯网络"></p>
<h3 id="结构形式"><a href="#结构形式" class="headerlink" title="结构形式"></a>结构形式</h3><p>1、 a-&gt;c b-&gt;c</p>
<p>P(a,b,c) = P(a)P(b)P(c|a,b)成立，即在c未知的条件下，a、b被阻断(blocked)，是独立的，称之为head-to-head条件独立。</p>
<p>2、c-&gt;a c-&gt;b</p>
<p>考虑c未知，跟c已知这两种情况：</p>
<p>在c未知的时候，有：P(a,b,c)=P(c)P(a|c)P(b|c)，此时，没法得出P(a,b) = P(a)P(b)，即c未知时，a、b不独立。</p>
<p>在c已知的时候，有：P(a,b|c)=P(a,b,c)/P(c)，然后将P(a,b,c)=P(c)P(a|c)P(b|c)带入式子中，得到：P(a,b|c)= P(a|c)*P(b|c)，即c已知时，a、b独立。</p>
<p>3、a-&gt;c-&gt;b</p>
<p>还是分c未知跟c已知这两种情况：</p>
<p>c未知时，有：P(a,b,c)=P(a)P(c|a)P(b|c)，但无法推出P(a,b) = P(a)P(b)，即c未知时，a、b不独立。</p>
<p>c已知时，有：P(a,b|c)=P(a,b,c)/P(c)，且根据P(a,c) = P(a)P(c|a) = P(c)P(a|c)，可化简得到：<br><img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/headtail.png" alt="headtail"><br>所以，在c给定的条件下，a，b被阻断(blocked)，是独立的，称之为head-to-tail条件独立。<br>这个head-to-tail其实就是一个链式网络。</p>
<h3 id="因子图"><a href="#因子图" class="headerlink" title="因子图"></a>因子图</h3><p>wikipedia上是这样定义因子图的：将一个具有多变量的全局函数因子分解，得到几个局部函数的乘积，以此为基础得到的一个双向图叫做因子图（Factor Graph）。</p>
<p>通俗来讲，所谓因子图就是对函数进行因子分解得到的一种概率图。一般内含两种节点：变量节点和函数节点。我们知道，一个全局函数通过因式分解能够分解为多个局部函数的乘积，这些局部函数和对应的变量关系就体现在因子图上。</p>
<p>根据贝叶斯网络的例子，</p>
<p float="left">
  <img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/factor.png" width="48%">
  <img src="/2021/03/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/推导.png" width="48%"> 
</p>


<h3 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h3><p>1、求某个变量的边缘分布是常见的问题：这问题有很多求解方法，其中之一就是把贝叶斯网络或马尔科夫随机场 转换成 因子图，然后用sum-product算法求解。换言之，基于因子图可以用sum-product 算法高效的求各个变量的边缘分布。</p>
<p>2、</p>
<p>reference：<br><a href="https://blog.csdn.net/v_july_v/article/details/40984699">https://blog.csdn.net/v_july_v/article/details/40984699</a></p>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>货币</title>
    <url>/2021/05/22/%E8%B4%A7%E5%B8%81/</url>
    <content><![CDATA[<br>



<h5 id="货币需求"><a href="#货币需求" class="headerlink" title="货币需求"></a>货币需求</h5><span id="more"></span>

<ul>
<li>1马克思货币量</li>
</ul>
<p>​    M = P*T/V</p>
<p>​    货币需要和商品流通相适应。</p>
<br>

<ul>
<li>2费雪方程式</li>
</ul>
<p>​    MV = PT</p>
<p>​    M是外生变量，V是制度型因素、短期不变、视为常数，T对产出水平保持一定比例 =&gt;P取决于M</p>
<p>​    只有M和T保持1/V的比例，才能保持既定的价格水平。</p>
<br>

<ul>
<li>3剑桥方程式</li>
</ul>
<p>​    Md = kPY</p>
<p>​    资产的角度而非交易；存量而非流量；微观而非宏观</p>
<br>            

<ul>
<li><p>4凯恩斯货币需求</p>
<p>交易、预防、投机</p>
<p>预期利率下降，则人们选择多持有债券，Md减少。</p>
<br></li>
<li><p>5弗里德曼货币需求</p>
<br></li>
<li><p>6</p>
</li>
</ul>
<br>]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>达叔</title>
    <url>/2021/08/04/%E8%BE%BE%E5%8F%94/</url>
    <content><![CDATA[<h4 id="经济"><a href="#经济" class="headerlink" title="经济"></a>经济</h4><ol>
<li>全球化<span id="more"></span><ol>
<li>商品货币在体系中流动。体系：中心国家、准中心工业国家、外围工业国家、外围原材料国家</li>
</ol>
</li>
<li>废票、废政策分房、铁饭碗、重视大学-扩招等。双轮驱动，要先富起来，富的有：基建 房地产、教育、商贸(进出口 制造业)、股市(放开借贷)。=&gt;转到现在的趋势，更多是芯片、半导体、新能源、互联网 ai 大数据、数字货币、虚拟货币等高新技术产业。</li>
<li>城乡结构差异明显，劳动力在城市中估价更高(价值更高，因为城市的消费人群更愿意支付高的价格，他们的收入也高–正向循环)。因为城市的流动性更高、出口带来巨大的相关业务量、资源和信息也更有效率。而城乡的差异也造成了房价的天壤之别。</li>
<li></li>
<li></li>
</ol>
<br>



<h4 id="政治"><a href="#政治" class="headerlink" title="政治"></a>政治</h4><ol>
<li>省 地 县 乡，四级行政层级</li>
<li>职位：县处级、厅局级、省部级、国家级</li>
</ol>
<br>

<h4 id="厚黑学"><a href="#厚黑学" class="headerlink" title="厚黑学"></a>厚黑学</h4><ol>
<li>信任。一旦获取到了信任，事情大多好办。找到了角色和别人的信任，越来越多的自己人，才是实力。索取只会失去，谦让反而获得。人的情绪价值利用好了，理解对方想法，就是情商。</li>
<li>声音更大的、立场更坚决的、利益更大的、组织更大的，往往是胜利一方。</li>
<li></li>
</ol>
<br>

<h4 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h4><ol>
<li>无非是卖东西的。有哪些东西，类别，以及为什么用户要买。以及对标的用户群。销售的渠道等。卖东西就有各种营销手段和活动。那么特殊的活动的数据、用户的数据、商品的数据。各有怎样的分类和定位。</li>
<li>基础模式搞明白了 ，就是细节的数据。比如，行为数据及特征、商品的热点、头部，推荐的召回路径，各个场景，推荐的工程框架等。从业务和代码层面去梳理。各自的量级和变化。以及数据变化背后的业务和用户想法。</li>
<li>更加细节：能够跟上老板的思路和规划。甚至可以预先考虑到老板的想法。更多创新的点子，从模型的角度、产品的角度、推荐召回的角度。</li>
</ol>
<br>



<h4 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h4><ol>
<li>需要有预判，有自己的逻辑，然后通过搜集现象来印证或者伪证，才能真的抓住”未来趋势“。比如，如果思路是从政策展开，看财政支出等重要投资公司的走向，分析相关联的公司、行业等等。现在重点就是半导体。芯片和半导体什么关系… 等等再分析出来芯片。但是时间点呢。无法确定是否就是在5月下旬启动的。但是大的方向总是可以get 的。但是是不是政策的原因呢。还是自然规律，非人为的。</li>
<li>全分类地对情况做出反应。是自己对各个情况都有一定的预设和把握，才是真的了解。</li>
<li>了解你的用户和你的受众。</li>
<li>思想实验。</li>
<li></li>
</ol>
<br>]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>通货膨胀-下</title>
    <url>/2021/02/21/%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80-%E4%B8%8B/</url>
    <content><![CDATA[<p><strong>摘要</strong></p>
<p>定义、相关概念及现象、分类、成因、影响、相应政策。</p>
<p><br> <br></p>
<h1 id="谨记"><a href="#谨记" class="headerlink" title="谨记"></a>谨记</h1><p><strong>只是看了一本书，必然受作者局限。作者有误，不要认为天经地义。勿从众。勿从威。</strong></p>
<p><br> <br></p>
<h2 id="货币幻觉"><a href="#货币幻觉" class="headerlink" title="货币幻觉"></a>货币幻觉</h2><span id="more"></span>
<ul>
<li><p>货币幻觉是新凯恩斯主义的代表人物之一阿克洛夫再次提出的。<br>简单说是100元的钱，被认为有100元的购买力。但实际只有50，另一半则为“铸币税”被征收。<br>而幻觉的存在，因为我们不能够完全理性，价格的传导存在时滞，我们私心喜欢虚幻的“富裕”。</p>
</li>
<li><p>货币本质<br>一般等价物？资产？负债？债券？税票？<br>一般等价物是在交换中起的作用。并不能表现出货币的真实所值，或者说购买力。<br>劳动或资产所得的个人财富，通过货币形式，用于购置资产则为资产。而借来的货币，用以购置资产则为债务。<br>对国家而言，可以将美元看做债券，人民币看做税票，这样的视角更能看出货币所值的变化。</p>
</li>
<li><p>成本<br>借贷的利息，意味着货币资源的机会成本。</p>
</li>
<li><p>货币是一种符号。意味着涨涨跌跌的可能性。货币的乘数效应既能放大资产，也能放大负债。能熬得住，承受得了时间成本，则货币将沉淀为资产。</p>
</li>
</ul>
<h2 id="影响"><a href="#影响" class="headerlink" title="影响"></a>影响</h2><ul>
<li>微观</li>
</ul>
<p>1、货币幻觉导致财富再分配：银行存款名义利率往往不能随通胀充分调整以保证实际利率不变，短期内会出现挤兑， 长期内会造成负储蓄、负投资、负就业、负产出。<br>2、实体企业投资:由长期投资转向短期投资，由生产性投资转向非生产性投资<br>3、累进税制下税收扭曲:通胀税<br>4、商品之间相对价格的变动，价格信号配置资源的效率下降– 模糊了工作效果和报酬的关系、使得投机赌博盛行。</p>
<ul>
<li>宏观<br>  1、对于产出的影响<ul>
<li>促进论:凯恩斯名义工资刚性理论、新凯恩斯粘性工资价格模型;弗里德曼和卢卡斯的预 期错误模型;</li>
<li>促退论:通胀造成微观效率损失;</li>
<li>中性论:“二分法”</li>
</ul>
</li>
</ul>
<p>2、对于就业的影响:菲利普斯曲线，附加预期菲利普斯曲线的运动 &amp; 长期垂直的菲利普斯曲线<br>货币政策的短期有效性及长期无效性、中央银行通胀预期管理的重要性;</p>
<p>3、对利率、汇率等重要经济变量的影响</p>
<h2 id="成因"><a href="#成因" class="headerlink" title="成因"></a>成因</h2><ul>
<li>需求拉上：大量的货币发行，导致过多的货币和商品供给的增长不平衡，供给弹性不高不能及时地跟上货币发行。</li>
<li>成本推动：劳动力市场的不完全或产品市场的不完全等造成的，主要包括工资推动型、利润推动型、进口成本推动型。如70s两次石油价格上涨和次贷危机后的大宗商品价格上涨。</li>
<li>预期：当期高通胀率带来市场主体的高通胀预期，进而导致下期高通胀。</li>
<li>结构：在供求总量基本相同的情况下，由于某些结构性因素，如本国产业结构老化，资源流动效率较低等造成的通胀–本质是新部门的供给未能及时跟上。</li>
</ul>
<br>
而各种政策环境，如金本位下的铸币成色、白银涌入、转型为纸币制、战争等历史聚变、计划经济、金融秩序等都通过间接影响到以上四个成因因素而导致通胀。



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>1、货币在流通过程中的乘数效应导致的通胀是几何级数的，与通常讲的CPI所标示的通胀不是一个概念。CPI是有欺骗性的，最真实的通胀就是M2与GDP的比值。</p>
<p>2、对偏离正常现象的分析，会带来更深刻的认知。</p>
<p>3、价格的绝对值无意义，房价回不到过去。</p>
<p>4、一般来说，批发价上涨幅度高于零售。</p>
]]></content>
      <categories>
        <category>货银</category>
        <category>货币政策</category>
      </categories>
      <tags>
        <tag>货银</tag>
        <tag>货币政策</tag>
      </tags>
  </entry>
  <entry>
    <title>通货膨胀</title>
    <url>/2021/02/17/%E9%80%9A%E8%B4%A7%E8%86%A8%E8%83%80/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>摘要</strong></p>
<p>通胀的基础知识、历史</p>
<p><br> <br></p>
<h1 id="谨记"><a href="#谨记" class="headerlink" title="谨记"></a>谨记</h1><p><strong>只是看了一本书，必然受作者局限。作者有误，不要认为天经地义。勿从众。勿从威。</strong></p>
<p><br> <br></p>
<h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><h4 id="古罗马的铸币成色下降"><a href="#古罗马的铸币成色下降" class="headerlink" title="古罗马的铸币成色下降"></a>古罗马的铸币成色下降</h4><p>当铸币被减少成色时，首先感知的是发行者，最终才是到物价上。最终感知的人的财富被转移到发行者手里。在刚减少、甚至未减少前感知到时，购入资产–窖藏黄金。此时流通的多为劣币。随着铸币数量增多，价值越跌。而执政者采取的冻结物价的措施，结果却是，规则越来越细、维护规则而设立的官员数量越来越多，触犯规则而死亡的人越来越多，市场越发萧条、充斥恐惧。</p>
<span id="more"></span>
<p>=&gt;<br>无法用立法(价格控制)取代经济规律。通胀是更深问题的表象。</p>
<h4 id="西班牙的白银涌入"><a href="#西班牙的白银涌入" class="headerlink" title="西班牙的白银涌入"></a>西班牙的白银涌入</h4><p>特殊的是，大量涌入没有带来恶性通胀。</p>
<p>1白银涌入<br>2西班牙物资缺乏，多进口<br>3白银官价低于真实价值<br>4人们没有认识到价格上涨的原因在于白银本身供给</p>
<p>=&gt;<br>物价上涨，人们窖藏白银，市场中白银少，流通的为劣币。拥有白银的人以为富有、白银价值不变，不进行生产投资。在舶来品和军事冒险上消耗白银，却未能将白银投入促进生产。流通货币成色降低导致物价进一步上涨。但因为滞后的工资上涨、军事冒险、皇室奢靡、文艺的黄金时代，西班牙一度凭借美洲白银输入和通胀，而使得其王室贵族强极一时。富有者的财富得到巩固，农奴制进一步加强。北欧的革命、宗教革命都未能影响西班牙。</p>
<p>而当美洲白银输入回落，西班牙落后。</p>
<p>=&gt;<br>货币更多不等于财富更多；西班牙最终并未能将货币转为资产、留住涌入的白银的价值；白银收入减缓了制度变革的压力，但并非一件好事；通胀具有传导性，通过贸易传导到其他国家；温和的通胀有助于短期繁荣；虽然金属货币铸造需要成本和受采矿影响，但同样会产生通胀，而纸币受发行者–中央政府官员的良心影响，与通胀的结合就更为天然。</p>
<p>=&gt;<br>工资上浮，并不意味这购买力增加，需要警惕货币幻觉；<br>财富如果不投资、自己不进行变革成长，则长期自然落后，受货币幻觉影响而躺在舒适里，并非好事。<br>大水漫灌的虚假繁荣。</p>
<h4 id="约翰劳的纸币"><a href="#约翰劳的纸币" class="headerlink" title="约翰劳的纸币"></a>约翰劳的纸币</h4><p>法国政府财政赤字重,政府用纸币支付、削减债务<br>1716/05，通用银行成立<br>10月，银行券可支付税收</p>
<p>1717/8，西印度公司成立</p>
<p>开始疯狂:</p>
<blockquote>
<p>1718/12，承诺12%-40%股息回报，垄断烟草业，获得铸币权，接手包税公司(交税给国家、从民众手里征税)，银行大量发行纸币，将国债成功转为股票。<br>出售更多的股票来支付股息。高于市场价33%购入自己公司股票期货。向原始股东限售股票。继续发行大量纸币。</p>
</blockquote>
<blockquote>
<p>1720/1，投放一年前9倍的纸币，新股发行价为40倍，保证金交易出现。</p>
</blockquote>
<p>开始破灭:</p>
<blockquote>
<p>1720上半年，亲王要求兑成硬通货。和东印度合并、高股价回购都不能挽回局势。<br>1720/05，发布规定6个月内纸币贬值，股票降价各50%。</p>
</blockquote>
<p>工薪和小店主受打击小–物价和工资同步地涨跌、且幅度不算太高，投机者有的丧失了所有财富。<br>疯狂到破灭，也就一年时间。</p>
<h4 id="大陆币"><a href="#大陆币" class="headerlink" title="大陆币"></a>大陆币</h4><p>1、战争时期，货币发行来获得物资，导致事后的物价抬升、货币贬值。通过通胀转移私人财富资源用以战备。多种手段，通胀是其一。<br>2、战后的纸币贬值，此时纸币退出流通，或者限制物价，导致通货紧缩。则债务人负担被动加重、物价骤降、经济萧条，价格秩序又被流动性减少而打破。因此，更好的策略是维持战时的物价水平，并保证流动性，促进经济复苏。<br>3、工薪阶层希望价格恢复到战前水平，但是价格本身就是相对的，之前价格的绝对值并不具有意义。战后的价格本应是绝对值高于战前的，而和货币发行相适应。</p>
<p>=&gt;<br>如果流动性和通胀不断地上升或者继续，房价水涨船高，并不会跌回十年前。除非极大萧条，利率极高，央行官员不热爱财富、国家不追求GDP。价格的绝对值，无意义。</p>
<h4 id="法国大革命和分配券"><a href="#法国大革命和分配券" class="headerlink" title="法国大革命和分配券"></a>法国大革命和分配券</h4><p>1788，旱灾导致物价上涨<br>1789，法国政府严重赤字<br>1789/07，攻占巴士底监狱<br>1790/04，革命者没收教会财产，发行分配券，首次利率3%<br>1792，反革命战争爆发<br>1793，路易十六上断头台，9月价格管制出台，开始了恐怖统治；没收有钱人的不动产<br>1794，罗伯斯庇尔遇害，开始了白色恐怖，直到1799拿破仑掌权<br>1795，限价法被废，通胀、经济复苏，粮食价格飞涨，开始了反革命行动 – 烧纸币，恢复铸币，经济秩序(价格)迅速恢复稳定<br>1796，距离首次发行分配券6年不到，分配券面值已经为最为保证的教会财产的20倍。</p>
<p>1770-1787通缩；89-1796通胀；97-1870通缩。</p>
<blockquote>
<p>发行分配券后很快开始贬值，金银开始窖藏，劣币再次驱逐良币。<br>当政府宣布废弃分配券时，大量的分配券沉淀在普通公民手中，他们没有将财富及时转换为永久价值的物品。<br>而工资滞后于物价上涨，工薪和固定收入者丧失了购买力。<br>社会里都是暴发户、投机者、穷人。<br>民众抢掠商人。商业活动萧条，易货贸易、违法交易层出不穷。</p>
</blockquote>
<blockquote>
<p>起初，企业主和大众即布尔乔亚和工人，联合起来对付王室。王室倒台后，开始内斗。最高限价法标志这个马克思阶级斗争的开始。企业主获得了胜利，管制被解除。</p>
</blockquote>
<p>=&gt;<br>历史再度重演。和当初古罗马的通胀没什么区别，都是价格管制来控制，结果无效。和大陆币也区别不大，通胀都是在战争时的手段(通过抬升物价，降低人民生活水平而获得资源，人物力投入到战争)，并以管制解决，然后无效。战争和革命，虽然让社会进步，但不止以士兵生命为代价，还有穷人和富人们。投机者却大发横财。</p>
<p>而人们却没有察觉。</p>
<h4 id="美国内战时期北方"><a href="#美国内战时期北方" class="headerlink" title="美国内战时期北方"></a>美国内战时期北方</h4><p>1836，各州私立银行涌现<br>1861的联邦开支占GDP2%，1865则为26%。联邦政府债务高达28亿美元，33倍于战前，占GDP一半<br>1861年底，纸币不再兑付黄金，18年后才恢复兑换<br>1862，纽交所开设黄金交易</p>
<p>1860-1864，<br>教师工资上涨20-30%，物价上涨到两倍多。则实际工资下降了40%。</p>
<p>1864-1896，<br>由于铸币拥护者的政策导致的绿币升值(与黄金的比对由61%上升到战前的100%)+贸易里其他国家物价下跌，导致物价下跌。最大跌幅65%。30年物价回到世界平均水平。<br>其他国家物价下跌，由于黄金增长低于商品增长。</p>
<blockquote>
<p>白银黄金比价：1867，16：1；1896，31：1；1989，71：1。<br>如果在1867年实行白银本位，则劣币驱良。黄金被窖藏。而流通的白银(因其贬值)，反而可能会促使价格回升。</p>
</blockquote>
<p>=&gt;<br>温和通胀有效，而通缩不利于经济。<br>一国货币贬值，则物价相对于其他货币稳定的国家是上涨的。<br>硬通货的拥护者占了上风，要求按照战前水平兑付黄金，使得货币升值，通缩产生。</p>
<h4 id="德国马克"><a href="#德国马克" class="headerlink" title="德国马克"></a>德国马克</h4><p><strong>1、事实：</strong></p>
<p>每月价格上涨50%–&gt;恶性通胀；马克的恶通胀认为导致了希特勒。</p>
<ul>
<li><p>通胀从小跑进入狂奔<br>一战后的德国，物价是战前的2.5倍，一年后上涨3倍，一年后又上涨2倍多，后来的几个月停顿了下。<br>1921年11月，为战前的40倍。<br>1922年11月开始进一步加速。23年年中开始恐怖狂奔。年末停止。<br>算下爬行阶段耗时2年多，小跑1年半多，狂奔半年多。最高能一个月涨二十多倍。小跑中间也有几个月算温和。<br>23年1月，法国进入鲁尔盆地，8月政府宣布征税，11月与法国战败。此种种不满终于带来了希特勒发动的啤酒馆暴动。</p>
</li>
<li><p>现象<br>民间的借贷利息是政府的120倍。<br>政府和企业主精英重建了工厂，巩固了权力，而对各个家庭生活或者不同经济部门，何等不平等不公正。<br>低利息时大肆借贷购入不动产和商品，马克贬值再用马克归还，产生了新富。<br>政府注销了所有内债。<br>农民享受了高的粮食卖出价格，抵押贷款偿债压力减弱。<br>实际工资下降，真实工资已经低于维持生活的需要。<br>工会要求工资同生活费用指数挂钩，但是消费在一周后进行，当前的指数仍然无法避免手里的钱一周后失去价值。<br>死亡、移民、犯罪上升。<br>耐用品的价格不再取决于需求，而是取决于一周后获得的成本，一周后可能需要双倍的当前价格。<br>汇率贬值-&gt; 国内物价上升-&gt; 增发纸币-&gt; 进一步贬值-&gt; 物价进一步上升  的恶性循环。<br>但，<br>20-23年间国GDP表现不错。钢铁产量稳定。</p>
</li>
<li><p>1923年<br>通胀狂奔后，马克失去信用。11月，失业23%，煤产量44%等经济毁灭。<br>对策：财政部领导人变更，确定结束通胀的新方案，严格削减开支、建立新的税种，德国回复了预算平衡。引入新马克作为临时货币。</p>
</li>
</ul>
<p>=&gt;<br>通胀到了极限，就停顿了。</p>
<p><strong>2、原因探究：</strong></p>
<ul>
<li><p>赔款<br>巨额赔款需要金马克支付，国际市场美元升值高于国内，外国银行购入马克，而使得德国免费获得大量的食品和原材料。但22年7月停止以外汇支付赔款，通胀却加速了。</p>
</li>
<li><p>国际收支<br>出口商品以支付赔款的压力(需要大量出口来获得用以赔偿的外汇，主动性地贬值来刺激出口) 导致马克贬值，国内物价上涨，为维持国内商贸不得不继续印发纸币。</p>
</li>
<li><p>马克投机<br>卖空马克者</p>
</li>
<li><p>避免革命<br>失业和商品短缺会带来俄国一样革命。通胀至少有表面的繁荣。</p>
</li>
<li><p>通胀获利团体的助力<br>马克贬值，外国人希望用马克购买德国艺术品(马克需求增多、而增发的马克进一步大于需求？)；担心受损，资产纷纷被转移到国外。这些都进一步导致货币进一步超发。</p>
</li>
<li><p>政府赤字</p>
</li>
</ul>
<p>=&gt;美元升值和出口压力，为超发找到了借口。本质是巨额赔款成为超发货币的借口。而且政府是通胀获利的最大团体，政府不想看到革命，而马克投机者里，更多是否是政府中人呢。经济现象的解释里，很多结果会反作用于现象，造成现象的强化，而这样的结果被认为成”原因”。这并非根因。</p>
<p><strong>3、问题</strong></p>
<ul>
<li>如果是超发，那么当时真的减少发行，又会出现什么样子呢，会不会紧缩–失业和短缺呢。会不会赔款付不了呢。受害的会否比这样的通胀好？为什么这样的恶性通胀，竟然伴随着就业和GDP的稳定甚至增长呢？</li>
</ul>
<p>(减少发行，如果控制得好能够不会恶性通胀，但是亦不能因此通缩。需要把握节奏。并且，大量的赔款如果不发行足够的货币，除非经济的增长速度快，能够产生相应的流动性需要，否则赔款就是突发的流动性需求。会付不了，或者时间很长。就业和GDP稳定因为，也是算温和的。23年狂奔后导致了大量失业和降产，失业大幅增多现象比通胀发生地慢一些。也可能是失业统计的数据，发生在通胀之后。)</p>
<ul>
<li>新马克和分配券有什么不同呢，都是以土地作为抵押保证。新马克的背景：赔款和政治经济都还是老样子，而新马克却使得恶性通胀恢复地惊人。劣币无法流通后良币取而代之。–海温斯坦为啥能取得稳定货币的成就？</li>
</ul>
<p>(本质是，新马克保证了其价值、控制了发行；而分配券还是超发。似乎根本不在于表面的这些相似，而在于对欲望的态度。)</p>
<ul>
<li>为什么通胀是旧价格的延续而通缩不是？</li>
</ul>
<p>(如果货币价值✖️价格 = 商品劳务价值，那么通胀不改变商品劳务价值，则旧的价格是由于货币价值而反向变动。通缩则是流动性不足，当人们不愿意拿钱买、不愿意出钱投资，认为商品都不值得买，并不是因为觉得货币更值钱，更多可能出于谨慎因素，回报低或者对未来悲观。商品也会慢慢退出市场。由于悲观，影响到投资和产出，变化来自等式右边。)</p>
<ul>
<li>为什么会小跑到狂奔？</li>
</ul>
<p>=&gt;<br>惊人的数字让人记忆犹新。德国现在仍会谨慎。</p>
<h4 id="俄国计划经济"><a href="#俄国计划经济" class="headerlink" title="俄国计划经济"></a>俄国计划经济</h4><p>todo                    </p>
<br>]]></content>
      <categories>
        <category>货银</category>
        <category>货币政策</category>
      </categories>
      <tags>
        <tag>货银</tag>
        <tag>货币政策</tag>
      </tags>
  </entry>
  <entry>
    <title>银行拨备</title>
    <url>/2021/02/17/%E9%93%B6%E8%A1%8C%E6%8B%A8%E5%A4%87/</url>
    <content><![CDATA[<p><br> <br></p>
<p>拨备有一个重要的功能，就是“以丰补歉”，以平滑利润。</p>
<span id="more"></span>

<blockquote>
<p>为什么要平滑利润？<br><br>如果银行全部股东一直没有变动，那么其实要不要以丰补歉都是一样的，反正赚和亏都是这些股东的。但是，因为股东每个交易日有进有出，所以得考虑利润跨期平滑问题。<br><br>假设有一家银行，过去每年盈利100元，非常稳定地持续了5年以上。突然有一年，发生了一笔不良资产，当年会亏损200元。如果这家银行没有以丰补歉，而是前5年把利润全分红了，然后有股票换手，有新股东进来，然后到了第6年亏损200元，那么新股东怎么都不会开心的。他们会觉得，前面的股东赚走了贷款的利息，而这笔贷款发生违约时，亏损让自己承担，这太不公平了。<br><br>这跟信用债交易有点类似。有些信用债在违约发生前发生交易，新买入的投资者成了接盘侠……</p>
</blockquote>
<p>如果按照要求计提拨备，也很做到平滑利润（不良太多，把超额拨备消耗完毕之后还不够）。那么，还有一个方法，就是在不良贷款的确认上动手脚。因为不良资产和非不良资产之间，并没有清晰、客观的边界，边界划分在哪，是有一定的主观性的。</p>
<p>基于拨备覆盖率指标，我们可以有这样一个假设（确实存在例外的情况）：<code>银行不太可能一边宽松认定不良（甚至隐藏不良），一边又计提大量超额拨备，保持很高的拨备覆盖率。</code></p>
<blockquote>
<p>（1）如果一家银行拨备覆盖率远超监管标准（比如150%），并且还在持续提升，那么很明显是处于丰的阶段。很显然，市场上，这样的银行股，估值一般不低，而且主升浪是从它们拨备覆盖率开始显著上升开始的。<br><br>（2）如果一家银行拨备覆盖率在较长时期内仅维持在监管标准附近，那么有可能是：每年的营业收入用于消化存量不良之后，无能力留存额外的拨备。甚至，它是每年收入能消化多少不良就确认多少不良，并且还有存量不良还未消化完毕（存量不良有可能还未确认到报表中），还在补欠。<br><br>（3）如果一家银行拨备覆盖率高位回落，可能也是处于补欠的阶段，即每年收入已经不足以消化新发生不良，而是需要拿过去的超额拨备去消化新发生的不良。<br><br>（4）一家银行的拨备覆盖率从监管标准开始起飞，则有可能是存量不良处置完毕了，开始进入丰的阶段，积累超额拨备。</p>
</blockquote>
]]></content>
      <categories>
        <category>货银</category>
        <category>商业银行</category>
      </categories>
      <tags>
        <tag>货银</tag>
        <tag>商业银行</tag>
      </tags>
  </entry>
  <entry>
    <title>长期价值</title>
    <url>/2021/06/25/%E9%95%BF%E6%9C%9F%E4%BB%B7%E5%80%BC/</url>
    <content><![CDATA[<br>

<br>

<p>如果以供需平衡来看待现象，很多时候更容易看到本质。</p>
<span id="more"></span>

<p>比如，为什么越贪婪，在当今这个市场中更受追捧。即所谓强势文化。本质是，贪婪是用来抵抗非理性的。因为恐惧的人多，大多人在追随，所以贪婪更稀缺。</p>
<p>这里隐藏的逻辑是，理性是最终目的。这是当然。</p>
<p>理性的本质是避免受干扰。我们每个人都是有限理性，并且由于惯性，很容易受他人、他事干扰，甚至于会受”情商“干扰。有些人的惯性就是不受干扰。他们属于幸运的。</p>
<br>

<p>避免干扰的有效途径，即信念。对自己认知的高度清醒地自信和坚持。遵循自己的认知，继而调整。做到有序，而非一口一个胖子，各种角度都要兼得，只会显得为人冗长。</p>
<p>信念塑造行为模式。而行为模式最终导向各种生态，自我的生态和自我与他者形成的生态。人的心理组成，无非贪婪和恐惧的平衡。越受恐惧支配，则越发减少能量消耗，信念度也随之降低。</p>
<p>那些硬刚的人，失败也刚的人，只要不是愚蠢，那便是有大智慧的人。</p>
<p>理性和信念，这便构成一个良性循环。一般有了这个循环，就可以持续。</p>
<br>

<p>基本概念和初心一样，是需要不断被重复，才能形成惯性的。这就启迪在实践上，程序性上，要做的则是重复和提醒。结合判断。这种理论到实干的沟壑不给打通，永远只是酒囊饭袋。人就是在干。</p>
<p>不要以为批判看待事物就是最佳的。既然你是批判的，那你就应该批判地看待批判。达到一个新的自我认知，并且认可自由。认可自由的人，也是有责任意识的人。否则就是在要求兼得。批判不是要求兼得。</p>
<br>

<p>理性、信念或者说初心，都是自我表达。有的人的表达就是感性。</p>
<p>每个人都在学习自我表达。大多数人凭着惯性展示自己。戒掉不好的惯性，第一要就是，慢。然后再快。不慢，永远快不起来。</p>
<p>追求价值的第一要，就是放弃眼前利得，因为价值的属性本来就是长期。价值就是要经得起沉淀。</p>
<br>

<p>着眼于长期价值，即找到不患而无位次之处。考虑长远的事情，更容易得到合理的、可预期的结果。确定性远远比更大更快重要。</p>
<br>

<p>整体的逻辑是：</p>
<p>首先要有责任感，对认知有自信，这样才会坚定，然后宁慢勿急，保证理性的环境，在理性的状态下，考虑长期，根据知识，做出相对合理的判断，每一个小判断构成一各个上上下下的波动轨迹，日久，就拉开了趋势性的倍数差距。细节里都需要对此初心进行秉持。</p>
<p>这个是大范围的。</p>
<br>

<p>小范围的，则是，具体的长期价值，如何着眼的？</p>
<p>搞清楚你背后的逻辑、看好你自己的那个篮子，严格要求逻辑清楚、并且能够确定边界和异常。</p>
<br>

<p>和几十年前所不同的在于，之前拉开差距的只是有无，而现在，信息便捷，摩擦成本小，套利机会少，更多拉开差距的是深刻和广泛性。以及性格。</p>
<br>

<p>长期思维是很大的区别之处。包括对基本概念的理解。</p>
<p>抓住了长远，就能立足，而获得长久的不败，而非短期的冲高。</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>阶段性总结</title>
    <url>/2021/06/29/%E9%98%B6%E6%AE%B5%E6%80%A7%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<br>

<br>

<h4 id="看到的"><a href="#看到的" class="headerlink" title="看到的"></a>看到的</h4><span id="more"></span>

<ol>
<li>《三一恨别长沙–梁稳根 的内心独白》和《恨别长沙谎言曝光》。中联重科和三一的斗争，有国资、非法手段。</li>
<li>到了后来，爷爷只有背影和沉默。只有零星几张吃饭的照片，很少娱乐，很少活动。人离世之时都是落寞的。万物带不走，唯一能缓解死亡降临恐怖的，只有爱和陪伴。相视无言。却留不住。只能想，都是要走的，我也是要走的，才能通透些继续这生活。和你面对死亡的，能有谁？父母来不及，孩子已出门，唯有伴侣。走时若是充实无遗憾的，心里也坦然。不枉岁月。</li>
<li>让子弹飞： 站着还把钱赚了。</li>
<li>中国知识分子招聘会</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>笛卡尔积</title>
    <url>/2021/03/11/%E9%9B%86%E5%90%88%E7%9A%84%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF/</url>
    <content><![CDATA[<br>
<br>


<h2 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h2><span id="more"></span>

<br>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static String cartesianProduct(final String[][] inputs) &#123;</span><br><span class="line">        if (inputs &#x3D;&#x3D; null) &#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        final StringBuilder sb &#x3D; new StringBuilder();</span><br><span class="line">        </span><br><span class="line">        product(&quot;&quot;, 0, inputs, sb);</span><br><span class="line">        </span><br><span class="line">        return sb.toString();</span><br><span class="line">    &#125;</span><br><span class="line">public void product(String prefix,int index, String[][] input,StringBuilder sb)&#123;</span><br><span class="line">        for (int i &#x3D; 0; i &lt; input[index].length; i++) &#123;</span><br><span class="line">            if (index &gt;&#x3D; input.length - 1) &#123;</span><br><span class="line">                sb.append(prefix + input[index][i]);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                product(prefix + input[index][i], index + 1, input, sb);</span><br><span class="line">            &#125;</span><br><span class="line">            if (i &lt; input[index].length - 1) &#123;</span><br><span class="line">                sb.append(&quot;, &quot;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>两个变量关键prefix和index，作为递归方法的参数时进行变化<code>prefix + input[index][i]</code>和<code>index+1</code>。</p>
<p>从数列角度看，sb(j) = sb(j-1) + charAt(i) ,charAt(i)需要一个for循环即<code>for (int i = 0; i &lt; input[index].length; i++)</code>。sb(j-1)为<code>prefix</code>，下一步需要<code>prefix + input[index][i]</code>；其中<code>j-1</code>为<code>index</code>，进入下一步需要<code>index+1</code>。</p>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>零散问题repo</title>
    <url>/2021/04/05/%E9%9B%B6%E6%95%A3%E9%97%AE%E9%A2%98repo/</url>
    <content><![CDATA[<p><br> <br></p>
<p><strong>无中心、碎片知识点</strong></p>
<span id="more"></span>


<ul>
<li><p>get post</p>
<ul>
<li>GET请求是通过URL直接请求数据，而POST请求是放在请求头中</li>
<li>GET提交有数据大小的限制，POST请求在HTTP协议中也没有做说明，一般来说是没有设置限制的，但是实际上浏览器也有默认值</li>
<li>登录操作的时候，尽量使用HTTPS请求，安全性更好</li>
</ul>
</li>
<li><p>gmv gtv</p>
<ul>
<li>总销售额，总交易额</li>
</ul>
</li>
</ul>
<ul>
<li>感受野<br>如果两个3<em>3和一个5</em>5对原始输入后卷积都得到了1<em>1的输出<br>那么哪个filter好呢？<br>从计算量和参数个数来看，选择多层卷积，而每个卷积为3</em>3的更好。</li>
</ul>
<ul>
<li>为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？</li>
</ul>
<p>CNN抓住此共性的手段主要有四个：局部连接／权值共享／池化操作／多层次结构。<br>如果每一个点的处理使用相同的Filter，则为全卷积，如果使用不同的Filter，则为Local-Conv。</p>
<p>人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取。</p>
<ul>
<li>用贝叶斯机率说明Dropout的原理</li>
</ul>
<p>从训练集有替换采样 构造 k 个不同的数据集,然后在训练集上训练模型 i。<br>Dropout的目标是在指数 级数量的神经网络上近似这个过程。</p>
<ul>
<li>在n维空间中，以下哪一个方法最适用于异常点检测？</li>
</ul>
<p>Mahalonobis 距离是基于卡方分布的多变量异常的程度的统计量</p>
<ul>
<li>共线性</li>
</ul>
<p>vif检测<br>正则、删除</p>
<ul>
<li>异常点</li>
</ul>
<p>卡方分布</p>
<ul>
<li>聚类</li>
</ul>
<p>划分聚类 k-means、k-medoids、k-modes、k-medians、kernel k-means<br>层次聚类  Agglomerative 、divisive、BIRCH、ROCK、Chameleon、HAC<br>密度聚类 DBSCAN、OPTICS、HDBScan<br>网格聚类 STING<br>模型聚类 GMM<br>图聚类 Spectral Clustering（谱聚类）</p>
<ul>
<li>使用SVM模型遇到了欠拟合的问题，以下哪个选项能提高模型性能？</li>
</ul>
<p>增加 c gamma。<br>c越大，支持向量和分离平面距离越小，要求越严格。<br>gamma越大，空间维度越高。</p>
<ul>
<li><p>说明如何用支持向量机实现深度学习(列出相关数学公式)</p>
</li>
<li><p>广义线性模型是怎被应用在深度学习中?</p>
</li>
</ul>
<p>深度学习从统计学角度，可以看做递归的广义线性模型</p>
<ul>
<li>Weights Initialization. 不同的方式，造成的后果。为什么会造成这样的结果。</li>
</ul>
<p>几种主要的权值初始化方法： lecun_uniform /  glorot_normal / he_normal / batch_normal</p>
<ul>
<li><p>为什么网络够深(Neurons 足够多)的时候，总是可以避开较差Local Optima？</p>
</li>
<li><p>Loss. 有哪些定义方式（基于什么？）， 有哪些优化方式，怎么优化，各自的好处，以及解释。</p>
</li>
</ul>
<p>Cross-Entropy / MSE / K-L散度</p>
<ul>
<li>backbone</li>
<li>hdcnn</li>
<li>cascade</li>
<li>小目标检测</li>
<li>caffe源码里的底层设计模式、数据流、cuda细节、如何添加新层</li>
<li></li>
<li>更倾向于给你一个场景看你的思路，场景里你怎么分析<br>怎么挑出来好的样本、选择什么样子的模型<br>特征工程的做法–<br>ab背后的正交<br>特征管理的方案<br>知识图谱好的应用方式<br>模型选型，为啥用图模型<br>一维的卷积的加速<br>模型 的可解释性、业务上解释<br>长尾item<br>采样方式–正负样本的做法<br>实时模型<br>实时特征落地<br>adam优化器背后的优化，解决了什么问题<br>sgd有什么问题<br>你是如何调参的<br>图模型有什么弊端<br>xgb的特征重要性怎么算出来的<br>ab正交机制<br>gnn的输入 gnn的思路是？<br>文本的情感类别，比如 美食领域的正向还是负向 怎么判断<br>Self attention和普通attention的区别<br>多种transformer的架构、对比<br>cnn输入的如果是长文本 怎么处理<br>各种loss<br>图模型还有哪些<br>同构图和异构图的区别<br>vae的loss<br>分裂加速<br>Mmoe<br>Highway network<br>Bidaf<br>Elmo<br>esmm<br>逻辑回归的时候问一下odds的概念，<br>贝叶斯线性回归要是需要强制系数非负该怎么办，<br>核函数，relu函数是什么意思。<br>半监督学习或者时间序列<br>lasso，逻辑回归，非负矩阵分解，svm的目标函数<br>什么叫显著，什么叫p值，<br>rerank阶段的强化学习、多目标任务<br>tf-serving的搭建<br>推荐中多样性、头部问题<br>其他的排序模型、召回框架<br>召回更好的做法，优化思路<br>计算量（样本量、分类效果or打分效果、epoch、时间，在线学习设计）<br>各个场景推荐策略的区别<br>目的是什么。<br>CET<br>概率图的条件独立？<br>spark解决数据倾斜<br>widedeep，为什么需要wide+deep好处的解释<br>如何避免落入局部最优<br>约束方程怎么解 – y&gt;=0的条件下……<br>hmm具体内容<br>attention bert transformer<br>pytorch的常用函数整理<br>全连接层有什么作用？做一个图像识别的网络，可以不要全连接层吗？<br>机器学习训练误差由哪些构成；<br>BN层加在激活函数前与后效果有何不同；<br>如何判断异常点；<br>逻辑回归的分布函数；<br>逻辑回归的参数求解在优化方面属于什么类型；<br>seq-seq有哪些结构形式；<br>soft-attention与hard-attention的区别；<br>数据分类不均匀的话，要做哪些处理；<br>手写逻辑回归极大似然函数的数学推导；<br>Glove与word2vec的比较；<br>PageRank是怎么回事；<br>在resnet中，什么是残差，有何意义？<br>在GEMM中，如何优化缓存？<br>在ARM平台上，SIMD(单指令多数据)介绍一下大概？<br>是否了解其他平台的SIMD指令？intel的AVX和ARM NEON有何不同？<br>在实现一个SIMD程序时，应该注意哪些方面？如何判断一个算法适不适合SIMD加速？<br>如何证明，SIMD已经达到了最优化性能？<br>在大量的SIMD指令中，如何选择性能最佳的指令？<br>实现memcpy.<br>如何在main函数之外之行一个函数。<br>这样声明变量有没有问题：int a[10000000].<br>static修饰符有什么用？如果不加会出现什么后果？<br>常见的语言模型；<br>文本表达方法；<br>新词如何发现；<br>句子中关键词如何提取（tfidf，textrank）；<br>如何计算两个句子相似度；<br>讲一下Bert；讲一下fastText；<br>文本处理常用步骤；<br>分词分得不准确的话，该如何处理。<br>DenseNet的网络结构？与全连接有何不同？<br>简历项目中网络用了多少层FeatureMap，每层面积？<br>FeatureMap提取了哪些特征？<br>词向量怎么训练的？<br>word2vec两种常用模式？<br>词向量后面的Softmax如何优化？<br>CBOW和Skipgram哪个更适合采用？（大规模训练的话Skipgram要更好）<br>Glove的原理？（简历上有体现）<br>讲一下fastText（简历上有体现），说一下与word2vec的联系；<br>Huffman树；<br>如果用seq-seq进行embedding，做相似度计算，会如何（替代Glove的话）；<br>Glove如何训练的，用的多少维；<br>解释下TextRank（简历有体现）；<br>为什么数据量越大SVM训练越慢；<br>分词是如何处理的；<br>语料中遇到新词如何处理；<br>决策树和SVM在数据预处理上有何不同（缺失值）；SVM对文本要先做什么处理；<br>fastText最初先对文本如何处理（固定格式）；<br>词性标注比较好的方法是哪些，有没有最新的模型。<br>手推SVM；LSTM的结构，优势在哪里。<br>生成模型主流就那几个<br>VAE系列<br>Glow系列<br>GAN系列<br>思考，VAE的优缺点有哪些，为什么VAE的结果通常是比较模糊的？VQVAE的提出是希望解决什么问题，他存在VAE类似的问题吗？如果存在，那是为什么。<br>Glow的优缺点？为什么用Glow？Glow的分布会对结果产生什么影响？<br>GAN的稳定训练措施有哪些？各个稳定方法的优缺点比较？pair与非pair数据对GAN的训练影响？<br>从生成模型的角度来看，当我们希望对生成内容的属性进行控制的时候，你会选哪些方法，为什么？<br>进一步地，从模型训练的角度。参数初始化，激活函数选择，数据的均值方差，正则化，归一化，优化器（adam以及变种），自回归与非自回归的选择。<br>最后的最后，从训练数据而言。因为我接触信号比较多，那么，这段信号有什么特点，怎么提取feature输入网络？干净的数据怎么做，带噪的数据怎么做，数据的干净程度对结果的影响，一定要用深度学习吗，传统的信号分析能作为辅助loss吗？怎么衡量你的模型效果？<br>最最后的深坑，这个东西可以用强化学习做吗</li>
</ul>
]]></content>
      <categories>
        <category>零散知识点</category>
      </categories>
      <tags>
        <tag>零散知识点</tag>
      </tags>
  </entry>
  <entry>
    <title>面试repo</title>
    <url>/2021/07/28/%E9%9D%A2%E8%AF%95repo/</url>
    <content><![CDATA[<br>



<h4 id="面试"><a href="#面试" class="headerlink" title="面试"></a>面试</h4><ol>
<li><p>微服务架构<span id="more"></span></p>
<ol>
<li><p>RPC原理：RMI ，原理，手写</p>
</li>
<li><p>Dubbo应用及源码</p>
<ol>
<li>原理</li>
<li>eureka和zk作为注册中心区别</li>
</ol>
</li>
<li><p>SpringBoot底层：分布式事务，性能优化，@SpringBootApplication</p>
<ol>
<li>启动原理</li>
</ol>
</li>
<li><p>SpringCloud进阶</p>
<ol>
<li>各个组件及集成</li>
</ol>
</li>
<li><p>Docker虚拟化技术：compose、service、redis分布式集群部署、file构建…</p>
<ol>
<li>基于Swam构建Docker集群实战</li>
</ol>
</li>
<li><p>Spring</p>
<ol>
<li>ipc aop 事务</li>
</ol>
</li>
<li><p>SOA和微服务区别和联系</p>
</li>
</ol>
</li>
<li><p>高并发分布式</p>
<ol>
<li>分布式锁</li>
<li>分布式场景及解决方案：单点登录、分布式任务调度</li>
<li>分布式协调和分流：ZK、Nginx</li>
<li>FastDFS分布式文件存储</li>
<li>RunnableFutrue、FutureTask、Thread、Runnable、ExecutorService…</li>
<li>lock、ReentrantLock、Condition、ReadWriteLock、LockSupport</li>
<li>atomic、ThreadLocal、ABA、JMM、cas算法、乐观锁</li>
<li>CountDownLatch、CyclicBarrier、Semaphore、Exchange</li>
<li>BlockingQueue、ConcurrentHashMap、HashTable…</li>
</ol>
</li>
<li><p>jvm</p>
<ol>
<li>垃圾回收算法</li>
<li>优化策略</li>
<li>类加载流程</li>
<li>进入老年代？</li>
</ol>
</li>
<li><p>mysql</p>
<ol>
<li>事务四大特性，隔离性，索引，索引优化，主从复制</li>
</ol>
</li>
<li><p>redis</p>
<ol>
<li>数据类型</li>
<li>list底层实现</li>
<li>分布式做法</li>
</ol>
</li>
<li><p>消息中间件</p>
<ol>
<li>怎么存储的</li>
<li>考虑哪些问题</li>
</ol>
</li>
<li><p>优化及debug</p>
<ol>
<li>接口响应慢的优化思路</li>
<li>缓存一致性问题</li>
<li>10个线程，让某一个最后执行，有几种方式</li>
</ol>
</li>
<li><p>netty</p>
<ol>
<li>Tomcat线程模型</li>
<li>Tomcat的NIO NIO2 ,MAX Threads、Max Connections…</li>
<li>Netty线程模型</li>
<li>Eventloop EventLoopGroup</li>
<li>Channl如何处理的，线程安全吗</li>
<li>Netty如何实现零拷贝</li>
<li></li>
</ol>
</li>
<li><p>linux</p>
<ol>
<li>内核IO 操作实现原理</li>
<li>内核如何零拷贝</li>
</ol>
</li>
<li><p>工程化：</p>
<ol>
<li><p>Maven</p>
<ol>
<li>scope</li>
<li>类冲突、包冲突的问题定位</li>
<li>lifecycle、phase、goal</li>
<li>Maven生成Archetype</li>
<li>手写插件</li>
<li>Nexus使用、上传</li>
<li>对比Gradle</li>
</ol>
</li>
<li><p>Jenkins</p>
<ol>
<li>集成maven和git</li>
<li>多环境配置、权限管理和插件使用</li>
</ol>
</li>
<li><p>git</p>
<ol>
<li>best practise</li>
<li>规范</li>
</ol>
</li>
<li><p>Sonar</p>
<ol>
<li><p>质量管理</p>
</li>
<li><p>FindBugs 、PMD运用</p>
</li>
</ol>
</li>
</ol>
</li>
<li><p>前端</p>
<ol>
<li>NodeJs internals：<ol>
<li>V8 internals</li>
<li></li>
</ol>
</li>
<li>js<ol>
<li>general JavaScript syntax</li>
<li>map reduce</li>
<li>array manipulations</li>
</ol>
</li>
<li>typescript<ol>
<li>generics</li>
</ol>
</li>
<li></li>
</ol>
</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>2021.06</title>
    <url>/2021/07/28/2021-06/</url>
    <content><![CDATA[<br>



<h4 id="0610"><a href="#0610" class="headerlink" title="0610"></a>0610</h4><p>做了些总结<span id="more"></span>，讨论了几个问题(主要是deep wide各自的区别，每一侧特征的选择，以及负采样的一些问题)，脑子足够的，但是一些基本概念会混淆。真是太随性了。</p>
<p>中午又买东西，买了一个多小时。早上tm没车。时间在我这里不是金钱。反过来了。还是贫穷没脱圈。看着小羊的消费，一下子生了相心。想写写抖音。观察下。晚上被洗了几个小时的脑。</p>
<p>一楼就是，返水、采光、潮湿、蚊虫、烟头。可避免也不可。有法子也费心。看机缘吧。</p>
<br>

<blockquote>
<p>全分类和框架、悬空与抽象性、实践性–即程序、细节、oneshot</p>
</blockquote>
<br>



<p>这几天没有记录时间和写zhimap。总是分散和旁支地被干扰。一周又将凋谢。</p>
<br>

<p>王朔(两个节目一个锵锵)，看了杂七八的。做了几道题。看了些高数。看了些缠书。看了些c++。抖音洗脑性太强了。</p>
<p>chamath让我开始对英文的news重视起来。以及带着我那些讨论的问题看论文的方案。我还没有真的解决一个问题去查找文献。</p>
<p>看了很多阴谋论的东西。资本市场的血雨腥风。以及赤裸地直面程度。没有艺术面纱。</p>
<p>讨论了些问题，仍然纷乱。自己的思想还没有构建起来。</p>
<p>上周看了些顾颉刚的书。不行。周末看了房子。</p>
<p>我还干嘛了呢上周。对了花了一个晚上时间对tf来debug，还是内存不够。没能跑起来。整个框架光看代码还是头绪不多。没能坚持日结。</p>
<p>也有大部分时间在购物。</p>
<br>

<br>

<h4 id="0612-13-14"><a href="#0612-13-14" class="headerlink" title="0612 13 14"></a>0612 13 14</h4><p>和老公吃饭，看金水苑二手房，练车，写了篇不太价值的文–这类文章还是别写了、行为也别有了，去大嫂那边，和李聊，来公司吃喝加简单看了点–效率几乎无。</p>
<p>大部分时间在车上。瞎聊。</p>
<p>“不要从手机获取资讯。而只是必要时用。”</p>
<p>没用心想，也是没有精力。没有锻炼。没看论文。没写啥笔记。</p>
<br>

<h4 id="0615"><a href="#0615" class="headerlink" title="0615"></a>0615</h4><p>1、复习到20课。√</p>
<p>2、活动开发，哪里开发，怎么测试√</p>
<p>3、c++对象模型√</p>
<p>比如，const和return by reference，以及析构–基本的对象是否含有指针，以及inline，以及构造函数的默认初始化。这些细节，trick，影响着性能。</p>
<p>人生就是各个trick。有能力从理论上自动地自然地，捕捉到trick，使得人生越走越高。其他很多情况，是各种碰了。比如胆子大比知识多，大概率地碰对。就对准到了那个情境下的trick。</p>
<p>他会告诉你，从那个全分类的角度，为什么需要这样的trick。</p>
<p>比如析构函数，为什么需要析构，为什么编译器不能自动地销毁。那么就需要手动。</p>
<p>比如，为什么这样写更好…</p>
<p>比如，为什么要有class，struct哪里不够。</p>
<p>比如，这个语言根本的模式好在哪里。是多态、封装，那么这些有体现在哪些写法或者设计上。而这个特性又将产生哪些影响。</p>
<p>这样的思考，就会从根源知道区别。并且更加理解。</p>
<p>4、go反射√</p>
<br>

<h4 id="0616"><a href="#0616" class="headerlink" title="0616"></a>0616</h4><p>为什么资本泡沫破裂来沉淀市场里的优质公司？</p>
<p>院线为什么能保证万达商业地产的业态能够快速的扩张？</p>
<p>资本市场商誉减值的问题，什么引起的？</p>
<p>其实资本还有一个很大的作用，把这个公司作为一个基础，把我们的产业链可以高度的整合融合起来。</p>
<p>四家影视公司的市值目前分别是224亿元、350亿元、336.5亿元和101.4亿元，市值之和为1011.9亿元。</p>
<br>

<p>一天了，活动开发还没完成</p>
<p>这些代码都很简单，在清楚了大概框架逻辑之后去看。不能不看。其实很简单。从上到下和从下到上的思维都有了。</p>
<br>

<br>

<h4 id="0617"><a href="#0617" class="headerlink" title="0617"></a>0617</h4><p>极慢。事情零散地做。不集中快速消灭。今天至少把代码看完、该改的开发完。</p>
<br>

<br>

<h4 id="0621"><a href="#0621" class="headerlink" title="0621"></a>0621</h4><p>1、30课尽力吧</p>
<p>2、高数看完两ch吧 </p>
<p>3、想好自己的反馈机制。</p>
<br>

<br>

<h4 id="0622"><a href="#0622" class="headerlink" title="0622"></a>0622</h4><p>30课</p>
<br>

<p>巴菲特 – 真正的价值在于不患；搞清楚你背后的逻辑、看好你自己的那个篮子，严格要求逻辑清楚、有理据、想清楚前后，才能够坚定，要的就是坚定性；材料要公开的。</p>
<p>1、本质上来说，短期投资和长期投资的理念是一致的。一致性在于 长期的时间也是有限的。长期需要估值，那么是一年期的估值，还是永久现金流的贴现估值呢。是一两年的增长，还是十年呢。长期短期总是有分界和阈值的。</p>
<p>2、长期的逻辑：进行估值，和价格成本相比较。</p>
<p>背后是因为，市场是不完美的。市场的波动性是随机、疯子一样的。而根本的价值在于，管理层以及企业未来的前景。投资的能力：是能够对企业内在价值有判断，并且能和市场的噪音想隔离。</p>
<p>市场不完美：资金力量、各种因素都在用影响着短期的供需的均衡。没有办法找到里面的不患。</p>
<p>对于内幕：知道是无意义的。市场节奏难以踏准。因为这样的短差带来了因小失大。主要因为，其中没有规律可循。</p>
<p>对于持有期：他们甚至想一直持有，只要增长是符合他们的预期。</p>
<p>但是缠论，理论基础在于说，他找了不患。</p>
<p>3、“捡烟头”的思路在当下还可行吗，他那时候是57年之后，每年的收益高则近50%，低也在8%，我觉得这些数字就算一个上限或者阈值了。并且波动性是小于指数的。他的资料是一手的研报和财报，他从中获得对于价值的分析(财报上作假？比对…等等，还是对于材料有很大的运用能力)。</p>
<p>虽然那些公司的安全边际也不是很大空间，但是足以能够开始，有了第一桶金。</p>
<br>

<p>4、材料是一致的，优质的。然后就是理解力的问题。</p>
<p>5、生活，真实的生活和想象的、憧憬的。现实和虚幻的。能够识辨已经很少人了。</p>
<p>6、强势文化和弱势文化的问题，是信仰的坚定性问题。以及信仰自身和行动，还是信仰别人–消息、见解等。做长期的，要信仰长期价值，不轻易所动。但是都是自己去决断，而不受别人左右。要严格。对思路、逻辑的严格。</p>
<p>7、贫穷的本质，健康、教育质量、以及经济上的做法。经济上的问题，是本质。”电视机比食物更重要“：代表娱乐、希望，而越缺乏则越渴望。渴望的层级也只是到电视机的层次了。那么也就不可能像富人一样去思考向富裕进步。</p>
<p>富人不用太过节制欲望，而是将时间放在思考进步上。而穷人在一天劳作之后，根本只想享受。这样看，大多数的工薪阶层，都是穷人。因为他们只能通过放纵一些”消费“性欲望来排解。而不能像富人一样，相对自由，不那么报复性地”满足“，因为他们的消费欲望是不需要节制的。他们能够完全满足。</p>
<p>只有当你没有被限制，而一直是满足的状态，你才能够不去陷入。只要缺乏，必会陷入。越低层次的缺乏，则越低层次的渴望，激发出来报复性地满足，而不是正向地去思考脱离困境的方法。只有你相信能够脱离，才会慢慢地采取行动，哪怕是推迟那种强烈的渴望。否则，最多，也只是拥有一个电视机而已。</p>
<br>

<p>反馈机制：</p>
<p>没有按照原来的想法做。思考更重要。哪怕做了一点，是否有自己的思考。</p>
<p>对问题的总结就是反馈。思考和复盘都是有效的反馈。</p>
<br>





<p>业务指标：</p>
<p>大多的badcase，类型就几种：1 相关的（实时行为或者是）却没有及时地推出来；2 该排上来的没上来 – 低效；3 ee的部分做的不够，太过头部、指向指标，不利于长期 ； 4 gmv和其他指标的此消彼长，指标变动的解释</p>
<p>解决方案：1 没召回出来，要么是召回路数需要增加、在粗排排分时候考虑更多行为和交叉特征。多队列 ； 2 精排优化，或者精排后人工规则的重排 ； 3 指标之间的权衡，需要考虑如何在尽量不变成本的情况下，增加新策略的好处。即一个 f ；</p>
<p>总结，目前的一贯方式，要么是找到“公式”，也是一个分解目标的过程，拆解出来因子后，分析关系。构成f，即建模和进行假设。其次的方式，就是从头部角度去解释。比如，为什么某个没有上来，因为这个的整体转化低或者这个的整体ctr低，那么我不推其他的是更利于全局指标的。–即个例不能保证。因为有全局的目标。这就是整体上指标和个例的差异性。</p>
<p>大多数的人，基本对数据是不够了解的。最基本的，基础信息在哪里、埋点数据在哪里、有没有问题、有没有检验。以及指标的大概值。大部分的工作，也只是在追查而已。创造性不多。</p>
<h4 id="0623"><a href="#0623" class="headerlink" title="0623"></a>0623</h4><p>t检验中用户群pair</p>
<p>模拟交易和行情数据</p>
<p>35课</p>
<br>

<h4 id="0625"><a href="#0625" class="headerlink" title="0625"></a>0625</h4><p>1、我没有去看数据，再进一步思考和分析。包括一些正在出现的问题。没有去发现。</p>
<p>2、我的思考会漫游出去，但是很多相关是否真的有价值，还没来得及深入。这也显得自己特别不清晰。初心要坚持，一些自己总结的也应当在行为里避免。</p>
<p>3、下午来罗森和星巴克了。如果我工程上能够cover，策略上能够专注于数据找到问题。并且比他们更加敏感和迅速。为什么不被认可。承认自己的不足。对代码不够熟悉。没能一下子get。</p>
<p>3：50出来，15min在罗森，4：20到星巴克。</p>
<p>1h看了几页，和同事聊。5：20-6：20，买饭，菜。-7：10，看了企微。 3h，效率非常低。如果我不以15min</p>
<p>4、活动总结：</p>
<p>被推动的时候，我都是能够找到原因的。但是没有推动力，我总是在拖。我不想主动地投入。总觉得小事，不想去付出。</p>
<p>只是捡一些边边角角无关紧要的东西，扣一扣。太过因小失大。</p>
<br>

<br>

<p>初心：</p>
<p>1、和别人沟通，考虑这几点：逻辑是否清晰、表达是否扼要、是否本质。</p>
<p>2、开始一件事情：短期目的是、长期目的是</p>
<p>3、别人无法帮你决断。相信自己。并且看明白别人的优劣。</p>
<p>我能做的，更深刻提升我的处境的，有哪些。1搞清楚几个项目的结构和代码、努力去重构；2考研的事情；3投资回报–书</p>
<p>而我更多的时间花费在消费、琐事。我没有找到自己的重心。受”恐惧“桎梏。本质上，对金钱和地位的渴望，都来自于恐惧和贪婪。而对问题的解决、业务的精进、真理的探求、原因本质的追思，却都是过程里，需要去享受的。</p>
<br>

<p>核心在我如何找到自己更惬意轻松的思考状态。书写是的。但是与人沟通无法进行书写。</p>
<br>

<br>

<h4 id="0628"><a href="#0628" class="headerlink" title="0628"></a>0628</h4><p>1、独立和干货。要负责。</p>
<p>论文，整理，学习代码。考研。复习。</p>
<p>2、周末晚上睡不太着。青春不再。积累不够。飘摇。但是飘摇也应乐观，有坚持才能自信。</p>
<p>35课</p>
<p>3、发现还是没有应用。比如去看万达的时候，盘整、背离、趋势，这些概念还是没法说清楚。买点卖点如果不是对照着结局看，很难看到迹象似的。</p>
<p>重复的遍数少了。</p>
<p>4、发现一个道理</p>
<p>如果你能发现价值，无论是长期还是短期，只要你比别人看到真实的价值，基于你的经验、知识或者睿智，只要你能够评估和判断出来真正的价值或者看到不同、差值。就是你的价值。因为你可以利用。</p>
<p>而这才是核心。是关于理解、评判的核心。大的方向的把握。</p>
<br>

<h4 id="0629"><a href="#0629" class="headerlink" title="0629"></a>0629</h4><p>1、网络ch 1和2 ，ch3</p>
<p>2、35课</p>
<p>3、无穷级数和常微分方程  多元微分多元积分</p>
<br>

<h4 id="0630"><a href="#0630" class="headerlink" title="0630"></a>0630</h4><p>1、价值的识别</p>
<p>2、对记忆的提取，才能强化</p>
<p>3、我能感受到爷爷还是有感觉和思维的，只是开始对身体没有了控制能力。即使生活在一个屋檐下，也难以产生强烈的感情。爱。因爱而生的动力，才会让你一直想去床边陪伴。而我只是逃避。谁真的爱谁。是爱恐惧的抚慰还是爱那个人。</p>
<p>4、对言辞激动的最大化解，就是不要陷入，高一个阶级的胸怀和礼貌来对待，客观地描述和解释问题。不要遮掩。同样，最坏的情况，就是自己被带入。而情绪和思绪是别人供给的情况下，就陷入了很大程度的被动。</p>
<p>5、你不懂电动车，买什么byd</p>
<p>6、风口、创新和品味、开放、表达、长期主义、品牌</p>
<p>雷军(人情味 文化 低调 顺势而为 风口)、乔布斯(品味 抓住机会 坚持做 产品的文化 品牌)、段永平(有想法讨论的时候 他很直接，并且很接地气，没有空的 考虑价值的部分 和雷军不太一样的风格 )、曹德旺(很实在 ，出轨也坦然地讲，毕竟的确是人生感受，没有什么可忌讳的)、巴菲特(沟通 – 把你的想法都表达出来，不必要留着 ； 对身体和大脑 物尽其用；选择优秀的人结婚 – 都很现实的价值观，爱优秀的人总是更好的； 从小公司入手、没人能够告诉你)、张磊(重仓中国 长期主义理念–初心的坚持(王兴)即足够专注、持续学习)</p>
<p>7、发现巴菲特非常能够了解死人的思维。</p>
<p>8、穷人，输的都是心态</p>
<p>9、每年10%，持续10年，不容易。但是一次2倍，持续2次，就 远 高于10年的。勤奋固然重要，但是功夫都在之外。风口和顺势的眼光，更加重要。</p>
<p>10、李永乐的小知识课堂</p>
<p>​    1.地球半径测量：关键在于引力常数 – 实验</p>
<p>​    2.基尼系数：对角线和曲线面积 比上 洛伦兹曲线下的面积。香港&gt;大陆&gt;台湾，台湾最低，相对公平 。这个可以和ck的1% 的 1%联系。100w美金的已经是世界的1%。要知道1%并不是尖子生，是近7000w人。其中，高于500w美金的，就是70w人</p>
<p>​    3.北京摇号：数学上算政策给的几率。</p>
<p>​    4.买股票的为什么总是输：</p>
<p>​        1)可以出正面和反面，各自采取策略。概率上看正+正，反+反和正+反的概率是一样的。但是给的效果不一样，即正正3，反反1，正反-2，这样的话，就可以控制期望了。所以期望比概率更重要。</p>
<p>​        2)这个和赔率的概念是相近的，即高盈亏比、低胜率，收益会高的，可以接收盈亏各半，但是只要控制回撤即可。这是有数学逻辑的：信仰价格投机+截断亏损+让利润奔跑。</p>
<p>​        3)那么庄家是可以拉升和压低的，即正或者反，而散户可以买多也可以卖空(相当于空仓吧)，即正和反。拉升和做多，则散户收益为3，拉低和做空则收益为1，反之则输-2。立论是，庄家可以控制期望。</p>
<p>​        4)这个是博弈论。首先散户是不知道具体数值的，就很难猜测到庄家的策略。</p>
<p>​        5)没有期望大于0的理论上证明么？ – 这个分析技巧还是应该具体分析的。有漏洞…</p>
<p>​    5.贫穷的本质：”下班了就想着满足下疲惫的自己“，就是穷人思维了。毕竟富人都是享受工作的。</p>
<p>​    6.虎门大桥的抖动：物理上解释。</p>
<p>​    7.退税：数学上算政策给的几率。</p>
<p>​    8.最有钱的公司 以及泡沫：最高都是7w亿级别的。全民财富都在这里吧。当时人们不懂。多了几次，到现代互联网普及就懂了。但是这些概念 比如国债转股、股价上升到泡沫破裂，又被包装起来，重新售卖。都是从被收割到收割。</p>
<p>​    9.切尔诺贝利:。宇称不守恒：实验挺有意思的。但是没深究。墨菲定律:。粒子对撞机:</p>
<p>11、youtube</p>
<p>基本锁定在 文昭、破空、ck go、缠变、chemath？、天狗、温相…也是头部。</p>
<p>一些历史的、投资的、政治的。</p>
<p>12、b的季度会</p>
<p>13、不同博客，不同文章，不同的人的断片，或者只言片语的，或者十几秒的视频，或者几个问答的采访。都可以给以灵感和启发。生活处处是启发。无字之书，时刻在无言地谆谆教诲你。这些散落的发光点和思路，难以被收纳，难以形成记忆和直觉，没有能够及时地被提取。无字之书需要梳理，但是如果大部分都沉浸在对此的分析之中，难免纷杂。因为没有框架。并且各自不够有逻辑地被融合。</p>
<p>如果能将这些点点滴滴融入到已有的框架里，更能够被提取。</p>
<p>14、林园</p>
<p>15、张磊、段永平</p>
<p>16、马男</p>
<p>​    影视行业：PE PB ，增长预期、周期性、技术壁垒、优质公司韧性、类刚需产品的长期复苏逻辑、</p>
<p>17、招财大牛猫</p>
<p>18、清华游资女</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>2021-09下</title>
    <url>/2021/09/22/2021-09%E4%B8%8B/</url>
    <content><![CDATA[<br>

<br>

<h4 id="0922"><a href="#0922" class="headerlink" title="0922"></a>0922</h4><h5 id="骑手谜云：法律如何打开外卖平台用工的「局」？"><a href="#骑手谜云：法律如何打开外卖平台用工的「局」？" class="headerlink" title="骑手谜云：法律如何打开外卖平台用工的「局」？"></a>骑手谜云：法律如何打开外卖平台用工的「局」？<span id="more"></span></h5><p>1、外卖骑手商业化模式演进：</p>
<blockquote>
<ul>
<li><p>餐馆则会自行雇佣员工进行配送</p>
</li>
<li><p>餐馆开始与外卖平台合作，在平台上发布信息，由外卖平台统一提供配送服务。劳务派遣用工曾经猖獗一时，被众多企业用来规避用人单位的法律责任。但没过多久，相关法律的修订对劳务派遣用工施加了一系列限制，如今劳务派遣的落地场景十分有限，因此也就「不太好用」。</p>
</li>
<li><p>外卖行业发展中期，各外卖平台的竞争进入白热化阶段。若要提升核心竞争力（外卖配送时效和准时率），最直接的方式当然是招募更多的骑手；但这将导致用工成本大幅增加。为此，外卖平台开始优化运力池（骑手）结构，以求在配送体验和配送成本之间找到最佳的平衡。</p>
</li>
<li><p>2015年10月左右，出现了「众包」这一新型用工模式。起初，平台往往会直接招募众包骑手，与众包骑手签订合作协议，并为其购买意外险（模式4）。</p>
</li>
<li><p>但很快，外卖平台开始与众包服务公司合作，由众包服务公司与众包骑手签订协议、支付报酬、购买保险（模式5）。原本由外卖平台承担的成本和风险成功转嫁给众包服务公司。</p>
</li>
<li><p>为了进一步降低成本、区隔风险，外卖平台开始与配送商（即劳务外包公司）合作，将配送业务「外包」，由配送商招募专送骑手并对其进行直接的日常管理（模式6）。</p>
</li>
</ul>
</blockquote>
<p>从法律上来说，模式6的出现是真正意义上的里程碑。</p>
<p>与众包骑手不同，专送骑手是受到劳动法全面保护的固定工，因此将专送骑手外包给配送商之后，外卖平台实打实地节省了约40%的成本（包括社保成本和法律风险），从此坐稳了「讲究轻资产，追求高毛利和边际效应」的互联网平台方地位。</p>
<p>外卖平台由直营（模式2）转为外包（模式6）的根本逻辑是为了彻底甩掉高昂的人力成本、规避与骑手签订劳动合同可能带来的法律风险和责任义务。可如此一来，相应的成本和风险便落到了配送商的头上。</p>
<p>配送商为了继续向外「甩锅」，开始将全部或部分配送业务「转包」或「分包」给其他多个公司甚至个人 </p>
<p>最终形成的是外卖平台联合多家公司对骑手进行共同管理的网络状外包模式（模式7）。</p>
<p>2、由于中间多出了交易成本，它们只会变得越来越烫，并迫使配送商继续向外抛出。而抛到最后，「烫手的山芋」只能由骑手自己吞下去——外卖平台用工模式演变的终极形态「个体工商户模型」也就此诞生（模式8）。</p>
<p>3、一步先手：</p>
<p>仲裁都赢了，怎么到法院又输了呢</p>
<p><strong>当被问及跟谁签了劳动合同时，邵新银一脸茫然。</strong></p>
<p><strong>按照迪亚斯公司在重庆法院的说法，其已经把相关配送业务「外包」给太昌公司了。可是邵新银从始至终都没怎么听说过这家公司。</strong></p>
<p>按照我国税法的规定，个税扣缴义务人必须是支付其工资薪金所得的单位，一般而言就是用人单位。但邵新银每月工资薪金的个税扣缴义务人至少有2-3家公司，其中不但有迪亚斯公司和太昌公司，还有一些邵新银此前从未听闻的公司，比如天津某建筑公司、上海某外包公司。</p>
<p>「劳动仲裁败诉方才能提起法院诉讼」的规则，以及当初迪亚斯公司输了北京的劳动仲裁后立刻选择回到重庆法院起诉的整套「一气呵成」的操作。</p>
<p>4、因为在我国法律框架下，一旦成为「自担风险、自负盈亏」的个体工商户，就意味着被剥夺了「劳动者」的主体资格，不再可能受到劳动法的保护。<strong>实际上，对于任何一位稍微懂点公司法和劳动法的律师来说，「个体工商户」都可能意味着一场同时「避税」和「避社保」的架构操作。</strong></p>
<p>5、一位曾在外卖平台发展初期参与过相关架构设计的公司律师对我们说，「<strong>曾经有客户问我中国目前最大的法律漏洞是什么，我毫不犹豫地告诉他，是劳务外包。</strong>」</p>
<p>「在资本眼中，他们不是需要珍惜的资产，而是急欲摆脱的负债；与其把他们当作一个个活生生的个体，不如用一串数字符号代替来得方便。」「我本来不该说这么多，」公司律师突然停下来叹了口气，「但就当是赎罪吧。」</p>
<p>6、<strong>如今，配送商早已退化成从中抽佣赚钱的「二道贩子」，不与专送骑手建立正式的劳动关系，从而逃避社保缴纳义务。</strong>同时由于配送商市场极其脆弱，骑手权益受到侵害时，许多配送商根本无力承担相应的赔付义务。</p>
<p>7、尤其当发生严重事件时，众包服务公司自身都朝夕难保，遑论骑手的权益保障。此外，众包服务公司更换频繁，许多公司与外卖平台的合作协议不足1年，这无疑增加了众包骑手后续的维权成本。</p>
<p>8、法院会视「场景」严重程度（人身损害抑或财产损害、伤残等级等）决定是否穿透整套法律安排以追究相关主体的责任。「总之我们可以帮您降低概率」，而降低概率就意味着降低成本和风险。</p>
<p><strong>当平台用工迭代和升级到模式7（网络外包）和模式8（个体户）后，我们清晰地看到配送商将承担用人单位责任的概率从82%成功地降到了46-59%，而外卖平台则基本控制在1%以内。</strong></p>
<p>一位前法官对此解释道，「我们只敢追到配送商这一级。」</p>
<p>9、资本市场对于外卖盛世之下的趣活始终犹疑不决</p>
<p>天风证券在同年发布的趣活报告中写下了这样一条政策风险提示：「公司与平台劳动者是业务外包关系，若相关政策变动，公司用人成本可能承压。」<strong>如此推断，投资者们似乎早已洞悉这是一场针对国家政策的「豪赌」，只是看破不说破罢了。</strong></p>
<p>10、「全球最大的出租车公司Uber没有一辆出租车；全球最热门的媒体所有者Facebook没有一个内容制作人；全球市值最高的零售商阿里巴巴没有一件商品库存；全球最大的住宿服务提供商Airbnb没有任何房产。」</p>
<p><strong>「平台经济会透支社会保障」</strong></p>
<p>11、人社部开始在全国范围内调研企业用工灵活化的具体情况：出现了「核心员工合伙化、 非核心员工合作化」的情况，去劳动关系化趋势显现。</p>
<p>12、传统经济遭遇平台经济和疫情的冲击后劳动力从二产、三产转移到外卖平台。而且可以想见，这些骑手原本拥有固定的工作，且这些工作往往能提供社保等一系列劳动权益。</p>
<p>尽管目前大多数国家对「依赖性自雇者」均没有相关立法（因其所占比例不高），但基于我国的劳动力结构和平台经济的蓬勃发展趋势，「依赖性自雇者」人数将越来越多，这也就意味着我国必须先于国际实践、积极探索对「依赖性自雇者」的权益保护。</p>
<p>13、在我们收集外卖骑手相关的数据过程中还惊讶地发现，美团研究院每年公布的骑手工作时间远低于各类研究机构报告所显示的工作时间；此外，美团在公布骑手收入时并未对全部数据进行公开，而是有意在专送骑手中抽样。两相对比，读者几乎必然产生「骑手整体工作时间短、收入却很高」的错觉。</p>
<p>14、两年前短暂探索过的「Uber Works」似乎是在往反方向退化成对劳动力毫无掌控的单纯的信息中介平台。</p>
<p>我们需要反思的是，作为平台经济祖师爷的Uber自上市后一路坎坷，而我们的外卖企业为何却如火箭般一飞冲天。</p>
<br>

<br>



<h5 id="屌丝十九问-F2230"><a href="#屌丝十九问-F2230" class="headerlink" title="屌丝十九问 #F2230"></a>屌丝十九问 #F2230</h5><p>1、产权和折现因子</p>
<p>因为不管他怎么算，他采取的“贴现率”一定是错误的。不管他采用的10%，7%，或者5%利率贴现，这个回答一定是错的。真正的贴现率，应该是0%甚至-2%</p>
<p>2、权威型和宽容型在认知能力和非认知能力的应用？</p>
<p>3、子产不毁乡校历史</p>
<p>4、但这个标准答案是错误的，真正的货币金融学，应该是“货币中立论”。</p>
<blockquote>
<ul>
<li>央行加息，企业不变</li>
<li>央行加息，金融市场也不变。</li>
</ul>
</blockquote>
<p>5、标准答案是“比较优势”。A国完全注重高端产业，B国完全注重低端产业，再进行贸易。</p>
<p>而真正的答案，应该是“经济学第二定律”，R&gt;T。B国依然会把持军工业不可开放。安全不可寄托于他人之手。哪怕损害效率。</p>
<p>经济学第二定律：</p>
<blockquote>
<p>经济学第一定律：dT &gt; 0 ： 交易创造财富</p>
<p>经济学第二定律：市场创造一切，除了市场本身。暴力是能够打破市场规律的，因为他反抗的是整个市场本身。在做事的时候需要注意不被原有的规则所限制，因为规则都是可以被颠覆的。最好能做一个创造新市场，定义市场规则的人。</p>
<p>经济学第三定律：R≠0(R代表抢劫) 历史演化的结果，一定是剩下一个最大暴力，而不是毫无暴力。暴力无法根除，只能被尽量削减。</p>
</blockquote>
<p>6、通胀其实很难用CPI来表示，因为CPI基本都是日常消费品，而没有计算房产和证券。可参考：M2增速减去GDP增速。但目前M2的数据可能也会被篡改，我们更好的是参考“金融业总资产”这个数值。</p>
<p>*“金融业总资产”<a href="https://zhuanlan.zhihu.com/p/65113197">https://zhuanlan.zhihu.com/p/65113197</a></p>
<p>7、简单理解：经济学是研究“资源配置”“统筹规划”“物资调度”的科学。把最合理的生产要素捏合起来，大工业高效率，社会分工。就可以产出最大效率。 更多的理解：“我愿意放弃什么，我想要得到什么”。</p>
<p>8、微观经济学，指的是“为人处世”，“在储蓄和消费之间选择”，“在价格和品质之间选择”的一门学问。微观有序，宏观无序—— 资本论</p>
<p>9、一眼就可以看出这个人“基本功不扎实，逻辑推理断层。肤浅得意炫耀”。门门都是皮毛，你还不如专精一课。志大才疏，不失败才见鬼呢。</p>
<p> 郭嘉说袁绍“色厉胆薄，好谋无断”。早早预言了他的失败。</p>
<p>10、传统教育认为，大学是一种“博雅”。致力于掌握广泛的知识面，融会贯通，并自我发展出新的思考。</p>
<p> “博雅”这个要求，其实是一种贵族要求。门槛高得<del>高得</del>高得吓死人。只有极极极极少数人能做到。</p>
<br>

<br>



<h5 id="拿破仑是继承了还是颠覆了法国大革命"><a href="#拿破仑是继承了还是颠覆了法国大革命" class="headerlink" title="拿破仑是继承了还是颠覆了法国大革命"></a>拿破仑是继承了还是颠覆了法国大革命</h5><p>1、在他之前的形态：</p>
<p>1794之后到1799：热月党建立督政府，三分之二从当前公会选 ，引起保王党的起义；督政府完全没有倾向，只是杀强党。</p>
<p>经济上，土地购买券的一次性支付带来恶性通胀。收间接税等。。</p>
<p>英国的第二次反法同盟，全战线失败。</p>
<p>2、为什么他能赢得地位：</p>
<p>拿完成了法国政府的收支平衡。</p>
<p>此时的军队不同，战争锻造了他们，他们成为拿破仑最为忠诚的盟友。</p>
<p>民法典的颁布。确认了资产阶级在这一过程获得土地的合法性。新的社会秩序被确立起来。</p>
<p>再次建立了天主教与国家事务的联系。</p>
<p>奴隶制被恢复。</p>
<p>拿破仑以名流的名义恢复了贵族等级。拿破仑将归来的流亡贵族与拥有土地的资产阶级融合为新的贵族，和军队共同构成了帝国统治的支柱。</p>
<p>选择取消所有有意义的政治参与，投票和政治俱乐部让位于强大的政治宣传，积极参与让位给被动旁观，它保留了人民主权的原则却声称自己就代表了人民的声音。</p>
<p>3、形势上的原因：</p>
<p><strong>与英美不同，在法国革命的语境下，政党意味着勾心斗角与私人利益，因而他们从革命一开始，便排除了政党政治的可能</strong></p>
<p>与英国的封建自由不同，法国在路易十四时代以来便建立了绝对君主制，在法国的政治体制内，从未出现过英国式的议会。在正式议会场所缺失的情况下，法国形成了另外一种政治空间，即民间社交团体，他也成为雅各宾俱乐部的前身。在这种俱乐部中，受过教育的精英阶层宣扬启蒙思想、评论公共政治，由此形成一套公共舆论，到旧制度后期，公共舆论逐渐成为合法性与权威性的基础，甚至于国王也不得不讨好公共舆论。</p>
<p><strong>“场所塑造行为”的发生学理论而言，这种不同的舆论场所，导致法国形成了与英美不同的政治理念。</strong></p>
<p>他们相信国家和民族将被重塑，人与人之间将真诚而透明，与英国议会中精英阶层就具体政策激烈争辩不同，<strong>在法国，辩论让位于演说、差异让位于公共、妥协让位于理想。</strong></p>
<p>4、总结：</p>
<blockquote>
<ul>
<li>他对于民主选举的信仰远不如秩序与荣耀坚定，拿破仑适时的出现，满足了社会精英对秩序的渴求，他以自己的方式巩固了革命，或许可以说他拯救了作为革命载体的法国，但绝不能说他拯救了革命</li>
<li>即使他成为了皇帝，共和国的理念依然在运行，即使他恨透了议会体制，他依然不能将其取消，在经历了那一场革命后，任何一个统治者都必须遵循宪法和人民。<strong>所以真正拯救革命的，恰恰是革命者自身</strong>。</li>
<li>革命者在演说、装扮、节庆的过程中传播了革命价值，也动员起更多的人民参与政治，从此政治不再是国王、贵族甚至革命者的特权</li>
<li>“她是我们所有人的母亲。”</li>
<li></li>
</ul>
</blockquote>
<br>

<br>



<h5 id="深度学习与计算机视觉系列-8-神经网络训练与注意点"><a href="#深度学习与计算机视觉系列-8-神经网络训练与注意点" class="headerlink" title="深度学习与计算机视觉系列(8)_神经网络训练与注意点"></a>深度学习与计算机视觉系列(8)_神经网络训练与注意点</h5><h6 id="1-1-关于梯度检验"><a href="#1-1-关于梯度检验" class="headerlink" title="1.1 关于梯度检验"></a>1.1 关于梯度检验</h6><p>1、比对数值梯度和解析法求得的梯度，<strong>使用中心化公</strong></p>
<p>2、<strong>使用相对误差做比较</strong>。1<em>e</em>−7&gt;相对误差，放心大胆使用。随着神经网络层数增多，相对误差是会增大的。这意味着，对于10层的神经网络，其实相对误差也许在1e-2级别就已经是可以正常使用的了。</p>
<p>3、<strong>使用双精度浮点数</strong>。<strong>要留意浮点数的范围</strong>。我们得保证计算时，所有的数都在浮点数的可计算范围内，太小的值(比如h)会带来计算上的问题。</p>
<p>4、Kinks。它指的是一种会导致数值梯度和解析梯度不一致的情况。会出现在使用ReLU或者类似的神经单元上时，对于很小的负数，比如x=-1e-6，因为x&lt;0，所以解析梯度是绝对为0的，但是对于数值梯度而言，加入你计算f ( x + h ) f(x+h)f(x+h)，取的h&gt;1e-6，那就跳到大于0的部分了，这样数值梯度就一定和解析梯度不一样了。</p>
<p>而且这个并不是极端情况哦，对于一个像CIFAR-10这样级别的数据集，因为有50000个样本，同时每个样本会对应9个错误的类别(给损失函数贡献9个loss值)，会有450000个 max(0,x) ，会出现很多的kinks。</p>
<p>不过我们可以监控m a x maxmax里的2项，比较大的那项如果存在跃过0的情况，那就要注意了。</p>
<p>5、设定步长h要小心。因为h太小程序可能会有精度问题。很有意思的是，有时候在实际情况中h如果从非常小调为1e-4或者1e-6反倒会突然计算变得正常。</p>
<p>6、不要让正则化项盖过数据项。有时候会出现这个问题，主要的梯度来源于正则化项，那这样根本就做不到正常的梯度回传和参数迭代更新。</p>
<p>7、注意dropout和其他参数。</p>
<h6 id="1-2训练前的检查工作"><a href="#1-2训练前的检查工作" class="headerlink" title="1.2训练前的检查工作"></a>1.2训练前的检查工作</h6><p>1、<strong>在初始化之后看一眼loss</strong></p>
<p>2、加回正则项，接着我们把正则化系数设为正常的小值，加回正则化项，这时候再算损失/loss，应该比刚才要大一些。</p>
<p>3、试着去拟合一个小的数据集。最后一步，也是很重要的一步，在对大数据集做训练之前，我们可以先训练一个小的数据集(比如20张图片)，然后看看你的神经网络能够做到0损失/loss(当然，是指的正则化系数为0的情况下)，因为如果神经网络实现是正确的，在无正则化项的情况下，完全能够过拟合这一小部分的数据。</p>
<p>todo</p>
<br>

<br>





<h4 id="0927"><a href="#0927" class="headerlink" title="0927"></a>0927</h4><br>

<p>一天不知道做了啥。列的计划一样没写。看看这里看看哪里？</p>
<p>面试很大的一个感受是，多准备面试的问题，才是核心。对问题的解决和深入才是核心。</p>
<br>

<p>写个总结。</p>
<p>年初至今，已经10个月。</p>
<p>1月开始准备面试。到4月22来到b。3月开始面了的吧。来了b后，已经快半年。小部分时间写了一些业务代码。大部分在搞这个模型。心累在于成长慢。</p>
<p>5 6 月在b，会思考一些技术问题。</p>
<br>

<p>开始在算法上去下了点功夫。但是也不够深入。基础不扎实也是事实吧，短时间，又想搞工程，又想搞算法积累，又想刷题。</p>
<p>开始 对财经评论，进行持续关注。看了几个公众号的文章。更多知乎。还有几本书。缠，缠解论语。还有一些零零散散的。</p>
<p>开始看房子。</p>
<p>是有一定变化的。对现实生活更加理解了些。也能够关注到正在发生的变化。虽然对信号无法形成自己的解读。开始对职场有新的理解。</p>
<br>

<p>7 8 9月开始颓丧。停止了健身。停止了对技术的热切。</p>
<p>爷爷的离世，对我的打击很大。跟越多人接触，我越能看到自己是什么样子的状态。</p>
<p>对未来的期待和自己的想象越来越少。不再渴望完成什么。我只想好好地研究投资。不要在乎其他事情。能够对未来有基本的保障。因为太多的事情，几乎都是颅中高潮。</p>
<br>

<p>核心，你能不能知道这些，能不能解决问题，能不能有勇气，能不能得到成绩。</p>
<br>

<p>关于投资：</p>
<p>这是一件值得付诸兴趣的事。</p>
<p>孤独。但是有价值。也是追求。有风险。足够当下。足够磨炼。</p>
<br>

<p>关于考研：</p>
<p>可以上。没有太大决心。给了太多退路。</p>
<p>我希望这个过程，我能找到一个清晰简单的路子。比如，不要死记。而从自己出发去构建。并且放松地重复。</p>
<p>家庭也影响很多。相爱的过程，陪伴，都会侵蚀你对问题的执著。</p>
<p>我不觉得自己比他们任何人弱。但是要想到我自己的坑，自己的缺陷能不能。</p>
<br>

<p>没有必要加入讨论。自欺欺人。但是也有必要。因为激发。</p>
<p>激发很重要。所以重视激发就好。重视点，分析的支撑。而不要太过在意于眼光。</p>
<br>

<br>







<h4 id="0928"><a href="#0928" class="headerlink" title="0928"></a>0928</h4><p>1、复权</p>
<p>转送后除权。10送15，股价相应地变为原来的2/3。同时成交量也调整✖️1.5。</p>
<p>前复权是缩减以前的价格。以目前的股价为基准。</p>
<p><strong>前复权公式：复权后价格=(复权前价格-现金红利)/(1+流通股份变动比例)</strong></p>
<p>填权主要指在除权除息后的一段时间里，如果多数人对该股看好，该只股票交易市价高于除权（除息）基准价，这种行情称为<strong>填权</strong>。</p>
<p><strong>贴权</strong>是指在除权除息后的一段时间里，交易市价低于除权（除息）基准价，即股价比除权除息日的收盘价有所下降。</p>
<p>2、考研安排</p>
<p>90天。四本书+高数整理和真题。</p>
<p>必须一天一章节了。至少了。一个月能过完。加上复习数学1-2章。</p>
<p>政治每天带着点吧。每天50题。刷三遍吧。</p>
]]></content>
  </entry>
  <entry>
    <title>SVM</title>
    <url>/2021/02/04/SVM/</url>
    <content><![CDATA[<br>


<ul>
<li><p>reference：[非常好的两本书。再加上libsvm的源码与调参的论文。]<br>[1]<a href="http://files2.syncfusion.com/Downloads/Ebooks/support_vector_machines_succinctly.pdf">http://files2.syncfusion.com/Downloads/Ebooks/support_vector_machines_succinctly.pdf</a><br>[2]An Introduction to Support Vector Machines and Other Kernel-based Learning Methods<br>[3]<a href="https://pan.baidu.com/share/link?shareid=262520779&amp;uk=1378138793">https://pan.baidu.com/share/link?shareid=262520779&amp;uk=1378138793</a></p>
</li>
<li><p>干货</p>
</li>
<li><p>首先，SVM是解决supervised learning 中classification问题。有两种情况，看是否linearly separable。线性不可分则引入kernel，想法为先做transformation到其他空间进而转为可分问题。</p>
</li>
<li><p>对于线性可分的监督分类问题，SVM的目标是什么呢? find  the optimal separating hyperplane which maximizes the margin of the training data</p>
</li>
<li><p>为什么以最大化间隔为目标？因为it correctly classifies the training data and because it is the one which will generalize better with unseen data</p>
</li>
</ul>
<span id="more"></span>
<ul>
<li><p>这里的 间隔 指？关于间隔涉及到两种分类，一种分类为几何间隔与函数间隔；一种为软、硬间隔。几何间隔在二维则为点线距离，三维空间就是我们学习的点面距离。函数间隔二维中可以理解为<em>几个</em>点没有在线上，三维则为<em>几个</em>点没有在面上；或者结合几何间隔可理解为，是将几何间隔求解的分母去掉了，没有归一化(也因此SVM中不能选择以函数间隔衡量，否则maximizes是没有意义的)。关于软硬，是看噪声，There will never be any data point inside the margin.  If data is noisy, we need soft margin classifier.</p>
</li>
<li><p>继上面的目标，假设该超平面的公式为W*X=0，这里会有疑惑：</p>
<ul>
<li>Why do we use the hyperplane equation W<em>X instead of   Y=a</em>x+b? –&gt; the vector W will always be normal to the hyperplane</li>
</ul>
</li>
<li><p>澄清下要做的步骤：<br>  1 数据集<br>  2 选择两个超平面能够分类数据并在两平面间没有其他点<br>  3 最大化超平面间隔</p>
</li>
<li><p>将步骤整理成数学过程</p>
<ul>
<li><p>设两个超平面， W<em>X+b = -θ  和  W</em>X+b = +θ。这里，为什么我们需要两个超平面？我们设想的是，假定最佳的超平面在这两个超平面的中间。我们求得两个超平面即可求得最佳分类超平面。</p>
</li>
<li><p>θ取值无关，直接设为1。 即得W<em>X+b = -1  和  W</em>X+b = +1。这里要想明白W与b到底是什么关系？b依赖还是独立于W？显然，是独立的，可以想象为，我们需要求得W与b两个变量，能够最大化间隔。</p>
</li>
<li><p>需要满足两个约束: 1. 任何&gt;=1的为class 1 2.任何&lt;=-1的为class -1 –&gt;这个限制使在两平面间没有其他点</p>
</li>
<li><p>将两个约束写为一个式子即： y*(w*x+b)&gt;=1</p>
</li>
<li><p>最大化间隔 ？对于这个问题，目前我们已知条件是两个。一个是两个平面 W<em>X+b = -1  和  W</em>X+b = +1。一个是有一个点x在平面  W<em>X+b = -1 上</em>。得：<br><code>w*(x + m*w/||w||)+b=1</code><br>化简得 <code> m = 2/||w||</code></p>
</li>
<li><p>得到的公式意味着：如果||w||没有限制，那么m我们可以取得任意大的值。</p>
</li>
<li><p>现在自然就面临optimization problem。所有的点subject to  y*(w*x+b)&gt;=1, 在此条件下如何minimize ||w||?先引入<strong>理论1</strong>，该理论为两个条件，在两个条件满足的情况下，可以说我们得到了一个scalar function的local minimum。</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2021/02/04/SVM/theorem1.png" alt="theorem1"></p>
<ul>
<li>f为从集合σ(其元素为vector)到实数集(其元素为值)的映射，且在x处 连续、可二阶导。这里涉及到两个的概念：<ol>
<li>**gradient ：a generalization of the usual concept of derivative of  a function in one dimension to a function in several dimensions  ( the gradient of a function is a vector containing each of its partial derivatives.)**注意符号为 nabla,图中倒三角。 </li>
<li><strong>scalar function：A scalar valued function is a function that takes one or more values but returns a single value. In our case f is a scalar valued function.</strong></li>
</ol>
</li>
<li>positive definite：<br>A symmetric matrix A  is called positive definite if x.T<em>A</em>x&gt;0 , for all n维的实数向量x。</li>
<li><strong>theorem 2</strong>中的四个条件是等价的。因此可以通过其他三种情况来判断是否为正定。这里选择主子式来判断Hessian正定，涉及到三个概念：</li>
</ul>
<p><img src="/2021/02/04/SVM/theorem2.png" alt="theorem2"></p>
<ol>
<li>Minors： 删除某行和某列的所有值再计算行列式。remove the ith line and the jth column</li>
<li>Principal minors ：删除的行、列号一致。remove the ith line and the jth column and i=j</li>
<li>Leading principal minor ：The leading principal minor of A of order k is the minor of order k obtained by <strong>deleting the last n−k rows and columns</strong>.（这里包含一个正三角符号，标注删除哪些行列）栗子看图Leading principal minor</li>
</ol>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-42fb025afc8f45d5.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="leading principal minor.jpg"></p>
<ul>
<li>得到了local minimum，How can we find a global minimum?两步走， 1. Find all the local minima 2. Take the smallest one; it is the global minimum. 另一个思路是看我们的f是否是<strong>convex</strong> ,是then we are sure its local minimum is a global minimum.</li>
</ul>
<p><strong>Theorem: A local minimum of a convex function is a global minimum</strong> 这里又涉及到convex function, convex set的定义。</p>
<ul>
<li>What is a <a href="http://mathworld.wolfram.com/ConvexFunction.html">convex function</a>? A function is convex if you can trace a line between two of its points without crossing the function line.<br><img src="http://upload-images.jianshu.io/upload_images/8716089-fd9cfea722d77a1d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="convex  0&lt;λ&lt;1.jpg"><blockquote>
<p>A function is convex if its epigraph (the set of points on or above the graph of the function) is a convex set. In Euclidean space, a convex set is the region such that, for every pair of points within the region, every point on the straight line segment that joins the pair of points is also within the region. </p>
</blockquote>
</li>
</ul>
<p>栗子看图convex set</p>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-b484ebf0cfdf222d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="convex set.jpg"></p>
<ul>
<li>同样根据Hessian判断是否convex，这里需要看是否是<strong>positive semidefinite</strong>,而semi有对应三个条件是与之等价。看 theorem 3<blockquote>
<p>**More generally, a continuous, twice differentiable function of several variables is convex on a convex set if and only if its Hessian matrix is positive semidefinite on the interior of the convex set.**The difference here is that we need to check all the principal minors, not only the leading principal minors. </p>
</blockquote>
</li>
</ul>
<p>  <img src="http://upload-images.jianshu.io/upload_images/8716089-744d4e22dac7d5aa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="theorem 3.jpg"></p>
<ul>
<li>其中on the interior of the convex set是什么意思呢？定义：the domain of a convex function is a convex set，那么 a function is convex on a convex set意思就是在domain上是convex function，而interior只是意味着为两边开区间。</li>
<li><a href="http://www.math.cmu.edu/~ploh/docs/math/mop2013/convexity-soln.pdf">其他证明是convex function的方法</a></li>
<li>这里就谈convex function的optimization问题求解。涉及对偶概念，根据wiki，<blockquote>
<p>Duality :duality means that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem (the duality principle). The solution to the dual problem <strong>provides a lower bound</strong> to the solution of the primal (minimization) problem. </p>
</blockquote>
</li>
</ul>
<p>  给最小值以下限。lower bound中有一个值为<strong>infimum</strong> (即 greatest lower bound)。补充，相对而言</p>
<blockquote>
<p>The same logic apply with the relation “greater than or equal” and we have the concept of upper-bound and supremum.</p>
</blockquote>
<ul>
<li>对偶，在求最小值时求对应的最大值，求出的最大值将是=&lt;最小值，两者之差即为<strong>duality gap**。对应来说，在求最大值时求对应最小值，求出的最小值将是&gt;=最大值即upper bound。</strong>duality gap<strong>为正，我们称之</strong>weak duality holds<strong>，为0则为</strong>strong duality holds**。</li>
<li>拉格朗日乘子 ： <blockquote>
<p>In mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to <strong>equality</strong> constraints. </p>
</blockquote>
</li>
</ul>
<p>  <img src="http://upload-images.jianshu.io/upload_images/8716089-59b1f2660a4869b6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Lagrange_portrait.jpg"></p>
<ul>
<li><p>如何将3D图在2D平面表示：Contour lines  两个要点：1. 线上的点z值不变，for each point on a line, the function returns the same value 2. 颜色扮演标识，the darker the area is, the smallest the value of the function is<br><img src="http://upload-images.jianshu.io/upload_images/8716089-271a7e44b55a0424.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="contours.png"></p>
</li>
<li><p>那么梯度就可以用<strong>向量场</strong>进行可视化。箭头指向函数增长的方向。与Contour lines 图有什么关系呢？在Contour lines 图中，gradient vectors非常容易画出，1 垂直于Contour lines 2.指向增加的方向。<img src="http://upload-images.jianshu.io/upload_images/8716089-8ee3efb4b3646c71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="gradient_and_contour.png"></p>
</li>
<li><p>将约束函数和优化目标函数以contour lines 画在一幅图中，并画出两者的gradient vector。可得到最优点。图中的优化目标函数为x^2+y^2, 约束函数为 y=1-x。 <img src="http://upload-images.jianshu.io/upload_images/8716089-fc1f3051bbf27c8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="function_and_constraint.png"></p>
</li>
<li><p>▽f(x,y) = λ ▽g(x,y)<br>λ 这就是拉格朗日乘子。根据图中，当两个gradient vector平行时，我们得到最优解。无论是否同向。更无论是否等长。乘以λ 即意味着不必等长。即求▽L(x,y,λ )=0时的x，y。现在我们需要列出L并求解。</p>
</li>
<li><p>由于我们需要求f(w)=1/2*||w||^2的最小值，将每个约束函数乘以的 λ需要取正数。<img src="http://upload-images.jianshu.io/upload_images/8716089-76c3fc3fc413375d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="L.jpg"></p>
</li>
<li><p>又面临一个问题，求L(x,y,λ )=0， the problem can only be solved <strong>analytically when the number of examples is small</strong> (Tyson Smith, 2004 即只有当约束函数数量比较小的时候，λ 个数不多，我们才能用分析的方法求解). So we will once again rewrite the problem using the duality principle–&gt;we need to minimize with respect to w and b, and to maximize with respect to a at the same time.我们在上一步需要最小化<br><img src="http://upload-images.jianshu.io/upload_images/8716089-4ca65da333340058.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="duality_after_L.jpg"></p>
</li>
<li><p>这里需要讲清楚三个问题。1. 拉格朗日是对约束函数是等式的情况，那么我们在这里是不等式约束的问题，也用拉格朗日乘子解决，需要满足什么条件吗？(KKT)2.之前说了，对偶问题有强与弱，只有当强时，gap才为0，我们才能将对偶问题的最大值作为原问题的最小值。那么，这里是否满足是strong duality holds? （强对偶 即下文Slater’s condition）3.或许你对为什么能够是对w b求min，对a求max还是留有疑问。(拉格朗日到对偶问题这两个之间的转化过程)</p>
</li>
<li><p>仍需要引入两个理论。1.  duality principle 2.Slater’s condition 3.KKT<br>首先，L对w与b求偏导，令为0(这里两个等式)，再将这两个等式带入到L中，消去了w、b，只剩下变量a，即得L(a)。于是将问题转化为 Wolfe dual Problem<br><img src="http://upload-images.jianshu.io/upload_images/8716089-67cdfbf09b820630.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="wolfe dual.jpg"></p>
</li>
<li><p>慢着，这里又出现一个问题。</p>
<blockquote>
<p>Traditionally the Wolfe dual Lagrangian problem is constrained by the gradients being equal to zero. In theory, we should add the constraints θL/θw=0  and  θL/θb=0 . However, we only added the latter, because it is necessary for removing   b from the function. However, we can solve the problem without the constraint   θL/θw=0.</p>
</blockquote>
<p>这里就会不明白为什么不需要加上θL/θw=0约束仍能够solve the problem？暂且保留疑问。</p>
</li>
<li><p>Karush-Kuhn-Tucker conditions :<strong>first-order necessary conditions</strong> for a solution of an optimization problem to be optimal<br>除了KKT还需要满足一些regularity conditions，其中之一为Slater’s condition。</p>
<blockquote>
<p>Because the primal problem we are trying to solve is a convex problem, the KKT conditions are also sufficient for the point to be primal and dual optimal, and there is zero duality gap.</p>
</blockquote>
<p>这里说的，即只要为convex问题，KKT也满足，即可说得出的结果是原问题或对偶问题的最优解，因为Slater’s condition是一定满足了的，gap=0。对于SVM，如果结果满足KKT，那么即可说是最优解。(详细证明过程[2] ch5)</p>
<p>   <img src="http://upload-images.jianshu.io/upload_images/8716089-62f19539ba8a88bc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="KKT.jpg"></p>
</li>
<li><p>可见，1. stationary 即为偏导为0即为驻点,若无约束函数则直接gradient是0，有了约束则gradient of the Lagrangian为0。2. primal feasibility为原问题约束函数  3. dual feasibility 为我们对L求解时使用对偶理论时的约束函数  4.complementary slackness含义是，要么a=0，要么y*(w*x+b)-1=0.这里与<strong>Support vectors</strong>相关，</p>
<blockquote>
<p>Support vectors are examples having a positive Lagrange multiplier. They are the ones for which the constraint y*(w<em>x+b)-1&gt;=0  is active. (We say the constraint is active when y</em>(w*x+b)-1=0 )</p>
</blockquote>
<p>这里，是否会疑惑为什么不能同时为0？为什么multiplier一定是正数？在KKT中，我们只选取支持向量，即将不等号约束改为等号约束，其他的点不考虑。</p>
<blockquote>
<p>Solving the SVM problem is equivalent to finding a solution to the KKT conditions.” (Burges, 1988)</p>
</blockquote>
</li>
<li><p>现在有了L(a),求导即可。得到了a。再根据偏导为0的公式回代得到w 。再根据prime problem中的约束函数y*(w*x+b)-1&gt;=0，计算b</p>
</li>
<li><p>用QP solver来解对偶问题。用python CVXOPT包。将wolfe dual.jpg中我们需要求解的公式转化到下面CVXOPT支持的形式。这里引入了一个Gram matrix - The matrix of all possible inner products of X.</p>
</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-907dcfe22f62860e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="CVXOPT.jpg"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/8716089-ac41318c2c79efb1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="转化过程.jpg"></p>
<p>这个图里有问题，minimize部分最后一项需要<code>q.T*a</code>, 详见代码部分，需要q = cvxopt.matrix(-1 * np.ones(m))。</p>
<ul>
<li>code部分：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import cvxopt.solvers</span><br><span class="line">X, y &#x3D; 这里获取到数据</span><br><span class="line">m &#x3D; X.shape[0]  #data有多少</span><br><span class="line"># Gram </span><br><span class="line">K &#x3D; np.array([np.dot(X[i], X[j]) for j in range(m) for i in range(m)]).reshape((m, m)) </span><br><span class="line">P &#x3D; cvxopt.matrix(np.outer(y, y) * K)</span><br><span class="line">q &#x3D; cvxopt.matrix(-1 * np.ones(m))</span><br><span class="line"># 等式约束</span><br><span class="line">A &#x3D; cvxopt.matrix(y, (1, m))</span><br><span class="line">b &#x3D; cvxopt.matrix(0.0)</span><br><span class="line"># 不等式约束</span><br><span class="line">G &#x3D; cvxopt.matrix(np.diag(-1 * np.ones(m))) h &#x3D; cvxopt.matrix(np.zeros(m))</span><br><span class="line"># 求解</span><br><span class="line">solution &#x3D; cvxopt.solvers.qp(P, q, G, h, A, b)</span><br><span class="line"># 拉格朗日乘子</span><br><span class="line">multipliers &#x3D; np.ravel(solution[&#39;x&#39;])</span><br><span class="line"># 支持向量</span><br><span class="line">has_positive_multiplier &#x3D; multipliers &gt; 1e-7 </span><br><span class="line">sv_multipliers &#x3D; multipliers[has_positive_multiplier]</span><br><span class="line">support_vectors &#x3D; X[has_positive_multiplier] </span><br><span class="line">support_vectors_y &#x3D; y[has_positive_multiplier]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#计算w，b</span><br><span class="line">def compute_w(multipliers, X, y):</span><br><span class="line">    return np.sum(multipliers[i] * y[i] * X[i]  for i in range(len(y)))</span><br><span class="line">def compute_b(w, X, y):</span><br><span class="line">    return np.sum([y[i] - np.dot(w, X[i]) for i in range(len(X))])&#x2F;len(X)</span><br><span class="line"></span><br><span class="line">w &#x3D; compute_w(multipliers, X, y)</span><br><span class="line">w_from_sv &#x3D; compute_w(sv_multipliers, support_vectors, support_vect</span><br><span class="line">b &#x3D; compute_b(w, support_vectors, support_vectors_y)</span><br><span class="line"> </span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<p>We saw that the original optimization problem can be rewritten using a Lagrangian function. Then, thanks to duality theory, we transformed the Lagrangian problem into the Wolfe dual problem. We eventually used the package CVXOPT to solve the Wolfe dual.</p>
</blockquote>
<ul>
<li>为什么需要将拉格朗日函数转化为对偶问题到wolfe dual？<br>这里还差对偶原则及Slater’s condition 概念。</li>
</ul>
]]></content>
      <categories>
        <category>ml</category>
      </categories>
      <tags>
        <tag>ml</tag>
      </tags>
  </entry>
  <entry>
    <title>Sai.L</title>
    <url>/2021/08/06/Sai-L/</url>
    <content><![CDATA[<br>



<h4 id="5月"><a href="#5月" class="headerlink" title="5月"></a>5月</h4><span id="more"></span>

<br>

<h5 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h5><ol>
<li>顺周期行业、大宗商品价格</li>
<li></li>
</ol>
<br>



<h5 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h5><ol>
<li>消息以及其其影响的程度，在消息出来之后会有很多的解读。个人也是有自己的连锁反馈地行为序列。比如查相关的资料(不同人查的也不同)，这已经体现视角的差异。当然最一手的，未必有。于是造成，大多都是观点的重复和抄袭。我不认为达叔和牛猫的影响力，代表了实力。而真的身在其中的人，又未必能够有意识利用这一手的信息。</li>
<li>很少地整理，更多的思考</li>
<li>如果可以预先地设想，并作出估值判断。(巴菲特的估值方式就是好公司好的管理人)。但是这好，又从何而言呢。什么数据呢。没有说。相当于没说(但是有示例和描述，多少人去研究这个事情呢)。但是理念也很重要就是了。</li>
<li>我现在不再以解谜的心态来对待未知了，像原始人一样去迷信去猜。这个时代不需要这样。因为信息和知识足够让你对事件有一个边界性的认知。哪些是已知且公认的，哪些是未知的。哪些是待探索的问题。以及前景。都是明牌。虽然真的有各种私底下的操作，但始终，这个假设出来了，你也能够对假设之后的场景有一定的认知。</li>
<li>大佬也是常人，非常的地方不在于看了哪些东西，而在于他自己对那些问题认为有价值，以及他的理解。他的理解也可能是依赖于经验，也可能是依赖于分析。你没有对方的经验，运用你的常识和检索，以及知识储备。总之，他们有过的积累，你也去积累就好。他们关注的，也留心和思考就好。</li>
<li>除了态度和理念，要记忆和重复一些知识的。</li>
<li>钱有没有更多地往这里走。</li>
<li>为什么自黑，因为拉近了距离。也增添了自己的人。</li>
<li></li>
</ol>
<br>

<h5 id="0506"><a href="#0506" class="headerlink" title="0506"></a>0506</h5><p>五一假期的梳理</p>
<ol>
<li>旅游出游2.3亿人次，收入1132.3亿元，按可比口径恢复至疫前同期的77.0%。</li>
<li>印度疫情的持续恶化，港股假期期间疫苗相关上市公司股价大涨，投资者也因此普遍预期假期后，A股市场相关上市公司的股价也会随之上涨。然而今天的盘面实际情况当中，这两个板块反而跌的惨不忍睹</li>
<li>相关个股在“五一”假期之前就出现了一段时间的持续上涨，当“五一”假期的出行和消费数据出来之后，这些大资金借机兑现离场。利好往往是兑现的时机。真的有时间点对应的利好要提前埋伏。</li>
<li>美国政府5日宣布，支持放弃对新冠疫苗的知识产权保护，以扩大全球范围内的疫苗接种。他给的解释是，这个事更多是一种人道主义的“姿态”，现实意义并不是很大，通俗点说就是生产疫苗要用的原材料和供应链是相对固定的，即使是放开知识产权保护，谁能生产谁就可以生产的话，也不是谁都能造得出来的，还得是实力较强的那些头部疫苗相关生产方才是收益的最大获得者</li>
<li>创业板指数当中医药和医疗的权重比较高，生物科技行业大跌，也导致了创业板指数快速下挫，盘中最大跌幅一度超过4%，好在下午跌幅缩窄，收盘时跌幅为2.48%</li>
<li>美国三月份商品和服务贸易逆差进一步扩大至744亿美元，高于修订后前值705亿美元，再写历史新高。这似乎意味着关税并未改变美国的经贸现实，反而增加了贸易参与各方的税收成本，这其中也包括许多美国的企业。</li>
</ol>
<br>

<br>



<h5 id="0508"><a href="#0508" class="headerlink" title="0508"></a>0508</h5><ol>
<li>本来五一假期期间，很多投资者还对节后的市场有所期待，但是节后这两个交易日的实盘情况走出来之后，市场气氛瞬间滑落到了冰点，指数虽然还在3400点的位置上，但整个市场气氛的悲观和沮丧程度，已经快逼近当年2400点时的状态了</li>
<li>每次真正牛市都在绝望和悲观当中悄然展开</li>
<li>随着反弹的持续而转变成了更高的要求——能重新进入上涨通道最好，而节后这两天的行业，无论是创业板指数还是山西汾酒这类强势反弹股的情况，都无情的打破了投资者这方面的预期值，卖的话怕行情突然转好自己踏空了，不卖又怕C浪真的出现，这波回血的成果没守住，反而滑落到比之前更被动的境地，因此沮丧和悲观气氛一下子就不可抑止的扩散了。</li>
<li>因为从当时的情况来看，指数已经攻上3400点，而疫情和美股熔断时的最低点也不过2600+，市场很难回调到一个让所有人都感觉绝望和悲观的点位了。我当时给出的解释是，最悲观最绝望不一定发生在绝对最低点</li>
<li>过去两年市场虽然波折不断，但整体不断上涨的大背景，加上去年年底只要追大流买抱团股就可以“无脑”赚钱的情况而产生的高预期，在很多投资者的脑海当中，今年就是一个“赶在”赚钱的年份，压根就没有想过累计涨幅过大可能导致的回调，以及回调可能的深度</li>
<li>基金公司没有多少纯粹是自己的钱，而又不能印钞票买股，他们的钱都是从基民手里拿过来用于投资的，行情走到现在这样，很多基民在忙着赎回，以就出现了这种看似该抄底的时候却只能被动卖卖卖的结局</li>
</ol>
<br>

<p><img src="https://pic3.zhimg.com/80/v2-3b46b9efbe56edb215b0edcd624af9b6_1440w.jpg" alt="img"></p>
<br>



<h5 id="0510"><a href="#0510" class="headerlink" title="0510"></a>0510</h5><ol>
<li>大宗商品的价格飙涨带动周期性行业上涨的情况，期货盘面上，最近有三大品种的价格刷新了历史性的高点，LME铜价突破1万美元大关，创下历史新高；沪铝则逼近20000元关口，创近十三年新高；铁矿石历史首次突破200美元大关，与螺纹钢一起创下历史新高。</li>
<li>大宗商品期现货价格最近飙涨过程当中，市场认可的比较主流的原因有三项，一是后疫情时代的经济恢复会导致对大宗商品的需求变得旺盛；二是世界主要经济体在应对疫情对经济冲击时，大量增发货币，导致市场有强烈的通胀预期；三是拜登上台后一再强调的“新基建”计划落地实行的话，可能会进一步加大对大宗商品的需求量。在我看来这三项因素表明上都有一定合理性，但又都难以成为主要坚实的逻辑支撑，其中我最瞧不起的理由就是拜登的那个“新基建”计划</li>
<li>如果能通过搞基建项目，把直接发钱变成给参与基建的人员发工资，变现的“撒币”操作和挽救就业数据这两大需求就可以一箭双雕的解决了。如果从财政系统出钱的话，在目前的赤字水平和负债率基础上搞这么大一笔钱，对于美国财政系统来说，几乎是个不太可能完成的任务；如果从货币系统出钱的话，直接印钱拿去搞基建从法理上说不通，而且在已经印了那么钱的情况下，继续印钱的副作用没人敢背锅。</li>
<li>在A股C浪下探+大宗商品暂时冷静的双重作用下，A股周期行业回调到合理位置之后，再有一段稳妥上车的机会</li>
</ol>
<br>

<br>

<h5 id="0513"><a href="#0513" class="headerlink" title="0513"></a>0513</h5><ol>
<li>部分大宗商品如铁矿石是需要现挖的，钢铁是需要现炼的，由于疫情严重时很多大宗商品供应方因为疫情不能正常开工，同时又因为疫情期间的需求低迷和价格低迷而刻意控制了产能，没全力输出，今天天气转暖后，疫情消退预期和需求恢复导致需求超预期回升，形成预期差，供给方的存量不足以应当市场短期需求，形成时间差，在供应方预期与市场实际预期差和存量+产能满足市场需求的时间差没解决之前，就形成了市场抢购大宗商品和中间商囤货加剧供需失衡的短期局面，但是等供应方在需求回升和价格飙涨的刺激下完全反应过来，并且产能全开的，有矿的可劲挖，有高炉的可劲炼的情况下，预期差和时间差会随时间的推移而被逐步消解，而一旦需求的回升速度缓和下来，以及需求不能长期保持在高位的话，短期的猛烈供应可能会在不久的将来导致价格出现明显回落。</li>
<li>市场短期火爆的时候货车堵着煤矿和钢铁厂的大门等着抢货，货拉上车在奔赴目的地的路上还在不断吃涨价的盈利，中间商关门惜售不肯发货，囤着货等进一步涨价，价格短期很疯狂，但是等煤矿和钢铁厂产能全开三个月以上，货车也不排队了，中间商也敞开大门笑脸迎客求着你赶紧买了。因此，大宗商品近期的价格飙升在需求端的持续性上还有待接受考验，在经受住考验之前，市场短期供需失衡是更主要的价格推手，这种行情提前囤了的可以继续等等看，没有货的不能冒进。</li>
<li>另一个大宗商品价格飙涨的理由就是全球放水后的“通胀预期”。在我看了“通胀预期”短期内只是一个助推大宗商品炒作的借口，正确的通胀带动大宗商品价格上涨路径不是这个模样的</li>
<li>以“通胀预期”为理由，爆炒大宗商品的目的，更多的和近期美股垃圾股（游戏驿站为代表），垃圾虚拟币（狗狗币为代表），以及美股疯狂的IPO盲盒等博傻炒作是一脉相承的，虽然发动不同领域价格疯狂飙涨的主体不同，但原因都是实体经济不振，传统优质投资渠道过热，大量流窜型资金无处可去而引发的各领域非理性暴涨。</li>
<li>工业制造国的政府和央行视大宗商品非理性暴涨都是看待死敌一般，尤其在中美两国目前都在承受通胀的大背景下，一定会尽全力平抑非理性，这一点中美等经济体的利益诉求是一致的，政府近期也在表态和行动，会有所作为的。</li>
<li>上证指数目前的横向整理区间比较显著，目前风险警示线还在前期低点连线处，放量跌破3400整数关口的情况下有必要减仓避险，震荡上行的话，在指数未能有效突破前期高点连线之前不要急于加仓。</li>
</ol>
<br>

<br>



<h5 id="0514"><a href="#0514" class="headerlink" title="0514"></a>0514</h5><ol>
<li>一个操作模式，就是分出一部分仓位留给券商，并且把这部分预留仓位分出几个等份，在券商行业指数跌破通道下沿时，或者每次有长阴线时，拿出一个等份的预留仓位用定投的方式去买入一点券商ETF或者券商个股，这样可以得到一个长期平均成本很低的券商持仓成本，如果有全面牛市就可以吃最纯正的“牛市概念”红利，如果仅是阶段性超级反弹或者是去年那种阶段性牛市,在券商股都能赚到一部分确定性比较高的收益，只不过这个过程会比较煎熬。</li>
<li>单日大阴线的时候，分批收集了一点低价的券商ETF和个股，但是在券商行业上的总仓位目前还是没有超过去年7月份整体启动之前，今天这一个超级阳线出来之后，在券商行业上布局的仓位有明显的正收益出现。</li>
<li>券商部分筹码：第一阶段目标是券商行业指数的MA60均线，如果行业指数反弹到这个位置受阻回落迹象明显的话，我会考虑先卖出一半的仓位兑现一部分盈利同时拿回部分本金，然后在后续反弹结束再进入低迷期的情况下重新抄底，以便在原有基础上继续降低总成本。如果行业指数顺利突破MA60均线的压制，回到原来的通道震荡的话，就暂时不做任何操作，持有目前现有的仓位跟风观望情况再说。</li>
<li>A公墓今年一年产生的佣金是10亿元，然后要分给10个券商，这分钱就是在分仓。那这钱怎么分呢？一般来说是通过券商研究报告的质量来分。但是往往啊，如果这个券商和公募关系不错，或者这个券商是公募股东，又或者这个券商给公募提供的销售额最多。因为公募没有沪深交易资格，必须通过不同券商的交易席位才能交易。</li>
</ol>
<br>

<br>

<h5 id="0516"><a href="#0516" class="headerlink" title="0516"></a>0516</h5><ol>
<li>叶飞爆料：目前已经被叶飞爆料波及到的上市公司共有10家，除了最开始的中源家居外，还有隆基机械、 维信诺、 东方时尚、 昊志机电 、华钰矿业 、众应互联、 法兰泰克、 祥鑫科技、 今创集团。涉及到的券商有四家，申万证券、恒泰证券、民生证券和天风证券。而周五券商股刚经历了一波集体大涨，周一被“爆料门”事件波及的券商股价情况如何，大家可以关注一下。被爆料的10家上市公司一个共同特点就是市值普遍较小，其中仅维信诺（120亿）、今创集团（109亿）两家市值过百亿，东方时尚（90.3亿）市值接近百亿之外，其它市值都不足百亿，有六家目前市值规模在10亿—50亿之间，其中市值最低的众应互联（*ST众应）连10亿都不到（9.91亿）。</li>
<li>爆料最多的中源家居，从纯技术角度来看，其股价走势图当中确实有疑似“坐庄”的痕迹，不仅从2019年至今有多次不太符合常理的暴涨暴跌过程存在，同时该股K线在暴涨暴跌之间的横盘整理期内，也有我们之前更新当中说过的比较典型的庄股特征——“鱼骨”形态，即股价在横盘期内K线实体都异常的小，且上下影线较长，成交量也很低，</li>
</ol>
<br>

<br>

<h5 id="0517"><a href="#0517" class="headerlink" title="0517"></a>0517</h5><ol>
<li>不仅被叶飞直接点名的多只个股惨烈大跌，整个市场低价小市值类型的股票大面积暴跌，事件当中被提及的天风证券也是今天行业内跌幅最大的一只，原本上周五券商行业整体气势如虹，今天却没能延续周五的惯性上攻，这一情况可能也是部分受了“爆料门”事件的影响</li>
<li>大致就是，蒲菲迪要拉升股价，但是钱不够，筹码不够。找到叶飞，叶飞找了下家出钱。并且下家高位接盘代持，后面的卖出是下家的事情。但是接盘后，股价下跌的厉害，接盘方资金被套。原本说好操盘方主攻，接盘方辅助，大家一起割韭菜，然后接盘方个人拿到一笔好处费，同时还能让接盘资金（券商资管的钱）赚上一定涨幅，现在摆明了根本就一起“割韭菜”而是接盘方被当成了“韭菜”直接成了接盘侠。事件能搞成这么一出闹剧，实际情况应该是操盘方是个“蠢贼”团队，一方面没有足够的操盘水平，同时也没有配备足够的控盘资金，到处通过中间人去找辅助护盘资金，资金没找够导致计划执行一塌糊涂；另一方面没有能力约束上市公司大股东那边的行为，大股东那边见有人接盘，也不问是谁就直接出货变现拿钱，导致直接把自家操盘方拉来的接盘资金被砸在里面了。</li>
<li>今天A股市场低价小市值全军尽墨，倒是前段时间波折比较大的绩优抱团股趁势崛起了</li>
</ol>
<br>

<br>

<h5 id="0519"><a href="#0519" class="headerlink" title="0519"></a>0519</h5><ol>
<li>这两天锂矿和动力电池行业快速上涨。一个是我们昨天更新讲当中到的，全球最大锂生产商智利化工矿业（SQM）可能会因为政局变动而影响到产能，另一个是近期动力电池行业数据给了市场更大的期待，今年4月，我们动力电池产量12.9GWh，同比增长173.7%，环比增长15%；1-4月累计产量45.7GWh，同比增长252%。4月动力电池装机量8.4GWh，同比增长134%，环比下降7%；1-4月累计装机量31.6GWh，同比增长241%。从这个数据能直观的看到动力电池装机量近几个月在暴增，锂电池产业链相关原材料全面上涨，再加上智利锂矿可能减产的影响，这些情况就都直接反馈到了A股市场和“锂”有关的公司身上了</li>
<li>新能源电动车理论上应该受到成本上涨可能导致利润下滑的威胁，但这几个交易日，新能源电动车相关上市公司的股价也跟着锂矿指数一起上涨。近期特斯拉处于“水逆”期，诸事不顺，特斯拉近期的股价从高位持续回落，同时口碑和声誉在国内市场受挫，这样市场也会期待国内相关的新能源电动车企业分食原本属于特斯拉的市场份额，因此短期并没有因为潜在的成本上升问题而对新能车行业股价予以负反馈。</li>
<li>由于预期下半年宏观经济政策会向政府投资方向倾斜，部分周期性行业的业绩会因此保持在一个相对不错的水平上，所以钢铁煤炭行业指数这波回调到位之后，会考虑在原有底仓基础上再适当增加一点仓位。</li>
</ol>
<br>

<br>

<h5 id="0524"><a href="#0524" class="headerlink" title="0524"></a>0524</h5><ol>
<li></li>
</ol>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>牛猫</title>
    <url>/2021/08/02/%E7%89%9B%E7%8C%AB/</url>
    <content><![CDATA[<br>



<h4 id="股票"><a href="#股票" class="headerlink" title="股票"></a>股票</h4><ol>
<li>富瀚微 – 海思被禁，股东减持跌后收复<span id="more"></span></li>
<li>第一个圈是概念小票，第二个是老国企，第三个是银地保，第四个是家电酱油牛奶，圈里的还有：锂电、新能源车、医药、光伏、芯片。</li>
</ol>
<h4 id><a href="#" class="headerlink" title></a></h4><br>

<h4 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h4><ol>
<li>资金出入</li>
<li>分红和红利税</li>
<li>趋势模型</li>
<li>分级基金</li>
<li>缺口</li>
<li>量化</li>
<li>ETF</li>
<li>指数名称及相应etf代码<ol>
<li>沪深300 510300</li>
<li>创业板指 159977</li>
<li>中证500 510500</li>
<li>中证科技 515000</li>
<li>证券指数 512880</li>
<li>中证军工 512660</li>
<li>芯片指数 512760</li>
</ol>
</li>
<li>退市流程</li>
<li>股票增发</li>
<li>海龟交易</li>
<li>基金定投收益</li>
<li>低开买入模型</li>
<li>p2p</li>
<li>中途上车模型</li>
<li>股票观<ol>
<li>短期无法预测</li>
<li>交易效率和交易成本</li>
<li>止损</li>
<li>T+0</li>
<li>右侧</li>
<li>A股的价投</li>
<li>投资=选战友</li>
</ol>
</li>
<li>打新</li>
<li>灾难事件</li>
<li>读</li>
<li>公告</li>
<li>境外资金与A</li>
<li>逆回购</li>
<li>ma20</li>
<li>蓝筹行业分级表</li>
<li>股指期货</li>
<li>沪港通规则</li>
<li>融资打新</li>
</ol>
<br>

<h4 id="理财"><a href="#理财" class="headerlink" title="理财"></a>理财</h4><ol>
<li></li>
</ol>
<br>

<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ol>
<li>宽基指数？</li>
<li>ic升水？</li>
<li>滚ic：每个月的第三个周五之前卖掉当前的合约买入下个月的。一年操作十二次。ic每个月的贴水是40-50，如果贴到50甚至更多，就换，不一定等到交个附近。远月合约跌的很多，导致月均的贴水扩大，也会换到远月合约上蹲一两个月，虽小了再换回近月。</li>
<li>ttmpe</li>
<li>扣非利润</li>
<li>员工激励的员工购买价，不就是坑了中小股东么。为什么以前回购也是这个加购就不算是坑了</li>
<li>欧菲光的下场？</li>
<li>IF</li>
<li></li>
</ol>
<br>

<h4 id="日子"><a href="#日子" class="headerlink" title="日子"></a>日子</h4><br>

<br>

<h5 id="0713"><a href="#0713" class="headerlink" title="0713"></a>0713</h5><h5 id="0713-1"><a href="#0713-1" class="headerlink" title="0713"></a>0713</h5><h5 id="0713-2"><a href="#0713-2" class="headerlink" title="0713"></a>0713</h5><h5 id="0706"><a href="#0706" class="headerlink" title="0706"></a>0706</h5><ol>
<li></li>
</ol>
<br>

<h5 id="0707"><a href="#0707" class="headerlink" title="0707"></a>0707</h5><ol>
<li><img src="https://mmbiz.qpic.cn/mmbiz_png/NUnibBdYwCWiatRjnwYTnXVzGic5JYB1aSTZXUkNmQLNHIuHAvy4VIJP5FB5TZVjgXicjnuvM9liamo3dXibicca68Uqg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="Image"></li>
<li>**目前中国的准备金率是大银行13%，小银行11%**，最近一次降准是2020年1月。</li>
<li><strong>滴滴再次大跌7%，14美元的发行价，现在11.7美元。</strong></li>
<li><strong>中远海控上半年预计盈利370亿</strong>，这个是真的猛，我主要是上一个熊市周期里看中国远洋难以置信的下跌（-96%），印象太深刻了，对海运这样的周期股都不想参与。</li>
<li><strong>腾讯今天跌2%，把2021年最多的时候37%的涨幅全跌回去了</strong>，我这一路捂过来，内心也是有一股无力感。同样是捂龙头股的策略，美股那边真的很挣钱，A股和港股都是一言难尽。</li>
<li>上午加仓智飞生物，<strong>最新仓位85%<strong>，其实还有一个股我也想买，但它今天涨太多了，算了，再等等吧。另外我今天</strong>加仓了期指</strong>，IC加1手，IF加1手，目前累计是16手，今年股票没挣钱，盈利全是滚期指滚出来的。</li>
</ol>
<br>

<h5 id="0708"><a href="#0708" class="headerlink" title="0708"></a>0708</h5><ol>
<li><p>腾讯从773回撤到528，美团从460回撤到267，快手从417回撤到160，海底捞从85回撤到40，农夫山泉从68回撤到38</p>
</li>
<li><p>受<strong>国内资金南下热潮所带动</strong>，当时港股通通道每天南流超过100亿，那个时候估计谁也没想到半年之后就变成现在这个样子。</p>
</li>
<li><p><strong>那些用自有资金炒股的散户反而普遍比较淡定，倒是一些机构，比散户更着急。</strong>看谁先跑</p>
</li>
<li><p>万华半年预增380%，净利润135亿，感觉前几天逆势上涨就是提前埋伏这个业绩，我有种预感，明天见光后可能会炸营，哎，不是我乌鸦嘴，是最近这类剧本演太多了。==&gt;隔了几天才炸</p>
</li>
<li><p>今晚美股跌了1.5%左右，其中中概股表现惨淡 ==&gt;有了前兆 。。 但是谁能对这样的前兆有预估呢。。</p>
</li>
<li><p>酒鬼酒上半年净利润5-5.2亿，增长170%-181%，这个业绩要换几个月前，肯定会让二级市场激动一下，现在白酒整个板块都熄火了，感觉明天也翻不起多大的浪。</p>
</li>
</ol>
<br>

<h5 id="0711"><a href="#0711" class="headerlink" title="0711"></a>0711</h5><ol>
<li><p><strong>央行决定于2021年7月15日下调金融机构存款准备金率0.5个百分点</strong>（不含已执行5%存款准备金率的金融机构）。可以<strong>释放大约1万亿的资金</strong>，我国金融机构加权平均存款准备率仅为8.9%，创下过去15年的最低值。消息出来后A股已经收盘，但新加坡的A50期指还在交易，然后直接就起飞了。</p>
</li>
<li><p>A50的交易时间是早上9点到下午16:30，下午17点到次日凌晨4:45</p>
</li>
<li><p>另外和降准的消息配套出炉的，还有<strong>6月份的社融数据</strong>。新增人民币贷款21200亿，预期是18500亿，前值是15000亿。6月社会融资规模增量是36700亿，预期是28900亿，前值是19200亿。另外6月份的M2货币供应同比增长8.6%，预期是8.2%，前值是8.3%。</p>
</li>
<li><p>东方财富预计上半年盈利35-40亿，同比增长93-121%。这个业绩比较顶，考虑到目前A股整体的交易活跃度，三季度基本也不会太差的</p>
</li>
<li><p>市场监管总局禁止虎牙和斗鱼合并，这几年政府对互联网行业的整顿真就没停过。</p>
<br></li>
</ol>
<h5 id="0712"><a href="#0712" class="headerlink" title="0712"></a>0712</h5><ol>
<li><strong>上证指数在2000年前后就已经涨到了2000点附近，也就是说最近20年累计只涨了75%。</strong></li>
<li><strong>而A股的股民，有将近80%都是2007年后才开户入市的，所以客观讲上证指数对绝大多数股民而言表现极差，基本都把它当做废物乱骂。</strong></li>
<li>创业板指是2010年6月1日儿童节那天上线的，基点是1000点，11年间涨了250%，年化复利相当于9%</li>
<li><strong>上证指数的成分股是所有上交所股票（代码6开头）的总市值加权</strong>，里面有大量过气行业的权重股，基本上大家耳熟能详的知名坑爹股都在里面；创业板指的成分股是创业板里挑选的100家优秀公司</li>
<li>去年才上线的科创板50指数（代码000688）未来超过上证指数也是100%的事。</li>
<li><strong>中国证券圈真正影响力最大的是沪深300指数，机构最认，被动跟踪的资金盘也是最多的。</strong>如果你看沪深300指数的话，就会发现A股还可以，不算很差</li>
<li>考虑长线投资ETF基金。大盘指数基金它有一个特点很符合人性，就是它可以越跌越买，因为迟早是能涨起来的–行业ETF“迟早能涨起来”的时间周期不确定，可能需要等很久，没有大盘指数ETF稳当。</li>
<li>整个传统行业最近也都不太好，白酒、食品、重工、证券、家电今年都惨惨的。去年A股的主线是风格歧视，弄死小市值，爽死大市值。今年A股的主线是行业歧视，抽血老旧慢，狂怼高新尖。</li>
<li><strong>舍得酒业</strong>上半年利润7.1-7.5亿，增长332-356%。上周酒鬼酒预增业绩翻倍，第二天几乎跌停，最近的鬼行情已经不太认白酒行业，这种时候业绩好也容易成为高开跳水的板子。<strong>山西汾酒</strong>利润17.66-20.87亿，同比增长110-130%，同上，现在不是白酒的节奏。</li>
<li><strong>卓胜微</strong>，上半年利润9.91-10.25%，增长180%-190%</li>
<li><strong>中国建筑</strong>预告上半年利润245-270亿，上半年同比预增24-36%。“你以为我傻捏是吧，那…那是给死人用的纸钱！”——这就是我现在看传统行业预告利润的心情</li>
<li><img src="https://mmbiz.qpic.cn/mmbiz_jpg/NUnibBdYwCWhBAgO8SwfFr47UADuZH9ibiaP3icck82L4ktka56nlWKp3ou59EPc4iaQ0IPrpxe8FHwqiacdOWibDsBBQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="Image"></li>
</ol>
<br>

<h5 id="0713-3"><a href="#0713-3" class="headerlink" title="0713"></a>0713</h5><ol>
<li><p><strong>历年来创业板指权重股的变迁</strong>，从2010年到2021年: </p>
<ol>
<li>2010-2013年——核心板块包括信息服务（包括传媒和计算机）、机械设备、化工、电子、通信、公用事业。<a href="http://quote.eastmoney.com/SZ300002.html">神州泰岳</a>、<a href="http://quote.eastmoney.com/SZ300070.html">碧水源</a>、<a href="http://quote.eastmoney.com/SZ300027.html">华谊兄弟</a>等为代表性成分股</li>
<li>2014-2016年——传媒、计算机板块占比持续提升，化工、机械设备占比减少。乐视网、<a href="http://quote.eastmoney.com/SZ300059.html">东方财富</a>、<a href="http://quote.eastmoney.com/SZ300017.html">网宿科技</a>为代表性成分股。</li>
<li>2017-2018年——农林牧渔、电子板块占比提升，传媒、计算机占比下滑。<a href="http://quote.eastmoney.com/SZ300498.html">温氏股份</a>、<a href="http://quote.eastmoney.com/SZ300136.html">信维通信</a>为代表性成分股。</li>
<li>2019年至今——医药生物、电气设备占比提升，公用事业、农林牧渔下滑。此外，受到科技行情影响，传统行业占比快速下滑。</li>
<li>从乐普医疗——汇川技术——碧水源——神州泰岳——乐视网——三聚环保——温氏股份——迈瑞医疗——宁德时代，这一路切换过来吗？</li>
<li>历年的创业板龙头股里，<strong>有5个现在都已经坠机，坑爹率太高了</strong>。</li>
<li><strong>马太效应会在其中发挥作用</strong>，让厉害的公司通过时间脱颖而出，占领越来越多的权重，从而带领指数向上，让持有ETF的人赚钱。</li>
</ol>
</li>
<li><p>截至收盘，创业板权重股<a href="http://quote.eastmoney.com/SZ300750.html">宁德时代</a>总市值已达13113亿元，位列深市第一、A股第四，仅次于<a href="http://quote.eastmoney.com/SH600519.html">贵州茅台</a>、<a href="http://quote.eastmoney.com/SH601398.html">工商银行</a>、<a href="http://quote.eastmoney.com/SH601939.html">建设银行</a>。自2019年以来，创业板指持续大涨，2019年累计上涨43.79%，2020年累计上涨64.96%，2021年累计上涨18%。而在创业板指屡创新高的背后，多只权重股大涨成为关键。</p>
</li>
<li><p><img src="https://pic2.zhimg.com/80/v2-48868424ee28dce81a435b91d81bd581_720w.jpg" alt="img"></p>
</li>
<li><p><img src="https://pic1.zhimg.com/80/v2-fc969d648a909c768d001864aa5700e8_720w.jpg" alt="img"></p>
</li>
<li><p><strong>今天顺丰公布了半年度报告，第二季度预告净利润为16.29-18.19亿，这里面扣非后的净利润为6-7.3亿</strong>，比一季度自然是强多了，一季度亏损9.8亿，直接导致白马股集体恐慌。但这个业绩也不算很好，我们把坑爹的一季度刨除，去年四季度扣非净利润为10亿，去年三季度扣非净利润为17亿，都比今年二季度的数据要好。所以顺丰这波只能说经营已经切换回正轨，止损止血，但盈利能力依然和去年有明显差距。</p>
</li>
<li><p>这里面主要问题还是快递行业太卷了，尤其是低价快递有新进场的在搞价格战，顺丰为了抢市场份额只能压缩自己的毛利润。只有等这轮内卷告一段落，顺丰的盈利能力才会反弹，这股短线弃疗了，中长线倒是可以慢慢蹲。</p>
</li>
</ol>
<br>

<h5 id="0714"><a href="#0714" class="headerlink" title="0714"></a>0714</h5><ol>
<li><strong>牧原股份的二季报预告来了，预计利润94-102亿，同比去年下降5.4-12.8%。</strong>其中第二季度单算的利润是24-32亿，这个业绩环比下滑挺厉害的，原因大家都知道，猪肉最近跌惨了。之前几个季度的利润分别是69、65、102亿，等于是一下子掉了一半，所以上半年累计是下滑的。<strong>但这个业绩在养猪的行业里已经算是挺能打的</strong></li>
<li>今晚正邦科技也发布了预告，上半年亏损12-14.5亿，瘟氏和新绝望大概率也好不到哪去，牧原起码二季度还能赚30亿左右，我觉得不至于大跌。目前猪肉期货在一小波反弹后再度下探，<strong>但我觉得16元/每公斤差不多就是铁底了</strong>，没有太多继续下行的空间。关键在于会在这个附近调整多久，如果再盘2-3个月，养猪企业今年的业绩就没法看了。</li>
<li>牧原我有保持关注，公司是个好公司，就是周期太跳了。</li>
<li><strong>石大胜华</strong>预计上半年盈利5.75-5.95亿，去年同期是亏损4223万，这个公司今天上热搜了，因为从+9%到-10%跌停，最诡异的是我在龙虎榜上竟然看不出来是谁砸的。全天成交44亿，但卖一只出了8000万，<strong>我不信这种日内20%的回撤是散户齐心协力砸出来的</strong>，可能用了关联账户，这个监管层应该关注一下的。这种有操纵痕迹的股票散户不要参与，不是透明博弈。</li>
<li><strong>赣锋锂业上修业绩预告，上半年利润13-16亿，同比增长730-922%。</strong>这是盘后出的消息，赣锋锂业跌了5%，不过不是它单个的问题，今天整个锂电新能源板块都大幅回撤。景气赛道过去半个月大幅上涨，出现技术性回调也正常。</li>
<li><strong>同花顺多位股东计划6个月内合计减持不超过7.46%股份。</strong>那几个名字我都会背了，于浩淼，叶琼玖，王进，每年都会把祖传的7.46%股份拿出来吆喝一下。我不知道他们几个在想啥，每年都是这个公告，然后这么多年下来也没真的卖</li>
<li><strong>东鹏饮料</strong>预计上半年6.65-6.85亿，增长50-55%。这个公司也是上市就出名了，因为是上交所的新股，一签1000股，46块钱涨到了250元，<strong>新股中一个号赚20万</strong></li>
<li>我也有银地保，但我不会只有银地保，而且我的交易习惯就是亏钱的股票轻易不补仓，所以跌着跌着占比越来越低，影响也越来越小。股市最大的特点就是马太效应，所以千万不要搞持仓内的均富主义，千万不要汰强补弱。</li>
<li>苹果上涨2.4%，机构普遍预期苹果明年会增产，所以果链又能搞一下？但欧菲光的下场实在是太惨了，果链公司要搞也不能搞太多仓位。</li>
</ol>
<br>

<h5 id="0715"><a href="#0715" class="headerlink" title="0715"></a>0715</h5><ol>
<li><strong>今年上半年A股交易额107万亿，去年同期交易额是87万亿，同比增长22%，证券公司今年上半年整体利润增长2成是摊在桌面上的明牌</strong>。结果证券指数2021年累计下跌12%。</li>
<li>**新绝望公布了上半年的业绩，预亏29.5-34.5亿，同比下降193-209%**，这其中包括了10亿的货值计提。公司上半年累计新增借款208亿，用于扩张产能，同一个行业的企业，牧原今年遭遇反周期还能半年赚100亿，新绝望亏30亿，我们必须要承认企业和企业之间差距是很大的</li>
<li><strong>药明康德</strong>上半年利润9.1亿-9.4亿，同比增长53-55%，当初它和恒瑞我二选一选砸了，我菜。</li>
<li><strong>中国平安</strong>上半年保费4222亿，同比下降5%，这个业绩是不好，但上半年股价下跌30%确实过分了。</li>
<li><strong>长江电力</strong>上半年利润增长6.22%，主要是投资收益增长了，扣非利润是下降的。但这股的股息率接近5%，又是很稳定的业态，涨不上去也跌不下来。<strong>长电要是大跌的话，真的可以买一些当养老股。</strong></li>
<li><strong>洋河也做了一期的员工激励，总价10亿，员工购买价格是103.73元/每股</strong>。虽然比市场价便宜了一半，但这些股票是洋河前几年回购来的，当时回购的均价就是103.73元，所以并没有坑中小股东。另外行权条件是未来两年营收增长15%，说明公司对业绩增长还是有信心。整体看，比珠海那家的厚道多了，国企这方面还是比较注意影响。</li>
</ol>
<br>



<h5 id="0716"><a href="#0716" class="headerlink" title="0716"></a>0716</h5><ol>
<li>滴滴被七大部门入驻调查的新闻。我今晚开盘就把滴滴全抛了，没挣啥钱，就1%的屁盈。君子不立危墙之下，等调查结果出来，形势明朗了再说。看到 自然资源部的时候还有些困惑，找懂行的问了一下，才知道在中国境内做地图测绘是归它们管的</li>
</ol>
<br>



<h5 id="0717"><a href="#0717" class="headerlink" title="0717"></a>0717</h5><ol>
<li>很多人感叹那个年代的港台出美女，其实稍微想一下，香港+台湾一共才3000万人口，出美女的数量和质量怎么和11亿人口的大陆比。<strong>但当时他们有更繁荣的城市文化，港台女艺人在这些更有高级感的歌曲，影视，综艺加持下，确实对内地的土包子们有致命的吸引力。</strong></li>
</ol>
<br>

<h5 id="0718"><a href="#0718" class="headerlink" title="0718"></a>0718</h5><ol>
<li>结构性暴跌，<strong>中位数其实没咋跌，也就-0.26%<strong>，主要盯着创业板的权重股爆锤。</strong>宁德时代跌6.5%<strong>，这票现在占创业板指</strong>15%的权重</strong>，所以单单它一只股就导致了创业板指下跌1%。</li>
<li>整个锂电池、新能源板块都遭遇了回撤，还有最近一起涨的芯片板块也跌了。这一波从5月上旬启动，涨到现在也涨了2个半月，主升浪已经是强弩之末。<strong>通常这类调整都是围绕着20日均线进行</strong>，横向震荡，持续时间通常为2-3个星期。</li>
<li><strong>片仔癀</strong>业绩快报，上半年净利润11.14亿，同比增长28%。它们家的一颗药丸给炒到上万</li>
<li><strong>格力电器</strong>已耗资36.77亿回购1.16%股份。公司股价一直在50附近挣扎，我觉得在这里回购起码是不亏的，现在回购的股票后面都会注销。格力现在ttmpe是12倍，股息率是4%，如果是那种偏爱长线持有吃股息的投资者，现在真的是遍地黄金。</li>
<li>小米的安卓生态是和竞品打通的。苹果有自己的生态闭环，10亿用户，每年除了卖手机，还能躺着收30%的苹果税。</li>
<li><strong>美国军机在台湾机场降落，大陆启动新一轮军事演习</strong>，周五军工板块大涨4%，领涨两市。但军工板块渣男属性很严重，玩玩还行，炒出感情的全在山顶站岗。</li>
</ol>
<br>

<h5 id="0719"><a href="#0719" class="headerlink" title="0719"></a>0719</h5><ol>
<li>今晚还有一个大事，就是欧美跌的比较凶。道琼斯目前跌2.36%，纳斯达克跌1.7%。对于习惯了A股波动的股民来说，这好像也没什么大不了的，但美股那边的波动率大概是我们的40%，所以今晚的跌幅对应成A股差不多就是-4%、-5%的样子。明天A股应该有一个低开，但幅度不会很大，因为我看了一下A50期指今晚只跌了0.5%。</li>
<li>花旗将上调<strong>宁德时代目标价为645元</strong>，受益于全球电动车渗透率的提升。</li>
<li><strong>摩根大通上调赣锋锂业目标价至180港元</strong>，这个说的是港股，目前股价是149港币。</li>
<li><strong>恒大美元债创9个月最大跌幅。</strong>我去看了一下恒大的美元债，明年和后年到期的年化收益率都在45-50%左右，恒大要是真有钱，去回购一些自己的债券都能大赚一笔。</li>
<li><strong>华夏幸福公告累计未能如期偿还债务本息合计732亿元</strong>，哎，平安当初真是瞎了狗眼，去趟了这浑水，也是活该自己股价跌成狗屎。</li>
</ol>
<br>

<h5 id="0720"><a href="#0720" class="headerlink" title="0720"></a>0720</h5><ol>
<li><strong>此前有网友问我能不能捡便宜恒大的打折房，我就预警过这类风险，怕就怕资金被挪用，房屋烂尾。</strong>湖南邵阳的住建局还是很警觉的，起码保护了当地利益。</li>
<li>连着两天出事，<strong>恒大地产+恒大汽车的市值跌掉了1000多亿港币</strong></li>
<li>这两天金融地产跌的跟鬼一样，在很大程度上就是受恒大情绪的影响。<strong>你说开个公司借那么多的钱干嘛，</strong>我现在真的特别烦高杠杆暴富的企业家，押对了就上福布斯，押错了就是社会救救我。</li>
<li>年初恒大本来不是要回购1000亿的股份吗，就是苏宁也有200亿在里面的那笔钱，后来说恒大赖皮，让这些股东都放弃了回购。</li>
<li><strong>这一波下跌这1000亿已经亏了500亿以上了，可谓是血的代价。</strong>这些人当初把钱借给恒大，打的如意算盘是借壳A股上市，割股民韭菜。万一上市失败，由恒大进行回购。可谓包赚不赔的买卖。</li>
<li>结果证监会卡住不让恒大上A股，然后恒大没钱回购</li>
<li><strong>每逢水灾做水管的那几家公司就会涨一下</strong>，但都是冲高回落，抢这种热点股很容易挂旗杆。</li>
<li><strong>农业部：要提醒广大养猪场户，高利润阶段已经结束</strong>，不要再赌市场。</li>
<li><strong>融创斥资1.53亿回购港股。</strong>其实我觉得那些聚焦一二线城市的地产公司，这一波都能扛过来，因为一二线的房产流通性很好。千万别学碧桂园和恒大，拼命借钱，然后弄了一堆三四线城市的项目，害人害己。</li>
</ol>
<br>

<h5 id="0721"><a href="#0721" class="headerlink" title="0721"></a>0721</h5><ol>
<li><strong>涨得最好的主要有3个板块，新能源、新材料、芯片</strong>，都是+4%以上，另一边传统板块则继续被抽血，银地保再度下跌。</li>
<li><strong>最近两天大量公募披露的持仓有关，不少管理着千亿+规模的明星基金都在二季度做了调整，调整的方向就是从传统行业撤退，然后加仓到我上面提到的3个板块。</strong></li>
<li>短线如果想挣钱就只能随大流，如果不会选个股的话，可以考虑这三个行业的ETF基金，分别是芯片ETF（159995）、新材料ETF（516360）、新能源车（515030）。最近连着两天大涨有些偏离，后期可以等回踩均线的时候再择机上车。亦或者买点宽基ETF，创业板、中证500的那种，更适合今年这种风格来回横跳的市场。</li>
<li><strong>果链的明星股歌尔股份在14:50的时候突然放量暴跌，一度被砸至跌停</strong>，受其影响整个果链相关的股票今天表现都被拖下水。不太像是业绩有雷。港资那边今天是净买入2.5亿，所以<strong>砸盘的应该是卖方第二的席位，也就是中信建投潍坊福寿西街营业部。</strong>这个山东潍坊的营业部并<strong>不是活跃游资，整个2021年只有今天上了一次龙虎榜</strong>，再加上歌尔股份就是山东潍坊的，我有点怀疑出货的<strong>可能是月初解锁的员工持股计划“家园4号”</strong>。如果真是它们在卖的话，总规模接近20亿，今天这个出货量只占10%，后面可能还得卖。当然这仅仅是我的猜测，起码勉强说的通。</li>
<li>至于立讯精密、蓝思科技这两个倒霉蛋，就是看到歌尔股份突然暴跌，有人被吓的跟风出货罢了。</li>
<li><strong>恒大</strong>给灾区捐了2000万，许家印是河南人，所以这也是勒紧裤腰带支援家乡人民。</li>
<li><strong>片仔癀****大股东计划减持1%股份</strong>，还好了，也就30亿，以片仔癀目前的流动性肯定接的动。</li>
<li><strong>公募</strong>明星基金<strong>的二季报，还有一个动向，大量增配了银行股</strong>。这个是我没想到的，当然了，银行比地产保险稍微好一丢丢吧。买的最多的是<strong>招行、兴业、平安银行、江苏银行</strong>，所以这几只股票还过得去，至于传统的国有四大行，已经被主流资金抛弃了。</li>
</ol>
<br>

<h5 id="0722"><a href="#0722" class="headerlink" title="0722"></a>0722</h5><ol>
<li>FFIE <strong>提出一个破产重整的方案，用自己手里的法拉第股份来偿还债务，以交换债主们同意公司在美国上市。</strong>双方终于达成一致，<strong>确认负债是29.6亿美金</strong>。<strong>贾跃亭必须偿还40%以上，才能解除在中国境内的债务。</strong>另外贾跃亭的老婆放弃了财产优先分割权。</li>
<li>今晚上市的FFIE股权分为：<strong>只要这家公司的股价涨250%，贾跃亭就可以把所有欠债都还清</strong>，彻底摘掉老赖的帽子，堂堂正正的回国。<ol>
<li>法拉第原有股东持股51.1% – 有恒大，不亏的，浮盈大概翻倍了，但是现在也减持不了</li>
<li>本次上市公开发行的股份23%</li>
<li><strong>贾跃亭的债权人们转债股17.2%（这部分股票应该是从贾跃亭个人名下划转过来的）</strong></li>
<li>借壳之前的老股东持股6.8%</li>
<li>卖壳的人分到1.9%的干股</li>
</ol>
</li>
<li>今天A股行情整体还凑合，就是<strong>医药板块跌的比较多</strong>，中证医药指数跌了3%，所以含药量比较高的创业板指今天拉胯了。<strong>一是片仔癀大股东减持带来的负面影响</strong>，把最近涨势凶猛的中药板块给锤下来了，广誉远、同仁堂之类的今天都大跌。<strong>另一个被爆锤的是生物疫苗</strong>，康泰生物-10%，智飞、复星、康希诺也都大跌。</li>
<li><strong>恒大和广发和解了，官司撤了。</strong>我原先猜测恒大是提前把钱还了，但后来看新闻并没有，依然是明年到期的时候还。这说明恒大的商务和公关能力依然很强，在金融圈依然有信用。受此刺激恒大股价反弹，<strong>顺带A股的银地保也联动反弹</strong>，但弹性非常弱，保险涨1.5%，地产涨0.8%，银行涨0.56%</li>
<li><strong>歌尔股份</strong>中午继续跌，<strong>公司紧急开了个电话会议安抚军心</strong>。其实昨天的闪崩主要是潍坊那个营业部净卖出1.8亿造成的</li>
<li><strong>深南股份的实际控制人周世平被强制刑事措施了</strong>，他就是之前著名的P2P平台红岭创投的实控人，这次被抓我猜测和上市公司无关，可能是P2P的秋后算账。</li>
<li>韦尔股份比较失望，感觉是在正确的板块挑了错误的股票，虽然有盈利，但和同期的北方华创、三安光电比真的难过到内伤</li>
</ol>
<br>

<h5 id="0725"><a href="#0725" class="headerlink" title="0725"></a>0725</h5><ol>
<li>酱油老二<strong>中炬高新</strong>被知名基金经理张坤加持，但股价在上半年一跌再跌，终于在这个周末出了一套二连招来提振股价。<strong>回购3-6亿的股票</strong>，用于注销，这个没啥问题。<strong>定增募集77.91亿</strong>，用于扩增300万吨产能。这部分股票<strong>发行价32.6元</strong>，比周五的收盘价折让13.6%，<strong>全部由大股东认购。</strong>市值也才300亿，大股东哪来那么多钱，查了一下大股东是<strong>宝能姚振华</strong>，哦，大佬确实有钱。</li>
<li>海康威视<strong>上半年利润64.8亿，增长40.17%。挺好的业绩，半年报稳。</strong></li>
<li>社保基金未来6个月减持中国人保2%**。好家伙，所以估值低真的是个陷阱，你会发现大机构还在卖保险。</li>
</ol>
<br>

<h5 id="0726"><a href="#0726" class="headerlink" title="0726"></a>0726</h5><ol>
<li>今天两市涨幅第一的是<strong>半导体板块，这大概是目前最正能量的行业。</strong></li>
<li><strong>第二条线是监管层对垄断资本的整顿</strong>，腾讯今天暴跌将近8%，已经从年度最高价775跌到了490，回撤幅度37%。流动性最强的鹅厂尚且如此，其他那些涉垄断的互联网公司就更别提了。</li>
<li>今天两市跌幅第一的是白酒板块，-6.6%</li>
<li>最近行情跌这么多，为什么大部分指数都还在趋势的强侧，仔细看了一下，创业板、中证500、科技指数、军工、芯片，这还真都是市场上为数不多的热点。**我今天卖了一半的海天味业，4成的洋河，仓位降至75%**，茅台我忍住了没卖。</li>
</ol>
<br>

<h5 id="0727"><a href="#0727" class="headerlink" title="0727"></a>0727</h5><ol>
<li>情绪<ol>
<li>外资进一步减仓它们持有的中国股票。幅度大都在10-30%之间。北上资金净流出90亿，外资之前买A股主要都是白马股，现在砸盘的也主要是白马股。</li>
<li>互联网公司股票被资本市场加速看衰。</li>
<li>市场更关注正能量的行业，减持浓度不够的行业。</li>
<li>消费行业出现资金加速逃离。<strong>这个看起来会影响挺长一段时间</strong></li>
</ol>
</li>
<li><strong>恒生指数最近两天跌了将近10%</strong></li>
<li><strong>但斌道歉上了热搜</strong>，他说新产品亏损17%，其中美股是正回报。<strong>今天砸盘的那些人，很多都是去年信仰长线投资的变节者</strong></li>
<li><strong>最差情况也不会跌回3000点以下，从周线上看3152那里有一个缺口，有可能是回调的底线</strong>，这是悲观的预期，也有可能像前几次那样3300+就获得支撑，又弹回3500附近。</li>
<li>再买芯片板块我就买159995芯片ETF</li>
<li><strong>恒生互联ETF</strong>这个ETF春节后跌了42%，前十大重仓股都是阿里、腾讯、小米、美团、快手、京东这些公司，我还是相信它们的困难是暂时的，未来有更好的发展。</li>
</ol>
<br>

<h5 id="0728"><a href="#0728" class="headerlink" title="0728"></a>0728</h5><ol>
<li>指数方面，确实比较典型的遵循了<strong>暴跌次日公式</strong>，上午有恐慌性的急杀，然后快速拉起，几个重要指数收盘的时候有的翻红，有的小跌</li>
<li><strong>大买方的交流会纪要</strong>：<strong>欧美系资金</strong>这波是被教培行业的新政惊呆了，所以<strong>普遍减仓了20%左右</strong>，是这些机构<strong>自发的系统性避险</strong>，和政治因素无关，网传美国政府限制资本投资中国市场是谣言，没有的事。如果没有新的刺激，这一波的减持暂时告一段落。</li>
<li>境外资金为了规避政策风险，<strong>基本撤出了对军工股的投资</strong></li>
<li><strong>技术上的短线超跌依然存在，明天可以期待一下。</strong> ==误判。。很多的误判。是聊天吗</li>
<li>股指期货的IC主力合约出现了罕见的+6个基点的升水，说明有<strong>不少资金看好宽基指数明天反弹</strong>。风险主要是周五及下周，到时候五日线压下来，才是对盘面的真考验。==&gt;下周没有被压垮</li>
<li><strong>中信证券上半年121亿利润，增长36%<strong>，这样的业绩今年股价却跌了22%。公司也不着急，完全不做市值管理。而且</strong>最近几天还给证监会递了280亿的配股申请</strong>，已经被受理。券商是今年最曰了狗的板块。除了东方财富，其余都是一言难尽。</li>
<li>想靠暴跌次日公式操作一下。我股票持仓太分散，短时间内操作不过来，所以<strong>我做波段的标的是期指里的IC和IF</strong>，这样效率会高一些。</li>
<li>**A50比下午15点收盘的时候涨了2.3%**，明天有望继续回血。</li>
</ol>
<br>

<h5 id="0729"><a href="#0729" class="headerlink" title="0729"></a>0729</h5><ol>
<li>领涨的依然是<strong>创业板、芯片、新能源</strong>等热门焦点，但也有一些弱势白马股继续下跌，处于明显资金流出的状态。</li>
<li>最近公募基金披露了持仓，所以机构统计了<strong>公募基金增减持top20的表格</strong>。比如平安从1.03%减到0.18%，等于公募基金减掉了8成的平安。再有就是恒瑞医药的二季度走势也很衰，从0.7%减到了0.13%。茅台虽然在减持榜的榜首，从5.92%减到了4.75%，但减持幅度其实还好，大概就走了1/5的仓位，还有4/5的人选择留下。</li>
<li><strong>能从公募的增减持看到局部的一个缩影，因为机构之间的立场和观点有比较强的传染性，交易趋同性明显高于散户。</strong>机构：<strong>决策撤离某只股票，卖的时候会很决绝，而且卖完了短期内都不会吃回头草。</strong></li>
<li>平安二季度下跌16%，当然不只是公募的那0.85%给砸下来的，出货的可能还有<strong>险资、社保、养老、理财信托、大户，以及一部分散户</strong>，只是这些交易数据不像公募那样便于统计。</li>
<li><strong>减持的百分比进度，能看出一只股票未来下跌的潜力</strong></li>
<li><strong>被减持的股票，超过一半以上都来自消费板块，机构现在普遍看衰老百姓消费力下滑已经不是什么秘密</strong></li>
<li><strong>滴滴今天盘前交易突然拉升接近50%，有传闻说是要私有化退市，但滴滴随后对这个消息进行了辟谣</strong>，目前股价又回落到+20%附近。从目前的形势看滴滴从美股退市的概率确实挺大的，比较好的结局是监管整顿完毕后去香港上市。</li>
<li><strong>武汉的房子要凭房票购买</strong>；<strong>道琼斯，标普500，今晚继续刷新历史新高</strong>；<strong>好未来、高途后续都准备大规模裁员</strong>；<strong>中概股普遍回调，再加上明天五日线压下来，所以盘面有一定的压力。</strong></li>
</ol>
<br>

<h5 id="0730"><a href="#0730" class="headerlink" title="0730"></a>0730</h5><ol>
<li>未来投资的主要范围还是有历史优秀基因的白马股，但会比较关注流动性溢价的变化，总结一下就是三句话：<ol>
<li>机构已经卖透了的过气白马股，可以适当仓位长线蹲一下。</li>
<li>机构正在退出的白马股，要尽量回避。</li>
<li>机构还在往里怼的白马股，可以趋势性参与，见机撤退。</li>
</ol>
</li>
<li>消费和白马回避下</li>
<li>太宗皇帝说的好，疾风知劲草，板荡识诚臣。勇夫安识义，智者必怀仁。</li>
<li>中国价投：公司价值之上的流动性博弈。教培政策让资金应激后，往正能量行业里怼。</li>
</ol>
<br>



<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ol>
<li>大多消息还是业绩。只是看看业务和扣非的净利。很多估计也不太准确。当然思路比判断重要。所以我也不要在于去估计。正反的看越看越看不懂。</li>
</ol>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>股市整体价值理论</title>
    <url>/2021/02/22/%E8%82%A1%E5%B8%82%E6%95%B4%E4%BD%93%E4%BB%B7%E5%80%BC%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[<br>
<br>

<h4 id="大牛股"><a href="#大牛股" class="headerlink" title="大牛股"></a>大牛股</h4><ul>
<li>现在没涨，一年内会涨；<code>现在涨了，预计继续上涨（至少在未来3个月内）</code>；曾经大涨，但现在已经停止上涨或者开始下跌或者已经大跌，且在短期内股价不存在大涨（创新高）的可能性。</li>
<li>“买大牛股，抓主升浪”，这两句话是不可分的。</li>
<li>高确定性的操作与盈利模式</li>
<li>只关注那9～10类强势股</li>
<li>股票投资要记住两句话：第一句话是不要赔钱，第二句话是记住第一句。<span id="more"></span></li>
<li>绝大多数赔钱者，其赔钱的主要原因是缘于<code>选股问题——选错了股票，或者选中了非牛股，甚至是大熊股</code>，而操作技巧方面的不当只占赔钱的次要因素。</li>
<li>短期暴涨型主升浪：<code>在短期内股价涨幅达到80%～100%的上涨波段，或者是股价涨幅能够在一年内至少翻倍的股票</code>。<br>
<br></li>
</ul>
<p><img src="/2021/02/22/%E8%82%A1%E5%B8%82%E6%95%B4%E4%BD%93%E4%BB%B7%E5%80%BC%E7%90%86%E8%AE%BA/tqly.png" alt="63.28/19.42=3.26倍"><br>63.28/19.42=3.26倍，11/06-01/09，<br>10年50.22-&gt;488.57，年收益0.8729<br>PEG=[44.7 79]/87=[0.506 0.908]</p>
<br>

<ul>
<li>上涨动力：一是内在价值低估–&gt;估值修复力，二是内在价值成长–&gt;内在增长力，三是投机价值增长–&gt;投机价值增长力。</li>
<li><code>从低估值股票挖掘，再看其成长性，最后看其有无热门概念，这就是选股的逻辑。</code></li>
<li>一是信息系统，包括信息搜集、整理、归类与处理；二是分析系统，分析信息对于股价的作用，特别是对于主升浪的作用，这需要在事前建立相关的评价标准；三是操作系统，在“五面整体顺势”的情况下加仓，在“五面整体逆市”的情况下减仓，即，在判断正确的情况下持仓或者加仓，在判断错误的情况下不增仓或者减仓止损。</li>
<li>一个运动的物体受到的力可以分为四种：一是原动力，二是持续推动力，三是阻力，四是运动过程中新加入的作用力。</li>
<li>炮射导弹可以利用大炮而获得极高的初速度与相当远的运动距离，可以大量节省火箭发动机的燃料，这对于降低导弹成本、提高导弹射程是很重要的。</li>
<li>洲际导弹的变轨技术</li>
<li>主升浪是十分罕见的，主升浪所占的时间一般不到该股票的整体交易时间的30%。也就是说，大多数股票在**70%**以上的时间里是处于非主升浪状态，其中的多数又是处于振荡状态，或者说无明显的趋势状态。无趋势状态，是很难预测的，故可以归为随机波动范畴。</li>
<li>小市值低价股现在被公告注入热门资产后，这是一个巨大的利好题材，该利好题材带来“一字涨停板”方式暴涨，我们经常看到这类股票会以<strong>连续5～6个甚至是十几个“一字涨停板”暴涨</strong>。这样的暴涨过程，就是一个惯性运动，即，以巨大的初速去克服股价上涨的所有阻力，当股价运动速度无法抵消阻力的反作用时，股价的向上运动就会停止，甚至会因前期的过度上涨而发生走势逆转，导致短期大跌，最终形成暴涨暴跌之态。</li>
<li>股价原动力的爆发力度：估值法，就是对股票进行估值，看看股价是否被低估。绝对估值法，就是评估股票的绝对投资价值。相对估值法。</li>
</ul>
<br>

<p>综上，<br>1、不能选错股票、不能赔钱，核心：大牛股的主升浪，核心：低估值股票挖掘，再看其成长性，最后看其有无热门概念。<br>2、短期暴涨型主升浪<br>3、连续5～6个甚至是十几个“一字涨停板”暴涨<br>4、一是原动力，二是持续推动力，三是阻力，四是运动过程中新加入的作用力。<br>5、股价原动力的爆发力度：估值法</p>
<br>
<br>

<h4 id="绝对估值法"><a href="#绝对估值法" class="headerlink" title="绝对估值法"></a>绝对估值法</h4><p>评估股票的内在价值与股价的差值，当内在价值与股价的比值越大，则股价的上涨潜力越大，这种潜力一旦遇到合适的市场时机，就有可能爆发出来，转变为引发股价暴涨的原动力。</p>
<ul>
<li><p>投资价值估值法<br>1、目标股票的长期投资收益率与长期国债收益率进行比较，两者差值越大，则长期投资价值越高，这里的长期应该是指5～10年以上。巴菲特的标准更高一些，他的长期价值投资收益率基本定在10年内的年均复合收益率达到10%以上。<br>2、在考虑到风险溢价的情况下，择股标准应该趋于保守，所选择的长期价值股应该是低市盈率的成长股。按照价值投资的基本标准，对于长期成长股来说，其合理的股价范围应该满足PEG＜1（PEG=市盈率/收益增长率），且越低越好。当PEG＜0.5时，股票就存在足够的安全边际。<br>3、若一只股票未来的5～10年的年均复合收益增长率能够达到20%，则当该股的市盈率跌到10倍时，就具有极好的投资价值。应该注意的是，此时的股价虽然不高估，但也不是<strong>明显低估</strong>，还难以产生爆发性很强的估值修复力。假若此时，股市暴跌，该股股价随着股市暴跌而意外地跌到5倍市盈率，那么，此时该股股价就被明显低估了。</p>
</li>
<li><p>资产价值估值法<br>1、比较股票净资产值与总市值的比值，比值越大，则股价低估越多，股价上涨空间越大。<br>2、这类股票主要体现在隐蔽资产股上面。自2006年来，隐蔽资产股经常走出大牛股。<br>3、一隐蔽资产的大幅升值不被知晓，二隐蔽资产的增值过程是公开的、渐进式的，但隐蔽资产股的股价涨幅在起初滞后于隐蔽资产的增值幅度，最终股价会出现报复性补涨，使得隐蔽资产增值的这一事实成为了股价暴涨的原动力。</p>
<blockquote>
<p>2012年7月的罗顿发展：当《二十一世纪经济报道》披露该股在10年前曾花了2亿元在博鳌的黄金地段买下了1800亩土地，这些土地现值100多亿元，以罗顿发展总股本4.4亿股计算，仅这些土地价值就足以支撑该股股价到达15～20元。在该信息公告时，罗顿发展的股价只有3.9元。该信息公告后，罗顿发展的股价就出现了连续涨停板，不到一个月股价涨幅达到170%！在此期间，上证指数还是下跌的。<br><br>2013年1月28日至2月6日的西水股份就属此例。西水股份持有1.3亿股兴业银行，当兴业银行股价暴涨后，西水股份持有的兴业银行价值大幅增值，几乎等于西水股份的总市值，由于西水股份还持有未上市的天安保险11亿股，以及其它资产，这等于说除去兴业银行外，西水股份持有的其它资产被忽略不计了，这是明显不合理的，于是，在2013年1月28日至2月6日，该股就出现报复性的补涨，股价从6.6元涨到了11.8元，几乎翻倍。</p>
</blockquote>
</li>
<li><p>绝对套利估值法<br>1、上市公司被以高出市场价一大截全额要约收购时，其要约收购价格就是股票的真实内在价值，这个内在价值与现价的差值就是股价的无风险套利空间。假若市场是有效的，那么，股价就会几乎一步到位地涨到那个要约收购价格，于是，这个要约收购公告就是造成股价暴涨的原动力。<br>2、前几年被要约收购退市的石油大明、辽河股份等，就属此例。<br>3、当然，在有些时候，要约收购并非是全额的，而是部分的，如去年的重庆啤酒，那么，这类股票的上涨就可能不是一步到位的，而是渐进式的，二级市场就存在套利空间。在渐进式的情况下，要约收购公告虽然还是股价上涨的原动力，但它却会受到当时的市场运行状态、该股同板块股票运行状态等因素的影响，因此，股价运行就不仅受到原动力的作用，还受到其它新的作用力的作用。<br>3、这种情况还出现在某些上市公司大股东回购公司部分股票时，回购部分股票的股价并非是股票的真实内在价值，该价格只是大股东认可的价格而已，不能与完全要约收购价格相提并论，这是需要搞清楚的。</p>
</li>
</ul>
<p>综上，<br>1、按照逻辑，估值存在潜力时，股价未能反应的原因为，存在其他阻力，如市场运行状态、同版股票运行状态等。<br>2、概括，三个绝对估值方法为，PEG&lt;0.5的长期投资、隐蔽资产(所持股票或投资的公司或土地等)、全额要约收购与部分要约收购。</p>
<br>

<h4 id="相对估值法"><a href="#相对估值法" class="headerlink" title="相对估值法"></a>相对估值法</h4><p>一是基本面预期类，二是市场面预期类，三是技术面预期类，四是大盘面预期类。</p>
<p>1、就是领涨的市场、板块或者个股，它们是那些拥有比价效应、等待补涨的市场、板块或者个股的追赶的目标。<br>2、在补涨者开始补涨时，领涨者可以暂时停止上涨；在补涨者开始补涨时，领涨者还可以继续上涨，这对补涨者的领涨作用更大。<br>3、随着价格等自变量的变化，心理预期这个因变量也会随之变化。可见，若用一句话表述心理预期类投机价值，那就是–&gt;一切处于变化之中。<br>4、戴维斯双击，是指因股票收益持续成长，投资者会对股票未来的收益增长产生更高的预期，在未来收益还未实现的情况下，就以未来的高收益定位其市盈率，从而提升其市盈率定位水平，最终导致股价过度上涨。<br>5、<code>提升了股票的市盈率，也就是提升了股票的估值，这相当于产生了新的投机价值</code><br>6、戴维斯双击原理： 由于 <code>股价=每股收益*市盈率</code> ，当每股收益持续增长时，即使市盈率保持不变，股价也会同比上涨，这属于戴维斯双击中的第一击；当每股收益持续增长时，若还提升市盈率，则股价会更上一层楼，涨幅会更大，这属于戴维斯双击中的第二击。戴维斯双击能够导致乘数效应，使得股价涨幅加倍。由于戴维斯双击提升了股票的市盈率，也就是提升了股票的估值，这相当于产生了新的投机价值。戴维斯双击效应产生的根源，就是投资者依据历史数据的惯性，顺势推导、过度预期的结果。<br>7、更重要的是<code>因比价关系引发的股价跟涨而造成的股价虚高</code>，但这并不妨碍我们得出因基本面过度预期而产生新的投机价值的结论。<br>8、主流热点是市场热钱追逐的对象，处于热点中的板块与股票一定是短期内最牛的，基本上属于短线暴涨型品种。<br>9、市场面预期，就是对于市场未来炒作热点的预判。一旦判断某个板块或者个股刚刚成为或者在未来会成为市场炒作热点，那么，该板块或者个股就具有新的投机价值，股价就具有投机价值所赋予的上涨空间。<br>10、比价关系是领涨者已经给出了方向，给出了大致的投机价值空间，但市场面预期是市场才刚刚开始启动热点，或者还未开始热点，但投机者只是预见到了炒作热点将出现，但投机者一般还难以确定投机价值到底有多大，上涨空间有多大，这要走一步看一步。<br>11、在2012年12月中旬，当媒体披露北斗系统将投入商业应用时，超级主力立即抢进了北斗星通等龙头股，造成北斗概念股暴涨。我认为，这次对于北斗概念股的炒作，属于市场面预期主导下的“自我增强”型炒作模式。其基本逻辑是：超级主力判断北斗概念也许会成为市场热点，但到底能形成多大的热点，他们也许并没有很大把握，因为北斗概念是一个全新的概念，人们很陌生，超级主力担心投资者不认同；但超级主力知道，要让投资者认识且认同北斗概念，最佳的方法就是让北斗概念股暴涨，因为股票暴涨一定会吸引全市场的眼光，所有人都会好好研究的，这就是一种“胡干胡有理，越干越有理”的江湖思维。当然，平心而论，毕竟北斗导航属于国家级的概念，超级主力知道，即使胡干的风险也不大，最终，他们成功了。<br>12、赚钱的境界是这样的：最低的境界是打工，用自己的身体赚钱；次低的境界是做实业，让别人的身体为自己赚钱；较高的境界是玩钱（做金融），让钱生钱；而最高的境界是玩规则，让所有的人与所有的钱为自己赚钱。这就是赚钱的金字塔模型。<br>13、以二级市场来论，也存在一个投资者的金字塔模型：<br>最低的境界是交易，次低的境界是跟庄，较高的境界是坐庄（单只股票），而最高的境界是引领市场、发动行情（玩板块甚至玩整个市场）。北斗概念主力就属于最高境界的，不仅是北斗概念，去年市场的几乎所有的市场热点——3D打印、手游、传媒等，都是不同的超级主力们发动的行情。这些主力们“敢为天下先”的底气何在？有人可能认为是他们的资金实力雄厚，此言谬矣，你若有钱，去发动钢铁板块、水泥板块试一试？我认为，这些超级主力的过人之处，还是<strong>对于未来市场主流热点的预期、研判的功力深厚</strong>，任何人要是有这个本事，就一定会走在市场曲线的前面。<br>14、价量时空。<br>15、基本面分析公司未来价值，关注的是现在的股价，以及未来的股价定位，不太关心过去股价。<br>16、基本面分析的核心就是定价理论，即，对于现在股价是否合理进行评判——若低估了就可以买进。公司未来价值的预测，以期判断股价的上涨空间。巴菲特的办公室里是没有电脑的，但他非常关心现在的股票报价与其内在价值的关系，至于股票过去是什么价格或者什么走势，他也从来不看股价走势图。<br>17、市场面分析是通过比价关系来推测股价定位。所以，市场面分析主要是看现在股价与未来股价，对于过去股价也不是太关心。这很容易理解，若某个板块因热门概念而出现暴涨时，在依据比价关系挖掘该板块内的补涨股时，看绝对价格要比看历史走势重要得多，只要股价被低估，任何形态的股价走势都可以出现补涨。<br>18、技术分析的目的是为了预测股价未来走势，侧重于研究过去价格。它将股价走势图从基本面、市场面中抽提出来，让股价走势图完全独立了。走势图一旦独立，技术分析也就自成一统。技术分析将股价走势图看成一个活物留下的足迹，通过研究这些历史足迹，去预测这个活物下一步或者未来将走向何处。技术分析就是分析与预测<strong>这三种趋势的产生、持续与转换关系</strong>。<br>19、首先，技术分析要研究<strong>趋势的产生</strong>。<strong>一个新趋势的产生，一定是原有的其它两类趋势中的某一个终结的结果。</strong>如，一个新的上涨趋势的产生，就一定意味着一个原有的下跌趋势，或者横盘趋势的结束。所以，研究新趋势的产生，意味着要同时研究旧趋势的结束，两个相关趋势要同时研究。<br>20、其次，技术分析要研究趋势的持续。<strong>一个趋势一旦产生后，就会持续</strong>，这种持续就相当于趋势产生了顺势发展的惯性。很显然，假若能够认识到一个上涨趋势是处于持续发展中，或者惯性上涨中，那么，投资或者投机就变成了很简单的事情——买入持有，随着趋势惯性上涨就可以了；假若…<br>21、再次，技术分析还要研究趋势的结束。与研究趋势产生，本质上是一回事，只是研究的对象不同罢了<br>22、所有的技术分析方法，万变不离其宗，就是<strong>为了揭示趋势的产生、持续与结束</strong>，或者简言之，就是为了揭示趋势惯性。投机价值就是预期价值，而趋势惯性本身就具有预期性质，所以，趋势惯性能够提升或者降低投机价值。<br>23、波浪理论有某一浪的预期上涨高度、形态理论有形态突破后的预期量度涨幅、量价理论有放量突破后的惯性上涨高度，等等。224、股市有谚：横有多长，竖有多长。说的是一只股票若长期做底或者横盘，股价一旦启动向上突破，那么，涨幅机会十分惊人。这个惊人的涨幅，就是技术面预期类赋予的投机价值，因为这个投机价值的存在，使得股价最终具有了预期中的上涨空间。这是一个自我实现的预言——因为技术面给出了预期涨幅，那么，买方就会在涨幅到顶之前持续买入，以获取预期中的那个投机价值收益。<br>25、技术面预期类投机价值对于短线交易，特别是对于挖掘主升浪启动点或者启动阶段、主跌浪的启动点与启动阶段都是至关重要的。<br>26、对于短线交易来说，趋势、形态、量价关系是最重要的三个因素，而均线、指标等，就属于次级重要的因素了。<br>27、“济安金信价值分析系统”，该系统的主开发人杨健教授，说<strong>均线系统属于滞后指标</strong>，对于操作意义不大，而该系统运用了许多自创的新指标。<br>28、巴菲特说：“投资只需要学习两门课程就可以了，一门是如何评估企业价值，一门是如何看待股市波动。” </p>
<p>综上，<br>1、基本面预期<br>2、热点等市场面预期研判<br>3、戴维斯双击<br>4、领涨、补涨、跟涨虚高</p>
<br>

<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>大体说的，也是已知的，但是给了个分析框架。<br>基本面向好，为什么不能反映在股价。为什么8个月涨了4倍多。涨停的逻辑在哪。等问题在框架内还是有一定程度解释力。</p>
<p>框架基本操作思路是牛股的大波段，选股思路为低估值+成长+题材。<br>并由力学中四种力，四种力的不同合力模式构成物体运动。<br>股市中不同合力模式构成股价上涨，主要三方面估值修复+价值成长+投机价值。<br>转而对原动力进行估值法定量衡量。<br>绝对估值法包含：PEG、隐蔽资产、要约收购。<br>比价效应带来领涨、补涨、跟涨虚高。而预期无参照物，不同于比价，即一切处于变化之中。<br>相对估值法包含：基本面预期、市场面预期(热点)、技术面预期、大盘面预期。</p>
<p>再统一，即为投资价值和投机价值。</p>
<p>补充：<br>基本面的信息主要包括政策性信息、行业性信息、经营信息（产品价格信息、重大合同信息、新项目与新产品信息）、财务信息（营收信息、净利润增减信息）等。<br>技术面信息主要包括价格图表、技术指标与交易信息，其中，价格图表是技术分析的核心要件，不同的交易者关注不同的图表；技术指标更是五花八门，多达数百种，只能是各取所需了；而交易信息主要是各种交易数据的排行榜，包括涨幅榜、换手率榜、成交量榜、量比榜、成交金额榜等。</p>
]]></content>
      <categories>
        <category>股市理论</category>
        <category>数女-谷</category>
      </categories>
      <tags>
        <tag>股市理论</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机网络</title>
    <url>/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<br>

<br>

<h4 id="要刷的几个题目"><a href="#要刷的几个题目" class="headerlink" title="要刷的几个题目"></a>要刷的几个题目</h4><span id="more"></span>

<ol>
<li>海明码、crc</li>
<li>信道利用率=（帧长/发送速率） / [（帧长/发送速率） +  rtt]   – 衡量发送方是不是在等待信道而空闲</li>
<li> 信道吞吐率 = 信道利用率 *  发送速率</li>
<li>二进制指数退避重传算法，k=min{k,10} [0,2^k-1]</li>
<li>PPP/HDLC的帧格式、区别</li>
<li>以太网相关记忆</li>
<li>停等、GBN、SR、ALOHA、CSMA、CSMACA、CSMACD、令牌环 (随机访问介质访问控制才有冲突)</li>
<li>ip数据报的分片过程，及其中某些首部字段含义</li>
<li>子网掩码</li>
<li>CIDR、最长匹配前缀、CIDR和子网掩码</li>
<li>ipv4和ipv6的报文格式，以及区别</li>
</ol>
<br>

<br>





<h4 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h4><ol>
<li>计算机网络的功能：数据通信、资源共享、分布式处理、提高可靠性、负载均衡</li>
<li>组成：硬件软件 及协议。工作方式：边缘部分(主机 p2p c/s方式)及核心部分(路由器及大量网络)</li>
<li>功能组成：通信子网(实现数据通信 – 包括物理层、数链层以及网络层)和资源子网(实现资源共享和数据处理 – 包括会话、表示和应用层)，传输层属于两个子网的接口</li>
<li>分类：按照分布范围：WAN MAN LAN PAN ；公用网、专用网；按交换方式分，电路交换、报文交换、分组交换；按拓扑结构分，网状(广域网)；按传输技术分，广播式、点对点</li>
<li>RFC – 因特网标准形式</li>
<li>组织：ISO ITU IEEE IETF</li>
<li>性能指标：速率 带宽 吞吐量。带宽：带宽变大是发送变快，一定时间往链路注入的bit更多，而不是传播更快。吞吐量：单位时间通过某个网络(信道/接口)的数据量。</li>
<li></li>
<li></li>
</ol>
<br>

<br>

<h4 id="层次"><a href="#层次" class="headerlink" title="层次"></a>层次</h4><ol>
<li>TCP/IP协议栈：从下到上：物理层、数据链路层、[ARP] IP [ICMP/IGMP]、TCP/UDP、HTTP/FTP/DNS</li>
</ol>
<br>

<br>



<h4 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h4><br>

<br>



<h4 id="数据链路"><a href="#数据链路" class="headerlink" title="数据链路"></a>数据链路</h4><ol>
<li><p>功能</p>
<p>保证数据无差错地传输到相邻节点，实现透明传输。进行： 组帧、差错控制–校验码、流量控制、链路管理(有连接的链路进行连接维持和释放)。从而为网络层提供服务(有连接有确认，无连接无确认，无连接有确认)。</p>
</li>
<li><p>封装成帧</p>
<p>4种方法。一般用比特填充和违规编码。</p>
</li>
<li><p>差错类型，位错和帧错。帧错：重复、失序、丢失。纠错 - 海明、检错 - 奇偶 crc(填上n-1个0 除以生成多项式,加上余数 又叫冗余码 FCS帧检验序列。除法是异或)。</p>
</li>
<li><p>有线传输一般，无确认无连接，保证交给上层-网络层。无线传输一般是有确认的(不一定有连接)，尽量避免差错。</p>
</li>
<li><p>流量控制，收不下就不去确认。包括，停止等待协议、滑动窗口。</p>
<p>传输层的流量控制是公告窗口。</p>
</li>
<li><p>GBN：累积确认(捎带)；接收方只按序接收、否则丢弃，并确认帧催更；发送发确认序列号最大的、按序号达到的；超时重传；发送窗口2^n-1，接收窗口1(n为帧序号长度)；</p>
</li>
<li><p>SR：选择重传协议。只重传出错的；接收方有缓存的窗口；对数据逐一确认，收到一个确认一个；最大接收和最大发送窗口大小：2^(n-1)</p>
</li>
<li><p>点对点链路，广播链路。信道访问介质控制：静态、动态划分信道。动态分为 轮询和随机访问。随机访问介质控制：aloha、csma、csmacd、csmaca。</p>
</li>
<li><p>ALOHA ，纯aloha 和 时隙aloha。吞吐是一段时间的发送成功的帧数。一个想发就发，一个在时间片开始发送和重传。</p>
</li>
<li><p>CSMA：载波监听、多点接入(逻辑总)，分为1坚持，非坚持，p坚持。1坚持是忙了还在监听，空了就发。非坚持是忙了随机等会监听，空了就发。p坚持是忙了随机等会再监听，空了p概率发送。</p>
</li>
<li><p>轮询：轮流(3个缺点)，令牌传递协议。令牌：1过程 2缺陷 令牌开销；等待；单点故障 3应用 令牌环网-逻辑环，物理星；负载重和通信大的。节点在令牌后加上数据，修改令牌为忙，每个节点看到接收方不是自己则传给下一个，直到回到发送方，发送方发现数据正确，则说明发送成功。</p>
</li>
<li><p>CSMACD：总线型的以太网 有线。半双工网络。边发送边监听，电压检测。但是由于传播时延还是会冲突。争用期2*传播时延τ。重传：二进制指数退避重传算法(重传上限16次)。最小帧长：为了碰撞及时地被叫停–帧 的 传输时延至少2τ–以太网64B。</p>
</li>
<li><p>CSMACA：无线。先听后发，载波能量以及混合检测。无线局域网中无法全方位检测、隐蔽站问题，我发给b的时候不知道c发不发给b。– 检测空闲，a先发rts(包括接收方、发送方地址以及下一份数据持续发送的时间)，b接收端收到返回cts。c收不到b的cts，则不发了。a收到cts后，开始发送数据并预约信道(告诉其他人我要发送多久)，b收到数据后crc检验然后再发ack给a。a收到ack继续发下一帧，没有则重传(二进制指数退避)。</p>
</li>
<li><p>局域网：网络拓扑(星、总线、环、树) 以太网则是逻辑上总线型 ；传输介质：有线无线；根据介质访问控制，分为以太网(802.3 将数据链路分为了LLC和MAC两个子层，LLC主要为上面的网络层服务 比如建立确认连接的等，MAC主要帧同步之类)、令牌环网、FDDI、ATM、无线局域网(WLAN 802.11)。wifi是WLAN的一种应用。802.5令牌环、802.8光纤…</p>
</li>
<li><p>以太网：CSMACD，基带总线，无连接不确认(以太网提供的服务是不可靠的交付，即尽最大努力的交付。当目的站收到有差错的数据帧时就丢弃此帧，其他什么也不做。差错的纠正和重传由高层来决定。)，无差错接收，但不可靠，物理星逻辑总，</p>
</li>
<li><p>10baseT：基带、双绞线UTP、10Mb/s、物理星逻辑总、每段双绞线最长100m、csmacd有冲突、曼切斯特编码</p>
</li>
<li><p>适配器–网卡，每个网卡有mac地址，全球唯一，48位 – 前24位代表厂家 ieee确定 6个16进制数表示，在网卡的rom上有</p>
</li>
<li><p>以太网的MAC帧：前同步码7B + 帧定界符1B + <strong>以太网MAC帧 （源地址6B + 目的地址6B + 类型2B + 数据 + FCS4B）–这个是mac层</strong> = 物理层  数据46B-1500B</p>
</li>
<li><p>高速以太网：大于100M就是高速。全双工时候，用的是交换机，隔离了冲突域，无冲突。光纤上1g or  10g。</p>
</li>
<li><p>无线局域网：802.11、MAC帧头格式：分为四种(to ap–BSSID SA DA,from ap–DA BSSID SA,wds–RA TA DA SA,ibss–DA SA BSSID) 、每个ap站点相连的为一个基本服务集BSS,多个BSS构成ESS，需要一个分配系统DS–将所有的ap接入到有线的线缆中，将有线和无线结合。自组织的网络即把所有的主机安排在同一个网段….所有主机之间直接通信，无固定基础设施(DS或者有线)</p>
</li>
<li><p>广域网：分组交换技术，多个局域网组件而成，链路层设备交换机互联，(局域网只覆盖物理层和链路层，广域网还有网络层–路由器)，广域网强调资源共享</p>
</li>
<li><p>ppp：只支持全双工，封装帧、透明传输(字符填充法 转义字符)但是不需要序号纠错、不需要流量控制，简单差错检测，支持多种ip网络层协议，数据部分最大长度MTU，压缩，IP地址协商… 三个部分：1LCP身份验证2 NCP建立逻辑连接以支持各种网络层协议 3将ip数据报封装到串行链路。面向字节</p>
</li>
<li><p>HDLC：只支持全双工，不属于tcpip，面相比特，0比特插入法(51插入0)的透明传输，crc，有编号有确认 可靠，不纠错，主站、从站、复合站，三种数据操作方式：正常响应(主站同意从才能发)、异步平衡(每个复合站都可以发 地位平等)、异步响应(从站无需同意)，三种帧类型：无监息 帧</p>
</li>
<li><p>链路层设备：物理层：集线器(集线器相连的任何两个设备之间有通信、其他都不能通信)、主干集线器(冲突域)、光纤调制器(距离)，链路层：网桥or交换机(将以太网连接并根据mac地址进行过滤 分隔冲突域)。物理层虽然解决了距离问题，但是导致冲突域扩大。</p>
</li>
<li><p>网桥：透明网桥和源路由网桥。透明网桥通过自学习算法来填网桥的转发表，转发表即为主机和网桥接口之间的对应。源路由则是，发送方在帧首部加入 路由最少/时间最少 的 详细路由信息。交换机分为：直通式(查完6B的mac目的地址就直接转发)和存储转发式交换机(高速缓存中存放帧并检查是否正确，错误则丢弃)。交换机就是多接口的网桥。</p>
</li>
<li><p>冲突域和广播域：冲突域中同一时间只有一台设备才能发送数据。广播域是，网络中能接收到广播帧的所有设备集合。网络层设备都可以隔离，链路只能隔离冲突。</p>
</li>
</ol>
<br>

<br>

<h4 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h4><ol>
<li><p>功能 ： 把分组从源传到目的，单位是数据报。1路由和分组转发2异构网络互联3拥塞控制(闭环和开环两种控制方式)</p>
</li>
<li><p>数据交换方式：首先，为什么需要数据交换？(为了各个节点之间的通信，建立的链路不能过于复杂也需要远距离，就需要交换网络里的交换设备完成交换)。类别：电路交换、报文交换、数据报交换(分组交换)。电路：建立连接后通信。报文：先存储，等链路空闲后转发。分组交换：将报文分组。传送数据大且传送时间远远大于呼叫则选择电路交换。后两者都有存储转发技术。</p>
</li>
<li><p>数据报和虚电路：虚电路号，路由器记录虚电路号的转发接口，过程：建立连接、发数据、释放，保证有序到达。</p>
</li>
<li><p>路由算法：路由表(目的网络ip地址、子网掩码、下一跳ip地址、接口)。为了找到最佳路由而使用的算法，有两种分类：静态路由算法(非自适应，手动配置路由信息)、动态算法(路由器间彼此交换信息)。动态分为：全局性(链路状态路由算法–掌握完整的网络拓扑和链路费用信息)和分散性(距离向量算法–只掌握物理相邻的邻居以及链路费用)。OSPF 、RIP</p>
</li>
<li><p>分层次的路由选择协议：AS自治系统内的路由协议在其他AS是透明的。只要AS内的路由器都用本AS内的路由协议，并确定好和其他AS之间的路由方式。分为两类：内部网关协议IGP(AS内，OSPF / RIP)以及外部网关协议EGP(AS间，BGP-4)。</p>
</li>
<li><p>ip数据报格式</p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ip%E6%95%B0%E6%8D%AE%E6%8A%A5.png" alt="ip数据报"></p>
<p>ip数据报分为首部和数据部分(TCP/UDP段)。首部又分为固定部分和可变部分。固定<strong>20B</strong>。首部长度最小为0101(5)，单位是4B。总长度是包含<strong>数据部分的总长，单位1B，即总长度最大2^16-1</strong>，一般不会这么大，一般满足MTU。用于<strong>分组的”标识、标志、片偏移“</strong>。生存时间是ip分组在网络中的寿命，防止无限制地兜圈子。协议指数据部分所使用的协议，(即上层的协议)，如<strong>TCP – 6 ,UDP – 17</strong>。首部检验和–质检验首部。源地址、目的地址都是32bit。可变部分为：可选字段、填充字段。</p>
</li>
<li><p>ip数据报分片：以太网中MTU是1500B，其中至少20B为首部。首部：同一个报文的分片都有同样的标识；标志DF有0或者1，0是允许分片；最低位MF，0是最后一片/无分片；片偏移能够确定分组在整个数据报的哪个位置开始的，单位为8B。</p>
</li>
<li><p>一种八片首饰，总长度是1B，片偏移是8B，首部长度单位是4B。</p>
</li>
<li><p>ipv4地址</p>
<ol>
<li><p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E5%88%86%E7%B1%BBip%E5%9C%B0%E5%9D%80.png" alt="分类ip地址"></p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%89%B9%E6%AE%8Aip%E5%9C%B0%E5%9D%80.png" alt="特殊ip地址">    </p>
</li>
</ol>
</li>
<li><p>网络地址转换NAT：私有ip地址(A：10.0.0.0-10.255.255.255;B：172.16.0.0-172.31.255.255;C：192.168.0.0-192.168.255.255)网段个数分别为1 16 256个。如何实现私有ip地址和外部互联网主机的通信？安装NAT软件，NAT路由器会为本地网各个主机生成一个全球的地址代表，然后再分发给具体主机，转换表里表明外部的主机以及端口（这里的外部主机就是那个生成的代表地址），LAN表示本地网段里的主机ip和端口号。也就是在外部，各个内部网段的主机只是一个地址的不同端口。</p>
</li>
<li><p>子网划分，子网掩码</p>
</li>
<li><p>ARP：数据经过传输层分组以及网络层分片、加ip首部后，在数链层加上mac地址首部。每个主机有自己的ARP缓存，缓存本局域网中的ip对应的mac。如果ip不在本局域网中，则填入目的网关的mac地址。如果此时不知道目的ip的对应mac地址，则需要发一个广播ARP请求(包括源ip、目的ip、源mac地址)，目的ip主机收到后则 返回(本ip、本mac地址)。数链层加上mac后，加上FCS等，传给物理层，进行传输。ARP协议是自动进行的</p>
</li>
<li><p>DHCP：1主机广播DHCP发现报文(找到网络中的DHCP) 2DHCP服务器广播提供报文 3主机广播DHCP请求报文 4DHCP服务器返回广播DHCP确认报文。</p>
</li>
<li><p>ICMP：差错报告报文，几种类型：终点不可达、源点抑制、ttl=0、首部字段错误(参数问题)、重定向(让源主机知道下次更好的路由)。ICMP的前8个字节+IP数据报首部+IP数据报数据部分前8B = ICMP差错报告报文。再加上IP数据报的首部，构成一个ICMP报文的IP数据报。不发送ICMP：ICMP出错不发ICMP；第一个分片的数据报后的所有数据报片都不发ICMP；组播(1对多)地址的不发；特殊地址(环回等)不发。应用：比如ping（用了ICMP询问报文），用于回送请求和回答报文；时间戳请求和回答报文等。Traceroute(用了ICMP时间超过差错报文)，来跟踪一个分组从源到终点的路径(方法是将ttl设为不同的值，那么到了中间的路由器，就会返回ttl=0的差错报告报文，就能知道路由过程)。</p>
</li>
<li><p>ipv6数据报格式</p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/ipv6%E6%95%B0%E6%8D%AE%E6%8A%A5.png" alt="ipv6数据报"></p>
<p>流是用于确定一些数据报是同一个流的。下一个首部：下一个扩展首部或者上层协议首部。有效载荷：扩展首部+数据。跳数限制：ttl。源地址和目的地址都是128bit。v6的首部是固定的40B。可变的放在扩展里。</p>
</li>
<li><p>ipv4和ipv6的区别：地址位数的扩展；没有了首部校验；将可选移到扩展首部里；ipv6首部长度是8B倍数，而v4是4B的整数倍；v6即插即用，不需要DHCP协议；v6只能在主机分片，如果在后面的链路层有MTU的限制，路由器只能将此v6报文丢弃，并且发送一个ICMPv6的分组过大报文；v6有资源的预分配；没有了协议和总长度字段，多了下一个首部和有效载荷长度字段；v6没有了服务类型字段。</p>
</li>
<li><p>ipv6表示：一般冒号十六进制，压缩：删除0，每一组至少有一个数字表示；或者一连串的0用冒号取代 – 只能出现一次两个冒号。</p>
</li>
<li><p>ipv6的类型： 单播–目的or源、多播–目的地址、任播 – 目的(对多台主机里最近的一台主机通信)。</p>
</li>
<li><p>如何从ipv4迁移到ipv6：双栈协议(同时启用)、隧道协议(重新封装)。</p>
</li>
<li><p>RIP协议：适用于小互联网，16表示网络不可达，最多包含15个路由器。每30s，只和相邻 路由器交换自己的路由表。180s，没有信息来则更新自己路由表：这个邻居死了。在拥有所有信息后，用距离向量算法得到 距离和下一跳。</p>
</li>
<li><p>RIP报文</p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/RIP%E6%8A%A5%E6%96%87.png" alt="RIP报文"></p>
<p>是应用层协议，使用UDP传输数据。将RIP的首部和路由部分作为UDP的数据，加上UDP的首部，再加上IP首部，构成ip数据报。其中，路由部分最多包含25个路由信息，所以更多路由需要多发几个报文。</p>
</li>
<li><p>RIP好消息传得快，坏消息传的慢：不断更新，直到距离更新到16，才直到这个是不可达的。</p>
</li>
<li><p>OSPF协议：适用于大型互联网，通过划分区域的方式，区别区域内部路由器和区域边界路由器以及主干路由器，以及自治系统边界路由器。开放最短路径优先协议。使用了分布式的链路状态协议。只有变化时候，洪泛向所有路由器发送 <strong>本路由器相邻的所有路由器的链路状态</strong>，如何哪些路由器相邻以及费用。=&gt;所有路由器都会有一个全网的拓扑图。OSPF分组：数据加首部最为ip数据报的数据部分。暂时理解为是网络层协议。</p>
<p>TIPS:发送的信息是与本路由器相邻的所有路由器的链路状态 ,只涉及与相邻路由器的连通状态,与整个<em>互联网</em>的<em>规模无关</em></p>
</li>
<li><p>链路状态路由算法：1每个路由器发HELLO分组，发现邻居，并更新和邻居之间费用metric 2构造DD描述分组给邻站自己的数据库中链路状态信息的摘要 3DD信息我都有了，就不处理，否则发LSR链路状态请求分组来请求自己没有的信息 4收到LSR，发送LSU链路状态更新分组 5更新后返回LSAck链路状态确认分组   – 最后 用Dijkstra来计算最短路径。 (简单说，就是先hello找邻居，然后发摘要，然后看看别人的摘要再发请求，最后整理所有的信息)</p>
</li>
<li><p>BGP：网络较大，只要求到达其他AS的较好的路由。各个AS的BGP发言人，交换到达某个网络所经过的一系列AS，即交换的是一系列的路径(距离向量)。BGP的报文作为TCP的数据部分，为应用层协议，需要TCP传送，即首先要建立TCP连接，然后在此连接至上建立BGP会话，利用BGP会话交换BGP报文。支持CIDR，需要网络前缀。</p>
</li>
<li><p>最常用版本BGP-4的四种报文：OPEN报文(认证及发现邻居)，UPDATE(路径变更)，KEEPALIVE(邻居之间周期性连接，也可以作为OPEN确认)，NOTIFICATION（报告差错、关闭连接。）</p>
</li>
<li><p>RIP OSPF BGP区别：</p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E5%85%B3%E5%8D%8F%E8%AE%AE.png" alt="网关协议"></p>
<p><img src="/2021/06/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/%E7%BD%91%E5%85%B3%E5%8D%8F%E8%AE%AE2.png" alt="网关协议2"></p>
</li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title>2021-09</title>
    <url>/2021/09/13/2021-09/</url>
    <content><![CDATA[<br>

<br>

<h4 id="0913"><a href="#0913" class="headerlink" title="0913"></a>0913</h4><br>

<p>1、每个领域<span id="more"></span>都是一个系统。国际是一个个系统构成的全球系统。而经济、技术各自为一个系统。有的系统受人的行为以及群体心理影响，有的则完全不受。系统中，会产生各种各样的需求，为了实现需求，会不断地增加新的概念和机制。记住，这些都是为了实现需求。</p>
<p>那么一旦有新的需求，必然带来各种解决方式，其演进是越来越抽象地、一般性地解决问题。非抽象的方式会有各种各样的局限。比如各种模型，金融模型或者量化模型，或者技术里的各种解决方案。当更好的概念带来成熟的机制，能够去分析出对现有系统的影响，则是对系统有相当的了解了。</p>
<p>兵法也是一套系统，管理也是。为什么需要兵法，因起有内在可描述和重复的规律。以及兵法让你认清主次。如果说没有主次，事物各个角度都具有相当的重要性，没有很大的区分，则系统不产生竞争。有竞争之处，有高低之分，必然对应着系统内部隐含的认知层次。</p>
<p>认知的浅层带来基础的价值，对应获得的收益。越深则对应的收益则越大。但是经济系统的细部，又有各自的子系统。因此，有钱的人不必对大的系统有高层次认知，而只要在小的系统内战胜了局部。局部的规律，也是曲折而累积的，并不违背大的原则。</p>
<p>对于人的选择，也应该择小而谋。从而才能越发壮大。唯有第一桶金之后，后续才更从容，去以实践对另一些局部系统进行进攻。兵法上也是，不需要战胜所有敌人，不需要去打注定的败仗，只需要对基本诉求以满足，对所谋对象有基础性掌握。如果贪图大的系统，而能力不匹配，则是虚耗精力，虚妄自大，而不得志。面试时候，需攻取的是面试官，而不是所有的知识。知识是针对性的。是一种把握和理解。退一步，而不要退万步。求一全而不是万全。当然不是指不求万全之策。</p>
<br>

<p>2、重复后，才能真的理解。一遍遍地实践。</p>
<p>所有短时间内高度的成就和能量的聚集，一方面是正确的方向和清晰的目的，二方面则是高效率的重复。</p>
<p>高效率不易。因为诱惑颇多。人渴望大的系统里的认知层次，而对局部系统的深入不以为然。因为局部没有新鲜感。人受新鲜感和好奇心驱使而有更大的动力。是否好事还是得看能力是否匹配。</p>
<br>

<p>3、case很重要。尤其是有重要意义的case。</p>
<br>

<p>4、再复杂和抽象的概念或者思路，都是和质朴的东西联系起来才有意义。返璞才能归真。</p>
<br>

<p>5、大部分的系统的认知都是规则系统。专家系统。而不是深度学习那种，机器学习系统。</p>
<p>而机器学习也不过是在统计意义上去理解系统。</p>
<p>数据分析也是。在统计意义上构建起来的模型，更好地对规则进行了阐释。</p>
<br>

<p>6、系统本部复杂。而是人给他加上了对问题和认知的恐惧因素。</p>
<p>系统只需要逻辑自洽。甚至从细部的切入是自洽的，则可检测系统的完整性。</p>
<br>



<h4 id="0918"><a href="#0918" class="headerlink" title="0918"></a>0918</h4><br>

<br>

<h5 id="房圈的开放生态链"><a href="#房圈的开放生态链" class="headerlink" title="房圈的开放生态链"></a>房圈的开放生态链</h5><p>1、IBM要进军家用办公电脑(PC)市场，需要一个家用办公电脑的操作系统。比尔.盖茨两手空空，但是他说，我有，我有，我全都有。他拿到巨额订单之后，找到当时系统做得最好的一个人，并没提订单的事，以极低的价格买了一个操作系统。而且是<strong>买断</strong>，后面可以在上面继续开发的。</p>
<p>2、基于<strong>生态链赚钱，才是稳固的赚钱手段</strong>。Intel每发布一个新芯片，微软每发布一个新的操作系统，大家就跟着赚一波钱。这种模式养活了生态链上无数的人，产品让全世界的人获益。</p>
<p>3、大IP就是房圈的芯片。但是<strong>IP这东西是极度不保值的</strong>。为什么intel公司的IP保持的不错呢？Intel inside一度被认为是品质的保证。</p>
<p>4、赚一两次钱不算本事，<strong>一直持续赚钱</strong>才是本事。IT产业依靠升级，不断的赚钱，大家已经习惯。其实任何行业都是如此。你没看服装行业，每年都有一股潮流？那是人造的。</p>
<p>5、现在任何一个人去改那个<strong>心法</strong>，都是没有人认的。你可以去说，可以去骂，但是没有一个人能改。哪怕是改动一个字。那么如果认真的升级心法，还是会带动整个生态链的发展。</p>
<br>

<br>

<h5 id="愿意慢，未必慢-姜诚"><a href="#愿意慢，未必慢-姜诚" class="headerlink" title="愿意慢，未必慢 姜诚"></a>愿意慢，未必慢 姜诚</h5><br>

<p>总结：重要：深度思考、长期-谋定-耐心、公司意图、不以难度系数为追求。 </p>
<p>​           次之：碎片、      短期-频繁-高胜率、表面数据、追求难和极端。</p>
<p><strong>深度</strong></p>
<br>

<p>1、巴菲特曾说：一年的利润损失就好比邮寄过程中寄丢的一张红利支票，对一个企业的价值影响微乎其微。简言之，短期利润本身，就是噪声。</p>
<p>2、<strong>做准确的长期预测很难</strong></p>
<p>3、<strong>我们基于已知信息做决策，更具决定性的却可能是未知信息。</strong>在寻找正面证据的过程中，不要过于努力，或许是避免自欺欺人的好办法。</p>
<p>4、巴菲特曾说：疯狂的人们会在一段时间内自己创造真相。我觉得我们不仅会创造真相，并且乐在其中</p>
<p>5、“地狱有石油”的故事。人性基本经受不住考验，所以不要轻易将自己的人性暴露在考验之下，烫手的东西，离得远点为妙。</p>
<p>6、<strong>结果是形成群体极化现象，最终形成比原有倾向更极端的观点。</strong></p>
<p>7、<strong>同时使用放大镜和望远镜，效果会比较好：放大镜用来审查内部，望远镜用来观察外部。企业的表现是复杂的内外因共同作用的结果。</strong></p>
<p>8、<strong>适当调查，大量阅读，深度思考，才能穿越噪声迷雾；不轻易出手，出手的命中率才会更高。</strong></p>
<p>9、<strong>要掌握卸妆技。</strong>有人说投资是一门艺术，我觉得不是，会计才是艺术。会计准则赋予企业在会计政策和会计估计上灵活的“自由裁量权”，每一张报表都像一张画过妆的脸，想要她丑就丑，想要美就美，化妆品是“盈余管理”。所以，看上去好看不重要，卸妆后好不好才重要，真假面之间的差异所表现出的公司意图最重要。<strong>有时，知道了编报表的人的意图，报表就不重要了，但首先要能读懂报表，才能识别背后的意图。</strong></p>
<p>10、再比如做股票间比较研究，利润增速的差异是周期性的，持久的成本差异是结构性的；原材料价格波动导致的利润波动是周期性的，竞争格局优化或恶化导致的利润波动是结构性的；单一品种驱动的利润增长通常是周期性的，有规模经济和范围经济支撑的规模和品类扩张是结构性的</p>
<p>11、<strong>每一个涉足股市的人，要先想清楚你要从哪里赚钱，然后要对市场的复杂多变和未来事件的不可预测性有基本认知，最后才是建立合适的方法</strong></p>
<p>12、勇敢的人喜欢挑战高难度，投资中亦如此。猫口夺食是高风险活动，可一旦成功得到的报偿更让人艳羡。股市中也总上演猫捉老鼠的游戏，奇怪的是喜欢扮老鼠的人多，正是高赔率的诱惑。</p>
<p>13、为什么穷国人口出生率高而寿命短，因为在缺乏安全感和希望的情况下，“多生早死”是一种适应性强的生存策略。眼前明明有一条康庄大道，还去搏命不明智。<strong>好投资不以难度系数取胜，保守型投资者夜夜安眠，亦能赚钱。</strong></p>
<p>14、<strong>用深度思考代替碎片化思考，用谋定后动取代频繁调动，用耐心交换高胜率，跟时间做朋友而非跟它赛跑。</strong></p>
<br>

<br>

<h5 id="货币超发下，如何捕捉未来十年的优质机会？"><a href="#货币超发下，如何捕捉未来十年的优质机会？" class="headerlink" title="货币超发下，如何捕捉未来十年的优质机会？"></a>货币超发下，如何捕捉未来十年的优质机会？</h5><p>1、目前全球收入最高的国家是卢森堡，人均收入<strong>应该是超过了12万美元</strong>，人均12.59万美元，相当于每个人平均接近100万人民币的收入。但是全球最穷的国家（布隆迪），去年它的人均年收入不到300美金，换成人民币可能不到2000元。</p>
<p>2、到底什么因素产生的分化？<strong>我们回答是科技，是因为资本、也就是货币。</strong></p>
<p> 3、<strong>全球市值最大的10家企业排到一块，有6家是美国的科技企业，还有一家是巴菲特的伯克希尔。6家都是科技企业，包括像苹果，微软，亚马逊，谷歌，</strong>这几家企业的市值基本上都达到了2万亿美元附近。美国的科技企业，<strong>每年的研发投入接近1000亿人民币</strong></p>
<p>4、交易只要在苹果APP里面去做，它要分成30%，俗称“苹果税”。谷歌的安卓系统。实际上就是<strong>美国通过科技领域的霸主地位，在收割全世界的财富。</strong></p>
<p>5、自从疫情爆发之后，我们统计到现在最新的美国股市的市值，比2020年年初已经增加了10多万亿美金，差不多新增了一个中国的 GDP。<strong>去年美国的货币增速是25%，这个25%是二战结束以来的最高值，</strong>也就是去年美国开动了疯狂的印钞机。</p>
<p>6、当时美国的经济增速大概是每年3%，它的货币增速同样也是3%，所以金本位时期美国没有出现过货币的超发，但是非常遗憾，金本位时期已经结束了。大家现在发明比特币，其实有这样一个希望，能够锁住货币的锚，但是能不能成功我们不知道。</p>
<p>7、1933年，就是美国退出金本位这个时期，美国国债的余额大约是在200亿美金。最新的美国国债数据，在去年年末已经接近30万亿美金，也就是在过去不到100年的时间里，美国的国债余额增长规模超过了1000倍。</p>
<p>8、<strong>纸币时代和金本位时代的区别在于货币的基准发生了变化，从黄金转成了国债，纸币时代，印钞的核心是用国债来印钞。</strong></p>
<p>9、1990年中国的货币 M2一共是1.4万亿，到2020年中国的M2已经超过了200万亿，这30年中国的货币总量增长了140倍。</p>
<p>10、贫富差距在持续地扩大，中间在二战之后确实有所缩减，但在过去的40年又开始重新扩大。<strong>根本原因在于资本的回报率超过了经济的增长，也就是资本的回报率超过了劳动的回报率。那么关键就在于货币超发。在这种货币超发的环境之下，财富必然会向资本去集中，所以会导致贫富差距会越来越大。</strong>在美国就造成了分裂。<strong>一帮是金融资本，是科技的巨头，另外一帮就是中西部所谓的铁锈带，工人，农民，其实分裂非常的明显。</strong></p>
<p>11、<strong>未来整个世界的政策重心会从追求效率转向追求公平。供应链的全球化会使得这个世界会越来越平，效率会越来越高。特朗普是减税，</strong>减税依然是在激励市场去释放效率，但是<strong>拜登是加税，</strong>加税其实就是在强调公平，就要把钱从富人转到穷人。<strong>另外一个是反垄断</strong></p>
<p>12、美国，<strong>科技支撑去带动经济增长，</strong>如果你开始打压科技，开始加税，它的经济增速必然会遭到挑战，所以如果经济增长回落，他只能靠进一步的放水来兜底。近期大家在讨论美债这件事，大家很担心美债利率上升是不是全球通胀重来？美国的紧缩提前？是不是这个泡沫要破了？</p>
<p> 13、美联储其实已经明确地告诉大家，它已经不再重视通胀问题，这话说得非常直接，它认为<strong>美国的经济或者说就业才是美联储政策变化的核心。</strong></p>
<p>14、目前美国的最新失业率还在6.3%，按照上一次2008年金融危机之后，美国加息的规律，它是到2015年才开始退出宽松的货币政策。就2015年它的失业率降到了充分就业之下，降到5%以下。<strong>美国下一次加息，就真的有加息的话，我个人认为最早要等到明年，甚至明年下半年，就是美国的失业率重新降到5%以下之后，才有可能。</strong></p>
<p>15、从容投资的吕俊写的。他说美国目前在讨论K形增长，K形就是收入分叉，就是<strong>富人的收入越来越高，穷人的收入是往下走的，但中国不是，中国是富人的收入增速在上升，穷人也在上升，是类似//形的平行增长模式。</strong>我们在<strong>各个领域都出现了消费的升级。</strong>一方面是豪车，宝马卖得很好，但同时我们8万以下 的A级车市场在萎缩，而中级以上的车市场在扩大，其实大家都在消费升级。</p>
<p>16、三一总裁向文波的一个视频，他说其实我们在蓝领工人方面，相比国外未必有优势，可能搬到泰国，搬到缅甸，那边的工人可能也很便宜。<strong>但中国的优势在于有大量相对比较便宜，而且比较优秀的工程师，这是中国目前最大的一个红利。</strong></p>
<p>17、货币的超发是很难改变的。</p>
<p>18、我自己的判断就是未来10年为了保证每年4-5%的经济增长，中国的货币增速每年可能还是在8-10%，如果再持续下去，货币超发可能还是很难改变。我相信目前的政策房住不炒，应该会坚定不移，毕竟房价到10万还可以讲，如果真到了比如说100万一平，这个世界有点荒谬了，因为你远远超过了国外。<strong>所以未来10年我相信如果我们坚持房住不炒的话，超发的货币，它未必会继续流向房地产，未来新的希望可能是在于我们的资本市场，因为它对应一种新的发展模式。</strong></p>
<p>19、开一个新的市场，资本市场，我们把钱导过去，一方面它可以吸纳超发的货币，同时把这些过剩的资金流向科技领域，流向消费领域，它又可以帮助中国优秀企业的成长，然后我们的居民财富投进去，财富可以保值增值，它就是一个双赢，甚至是一个三赢。  </p>
<br>

<br>

<h5 id="如何挖掘未来10年的投资机会（续）"><a href="#如何挖掘未来10年的投资机会（续）" class="headerlink" title="如何挖掘未来10年的投资机会（续）"></a>如何挖掘未来10年的投资机会（续）</h5><p>1、石油行业，在100年前它是一个超级新兴的产业，比如说汽车行业，在50年前是一个超级新兴的产业，比如说在30年前，或许你对金融行业还有很乐观的展望，那么在我入行的15年前，煤炭行业是中国的超级成长产业，这是一个客观的规律。</p>
<p>2、什么叫高质量的内生增长？<strong>高的竞争优势带来的好的竞争格局，进而产生的高盈利能力，那么高盈利能力的突出表现形式是高的净资产收益率，高的净资产收益率是高内生增长的保障。</strong>净资产收益率*（1一分红率）是你的内生增长率</p>
<p>3、<strong>不增长的行业，它可以通过高的即期的现金的返还</strong></p>
<p>4、<strong>我学了接近7年的金融，</strong>最有用的一个模型就是股票价值评估最基本的模型就是股利折现模型，就是DDM模型。所以我猜测<strong>巴菲特对于国债收益率变化的关注，是源于他需要调整他的机会成本。如果你的机会成本变得越来越低，你就应该对股票的要求收益率降得越来越低。如果市场都是这样一种思考的方式的话，资产的价格本身就会越来越高。</strong></p>
<p>5、确实长期国债收益率它像是这么一个备选，刚才诚哥讲逻辑很顺，利率上升，我的机会成本提高以后，可能股票就要卖了，那价格就会跌，好像很顺。中国的国债是银行买出来的，它不是一个很好的市场无风险收益率的指标，比如说银行理财的收益率可能还更好一点，甚至是以前的信托，你怎么选？<strong>我个人以前是特别推崇一个指标，叫做社会融资总量余额增速。</strong>因为我发现其实所有利率变化它是一个信用周期，所以一旦我们的社融增速上升，就是信用扩张，就代表着社会的广谱利率开始上升，它是一个趋势开始上升，一旦信用开始收缩，那么利率就开始下降，所以这个指标确实也很好用。但后来我发现其实也没用。但你想想，假设你当时拿着一个优质的科技龙头股，你卖了，你会发现你可能就买不回来了，因为它后来又涨了，你卖有意义吗？你很多的这种结论放在当时是对的，但后来发现他又错了，所以还是要放弃这种靠一个指标来打天下，我觉得这个不准确，还是得具体情况具体分析。哪怕你宏观上惊涛骇浪，但我这个公司就是卖大家的这种必需品，就是柴米油盐酱醋茶，你都需要，可能经济波动跟我也没关系，所以我觉得其实一元论很容易影响大家的情绪。</p>
<p>6、最近的 A股的下跌，大家说是因为美国的国债利率上升，明显是错的，美国国债利率上升，美股都创了新高，咱们A股就跌了，所以明显不符合逻辑。所以我觉得其实不存在这种一元论，这个世界是复杂的，所以还是<strong>要敬畏市场，尽量少做这种大的判断，因为你判断一次可能会错一次。</strong></p>
<p>7、 其实就找那些你能够判断的东西，然后保证它10年以后还活着，然后货币超发，它应该是受益的，这样研究的范围就可以精准。</p>
<p>8、我认为，你要有一个合理的目标，因为大多数人是追求那种不切实际的目标，总想去买涨的最好的基金，后来一跌就卖。如果大家能够接受回撤的话，你才能够增长。<strong>我的目标就是战胜货币增速，就是货币超发，我比它高一点点，这样就保证了我不会被货币超发干掉。</strong></p>
<p>9、转行做投资这个事我也想了很久，看报告已经看了很久了，最开始我什么都看，希望找黑马，后来发现很难，所以也没必要，其实就是做你能做的事情，找到中国这些优质的企业，找到那些沙漠之花也行，成长股也行，（找到）这些好企业，它们只要不被消灭，我们把我们的资产配进去就好。</p>
<p>10、<strong>错过不是你的错，做错就是你的错了，所以就是宁可错过，但不要做错，我只要不要犯错，就不要买错就好。</strong></p>
<p>11、个公司所从事的业务本身有一定的技术含量，或者说它需要一定的理工科知识，是不是就表示这个行业的商业模式非常复杂？<strong>完全不是。</strong></p>
<p>12、<strong>它的研究难度取决于你能否理解它的优势在哪里。</strong>规模与范围经济。</p>
<p> 13、<strong>竞争优势当中最重要的一点是成本领先，第二点是差异化。</strong></p>
<p>14、<strong>从2018年年底，更早可能是2017年开始，我认为中国新一轮股市的大时代就已经开始了。</strong>因为我们中国的一类资产其实一直在走牛，近期虽然市场有一些波折，但这个波折我觉得也很正常，毕竟这些优质股票的涨幅已经非常惊人了，所以它短期出现回撤也很正常。</p>
<p>15、我还是比较认同，就是不能够随意做这种大的判断，我们只要确定它不是一个熊市。因为熊市你会有一些感受，比如说2017年是杠杆上得太快，可能2007年当时没有感觉，但你回看的话，当时是因为整个美国地产泡沫很大，你有一些指标。</p>
<p> 16、另外一个怎么来看港股？港股是这样，我自己看到很多公司以后，<strong>如果未来我做配置的话，可能大部分配置都是在港股。</strong>因为港股是这样，它是一个集合，中国很多的优质企业在香港上市，同时港币又和美元挂钩的，它跟美元是关联的，所以你会发现港股是比较倒霉的，因为目前中美关系不是很好，所以但凡哪边有事，港股都是先跌为敬，确实比较倒霉。</p>
<p>17、我觉得选基金的话，通常大家会做三个决策，大家最常做的就是<strong>对市场的判断，接下来市场风格在哪里，我去买什么样的基金。</strong>第二个判断是要判断这个人，但是这个人他会变，不是人本身变了，而是基金经理换人。这就引申出来第三个问题，就是<strong>你要判断一家基金公司团队的稳定性。</strong></p>
<p>18、你发现没有几个基金经理有十年的历史业绩，所以选人这个环节可能还要多（研究）。</p>
<p>19、我们考虑从外部引进新的基金经理的时候，我会把这个基金经理历史上管理的所有的基金产品的任何一个时间点上所有的重仓股，包括他的仓位变化，从头到尾去复一遍盘。</p>
<p>20、用一个负面清单，有知行不合一表现的基金经理就不要选了。</p>
<p>21、但是第一个决策，<strong>根据市场的判断来选不同风格的基金经理，这个事我建议大家直接就不要做了，因为大多数人基于这样一种方式的决策是错的，是增厚不了的。</strong>比如说，为什么现在我的关注度高起来了，因为有更多的人觉得深度价值的风似乎要吹来了，但看你什么时候进来。可能最好的时点是在于鼠年的最后一天，把抱团的基金经理的基金赎回来，然后在牛年的第一天买我的，其实你是很难做到的。</p>
<p>22、<strong>降低决策的频率，提高决策的质量是更好的，包括我在内的大多数基金经理也是这样，</strong>这就引申出来你说的换手率的问题。</p>
<p>23、高集中度和低集中度，它有另外一个维度的解释因素，就是<strong>基金经理的自信程度。</strong></p>
<p>24、假设你想做得更好一点，因为你可能不只是想获得一个平均收益，你想获得一个超额收益，而且确实在A股市场它又是可以实现的。因为存在这样一些所谓的制度缺陷，（A股）这么多散户，你确实可以做到这一点。</p>
<p>25、投公募的<strong>优势在于你的成本是最低的，你不用给业绩报酬付费，但你得知道这个人凭什么他不走。你要知道这个公司它有什么优势去留住这个人，所以你要去研究股权方面的优势，</strong>一个是股权，是不是会有额外的股份激励，他可以通过（这些激励）来实现所谓的人生价值，这是一个。其实还有一个就是**这个人的持仓。我知道有一个特别有名的基金经理，他应该是不会走了，因为他的个人资产全部在他基金里面，他又没有兴趣出去搞自己的事业，我就管好这个基金，相当于是搭一个免费的班车，你去买基金，公募基金不用交业绩报酬，而且他的业绩非常好，他应该也不会走了，因为他自己就愿意这么干。</p>
<p>26、条条大路通罗马，对于不同的人而言，同一条路经过的也不太一样，我只是觉得我选的这条路他人少，然后不太快，但是大概率能到，它又比较适合我的秉性，是这样的一种方式。所以<strong>我的建议是大可不必纠结于你是价值投资还是投机，1000个人眼中有1000种价值投资，正如1000个人眼中1000个哈姆雷特，你不用太纠结投资理念。</strong></p>
<p>27、买入时点的市值就是对应景气低点利润的10倍市盈率，这个低点我是怎么测算的？就是全行业不赚钱的时候它的利润规模，我觉得是一个很好的安全边际。</p>
<p>28、因为有的时候股价通常会领先于基本面，像上面这种情况领先了两年，所以股价的拐点其实是很难判断的。所以<strong>我不太会试图去追求更好的点，更好的点就是你觉得它便宜，你把股票价格本身当作一个外生变量即可，不要对它的趋势太多的苛求，</strong>我是从来不试图买得准，这是我的建议。</p>
<br>

<br>

<h4 id="0919"><a href="#0919" class="headerlink" title="0919"></a>0919</h4><h5 id="月饼经济学"><a href="#月饼经济学" class="headerlink" title="月饼经济学"></a>月饼经济学</h5><p>1、月饼毛利55%，茅台90%</p>
<br>

<h5 id="买地产爆仓了"><a href="#买地产爆仓了" class="headerlink" title="买地产爆仓了"></a>买地产爆仓了</h5><p>1、这位老哥也回顾了它投资融创的经历，主要是两点，第一是觉得这波下跌不会来得这么快，“毕竟一家pe为3，一家pe为7”。第二是他重仓融创很大一部分原因，是六年前他同样经历了融创和恒大的大幅下跌，但是迎来了绝地大反击。</p>
<p>2、哪怕是直接看线，也比只看pe强。pe的高低非常主观，线起码还是客观的。</p>
<p>3、伤我们最深的，往往是我们自认为最熟悉的。但是不熟悉又不敢下仓位。</p>
<p>4、面临的宏观环境差太多了。2017年融创恒大的大逆袭。从四万亿之后，我们国家对房地产的调控大概会有一个周期，水多了加面，面多了加水。选在2017年那波放松调控之前，疯狂上杠杆拿地，一举把之前的招保万金四位大哥甩在身后。2017年恒大股票翻了十倍，融创股票翻了二十倍。想回A股上市的恒大，在借壳深深房之后，一直得不到批准，为了对赌恒大上市的战略投资者，对恒大的资金注入，都是在2018年之前。</p>
<p>如果不是中美贸易战，2018年金融市场的股权质押危机，以及2020年的疫情，房地产行业的冬天，说不定会来得更早一些。</p>
<p>5、2014年阿土哥十倍杠杆买中行转债</p>
<p>6、1当时中国银行上市已经近8年时间，盈利增长了6倍，但股价却比上市价格还要低。A股那时候也熊了六七年。而且研究中行转债之前，他大概排除了二十多个品种。2中国银行发行普通债权可转债，当时利率为5%，阿土哥买入的价格是102，利息贴现后中行可转债理论价值为98元，到期后跌破98元的可能性很低。阿土哥根据自己的平仓线，计算了一下能够承受到的跌幅是92元，对银行可转债来说，98元价格对92元价格，是非常高的安全垫。3那时候中行转债到期时间剩下不到两年，如果到期后中行转债价格没涨，阿土哥只能获得约定收益，扣除融资利息之后，他会亏68万左右。他自己前两年通过股票大概赚了68万。也就是最差情况他两年白干。</p>
<p>7、<strong>最大风险可控，赔率适中最好，而且预定兑现时间不能太长。</strong></p>
<p>8、周期股就是来打破市场认知的。合盛硅业从17年上市，一直到年初都没涨，然后一年五六倍。如果持有四五年，虽然最终收益率也不错，但是持股体验真的是…不如跟师兄睡两晚。 我的建议是不专门看周期，而是在研究产业链的时候，顺便把上游看了。既能理解宏观，也能长期具备复利研究效应</p>
<p>9、风格只是景气度的附带的产物。消费我觉得持续大涨的概率很小，因为三季度大部分公司的消费数据还是会比较差。重点是周期股的景气度能不能超预期，我觉得三季度业绩环比增速，是很重要的观察指标。</p>
<p>10、苏泊尔，九阳？看你是什么预期了。如果是当做高分红的价值股，那还是值得一看的，如果是当做成长股，要求高回报，我觉得就不算有很大看点了。这些就属于消费股里面比较中不溜秋的，比调味品这种最惨的好点儿，比不上高端白酒，也比不上珠宝类和乳制品等。</p>
<br>



<h5 id="翻车了"><a href="#翻车了" class="headerlink" title="翻车了"></a>翻车了</h5><p>1、融创在港股已经跌到了1.5倍PE，看清楚哈，不是1.5倍PB。</p>
<p>2、有一则消息是最近公募基金受到了规范指导，有些基金明明叫做XX公用事业，结果比亚迪、亿纬、宁德买到飞起，挂羊头卖猪脚。这些基金有潜在的卖出需求。</p>
<p>3、以带头跌停的锂矿为代表，工信部副部长今天说目前中国新能源车成本依然偏高。电动车关键部件动力电池，面临锂钴镍等矿产资源保障和价格上涨压力。</p>
<p>4、下一个景气度和走势的预期差，在哪里？如果我们自上而下寻找，能够挖掘到最深的便是：今年企业盈利的大头都被上游拿走了，明年中游可能会好转。但是哪些中游会好转呢？如果要缩小范围，提高效率的话，我建议从下游景气度上，自下而上结合起来找。</p>
<p>5、在市场里，有时候能很快速得出的结论，可信度反而是最大的。要摸索半天，上九天揽月下五洋捉鳖，才能追到的妹子，并不一定能走得长久。</p>
<p>6、举个例子，PCB（印刷电路板）和覆铜板行业（覆铜板是PCB的原材料，这俩是比较靠近的中游，所以放在一起说），它们的上游是铜箔、树脂、玻纤布等偏大宗的行业，从去年开始涨价都比较厉害。下游行业是通信、消费电子、汽车电子、工控等，行业整体增速有保证，这个翻一翻几家公司的中报，以及行业资讯，就能快速得到结论。</p>
<p>通过这个快速的结论，再去跟相关公司的财报做交叉验证：比如是不是大部分公司<strong>营收都在增长（行业扩容），而且营收的增长比利润的增长快（原材料承压）。这个时候如果有头部公司，毛利率不降反升，说明转移价格的能力是比较强的，就可以优先研究了。</strong>也有一些小公司，毛利率是降低的，但是产能依旧在扩张，这种公司就可以放在上面的头部公司之后，做研究。</p>
<p>7、再比如风口上的锂电，上游从锂矿到正极负极电解液，今年全是大幅涨价，电池厂今年毛利率承压，但是它们依旧在逆势扩张份额，行业也是在扩容，那今年在上游锂矿和中游四大环节赚的盆满钵满的小伙伴，是不是可以开始准备研究电池厂了，甚至是整车厂？</p>
<p>8、这样的研究和操作，就比较有梯度和层次感。没有板块是永远的神，只有基本面周期跟股价运行阶段存在的预期差，才是永远的神。</p>
<p>9、比如光伏四五月份在底部，协会找相关企业开会控制硅料价格，光伏因此涨起来，是因为<strong>装机量是整个光伏行业的核心逻辑</strong>（对比一下，<strong>政策走势是房地产企业的核心逻辑，核心逻辑都比业绩对股价影响大，也远比估值对股价的影响大</strong>）。等到现在，虽然是到了旺季，但是产业链上下游都涨价了，市场又开始担心装机量了。尽管产业链还没有反应，但是股价已经在反映这个预期。</p>
<p>10、券商最大的问题是：业绩的可预测性太差。你说的现在业绩好是因为过去一个季度成交量都很高，万一熊市来了呢？这种思维久而久之，大家都把券商当渣男，来一发就跑。我觉得券商的稳固的玩法，是等这波跌下来之后，买券商发行的可转债，既安全，也适合上仓位。看个股就东财值得重点看，东方这种稍微注意点</p>
<br>

<h5 id="这对我们来说，就是个大事件"><a href="#这对我们来说，就是个大事件" class="headerlink" title="这对我们来说，就是个大事件"></a>这对我们来说，就是个大事件</h5><p>张是之老师公号永久封禁。观念的后浪，自由在深处被永久封禁。</p>
<p>甚至是因为十几天前没有发出的文章，一本正经的华仔封禁一个月，乌里单刀封禁一个月。</p>
<br>



<h5 id="28岁美女创始人，坐拥3-4亿用户！最困难时，投资人跑路，被逼四处借钱……"><a href="#28岁美女创始人，坐拥3-4亿用户！最困难时，投资人跑路，被逼四处借钱……" class="headerlink" title="28岁美女创始人，坐拥3.4亿用户！最困难时，投资人跑路，被逼四处借钱……"></a>28岁美女创始人，坐拥3.4亿用户！最困难时，投资人跑路，被逼四处借钱……</h5><p>1、中国最大国漫平台快看APP宣布完成2.4亿美元融资，估值超百亿人民币。本轮融资由建银国际、One Store、腾讯、Coatue、天图资本等投资。总用户超过3.4亿，月活近5000万，市场占有率超过50%，超过了市场第二名到第六名的总和。</p>
<p>2、安妮开始走上漫画创作的道路，并以“伟大的安妮”为名在微博连载作品。2011年10月，凭借调侃学校兼职班主任的《广外班导使用手册》，安妮开始走进网友的视线。安妮还和当时的学弟，后来的礼物说CEO温成辉一起创办了“M方工作室”。安妮作为一名漫画家获得的最高荣誉。同一时间，安妮的微博粉丝也将近1000万之多。千万粉丝的安妮，在每个月拥有几十万广告收入的情况下，可以轻轻松松地生活下去。</p>
<p>3、2014年4月，安妮受李开复邀请，参加创新工场在北京举办的一场网红聚会。</p>
<p>4、当时真的什么都没有，产品的雏形都是我自己在纸上画的，画完以后在微博上找比较有名的产品经理指导，还常常被人嘲笑着打回来。安妮把自己所有的积蓄都拿了出来，联合学弟温城辉的几十万元投资一起成立了快看漫画。</p>
<p>5、安妮在QQ上搜索加入一些技术群，在群里找到一个人就狂聊，聊完再约出来见面，然后又逼着他介绍另外一个人给她认识。就这样，快看app的开发慢慢打开了局面。据快看漫画技术合伙人李润超透露，他那时还是小米应用商店产品线的技术负责人，就是通过这种方式认识安妮的。</p>
<p>6、导致快看漫画从研发到上线耗费了整整八个月时间。这八个月里，安妮和团队中的12个人每天都在自我怀疑中度过，他们不知道这件事到底能不能做成。2014年12月14日，“快看漫画”app终于上线</p>
<p>7、场微博营销，就以自己的成长和创业故事为模板，创作一副漫画，名字叫做《对不起，我只过1%的生活》。姚晨和赵丽颖都转发了安妮的漫画。“快看漫画”app在当天就增加了30万用户，以后几天也一直以每天20万用户的速度增长。2015年4月6日，红杉资本和字节跳动向快看漫画合投了300万美元。</p>
<p>8、差异化竞争。锁定年轻女性用户。在阅读体验上。重视对原创作者的扶持。诞生了一批年入超过500万的头部作者，快看旗下签约作者平均月收入可以达到53604元。除了资金的支持，快看漫画为头部作者提供的经纪人服务，除品牌包装、作品全生命周期运营支持、商务支持等分内之事，还包括为作者购买商业保险，组织年度体检，心理咨询，调休关怀等分外之事。</p>
<p>9、这样一部数据惨淡的作品，本来没有任何成名的可能。但安妮发现后，却能敏锐地找出作品不同于“兄控”“妹控”脸谱化人设的生活气息，果断签下了作品。接下来这部《逗比兄妹》开始“变身”：作品从黑白改为彩色，增强颜值；名称改为《快把我哥带走》，增强人设；开通微博账号“快把我哥带走”，邀请大V转发，制造舆论……….</p>
<p>10、在漫画的世界，安妮拥有惊人的嗅觉，总能找到那根撩拨人心的弦。</p>
<p>11、快看漫画的各项数据快速增长，后续拿到了多轮融资，并在2017年就成为了行业第一，并延续至今。</p>
<p>12、2015年进行B轮融资时，因为投资人跑路，答应给的钱没有打过来，而快看又因为签了协议，在锁定期内不能拿其他投资人的钱，导致快看的资金链快要断裂。那个时候，安妮被迫把所有积蓄都拿了出来，又向亲戚朋友借了几百万</p>
<p>13、迪士尼的强大，不仅在于它有很多全球性的IP，还因为它有强大的IP及其衍生品的开发能力。</p>
<p> === &gt;</p>
<p>然后发下，温的礼物说不见了。。。没有看到新闻。。。为什么肖申匿迹。。。</p>
<br>

<h5 id="深圳突发！炒房的彻底凉了！！"><a href="#深圳突发！炒房的彻底凉了！！" class="headerlink" title="深圳突发！炒房的彻底凉了！！"></a>深圳突发！炒房的彻底凉了！！</h5><p>1、<strong>金茂府业主搞出了退房的六个理由，其实明眼人一看就知道了，说到底，就是因为觉得买贵了。金茂府本身就是一种赌博的行为，赌好学校，赌价格涨。</strong>两年前金茂府卖11万/平，硬生生比旁边高了近50%，很多炒房的，还是趋之若鹜，赌今后的升值空间。</p>
<p>2、<strong>深圳的房价已经出现明显降温和回调，特别是深圳在今年2月份率先成为全国首个发布二手房指导价的城市之后，二手住宅成交量一路狂降。</strong>深圳开始推行大学区招生，房子与对应学位脱钩了，同时了，建立义务教育学校教师交流制度，<strong>学区房彻底要凉</strong></p>
<p>3、<strong>8月31日，国新办正式在北京举行了“ 努力实现全体人民住有所居 ”新闻发布会</strong>，<strong>“十四五”期间，接下来将以发展保障性租赁住房为重点，进一步完善住房保障体系，增加保障性住房的供给，努力实现全体人民住有所居。</strong>–房住不炒，坚决不把房地产作为刺激经济的手段。</p>
<p>4、要拉动内需，要拉动消费，还要大家生娃，那就必须抑制住房价上涨。<strong>房地产与金融业深度关联。已经成了现阶段我国金融风险方面最大的“灰犀牛”，必须妥善解决！</strong></p>
<p>5、目前，我国房地产相关贷款占银行业贷款的39%，还有大量债券、股本、信托等资金进入房地产行业，因此，坚决实施“房住不炒”，坚决抑制房地产泡沫</p>
<br>

<h5 id="金融行业为什么普遍采用-PB-估值法，而不是-PE？"><a href="#金融行业为什么普遍采用-PB-估值法，而不是-PE？" class="headerlink" title="金融行业为什么普遍采用 PB 估值法，而不是 PE？"></a>金融行业为什么普遍采用 PB 估值法，而不是 PE？</h5><p>行业的利润率。对于大多数普通行业，一个企业要想利润率高，要么是提高单个产品的净利润，要么是提高企业的周转率，要么就是加杠杆提高负债率。基本就是这三条路。好，我们来看银行业。</p>
<p>第一条路，提高单个产品的净利润。这基本是不可能的，银行的产品同质化严重，建行给的抵押贷款和交行给的抵押贷款基本没有差别，你提价了就意味着客户没了。另外，央行还有个基准利率摆在那里，存款端的成本全行业也差不了太多。唯一可能有一点点差别的就是风控了（其实也差不了太多）。所以，这条路基本上是走不通的。</p>
<p>第二条路，提高企业的周转率。因为银行业的特殊性，这也是行不通的。五年期贷款你压缩一下变三年期，这产品性质就变了。再者，央行的存款准备金率摆在那里，多转几次钱就转没了。所以，这条路也是走不通的。</p>
<p>第三条路，加杠杆。这就更行不通了。巴塞尔协议对于银行的资本充足率、核心一级资本充足率这些都有明确的要求，这是一条死胡同。</p>
<p>综上，作为银行，你是很难通过所谓的金融创新亦或是风控管理取得超出行业平均的利润率的。那些短期内取得超出行业利润率的银行，往往都是极大地提高了风险容忍度，而银行业的风险爆发又具有滞后性，所以当下的利润率其实并没有多大的参考价值。</p>
<br>

<h5 id="陆奇：这是历史上最大的一次市场机会！"><a href="#陆奇：这是历史上最大的一次市场机会！" class="headerlink" title="陆奇：这是历史上最大的一次市场机会！"></a>陆奇：这是历史上最大的一次市场机会！</h5><p>1、科学四个范式：哲学时代、物理时代-理论分析和实验、工业能源时代、系统理论辅助计算模拟、数据时代–算力。对应农业、工业、信息时代。</p>
<p>2、人类历史上还没有这样一个商业化机会，同时占住信息工业和能源工业的制高点。</p>
<p>3、一定要关注视频，这里面有很大的机会。通过点击和上下滑动与这个思想空间(视频)交互。这样的交互，它意味着人开始和物理世界、自然社会做直接交互，这开启了一个新的大门，未来会有越来越多的软件驱动的直接交互，演化出新的内容，通讯，社交，电商，企业服务等等。</p>
<p>4、宏观格局：</p>
<p>数字化社会、生命科学(病毒)、可持续新能源；消费升级(新消费：需求在往上走，品牌有溢出性)、人口、城市化(还有二三十年要做)；新基建+双循环。</p>
<p>不光是经济中心，科学中心和创新中心也在向亚洲转移。</p>
<p>人类历史是由黑天鹅事件决定的，小概率事件影响很大。</p>
<p>技术永远和需求挂钩，技术驱动，需求拉动，同时用市场加速，这是这个时代的核心特征。</p>
<p>5、中国人均收入是美国的四分之一，中国为什么有全球一流的互联网公司和一流的高科技公司呢？原因在于它的劳力结构和产业结构。</p>
<p>6、从政府开始，用政策投入支持大学、研究机构做基础建设，然后有一个早期创业创新生态，让这些创业公司无处不在地、从零到一去追求<strong>技术商业化</strong>的机会。</p>
<p>7、有了对本质的认识之后，我们可以先看结构：结构是一个复杂体系里面各个组成部分之间的稳定关系；然后我们再看发展体系，它是形成新现象的机制；然后再看趋势，它是通过驱动力形成未来状态。这里要讲趋势，必须知道驱动力。</p>
<p>8、人的驱动力永远是不断地追求知识和财富，人类整个历史就是认识世界、改造世界，不断追求知识和财富。需要指出的是，财富是一个广义的概念，是满足人的需求和欲望的通用能力。</p>
<p>9、价值本质上是满足人类需求的能力，具体而言，就是“现在和未来通过市场来持续满足用户和客户需求的能力”。</p>
<p>10、a.用户和客户在市场上有区别，他们未必是同一个人，客户是付钱的，用户是使用者，他有可能付钱有可能不付钱；b.价值的本身是满足用户和客户需求的能力，本质是一种能力；c.它是“现在和未来”满足需求的能力，它必须具备持续性；d.它必须通过市场。这个定义里面缺一个字都不是价值。</p>
<p>11、不管是生物过程、物理过程、化学过程，都是能源转化，没有别的途径。</p>
<p>12、技术的内核是用信息+能源去改变自然现象，满足人类需求。技术和人的需求是分不开的，这是技术的核心结构体系。</p>
<p>13、 技术创造价值的关键：用长期主义把握好价值创造和市场价格之间的关系</p>
<p>14、我们要长期持续关注“我（是否）在满足越来越多人未来的需求”，要做一个价值越来越大的公司。</p>
<p>15、农业：光合作用+劳动+简单交易；工业：机械、电子设备+技能+市场化分工化；信息：数字+新能源+前沿技术+创新+成熟市场环境</p>
<p>工业有三个不同的阶段，主要是能源转换效率不一样。1.0是机械设备转换能源，2.0是电气设备转换能源，3.0是电子设备转换能源，转化效率层层递进。工业4.0会很不一样，主要是信息化和数字化驱动。</p>
<p>信息时代，起点是上世纪中叶，人类发明了通用计算机，它可以高效地处理信息，用计算来模拟和预测，<strong>设计如何更快更好满足人的需求；它能用很少的能源、很短的时间更高效地产生商业价值和社会价值</strong>。这条蓝色曲线涨得越来越快，因为它是一个 1+1&gt;4 的发展体系。</p>
<p>信息时代的核心产能不再是人的技能和设备，而是人才（研发和市场）+ 技术（资源快速组合)。</p>
<p>数据让我们更多更好地了解需求，软件驱动让我们更快更有效地满足需求，因此产出相对于核心产能是一个多重超线性增长的关系。</p>
<p>创业不管做什么生意，都一定要尽量站在蓝色曲线上做。我们要做得越来越多的是用数据和软件来做。</p>
<p>16、人类发展的历史主要是由通用技术发展而驱动的，技术的结构核心是信息和能源之间的关系；我们讲技术，最关键要看的是用什么样的信息结构做什么样的能源转化，别的都不重要，决定一切的是能源转化过程和信息的结构。</p>
<p>17、技术商业化和数据来自商业，因此产学研闭环。</p>
<p>18、今天的全球主流科学发展体系是由Vannevar Bush 1944年《科学无尽的前沿》所提议的，核心有几点：大规模建立研究型大学，让研究生和教授一起推动科研；研究型大学大量驱动工程性研究，不纯是基础研究；建立大量的国立实验室- 政府出资，科研方向以科学家自导为主。但是，今天的科学前沿逐步由一些大厂和新一代研究型创业公司在驱动。举个简单的例子，今天引领信息科学的是谁？不是斯坦福、MIT等高校，而是谷歌、微软、亚马逊、阿里、腾讯和字节等一系列的大厂。今天站在人工智能最前沿的不是大厂而是OpenAI和DeepMind（DeepMind虽然在谷歌里面，但它本质上还是个独立的创业公司）。同理，今天引领商业航天的，不是波音，NASA，而是以 SpaceX 为代表的商业航天创业公司。</p>
<p>19、因为在今天这个时代，你只要有顶尖技术，有一个大的idea，有能力有抱负，就有机会获得大量的资本，并跑在前沿。这是我们在场每一位的机会。</p>
<p>20、工业时代的第一阶段发生在英国，因为当时的力学、机械学、早期的物理学研究最多且传播最广的是在英伦三岛。工业2.0在德国等欧洲大陆，3.0到美国等北美地区，美国过去几十年基本上垄断了科研的前沿。这个前沿有可能向亚洲转移</p>
<p>21、今天大部分的信息是用符号和文字表达的，深度学习的革命本质是一种表达的突破，不再用文字符号，而是用基于浮点的重叠向量，这一下子打开了新一代感知和各种各样计算的可能性。</p>
<p>22、赫伯特·西蒙是人类历史上唯一一个同时获得图灵奖和诺贝尔奖的人，他有一个非常系统化的理论解析人是如何解决问题的，任何人的需求、问题，都可以设计一个任务通过计算来解决。</p>
<p>23、交付信息，就是把计算结果交付给人，去满足我们的需求。今天大量的信息交付是在屏幕上输出给人去读，越来越多的交付是直接去控制一个设备，甚至直接去控制一个生命体系等。</p>
<p>24、数字化的起步是针对某一种人类需求，获取信息、 表达信息、存储信息、传输信息、处理信息，最后交付信息，这是数字化的核心。</p>
<p>25、算力成本越来越低、软件驱动快速迭代、数据沉淀、社会生态网络协同。我有客户数据，我可以做产品设计、物流、客服和支付上的协同，把他们综合在一起。</p>
<p>数字经济是极为高效的增长模式，成本不断地呈指数下降，迭代越来越快，数据沉淀越来越多，更多的需求理解，协同不是在工厂里面协同，而是整个社会来协同，它是一个极为高效的价值增长飞轮。</p>
<p>26、一个平台一个平台往下走，大概12年左右，这是数字化发展的趋势。</p>
<p>27、数字化前沿：a.移动互联网/云，后台，在云的时代，大家要关注云原生。云原生不光是软件的体系变成以微服务、网关、Mesh这一系列组织企业和C端这些看得见的体验，更为重要的是，它会带动未来硬件的更新和软件体系的重构。视频原生会发生，毫无疑问，视频承载的信息远超文字和图像。未来的人机交互，企业管理都会以视频流为主。这会带来新的基础平台，新一代的Paas、新一代的Iaas，机会非常多。上海有一家创业公司叫做声网，非常出色，它代表了很多类似的机会。</p>
<p>b.AI/边缘及5G：AI把物理世界的信号直接投射到这个向量空间，所有的传感器、传控器，所有物理模态都可以，从人机交互的角度看，一切交互模态的大门通通打开。过去交互都是鼠标键盘，现在是对话交互、视觉交互，未来还有触觉交互等，自然交互都可以开启。</p>
<p>智能云/边缘，人类的发展永远是路径依赖的，人工智能的未来是在今天云和移动端的基础上往前走的，因此自然会生成智能云和边缘，特别是5G的边缘计算。</p>
<p>数据是人工智能时代的核心产能，但是数据的复制成本是零。所以如何用隐私计算等新的计算平台，让数据变成产能，有很多的机会，当然，像OpenAI做的，通过大模型、大算力、新的平台如GPT1、2、3、4、5……，这样一直往下走，未来的模型只要通过微调（fine tune）,经过 few shot learning，很快就可以做出来，所以这是新的Iaas/Pass的机会。</p>
<p>28、过去的历史，每一个时代都是定义性的体验产生的数字化生态，PC时代是Windows的发明，在移动时代是2007年的iPhone，下文会讲为什么这是定义性的体验，因为它启动了新的时代。  </p>
<p> 人工智能时代，定义性的体验还没有到，但是很快要到了。比方说一辆软件可更新、可延伸、可以自动驾驶的车子；一个完整的、软件定义并可延伸的医院、工厂、学校等；每一个这样的定义性的体验，都可以启动一个大的商业生态。人工智能时代机会非常多，包括新的生命科学、药物开发体系等等。</p>
<p>他们为什么造电动车？就是为了拿一张进场的票。进什么场？自动驾驶带来的新一代信息工业的制高点。</p>
<p>29、未来的汽车是软件可定义的电子电气设备（它将由好几亿行代码驱动），它将是信息工业的制高点，信息工业的发展永远有一个母生态，今天的母生态是手机生态，以苹果、华为、谷歌、Qualcomm 等为代表，其他很多产品都是基于这个产品衍生出来的。</p>
<p>30、下一个母生态将是汽车，同时，电动车又代表了新的能源产业的制高点，是人类能源体系从化石能源进入新能源的转折。人类历史上还没有这样一个商业化机会，同时占住信息工业和能源工业的制高点，因此你今天有能力就应该去拿一张门票，给自己留一个机会。</p>
<p>31、c.新前沿：量子虽然离商业化还有距离，但是技术一直发展很快，还需要时间。量子一旦出来，它是一个完整的生态，早期很可能是量子化学、量子材料等，当然量子计算会带来颠覆性的效果。</p>
<p>32、新能源、新生命科学、新材料科学、新航天等领域的新前沿：</p>
<p>a.新能源</p>
<p>碳捕捉技术、新的电池和存储技术、氢能、生物能源等，最终我们的解决方案可能是可控核聚变，因为地球上所有的自然能源，如太阳能和风能，最终都是太阳核聚变形成的；当然这个很难，但是核聚变如果可控的话，效益是最高的。</p>
<p>b.新生命科学– 基因组，基因测试的成本一直在往下降；其他感知系统如低温电镜等等</p>
<p>c.新材料科学-芯片  ；d.新航天</p>
<p>33、需求的结构、体系和趋势</p>
<p>贝佐斯三年前写给投资者的一封信，他个人的认知是，用户永远不满足。人的需求是永远不满足的，因为总有更好的方法可以去满足得更好。</p>
<p>34、如何考虑长期深层的需求，好的创业者往往在需求上想得很深，我给大家举几个例子，供大家参考：</p>
<p>2007年的iPhone，它满足人的什么需求？打不通电话，只有6款应用，为什么大家那么激动？核心我认为它是满足了人类随时随地要获得信息的需求，因为人是一个信息动物。</p>
<p>为什么爱迪生能够改变历史，做出了不起的发明？因为电满足的是人要随时随地转化能源的需求，照明只不过是一种应用。</p>
<p>35、好的创业者，尤其是真正能够改变世界的创业者，他是要开启这种根本性的创新，需要对需求考虑得很深，把大量人的需求满足得更好。</p>
<p>36、今天的信息工业为什么这么蓬勃发展，主要是过去四五十年沉淀的软件，我们每一个创业项目，每一个新的应用，程序员X写的代码不多，因为大部分代码已经有了，都是模组化，所以开源开放非常重要。</p>
<p>37、动手创造性地解决问题。不要老是盯着牛角尖钻理论，而是去解决问题，去做，到实验室里面，做化学和生物实验也好、写计算机代码也好，一定要去做。</p>
<p>在你所选择的创业赛道里，真正可以做好的永远是对未来判断得更靠前，比同赛道的人多看几步，并有独到的见解。</p>
<p>38、你得到机会并不是你能力很厉害，而是你认识很厉害的人</p>
<p>39、优秀创始人：1永远能够比同赛道的人多看几步；2很强的思考和沟通能力。创业的早期，核心能力是思考和沟通，融资、雇人、引入战略合作伙伴等背后都是沟通，创始人能够想得清楚，讲得明白，非常重要；3行动导向，解决问题；4持久的内在驱动力，一种心力，一种愿力。</p>
<p>40、核心能力–创造价值–持续满足客户需求：产品+商业变现能力+市场推广和运营以触达用户+融资能力(从种子、天使、PreA、A、B、C等，最后到二级市场上市等)。</p>
<p>41、估值：1你今天的产品，假定五年、十年之后你有10%的市场份额，你产品单价是多少，有多少人可以用，价值的简单计算公式就是P×Q（Price×Quantity）；2以后可以衍生的新产品；3用你的核心能力去跨界所能产生的商业价值(苹果造房子、小米智能家居、小米造车)</p>
<p>42、风险：1市场风险：很多创业项目是伪需求，这个需求不存在，不需要被满足；即使你找对需求了，但是渠道产品卖不下去，比如大部分 To B 企业都是死在Sales上面，卖的成本比你赚的钱多，这是市场风险的两种情况。2资本风险：非常需要烧钱，要大量的资本。3技术风险(实现层面可行性)、执行风险(踩了坑)</p>
<p>43、考虑：1我的目的–价值是什么、什么需求的满足、路径规划(如何五年内成为独角兽、如何实现年收入1亿美元)【阿里-技术让天下没有难做的生意；迪士尼-欢乐带入每一个家庭】 2我的行动–实现方式、到底做什么 3风口、宏观是什么–对的时间对的事情【苹果进场的时间点，不是第一个做手机、不是第一个做手表或者平板的，但是，进场的时间点非常赶风口】4建立壁垒–壁垒包括了网络效应、生态效应、规模效应、特殊资源、专利等等。 5团队–我可以找另外一个团队，为什么投你的团队呢？你要讲清楚为什么自己可以把这一件事情做好，这里关键是认知速度的提高和胸怀的打开。–胸怀宽广的人可以引入比自己强的人。</p>
<p>44、产品匹配市场：把你做的创业这件事情拆成一小步，不断地去验证，不断地降低你的机会成本。小步快跑，快速得到多方面反馈来迭代和验证</p>
<p>1收入在高速增长或者将要高速增长，一个创业项目早期看不见增长，要么产品没有做对，要么产品做对了，市场没有切对，要么这是伪需求。如果没有增长，对不起，重新考虑。</p>
<p>2我在满足用户和客户的需求，要验证我们在满足这个需求，这个需求以前没有人满足过，或者我满足得更好。</p>
<p>3要验证能赚到很多钱。一般都是有收入，有的时候你实在找不到收入，可以找到收入意向书，内容上约定我做到哪些条件，你答应用什么样的价格买我的产品，这是退而求其次。核心是一定有收入。</p>
<p>45、一个创业项目，早期就是挖一口井，第一滴水，第二滴水等等都是来自同一个水源，后面的水滚滚而来，这就是增长，同一种需求被你满足了，找到水源了。我们在YC经常讲，差的创业者挖了一个大坑，里面三滴水，三笔订单，都是不同的人，不同的需求，这样没有用，没有增长。</p>
<br>

<br>

<h4 id="0920"><a href="#0920" class="headerlink" title="0920"></a>0920</h4><p>1、将你的沟通目的改为，有价值的深度观点或幽默。否则沉默吧。唯有放松和真的思考别人的话，才会真的进入那个状态。状态就很重要。</p>
<br>

<h5 id="神经网络调参经验大汇总"><a href="#神经网络调参经验大汇总" class="headerlink" title="神经网络调参经验大汇总"></a>神经网络调参经验大汇总</h5><p>1、特斯拉高级总监Andrej Karpathy的A Recipe for Training Neural Networks – 防止的是一次引入大量“未经验证”的复杂假设，这必然会引入错误/错误配置，这将需要花费很长时间才能找到（如果有的话）。</p>
<p>2、<strong>第一步是从检查数据开始。</strong></p>
<blockquote>
<ul>
<li>非常局部的特征是否足够？</li>
<li>或者我们是否需要全局上下文的信息？</li>
<li>有多少变化，它采取什么形式？</li>
<li>什么变化是虚假的，是否可以被预处理掉？</li>
<li>空间位置重要吗？还是我们想把它平均化？</li>
<li>细节有多重要？我们能承受多大程度的减少图像采样？</li>
<li>标签存在多少噪音？</li>
<li>花大量的时间（以小时为单位）浏览数千个示例，了解它们的分布</li>
<li>寻找数据不平衡和Bias</li>
</ul>
</blockquote>
<p>如果你的网络给了你一些与你在数据中看到的不一致的预测，那么就有问题了。</p>
<p>写一些简单的代码来搜索/过滤/排序你能想到的任何东西（例如标签的类型、注释的大小、注释的数量等），并<strong>可视化它们的分布和任何轴上的异常值</strong>也是一个好主意。异常值几乎总是会暴露出数据质量或预处理中的一些缺陷。</p>
<p>3、建立框架和进行实验：</p>
<blockquote>
<ul>
<li><p>最好选择一些比较擅长的简单模型，例如线性分类器，或者较小的卷积网络等</p>
</li>
<li><p>可视化损失函数，评估指标（如准确性）等，在进行模型预测，并在过程中使用明确的假设进行<strong>消融实验</strong>。</p>
</li>
<li><p>固定随机种子。</p>
</li>
<li><p>尽可能简单，确保禁用任何不必要的假设。在此阶段，请务必关闭<strong>任何数据扩充的策略</strong>。</p>
</li>
<li><p>add significant digits to your eval. When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.</p>
</li>
<li><p><font color="#cornflowerblue">        verify loss @ init. Verify that your loss starts at the correct loss value. E.g. if you initialize your final layer correctly you should measure -log(1/n_classes) on a softmax at initialization. The same default values can be derived for L2 regression, Huber losses, etc.</font></p>
</li>
<li><p><font color="#cornflowerblue">init well. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias.</font></p>
</li>
<li><p>human baseline. Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it. Alternatively, annotate the test data twice and for each example treat one annotation as prediction and the second as ground truth.</p>
</li>
<li><p>输入相关的基线。训练输入独立的基线（例如，最简单的方法是将所有输入设置为零）。这应该比实际插入数据而不将其归零的情况更糟糕。即：您的模型是否学习从输入中提取任何信息？</p>
</li>
<li><p>过拟合一个batch。使用少数几个样本（例如，仅两三个样本）对单个批次进行过拟合。为此，我们增加了模型的容量（例如添加层或filters），并验证我们可以达到最低的可实现损耗（例如零）。我还喜欢在同一个图中可视化标签和预测，并确保一旦我们达到最小损失，它们最终会完美对齐。如果他们不这样做，那么肯定在某个地方存在一个bug，我们无法继续下一阶段。</p>
</li>
<li><p>验证是否减少了训练loss，在这个阶段，我们更加希望看到在数据集上欠拟合，因为你正在使用一个玩具模型。试着稍微增加它的容量。你的训练损失有没有像应该的那样减少？</p>
</li>
<li><p>在net之前可视化，可视化数据的明确正确位置就在y_hat=model（x）（或sess.run in tf）之前。也就是说，您希望准确地可视化进入网络的内容，将数据和标签的原始张量解码为可视化。这是唯一的“真理之源”。我无法计算这节省了我多少时间，并暴露了<strong>数据预处理和扩充方面的问题。</strong></p>
</li>
<li><p>可视化预测动态。在训练过程中，我喜欢在固定的测试批次上可视化模型的预测。这些预测的“动态”会让你对训练的进展有非常好的直觉。<font color="#cornflowerblue">很多时候，如果网络以某种方式摆动过多，暴露出不稳定性，人们可能会感觉到网络在努力适应数据。非常低或非常高的学习率在抖动量上也很容易被注意到。</font></p>
</li>
<li><p><strong>使用backprop来图表来依赖关系</strong>。<font color="#cornflowerblue">深度学习代码通常会包含复杂的、矢量化的和广播式的操作。我曾经遇到过的一个相对常见的错误是，<strong>人们错误地理解了这一点（例如，他们在某处使用视图而不是转置/置换），无意中在批处理维度中混合了信息。这是一个令人沮丧的事实，您的网络通常仍能正常训练，因为它将学会忽略其他样本中的数据</strong>。调试此问题（以及其他相关问题）的一种方法是将损耗设置为微不足道的值，如样本的所有输出之和，运行反向传递到输入，并确保仅在第个输入上获得非零梯度。例如，可以使用相同的策略来确保时间t时的自回归模型仅取决于。更一般地说，梯度为您提供了有关网络中哪些依赖于哪些的信息，这对于调试非常有用。</font></p>
</li>
<li><p>泛化一个特例。这更像是一个通用的编码技巧，从头开始编写一个相对通用的功能。 Often this applies to vectorizing code, where I almost always write out the fully loopy version first and only then transform it to vectorized code one loop at a time.</p>
</li>
</ul>
</blockquote>
<br>

<p>4、过拟合</p>
<blockquote>
<ul>
<li>首先获得一个足够大的模型，使其能够过拟合（即，关注训练损失），然后适当地调整它（放弃一些训练损失以改善验证损失）。</li>
<li>如果我们在任何模型上都无法达到低错误率，那么这可能再次表明一些问题、bug或错误配置。</li>
<li>选择模型。要达到良好的训练效果，您需要为数据选择合适的结构。在选择这个问题上，我的第一条建议是：不要做英雄。我见过很多人，他们热衷于疯狂和创造性地将神经网络工具箱中的乐高积木堆积在各种对他们认为有意义的结构中。在项目的早期阶段强烈抵制这种诱惑。</li>
<li>我总是建议人们简单地找到最相关的论文，然后复制粘贴他们最简单的体系结构，以获得良好的性能。例如，如果您正在对图像进行分类，请不要成为英雄，只需在第一次运行时复制粘贴ResNet-50即可。你可以在以后做一些更习惯的事情，并战胜它；</li>
<li>Adam会相对安全。在设定基线的早期阶段，我喜欢使用学习率为3e-4的Adam。根据我的经验，Adam对超参数（包括糟糕的学习率）更为宽容。对于ConvNets，经过良好调整的SGD几乎总是略优于Adam，但最佳学习速率区域要窄得多，且针对具体问题(注意：如果您使用RNN和相关序列模型，Adam则更常用。在项目的初始阶段，再次强调，不要做英雄，而要遵循最相关的论文。）</li>
<li>一次只复杂化一个。如果您有多个信号要插入分类器，我建议您一个接一个地插入它们，每次都要确保获得预期的性能提升。还有其他增加复杂性的方法-例如，您可以尝试先插入较小的图像，然后再将其放大，等等。</li>
<li>不要相信学习速率衰减默认值。如果您打算从其他领域重新编写代码，请务必非常小心使用学习率衰减。您不仅希望针对不同的问题使用不同的衰减计划，而且更糟糕的是，在典型schedule实现中，该计划将基于当前epoch，而当前epoch数仅取决于数据集的大小，可能会有很大的变化。例如，ImageNet将在第30 epoch时衰减10。如果您不训练ImageNet，那么您几乎肯定不希望这样。如果您不小心，您的代码可能会过早地秘密地将您的学习率降至零，从而导致您的模型无法收敛。在我自己的工作中，我总是禁用学习速率完全衰减（我使用一个常数LR），并在最后一直调整它</li>
</ul>
</blockquote>
<p>5、正则化：</p>
<blockquote>
<ul>
<li>我们现在所处的位置是，我们有一个至少拟合训练集的大模型。现在需要对其进行正则化，并通过放弃一些训练精度来获得更好的验证精度。</li>
<li>get more data. First, the by far best and preferred way to regularize a model in any practical setting is to add more real training data. It is a very common mistake to spend a lot engineering cycles trying to squeeze juice out of a small dataset when you could instead be collecting more data. As far as I’m aware adding more data is pretty much the only guaranteed way to monotonically improve the performance of a well-configured neural network almost indefinitely. The other would be ensembles (if you can afford them), but that tops out after ~5 models.</li>
<li>创造性的数据增加。如果有一半的假数据不起作用，假数据也可能起到作用。人们正在寻找扩展数据集的创造性方法；例如，域随机化、模拟的使用、巧妙的混合，例如将（可能模拟的）数据插入场景，甚至是GANs。</li>
<li>预训练：如果可以的话，即使你有足够的数据，使用预先训练好的网络也不会有什么坏处。</li>
<li>坚持监督学习。不要对无监督的预训练过度兴奋。据我所知，与2008年的那篇博文所告诉你的不同，目前还没有一个版本的NLP在现代计算机视觉方面取得了很好的效果（尽管这段时间来使用BERT处理NLP问题非常好，很可能是因为文本到特殊性质，以及更高的信噪比）。</li>
<li>较小的输入维度。删除可能包含虚假信号的功能。如果您的数据集很小，任何添加的虚假输入都只是另一个过拟合的机会。同样，如果低级细节无关紧要，请尝试输入较小的图像。</li>
<li>更小的模型size。在许多情况下，您可以使用网络上的领域知识约束来减小其大小。例如，过去流行在ImageNet主干的顶部使用完全连接的层，但后来这些层被简单的平均池所取代，从而消除了过程中的大量参数。</li>
<li>减少batch大小.        Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean/std are more approximate versions of the full mean/std so the scale &amp; offset “wiggles” your batch around more.</li>
<li>加上drop。将dropout2d（空间dropout）用于CONVnet。谨慎使用此选项，因为辍学似乎不能很好地处理批处理规范化。</li>
<li>权重衰减。增加weight衰减惩罚。</li>
<li>早停。根据验证损失停止训练，以便在模型即将过度拟合时捕捉模型。</li>
<li>试试大一点的模型。我最后一次提到这一点，而且是在提前停止之后，但我发现在过去的几次中，大型车型当然最终会过度拟合，但它们的“提前停止”性能通常会比小型车型好得多。</li>
</ul>
</blockquote>
<p>6、调</p>
<p><strong>随机搜索</strong></p>
<p>探索模型和超参数的美好和广阔空间的最先进方法是使用<strong>实习生</strong>：）</p>
<p>集成</p>
<blockquote>
<p>留着训练。我经常看到人们试图在验证损失趋于平稳时停止模型培训。根据我的经验，网络会持续很长时间的训练。有一次，我在寒假期间不小心离开了，留着一个模特训练，当我在一月份回来的时候，那是SOTA。</p>
</blockquote>
<br>

<br>

<h5 id="麦肯锡思维十大底层逻辑，认知升级深度好文！"><a href="#麦肯锡思维十大底层逻辑，认知升级深度好文！" class="headerlink" title="麦肯锡思维十大底层逻辑，认知升级深度好文！"></a>麦肯锡思维十大底层逻辑，认知升级深度好文！</h5><p>1、分类</p>
<p>2、矩阵：横纵轴。<strong>产品矩阵模型了（****BCG matrix</strong>）– 市场增长率和市场占有率</p>
<p>3、漏斗–<strong>AARRR模型</strong></p>
<p>要提高最终的收入，原理很简单，就是把每一层的<strong>容量</strong>都扩大，或者，把下钻的<strong>转化率</strong>给提高。作为增长黑客的你，首先要学会去用数据去判断，哪个地方的容量太小，或转化率太低，然后再去制定方案进行针对性提高。</p>
<p>4、相关</p>
<p><strong>大数据关注的不是因果关系，而是相关关系</strong>。相关分数矩阵。</p>
<p>5、决策树</p>
<p>一个<strong>流程图</strong>甚至是一个<strong>检查清单</strong></p>
<p>6、闭环</p>
<p>戴明环，又叫<strong>PDCA循环。</strong> DO - PLAN - CHECK - ACT 。<strong>它指明了一条****螺旋式上升的发展途径</strong>。一层比一层高。每次都能plan然后do然后check然后action(迭代优化)，才能知行合一。</p>
<p>7、逻辑链思维</p>
<p>归纳演绎。</p>
<p><strong>逻辑链的长度</strong>，决定了一个人的思维深度；一个人的思维深度，也决定了他的人生高度。</p>
<p>8、<strong>时间序列思维</strong></p>
<ul>
<li><strong>趋势</strong> – 长期思维</li>
<li><strong>较短的时间范围内所出现的一种波动规律</strong> – （环比是指这个月跟上个月比较，同比是指今年这个月跟去年这个月的比较）</li>
<li><strong>周期性</strong> – <strong>有周期意识的人，甚至比有趋势意识的人要更高一个水平。</strong></li>
</ul>
<p>9、试验思维</p>
<p><strong>科学的核心，就是它有可证伪性</strong>。</p>
<p>有时候不能先验地去认定用户喜欢什么，不喜欢什么。很多消费行为或决策都是无意识中、几毫秒中就产生好了。</p>
<p>最好的办法，就是<strong>以结果为导向，让用户用脚来投票</strong>。</p>
<p><strong>If you are not running experiments, you are probably not growing</strong></p>
<p>test &amp; learn</p>
<p><strong>我没有失败，我只是发现了10000种不成功的办法</strong></p>
<p>10、<strong>数字化思维</strong></p>
<p><strong>No measurement, no improvement</strong>。</p>
<br>

<br>



<h5 id="她一定认输，求回王思聪-2021-6-17"><a href="#她一定认输，求回王思聪-2021-6-17" class="headerlink" title="她一定认输，求回王思聪 2021.6.17"></a>她一定认输，求回王思聪 2021.6.17</h5><p>1、竞争并不激烈，因为聪明人的世界，寥寥无几。</p>
<p>2、才21岁，突然间有了双色球级，大笔收入，一夜横财，她会节俭么。超模17岁就可以进入上流社会。但是到了25岁，她们就被无情地踢出亿万圈子</p>
<br>

<br>



<h5 id="yevon-ou对2021楼市的判断"><a href="#yevon-ou对2021楼市的判断" class="headerlink" title="yevon_ou对2021楼市的判断"></a>yevon_ou对2021楼市的判断</h5><p>（2021/4/21）</p>
<p>1）最迟在 2026 年之前，北上深楼市将完成２０／１３／８。</p>
<p>2）在未来，楼市和调控都会变成小事。</p>
<p>3）未来将进入一个惊涛骇浪的时代，甚至纸币计价都不复有意义。</p>
<br>

<br>



<h5 id="创业7年，赚他个100亿身价"><a href="#创业7年，赚他个100亿身价" class="headerlink" title="创业7年，赚他个100亿身价"></a>创业7年，赚他个100亿身价</h5><p>1、14年在北京创业，公司叫石头科技，做扫地机器人的，20年2月在科创板上市，目前市值548亿，昌敬占股23%，价值126亿。</p>
<p>前不久石头发布了半年报，一共卖出123万台扫地机器人，营业收入23.48亿元，净利润6.51亿元，利润率高达28%，远高于一般制造业。</p>
<p>毕业四年，2010，在腾通讯任高级产品经理。第五年创立北京魔图精灵科技任ceo，第六年，在百度任高级经理。</p>
<p>2、何小鹏本科毕业后，在广州一家小型IT公司干了5年，离职创办了UC浏览器，十年之后40亿美金卖给阿里</p>
<p>3、昌敬微软技术+腾讯产品背景，正是李开复喜欢的，因此很快就拿到了创新工场的投资。</p>
<p>这次创业持续了10个月，项目不被看好，找不到后续融资，正好当时的百度想引进一些图像处理技术方面的人才，就以1200万美金的价格，连人带项目买下了昌敬团队。</p>
<p>4、14年正是万众创新创业的黄金时代，那一年小米风光无限，估值450亿美金，手握大把钞票，到处找智能硬件项目，来丰富小米生态链或者智能家居，而扫地机器人正是小米心许的项目。</p>
<p>5、昌敬从百度出来，准备做扫地机器人，拉上了前微软同事，以及前华为的合作伙伴。</p>
<p>昌敬在腾讯百度的产品经历，加上微软前同事的软件和算法能力，以及华为伙伴的硬件能力，产品、软件、硬件，铁人三项齐了，小米看好他们，很快就给出了投资意向，有小米背书，其他投资人自然就无脑跟进。</p>
<p>6、就像之前给小米做平衡车、手环的公司一样，打出名声之后，各自走上了独立品牌道路，昌敬他们也一样，2017年推出了自有品牌“石头智能扫地机器人”，这一年，石头自有品牌产品的销售额在总营收中占比仅为9.63%，严重依赖小米这个大客户的销售渠道。</p>
<h5 id="石头科技：一家活得轻轻松松的百亿美元市值公司"><a href="#石头科技：一家活得轻轻松松的百亿美元市值公司" class="headerlink" title="石头科技：一家活得轻轻松松的百亿美元市值公司"></a>石头科技：一家活得轻轻松松的百亿美元市值公司</h5><p>1、同期创办的中国科技、互联网公司中，只有拼多多和三家新造车公司蔚来、理想、小鹏的市值比石头科技高，再鼎医药（创新药研发公司）、中伟股份（锂电池原料公司）、满帮货运和 BOSS 直聘与之接近。</p>
<p>石头科技去年卖出 238 万台扫地机（含境外），净利润近 13.7 亿元。一台赚500，比市场份额第一的科沃斯高一倍</p>
<p>2、那时候石头只有 5 个人，花了 40 天时间，组装出一个 “其貌不扬” 的测试机：就是买来一个扫地机器人拆掉，改造了机台后，连接平板电脑运行算法，画出地图，机器人就按照规划路线慢慢地走。</p>
<br>

<br>



<h5 id="夷陵之战刘备为何军中所带众将都是一些水平与资历都很低的将领？"><a href="#夷陵之战刘备为何军中所带众将都是一些水平与资历都很低的将领？" class="headerlink" title="夷陵之战刘备为何军中所带众将都是一些水平与资历都很低的将领？"></a>夷陵之战刘备为何军中所带众将都是一些水平与资历都很低的将领？</h5><p>1、关羽黄忠已死，马超不可能受到重用，魏延和吴懿要驻守汉中，赵云因为政见不合被弃用，功臣宿将里只有张飞一人可用。</p>
<p>2、夷陵之战原计划就是用中生代将领去打的。<strong>对自己军队结构有长远布局，在将领老龄化的问题上早有预案。</strong>夷陵的阵容是经过慎重考虑的。</p>
<p>3、</p>
<p>为什么蜀国不重用马超？</p>
]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title>投资逻辑</title>
    <url>/2021/06/10/%E6%8A%95%E8%B5%84%E9%80%BB%E8%BE%91/</url>
    <content><![CDATA[<br>
<br>



<h3 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h3><p>1、2005年6月最暴跌时，一旦人民币升值、国有股流通，股市将大涨。</p>
<span id="more"></span>

<h3 id="108"><a href="#108" class="headerlink" title="108"></a>108</h3><p>1、12课：</p>
<p>由5日均线与10日均线构成的买卖系统，首先，两者的体位构成一个完全分类，女上位是牛，男上位是熊，还有一种是互相缠绕的情况，这种情况最终都要演化成女上位或男上位，只有两种性质：中继或转折。对多头来说，值得介入的只有两种情况：<strong>男上位转折，女上位中继</strong>，空头反之。</p>
<p> 例如，<strong>女上位趋势出现的第一次缠绕是中继的可能性极大</strong>，如果是<strong>第三、四次出现，这个缠绕是转折的可能性就会加大</strong>；还有，出现<strong>第一次缠绕前，5日线的走势必须是十分有力的，这样缠绕极大可能是中继，其后至少会有一次上升的过程出现</strong>；第三，缠绕出现前的<strong>成交量不能放得过大，一旦过大，骗线出现的几率就会大大增加</strong>，如果量突然放太大而又萎缩过快，<strong>一般即使没有骗线，缠绕的时间也会增加</strong>，而且成交量也会现在两次收缩的情况。</p>
<p>  <strong>女上位选择第一次出现缠绕的中继情况，而男上位的就相反，要寻找最后一次缠绕的转折情况</strong>，其后如果出现急跌却背弛，那是最佳的买入时机。抄底不是不可以，但只能选择这种情况。</p>
<p>没有人百分百确认那是最后一次缠绕，一般，<strong>男上位后的第一次缠绕肯定不是，从第二次开始都有可能</strong>，如何判断，最有力的就是利用好<strong>背弛制造的空头陷阱。</strong></p>
<p><br><br></p>
<p><strong>利用均线构成的买卖系统，首先要利用男上位最后一次缠绕后背弛构成的空头陷阱抄底进入，这是第一个值得买入的位置，而第二个值得买入或加码的位置，就是女上位后第一次缠绕形成的低位。</strong></p>
<p>即，男上转折、女上中继。即男上最后一缠后背驰、女上第一缠后低点。</p>
<p>卖点，即女上转折–女上位缠绕后出现背弛、男上中继–男上后第一次缠绕后的高点。</p>
<p><br><br></p>
<p>2、从来不觉得自己手里的股票有什么好，只知道他们能搞。但其实他是看基本面的。比如考虑人民币升值和股权分置。所以并不是无用，只是不能信“消息”。</p>
<p><br><br></p>
<p>3、</p>
<p><br><br></p>
<p>4、均线系统：</p>
<p>飞吻：短期均线略略走平后继续按原来趋势进行下去。</p>
<p>唇吻：短期均线靠近长期均线但不跌破或升破，然后按原来趋势继续下去。</p>
<p>湿吻：短期均线跌破或升破长期均线甚至出现反复缠绕，如胶似漆。 </p>
<blockquote>
<p>短期向长期靠拢，但是不破，进入缠绕或者按照原来的趋势继续(空头市场中说明反弹结束，多头则调整结束)。短期稍微平了继续原来的短期趋势(几率少、火爆后震荡)。短期向长期但是跌破或者升破。短期和长期反复缠绕(趋势转折，空转多)。</p>
<p>任何行情的转折大概率是突破引起。可能先突破然后拉升，然后陷阱诱再转折。还可能是转折箱型，拉高，然后转折。</p>
<p>第一次突破可能不一定真的转折。但突破之后必然有空多势能的转换。即使当时没有拉升很猛，但是势能变化了。</p>
<p>理解： 5上10下 女上位，10上5下，5 10缠绕。但最终都会是第一或者第二种。缠绕时等待转折。因此在缠绕末端介入，并且需要判断“转折为5上”或者“5上继续”的风险。</p>
<p>1、如果5上，且强势，且第一次产生缠绕，且缠绕前的成交不能过大(防止骗线)，则极可能是延续原来的趋势，还有一波。如果是第三四次则极可能是转折。</p>
<p>2、如果成交在缠绕前，突然放大又快速萎缩，则即使没有骗线，缠绕时间也会增加。成交可能再次收缩。</p>
<p>3、如果10上，且不是第一次缠绕，第二次开始往后、当然最好是最后一次缠绕转折，如果转折后还出现急跌并背驰，则为空头陷阱，为最佳的抄底买入。</p>
<p>(万达，我觉得后面疫情复苏有更高的可能。又是一次追。大部分只是压在这个逻辑。结果，没有任何好消息。也没有任何青睐。也没有势头。继续等待看看。）</p>
<p>4、第一个卖点，也就是<strong>女上位缠绕后出现背弛</strong>以及第二个卖点也就是<strong>变成男上位的第一个缠绕高点</strong>把东西卖了，这样就完成一个完整的操作。</p>
<p>5、买尽量买男上的转折，卖尽量卖女上的缠绕背驰。  </p>
</blockquote>
<br>

<br>

<p>5、实践理性</p>
<p>任何技术指标、系统，本质上都是一个评价系统，也就是告诉你在这个系统的标准下，评价对象的强弱</p>
<p>设置一套分类评价系统，然后根据该系统，对所有可能的情况都设置一套相应的应对程序，这样，一切的风险都以一种可操作的方式被操作了。</p>
<p>股票，主要有从0开始建立起来的系统。然后把技术和基本勾连起来，进行填充。心理面的建设，只有在实践里面练习吧。把操作融化为子程序。设计筛查均线上下关系以及缠绕关系的股票，由程序进行预警。然后在每个里面去看交易的起伏和势能。这个能够有评估系统进行对势能的筛查等。最后进入到图像上的走势关系。</p>
<p>比如，强势女上的时候，第一个缠绕后无放量的诱空，为买点。但如果是转折呢，转折的话，那么一定在短缠绕或者缠绕前就已经出货了。能够在前面看出出货的迹象吗。大笔出货还是小单多笔呢 – 这个从哪里看出来？todo</p>
<p>大量信息需要去噪和保持新鲜的及时反馈。分类考虑清楚了，全局和框架大体就出来了。实践需要一个评估系统和反馈系统。评估指标的数据化比较难，实时地记录也比较困难，</p>
<br>

<p>均线程序系统：</p>
<p>1、一个时间段，拉出来所有的k图，以及均线图</p>
<p>2、一个时间段，成交图，横轴为单价，纵轴为手。</p>
<p>3、筛选：均线女上到第一次缠绕。男上到第一次缠绕的股票。（判断是否是最后一次缠绕、以及是否继续还是转折）</p>
<br>

<p>迹象系统：</p>
<p>1、出货迹象：</p>
<p>2、主力迹象：</p>
<br>

<p>概率系统：</p>
<br>



<p>6、15课：“吻“的分类是基于对原趋势的反抗程度：</p>
<p>男上(均线为长期在上短期在下)转为女上时，连短线均线都不能突破，那期间出现的高、低点，肯定只是低级别图表上的，在本级别图表上没有意义 – 看不出和均线的关系，只能降低图标级别再看关系。</p>
<p>当走势突破短期均线却不能突破长期均线，就会形成“飞吻”–突破短期则短期均线会显示走平但是没有继续突破，则不会改变短期的趋势；</p>
<p>当走势突破长期均线马上形成陷阱，就会形成“唇吻”–靠近而不破，还按照原来的趋势，即是“诱惑转折”；</p>
<p>当走势突破长期均线出现一定的反复，就会形成“湿吻” – 有可能转折。</p>
<br>

<br>



<p>7、15课：转折</p>
<p>一般只有两种：一、“湿吻”后继续原趋势形成陷阱后回头制造出转折 – 最大的标志就是所谓的“背驰”了；二、出现盘整，以时间换空间地形成转折。：：：：这里的所有判断都只关系到两条均线与走势，和任何技术指标都无关。</p>
<p>理解：一个是最后一个湿吻之后的背驰之后转折，一个是盘整后转折。</p>
<p>看图上的话，没有吻也转折了 – 看的级别高了。</p>
<br>

<br>



<p>8、15课：背驰todo</p>
<p>在盘整中是无所谓“背驰”的</p>
<p>背驰是一个趋势中，趋势的力度，对应的现象。</p>
<p>量化：前一“吻”的结束与后一“吻”开始由短线均线与长期均线相交所形成的面积。（两条均线）同向趋势，但是面积变小，力度变弱，则形成了背驰。：：：：必须等再次接吻后才能判断，这时候，走势离真正的转折点会已经有一点距离了。解决：看低一级别的图，从中按该种办法找出相应的转折点。这样和真正的低点基本没有太大的距离。</p>
<p> 趋势平均力度：当下与前一“吻”的结束时 短线均线 与 长期均线形成的面积 除以 时间。一旦这次比上次弱，就可以判断“背驰”即将形成，然后再根据短线均线与长期均线的距离，一旦延伸长度缩短，就意味着真正的底部马上形成。</p>
<p><br><br></p>
<p>9、15课：第一个买点买入的基础在于男上位最后一个缠绕后出现背驰(往下跌，短线与长线均线面积比之前的小 ？)，而现在又出现男上位的缠绕，意味着前面引导买入程序启动的缠绕并不是最后一个缠绕，也就是程序判断上出现问题，因此必须退出。</p>
<p>第二个买点，一旦该缠绕中出现跌破前面男上位的最低位，就意味着买入程序出现问题。</p>
<p><br><br></p>
<p>10、16课：第一类买点且之前走势是“下跌+盘整+下跌”类型</p>
<p>**陷阱式：上涨+下跌；下跌+上涨。反转式：上涨+盘整+下跌；下跌+盘整+上涨。中继式：上涨+盘整+上涨；下跌+盘整+下跌。 **</p>
<p>在下跌时买入，唯一需要躲避的风险有两个：<strong>一、该段跌势未尽；二、该段跌势虽尽，但盘整后出现下一轮跌势。</strong></p>
<p>下跌走势用背驰来找第一类买点，就是要避开上面的第一个风险。而当买入后，将面对的是第二个风险。解决：就是<strong>其后一旦出现盘整走势，必须先减仓退出。</strong></p>
<p><strong>盘整也会耗费时间，对于中小资金来说，完全没必要。</strong>特别对于不想坐庄的大资金来说，这是一个最重要的问题，因为<strong>不想坐庄的大资金的安全建仓在六种走势中只可能在下跌+盘整+上涨这一种</strong>，其他都不适用。至于<strong>坐庄的建仓方法，和这些都不同。</strong></p>
<p>买卖入方法：在<strong>第一类买点买入后，一旦出现盘整走势，无论后面如何，都马上退出</strong>。这种买卖方法的实质，即只参与唯一的一种：<strong>下跌+上涨</strong>。对于资金量不大的，这是最有效的一种买卖方法。因为：<strong>对于下跌+上涨来说，连接下跌前面的可能走势只会有两种：上涨和盘整</strong>。</p>
<p><strong>如果是上涨+下跌+上涨，那意味着这种走势在上一级别的图形中是一个盘整</strong>，因此这种走势可以归纳在盘整的操作中，这在以后对盘整的专门分析里研究。对于“下跌+上涨”买卖方法方法来说，必须是这样一种情况：就是<strong>一个前面是“盘整+下跌”型的走势后出现第一类买点</strong>。显然，这个下跌是<strong>跌破前面盘整</strong>的，否则就不会构成“盘整+下跌”型，只会仍是盘整。</p>
<p>那么在该盘整前的走势，也只有两种：上涨、下跌。<strong>对于“上涨+盘整+下跌”的，也实质上构成高一级别的盘整</strong></p>
<p>=&gt;<strong>出现第一类买点且之前走势是“下跌+盘整+下跌”类型。</strong></p>
<p>标准程序=&gt; 一、首先只选择出现“下跌+盘整+下跌”走势的。二、在该走势的<strong>第二段下跌出现第一类买点时</strong>介入。三、介入后，<strong>一旦出现盘整走势，坚决退出</strong>。注意，这个退出肯定不会亏钱的，因为可以利用<strong>低一级别的第一类卖点</strong>(第一个卖点是女上最后一缠的背驰，第二个卖点是男下第一缠的)退出，是肯定要赢利的</p>
<p><strong>前面出现“上涨+盘整+上涨”走势的，一旦第二段升势出现第一类卖点</strong>，一定要走，因为后面很可能就是“上涨+下跌”的典型走势。</p>
<p>这种方法，无论买卖，都极为适用于中小资金，如果把握得好，是十分高效的。</p>
<p><br><br></p>
<p>11、17课：</p>
<p>  把实践中总结出来的、很难实用的、静态的“所有级别的走势都能分解成趋势与盘整”，转化成动态的、可以实用的“走势类型终要完成”。<strong>任何走势，无论是趋势还是盘整，在图形上最终都要完成</strong>。另一方面，一旦某种类型的走势完成以后，<strong>就会转化为其他类型的走势</strong>，这就是“不患”而有其位次。</p>
<p><span style="color:red"><strong>某级别走势类型中，被至少三个连续次级别走势类型所重叠的部分，称为缠中说禅走势中枢</strong></span>一般来说，对实际操作，都把这最低的不可分解级别设定为<strong>1分钟或5分钟线</strong>，当然，也可以设定为1秒种线，但这都没有太大区别。</p>
<p><span style="color:red">盘整：在任何级别的任何走势中，<strong>某完成的走势类型只包含一个缠中说禅走势中枢</strong>，就称为该级别的缠中说禅盘整。</span></p>
<p><span style="color:red">趋势：在任何级别的任何走势中，<strong>某完成的走势类型至少包含两个以上依次同向的缠中说禅走势中枢</strong>，就称为该级别的缠中说禅趋势。该方向向上就称为上涨，向下就称为下跌。</span></p>
<p><strong><span style="color:red">任何级别任何完成的走势类型，必然包含一个以上的缠中说禅走势中枢。</span></strong></p>
<p>  由原理一、二以及缠中说禅走势中枢的定义，就可以严格证明：</p>
<p><span style="color:red">“缠中说禅走势分解定理一”：任何级别的任何走势，都<strong>可以分解成同级别“盘整”、“下跌”与“上涨”三种走势类型的连接</strong>。</span></p>
<p><span style="color:red">“缠中说禅走势分解定理二“：任何级别的任何走势类型，都<strong>至少由三段以上次级别走势类型构成</strong>。</span></p>
<p><br><br></p>
<p>12、17课：</p>
<p><strong>在第一类买点出现后第一次次级别回调制造的低点，是市场中第二有利的位置</strong>，为什么？因为上涨和盘整必然要在图形上完成，而上涨和盘整在图形上的要求，是必须包含三个以上的次级别运动，因此后面必须还至少有一个向上的次级别运动，这样的买点是绝对安全的。</p>
<p>“缠中说禅买卖点定律一”：<strong>任何级别的第二类买卖点都由次级别相应走势的第一类买点构成。</strong>这样，就像前面曾说过的，任何由第一、二类买卖点构成的缠中说禅买卖点，都可以归结到不同级别的第一类买卖点。由此得到“缠中说禅趋势转折定律”：<strong>任何级别的上涨转折都是由某级别的第一类卖点构成的；任何的下跌转折都是由某级别的第一类买点构成的。</strong></p>
<p>  <strong>注意，这某级别不一定是次级别，因为次级别里可以是第二类买卖点</strong>，而且还有这种情况，就是不同级别同时出现第一类买卖点，也就是出现不同级别的同步共振，所以这里只说是某级别。</p>
<p><br><br></p>
<p>13、17课：</p>
<p>1、 连接两相邻同级别缠中说禅走势中枢的一定是趋势吗？一定是次级别的趋势吗？    </p>
<p>解：也可以是中枢吧。</p>
<p>但是中枢或者趋势都一定是次级别的趋势。    </p>
<p>正答：也不一定是次级别的，<strong>只要是次级别以下，例如跳空缺口，就属于最低级别，如果图上是日线、周线，就不会是次级别了；最后，往往相连走势类型的级别越低，表示其力度越大，这也就是为什么缺口在分析中有比较强技术含义的理论依据所在。</strong></p>
<p>2、 背驰是两相邻同向趋势间，后者比前者的走势力度减弱所造成的，如果用均线或MACD等判断其力度，一定要在同级别的图上吗？同级别的MACD红绿柱子背驰一定反映某级别趋势间出现背驰吗？是相应级别的趋势出现背驰吗？</p>
<p>本质是，判断力度能不能用次级别的MACD或者均线来判断。当然可以。因为本级别的背驰趋势是由次级别的走势形成的。</p>
<p>同级别的MACD红绿柱子指什么？反映什么级别的趋势间背驰？本级别还是？还是不反应？</p>
<p>​        </p>
<p>3、 盘整的高低点是如何造成的。（这个问题有点难度，提示，用缠中说禅走势中枢以及级别等进行分析。）</p>
<p>次级别的三个趋势构成本级别的盘整。多个次级别的趋势高低点构成本级别盘整的高低点。</p>
<p><br><br></p>
<p>14、18课：</p>
<p><strong>次级别的前三个走势类型都是完成的</strong>才构成该级别的缠中说禅走势中枢，完成的走势类型，在次级别图上是很明显的，根本就不用着再看次级别下面级别的图了。</p>
<p>缠中说禅盘整：在任何级别的任何走势中，某完成的走势类型只包含一个缠中说禅走势中枢，就称为该级别的缠中说禅盘整。</p>
<p>缠中说禅趋势：在任何级别的任何走势中，某完成的走势类型至少包含两个以上依次同向的缠中说禅走势中枢，就称为该级别的缠中说禅趋势。该方向向上就称为上涨，向下就称为下跌。注意，趋势中的缠中说禅走势中枢之间必须绝对不存在重叠。</p>
<p>“缠中说禅技术分析基本原理一”：任何级别的任何走势类型终要完成。</p>
<p>“缠中说禅技术分析基本原理二”：任何级别任何完成的走势类型，必然包含一个以上的缠中说禅走势中枢。</p>
<p>“缠中说禅走势分解定理一”：任何级别的任何走势，都可以分解成同级别“盘整”、“下跌”与“上涨”三种走势类型的连接。</p>
<p>“缠中说禅走势分解定理二“：任何级别的任何走势类型，都至少由三段以上次级别走势类型构成。</p>
<p><br><br></p>
<p>15、18课：</p>
<ol>
<li>走势的完成是必然的，但是如何判断走势的完成。即什么时候盘整结束之类的问题。</li>
<li>一个盘整，三个重叠的连续次级别走势类型后，盘整就可以随时完成，也就是说，只要三个重叠的连续次级别走势类型走出来后，盘整随时结束都是完美的，但这可以不结束，可以不断延伸下去，不断围绕这缠中说禅中枢上上下下地延伸下去直到无穷都是可以的。</li>
<li>所以，核心在于，走势何时停止延伸？判断不清，则会在趋势中中间的中枢时候被震出去，而无法坚持到卖点</li>
<li>对于趋势来说，其“延伸”就在于同级别的同向“缠中说禅走势中枢”不断产生；而对于盘整来说，其“延伸”就在于不能产生新的“缠中说禅走势中枢”。“走势类型延伸”是否结束的判断关键就在于<strong>是否产生新的“缠中说禅走势中枢”。</strong></li>
<li>“缠中说禅走势中枢定理一”：在趋势中，连接两个同级别“缠中说禅走势中枢”的必然是次级别以下级别的走势类型。</li>
<li>用反证法，该定理的证明是很简单的，而这也回答了上一章中的作业一“<strong>连接两相邻同级别缠中说禅走势中枢的一定是趋势吗</strong>？一定是次级别的趋势吗？”</li>
<li>首先，这不必然是趋势，任何走势类型都可能，最极端的就是跳空缺口后形成新的“缠中说禅走势中枢”；其次，也不一定是次级别的，<strong>只要是次级别以下，例如跳空缺口，就属于最低级别，如果图上是日线、周线，就不会是次级别了；最后，往往相连走势类型的级别越低，表示其力度越大，这也就是为什么缺口在分析中有比较强技术含义的理论依据所在。</strong></li>
<li>维持“缠中说禅走势中枢”的一个充分必要条件就是任何一个离开该中枢的走势类型都必须是次级别以下的并以次级别以下的走势类型返回，该问题很容易证明，因为无论是离开还是返回，只要是同级别的走势类型，就意味着形成新的“缠中说禅走势中枢”，这与原中枢的维持前提矛盾。该命题表述成如下定理：</li>
<li>“缠中说禅走势中枢定理二”：在盘整中，无论是离开还是返回“缠中说禅走势中枢”的走势类型必然是次级别以下的。</li>
<li>把1分钟图当成最低级别，那么最后<strong>连接离开与返回走势类型连接处的最低级别</strong>图，只能有两种可能：<strong>三根以上1分钟K线的来回重叠震荡后回头</strong>–比较少见；二、<strong>1分钟K线无三根以上K线重叠的V型走势</strong>–十分常见。</li>
<li>定理三：某级别“缠中说禅走势中枢”的破坏，当且仅当<strong>一个次级别走势离开该“缠中说禅走势中枢”后</strong>，其后的次级别<strong>回抽走势不重新回到该“缠中说禅走势中枢”内</strong>。</li>
<li>两个次级别走势的组合只有三种：趋势+盘整，趋势+反趋势，盘整+反趋势。其中的趋势分为上涨与下跌，分别代表从上方突破与下方跌破两种情况。</li>
<li>最用力的破坏，就是：趋势+盘整。例如在上涨中，如果一个次级别走势向上突破后以一个盘整走势进行整理回抽，那其后的上涨往往比较有力，特别这种突破是在底部区间。</li>
</ol>
<p> <br><br></p>
<p>16、21课： </p>
<ol>
<li>所有买卖点都必然对应着与该级别最靠近的一个中枢的关系。</li>
<li>第三买点：即 在中枢向上突破中枢高点后，回抽不跌破此高点，形成第三买点。</li>
<li>中枢有三种情况：延续、扩张与新生。延续，则是高低点没有被突破，扩张或新生则突破高点，形成一个新的趋势，要么是新的中枢要么是上涨趋势。延续则买点只能在中枢下。第三类买点，在中枢上，并且，这个中枢是扩张的。</li>
<li>所以中枢上的买点，只要避免延续情况即可。</li>
<li>对于中枢下形成的买点，但如果该中枢是在上涨之中的，在中枢之下并不能必然形成买点，中枢下的买点，只可能存在于下跌与盘整的走势类型中。</li>
<li>对于盘整的情况，其中枢的扩张与新生，都不能必然保证该买点出现后能产生向上的转折，因为其扩张与新生完全可以是向下发展的，而对于中枢延续的情况，中枢形成后随时都可以打破而结束延续，也不必然有向上的转折，所以盘整的情况下，中枢下也不必然产生买点。因此，只有在下跌确立后的中枢下方才可能出现买点。这就是第一类买点。</li>
<li>一个上涨趋势确定后，不可能再有第一类与第二类买点，只可能有第三类买点。</li>
<li>第一买点出现后的第二段次级别走势低点就构成第二类买点。即次级别出现的第一类买点则为本级别的第二买点。</li>
<li>⭐️⭐️⭐️⭐️⭐️<strong>第二类买点，不必然出现在中枢的上或下，可以在任何位置出现，中枢下出现的，其后的力度就值得怀疑了，出现扩张性中枢的可能性极大，在中枢中出现的，出现中枢扩张与新生的机会对半，在中枢上出现，中枢新生的机会就很大了。但无论哪种情况，赢利是必然的。</strong></li>
<li>第一类与第三类买点，一个在中枢之下、一个在中枢之上，也不可能产生重合。</li>
<li>只有第二类买点与第三类买点是可能产生重合的，这种情况就是：但第一类买点出现后，一个次级别的走势凌厉地直接上破前面下跌的最后一个中枢，然后在其上产生一个次级别的回抽不触及该中枢，这时候，就会出现第二类买点与第三类买点重合的情况，也只有这种情况才会出现两者的重合。</li>
<li>当然，在理论上没有任何必然的理由确定第二、三类买点重合后一定不会只构成一个更大级别的中枢扩张，但实际上，一旦出现这种情况，一个大级别的上涨往往就会出现。</li>
<li>一个最典型的例子，就是大盘在94年7月底部跌到325点后，8月1日跳空高开，5分钟上形成单边上涨突破前面的30分钟中枢，第二天大幅上冲后突然大幅回洗形成5分钟的走势级别的回抽，那时候最高已经摸到快500点，一天半上涨50%，又半天回跌15%，这样的回抽，一般来说是很恐怖的，但如果明白第二类买点与第三类买点的重合道理，就知道这是最好的补进机会，结果第三天又开始单边上扬，第六天达到750点。这是指数上最典型的一个例子了。而且，<strong>325点留下的缺口至今未补</strong>，中国几十年的一个大牛市，从指数上看，这是一个最重要的缺口了，将支持中国股市几十年甚至上百年的大牛市。</li>
<li>补充一句，站在特大型牛市的角度，中国就从来没出现过熊市，大家打开上海的年线图就可以看到，从1992年到2005年，一个完美的年级别缠中说禅中枢的三段次级别走势完成，时间刚好是13年，一个完美的时间之窗。站在年线的角度，中国股市的真正大牛市才真正开始，因为该中枢是中国股市的第一个年中枢，区间在998到1558点。站在年线级别，在下一个年线级别中枢确立之前，中国股市的调整只可能出现一个季级别的调整，而第一个出现的季级别的调整，只要不重新跌回1558点，就将构成中国股市年线级别上的第三类买点，其后至少出现如去年类型幅度的上涨。即使出现调整，最多就是季级别的，其后反而构成第三类买点。而且更重要的是，站在年线的级别看，目前还在第一段的次级别上扬中，要出现第二段的季级别调整，首先要出现月线级别的中枢，目前连这个中枢都没出现，换言之，年线级别的第一段走势还没有任何完成的迹象，这第一段，完全可以走到6000点才结束。</li>
<li>对卖点的分析是一样的，归纳起来，就有缠中说禅买卖点的完备性定理：市场必然产生赢利的买卖点，只有第一、二、三类。</li>
<li>市场中的任何向上与下跌，都必然从三类缠中说禅买卖点中的某一类开始以及结束。换言之，市场走势完全由这样的线段构成，线段的端点是某级别三类缠中说禅买卖点中的某一类。</li>
<li>思考题：任何一个线段，其端点必然是一买点及一卖点，请完全列出各类买卖点之间可能的组合。如果一线段的端点是同级别的买卖点，有什么组合是绝对不可能出现的。</li>
</ol>
<p><br><br></p>
<p>16、22课：</p>
<p>本ID在下面放上一个9999的买单，本ID顺着着这身体的轨迹轻扫着，还真有点体液</p>
<p>第二天，继续往下扭动身体，本ID的扫动越来越快，江浙派大概突然发现，这样继续下去，他就有被吸干的危险，尾市几笔就拉起来</p>
<p>第三天开始，在不断的摩擦中，面首开始挺立，每天尾市的游戏继续。</p>
<p>突然有一天，他也玩起打压恐吓的游戏。前两天，本ID就看热闹，不管他，第三天突然发狠，严重警告他，在乱恐吓就把他给杀了</p>
<p>江浙派果然是胆小之人，要挺立。</p>
<p>一定要在适当的时候突然狠狠一下，他就会惊吓得往相应方向惯性下去，一般来说，这种面首都是反应有点迟钝的，注定这种面首画出来的面相，总是反反复复，缠绵不断。</p>
 <br>

<p>真理是干出来的。</p>
<p>第一天的体液就不少，浮码很多，10几个交易日前那两根大量暗示着，浮码多，水就混，藏点大米还不简单？</p>
<p>周线图上的中枢强烈地勾引着走势往上，一般在一个组合里，一定要放一个这样老实巴交的面首，万一其他股票出现什么特殊的情况，马上变现这个去增援是能随时办到的，这样就一定不会出大乱子了。</p>
<br> 

<p>一般来说，这种阻击，<strong>在一个低位的大级别第三类买点进行是比较安全的</strong>，首先，第一类买点不适合，你先进去，大家都看着你，找机会吃你，你还找哪里潜伏下来？第二类买点是可以的，但一般都采取比较温柔的办法，慢慢来。第三类买点介入，有点硬来的感觉，这要求有一定的功力，否则给吃了都不知道怎么死的。但这样的安全性在于，第一，时间利用率高，第三类买点等于箭在弦上了，你这样突然进去横插一刀，除非是实力特别强，而且所用资金又没有什么期限，所弄的题材也没到迫在眉睫的地步，这样，他会留下来和你折腾。从而变成持久战。</p>
<p>高手就是高在一定要对盘中庄家的脾性有充分的感觉，对症下药，而且对阻击的目标有充分的了解，这样就能避免陷入持久战，互相在那里干耗着。当然，干耗其实也不怕，就是不断弄短差，把成本降下来，熬都熬死对方。这样的前提是资金必须绝对自由、没有期限。一笔自有的，没有利息压力的资金，是阻击的一个最安全的保障。</p>
<p>阻击一定要控制好量，最失败的阻击就是阻击成了庄家。为什么要在低位的第三类买点出手，这个位置，庄家已经货不少了，而成本还在附近，如果大力打压，你有实力在低位顶住，除非那面首钱出问题了，否则不可能亏钱把所有货倒给你，如果真是这样，就成全他算了。对于第三类买点的阻击，资金实力是很重要的，<strong>关键就是要顶住突然变向的打压，所以也要求一定只能在低位</strong>，不能与庄家的成本相差太远。</p>
<p><br><br></p>
<p>17、24课：</p>
<ol>
<li>任一背驰都必然制造某级别的买卖点，任一级别的买卖点都必然源自某级别走势的背驰。</li>
<li>日线上向上的背驰制造一个卖点，回跌后，在5分钟或30分钟出现向下的背驰制造一个买点，然后由这买点开始，又可以重新上涨，甚至创新高，这是很正常的情况。</li>
<li>用MACD判断背驰的前提是，A、B、C段在一个大的趋势里，其中A之前已经有一个中枢，而B是这个大趋势的另一个中枢，这个中枢一般会把MACD的黄白线（也就是DIFF和DEA）回拉到0轴附近。而C段的走势类型完成时对应的MACD柱子面积（向上的看红柱子，向下看绿柱子）比A段对应的面积要小，这时候就构成标准的背弛。</li>
<li>注意，看MACD柱子的面积不需要全出来，一般柱子伸长的力度变慢时，把已经出现的面积乘2，就可以当成是该段的面积。所以，实际操作中根本不用回跌后才发现背驰，在上涨或下跌的最后阶段，判断就出来了，一般都可以抛到最高价位和买在最低价位附近。</li>
<li>盘整中往上的情况，如果C段不破中枢，一旦出现MACD柱子的C段面积小于A段面积，其后必定有回跌。</li>
<li>比较复杂的是如果C段上破中枢，但MACD柱子的面积小于A段的，这时候的原则是先出来，其后有两种情况，如果回跌不重新跌回，就在次级别的第一类买点回补，刚好这反而构成该级别的第三类买点，反之就继续该盘整。</li>
</ol>
<p><br><br></p>
<p><img src="/Users/wyq/Downloads/satyrsBlog/source/_posts/%E6%8A%95%E8%B5%84%E9%80%BB%E8%BE%91/wddy.png" alt="wddy"></p>
<p><br><br></p>
<p> 18、25 26课：</p>
<ol>
<li><p>MACD的灵敏度，和参数有关，一般都取用12、26、9为参数，这对付一般的走势就可以了</p>
</li>
<li><p>但一个太快速的走势，1分钟图的反应也太慢了，如果弄超短线，那就要看实际的走势，即盘面和柱子。</p>
</li>
<li><p><strong>MACD的柱子伸长，和乖离有关</strong>，大致就是走势和均线的偏离度。</p>
</li>
<li><p>打开一个MACD图，首先应该很敏感地去发现该股票MACD伸长的一般高度，<strong>在盘整中，一般伸长到某个高度，就一定回去了，而在趋势中，这个高度一定高点，那也是有极限的，一般来说，一旦触及这个乖离的极限，特别是两次或三次上冲该极限，就会引发因为乖离而产生的回调</strong>。这种回调因为变动太快，在1分钟上都不能表现其背驰，所以必须用单纯的MACD柱子伸长来判断。即，冲击乖离而不能突破(虽然其形成的面积大于前面的，但两次柱子伸长都不能突破该高度)，意味着强暴的走势要歇，特别是两三次上冲，则大概率要回调。</p>
</li>
<li><p>不断一字涨停，不能用背弛来看，最简单，就是用1分钟的中枢来看，只要中枢不断上移，就可以不管。直到中枢上移结束，就意味着进入一个较大的调整，然后再根据大一点级别的走势来判断这种调整是否值得参与。</p>
<p>​    如果用MACD配合判断，就<strong>用长一点时间的，例如看30分钟</strong>。一般来说，这种走势，其红柱子都会表现出这样一种情况，就是<strong>红柱子回跌的低点越来越低，最后触及0轴，甚至稍微跌破，然后再次放红伸长</strong>，这时候就是警告信号，如果这时候在大级别上<strong>刚好碰到阻力位，一但涨停封不住，出现大幅度的震荡就很自然了</strong>。</p>
</li>
<li><p>注意，如果这种连续涨停是出现在第一段的上涨中，即使打开涨停后，震荡结束，<strong>形成一定级别的中枢后，往往还有新一段的上涨</strong>，必须<strong>在大级别上形成背驰才会构成真正的调整</strong>，因此，站在中线的角度，上面所说的超短线，其实意义并不太大，有能力就玩，没能力就算了。关键是要抓住<strong>大级别的调整，不参与其中</strong>，这才是最关键的。即，一定要避免大级别的调整。能不能看出来是大级别上有调整呢，只要出现了调整开始的信号，并且没有结束的信号就是~</p>
</li>
<li><p>此外，一定要先分清楚趋势和盘整，然后再搞清楚背驰与盘整背驰。–todo 盘整背驰和趋势背驰的区别</p>
</li>
<li><p><strong>盘整背驰里的三种情况，特别是形成第三类买点的情况</strong>，一定要搞清楚。注意，<strong>盘整背驰出来，并不一定都要大幅下跌</strong>，否则怎么会有第三类买点构成的情况。而<strong>趋势中产生的背驰，一定至少回跌到B段中</strong>，这就可以预先知道至少的跌幅。–todo1盘整背驰的三种情况 2趋势背驰的跌幅判断</p>
</li>
<li><p>对<strong>背驰的回跌力度，和级别很有关系</strong>  ，如果日线上<strong>在上涨的中段刚开始的时候，MACD刚创新高，红柱子伸长力度强劲，这时候5分钟即使出现背驰，其下跌力度显然有限，所以只能打点短差，甚至可以不管</strong> 。而在<strong>日线走势的最后阶段，特别是上涨的延伸阶段，一个1分钟的背驰足以引发暴跌</strong>，所以这一点必须多级别地综合来考察，绝对不能一看背驰就抛等跌50%，世界上哪里有这样的事情。即：<strong>在大级别的走势的不同阶段，小级别的背驰力度是不同的。一般来说，越小的级别，背驰对大级别的影响力越小，但是在大级别的走势末期，即使是小级别的背驰，也会有大的影响力。</strong></p>
</li>
<li><p>一般来说，一个标准的两个中枢的上涨，在MACD上会表现出这样的形态，就是第一段，MACD的黄白线从0轴下面上穿上来(即近期12日均值上升高于26)，在0轴上方停留的同时，形成相应的第一个中枢(短期一直高于长期，并且有一段盘整时间)，同时形成第二类买点(有过上涨后的回抽)，<strong>其后突破该中枢，MACD的黄白线也快速拉起，这往往是最有力度的一段，一切的走势延伸等等</strong>，以及MACD绕来绕去的<strong>所谓指标钝化都经常出现在这一段，这段一般在一个次级别的背驰中结束</strong>，然后进入第二个中枢的形成过程中，同时MACD的黄白线会逐步回到0轴附近，最后，开始继续突破第二个中枢，MACD的黄白线以及柱子都再次重复前面的过程，但这次，<strong>黄白线不能创新高，或者柱子的面积或者伸长的高度能不能突破新高，出现背驰</strong>，这就结束了这一个两个中枢的上涨过程。明白这个道理，大多数股票的前生后世，一早就可以知道了。todo<strong>macd指标钝化、走势延伸</strong>即，次级别的背驰，带来更高级别的走势结束。</p>
</li>
<li><p>调整是日线级别还是周还是月？</p>
</li>
<li><p>⭐️⭐️⭐️⭐️⭐️MACD在0轴附近盘整以及回抽0轴所形成的中枢，不一定就是相应级别的中枢，而是至少是该级别的中枢。例如日线MACD的0轴盘整与回拉，至少构成日线的中枢，但也可以构成周线的中枢，这时候就意味着日线出现三段走势。</p>
</li>
<li><p>股票是无须选择的，唯一值得选择的，就是波动大的股票，而这个是不能完全预测的。对于资金量小的投资者，完全可以全仓进出，游走在不同的凭证之间。这样的效率当然是最高的，不过这不适用于大资金。</p>
</li>
<li><p>一般来说，本ID只在月线、最低是周线的买点位置进去，追高是不可能的，这样会让变负数的过程变得太长，而且都是在庄家吸得差不多时进去，一般都是二类或三类买点，这样可以骗庄家打压给点货，从散户手里买东西太累，一般不在月线的第一类买点进去，这样容易自己变庄家了。</p>
</li>
</ol>
<p><br><br></p>
<p>整理：</p>
<p><br><br></p>
<p>19、</p>
<p>：消息，必有人知道，且是大资金知道，他们有实力利用消息，因此消息早已在盘面中。走势是钱堆的，那么走势是被知道消息的人、为了赚钱走出来的。只有钱是唯一值得信任的，而钱在市场上运动的轨迹，就是走势。这是唯一公开的可观察的东西。</p>
<p>：市场先生是疯子。但最终会回归到内在价值。消息不过是会让你因小失大的无用信息。真正地影响价值的只有管理者以及公司业务和未来。</p>
<p>两者的共性是都认为消息是无用的。前者希望在走势中分析出资金的意图。后者希望在公开年报以及行情研究中，分析出公司的内在价值。长期的确是后者均衡回归。而短期，走势如果的确有一定的规律和必然性在，那么也是更加反应资金情绪的最佳被观察客体。</p>
<p>第一手的、有价值的资料永远不是消息或者内幕。内幕的真假不可靠。可靠的是现象，而现象到背后的意图，之间的差距需要全分类以及评价体系去给出反馈。两者没有矛盾。只是逻辑不一样。</p>
<p>不同的逻辑，只要经得住推理，即使不同，也会获得各自</p>
<p><br><br></p>
<p>20、</p>
<p>和算法的业务指标一样。数据纷杂。</p>
<p>但是需要厘清重点和方向。</p>
<p><br><br></p>
<p>21、</p>
<p>不要有依赖心理，只有自己在实践中成为自己一部分的，才是真实的。</p>
<p>一个坏习惯足以毁掉一切，每次操作后一定要不断总结，逐步提高。</p>
<p><br><br></p>
<p>22、27课：</p>
<ol>
<li><p> 第二个中枢后就产生背驰的情况，一般占了绝大多数的情况，特别在日线以上的级别，这种就几乎达到90%以上，因此，如果一个日线以上级别的第二个中枢，就要密切注意背驰的出现。而在小级别中，例如1分钟的情况下，这种比例要小一点，但也是占大多数。一般4、5个中枢以后才出现背驰的，都相当罕见了。</p>
</li>
<li><p> 如果在第一个中枢就出现背驰，那不会是真正意义上的背驰，只能算是盘整背驰，其真正的技术含义，其实就是一个企图脱离中枢的运动，由于力度有限，被阻止而出现回到中枢里。</p>
</li>
<li><p> 一般来说，小级别的盘整背驰，意义都不太大，而且必须结合其位置，如果是高位，那风险就更大了，往往是刀口舔血的活动。但如果是低位，那意义就不同了，因为多数的第二、三类买点，其实都是由盘整背驰构成的，而第一类买点，多数由趋势的背驰构成。即：高位的小级别背驰往往是前驱信号–预警风险。为什么盘整背驰多构成第二三类买点？(第一缠上涨后的回抽、中枢上)。第一类买点是转折形成的，男上转女上，因此是趋势背驰。跌过了。</p>
</li>
<li><p> 一般来说，<strong>第二、三类的买点，都有一个三段的走势，第三段往往都破点第一段的极限位置，从而形成盘整背驰</strong>，注意，这里是把第一、三段看成两个走势类型之间的比较，这和趋势背驰里的情况有点不同，这两个走势类型是否一定是趋势，都问题不大，两个盘整在盘整背驰中也是可以比较力度的。</p>
</li>
<li><p>在某级别的某类型走势，如果构成背驰或盘整背驰，就把这段走势类型称为某级别的背驰段。</p>
</li>
<li><p>盘整背驰最有用的，就是用在大级别上，特别是至少周线级别以上的，这种盘整背驰所发现的，往往就是历史性的大底部。配合MACD，这种背驰是很容易判断的。这种例子太多，</p>
</li>
<li><p>例如000002，谁都知道该股是大牛股，但这牛股的底部，如果学了本ID的理论，是谁都可以发现的。请看该股的季线图，也就是三个月当成一个K线的图。1993年第一季度的36。7元下跌到1996年的第一季度的3。2元，构成第一段，刚好前后13季度，一个神奇数字；1996年的第一季度然后到2001年第三季度的15。99元，构成第二段，一个典型的三角形，中枢的第二段出现三角形的情况很常见，前后23季度，和21的神气数字相差不大；2001年第三季度下跌到2005年的第三季度的3。12元，前后刚好17周，神奇数字34的一半，也是一个重要的数字。第一段跌幅是33.5元，第三段是12.87元，分别与神奇数字34和13极为接近。因为13的下一个神气数字是21，加上前面说过的17，都不可能是第三段的跌幅，因此，站在这种角度，万科的2.99元附近就是铁底了。不过这种数字分析意义不大，最简单的判断还可以用MACD来，第三段跌破第一段的3.2元，但MACD明显出现标准的背弛形态：回抽0轴的黄白线再次下跌不创新低，而且柱子的面积是明显小于第1段的，一般来说，只要其中一个符合就可以是一个背弛的信号，两个都满足就更标准了。从季度图就可以看出，万科跌破3.2元就发出背弛的信号。而实际操作中，光看季度线是不可能找到精确的买点的，但对大资金，这已经足够了，因为大资金的建仓本来就是可以越跌越买，只要知道其后是一个季度级别的行情就可以了。而对于小资金来说，这太浪费时间，因此精确的买点可以继续从月线、周线、日线、甚至30分钟一直找下去，如果你的技术过关，你甚至可以现场指出，就在这1分钟，万科见到历史性大底部。因为季度线跌破3.2元后，这个背驰的成立已经是确认了，而第三段的走势，从月线、周线、日线等，可以一直分析下去，找到最精确的背驰点。</p>
</li>
<li><p>学过数学分析的，都应该对区间套定理有印象。这种从大级别往下精确找大级别买点的方法，和区间套是一个道理。以万科为例子，季度图上的第三段，在月线上，可以找到针对月线最后中枢的背驰段，而这背驰段，一定在季度线的背驰段里，而且区间比之小，把这个过程从月线延伸到周线、日线、30分钟、5分钟、1分钟，甚至是每笔成交，这区间不断缩小，在理论上，甚至可以达到这样一种情况，就是明确指出，就这一笔是万科历史底部的最后一笔成交，这成交完成意味着万科一个历史性底部的形成与时代的开始。当然，这只是最理想的情况，因为这些级别不是无限下去的，因此，理论上并不能去证明就是一个如极限一样的点状情况的出现，但用这种方法去确认一个十分精确的历史底部区间，是不难的。</p>
</li>
<li><p>推而广之，可以证明缠中说禅精确大转折点寻找程序定理：某大级别的转折点，可以通过不同级别背驰段的逐级收缩范围而确定。换言之，某大级别的转折点，先找到其背驰段，然后在次级别图里，找出相应背驰段在次级别里的背驰段，将该过程反复进行下去，直到最低级别，相应的转折点就在该级别背驰段确定的范围内。如果这个最低级别是可以达到每笔成交的，理论上，大级别的转折点，可以精确到笔的背驰上，甚至就是唯一的一笔。（不过这些其实都意义不大，1分钟的背驰段，一般就是以分钟计算的事情，对于大级别的转折点，已经足够精确了，对大资金，基本没什么用处。）</p>
</li>
<li><p>各位有时间可以参考一下，600640、000001、000006、000009、000012、600643的季度图，看看历史底部是怎么形成的。当然，只有特别老的股票才可以用季度图。而月线图的，看600663、一个标准的例子。</p>
</li>
<li><p>上面说的是背驰构成的买点，注意，<strong>第一类买点肯定是趋势背驰构成的，而盘整背驰构成的买点，在小级别中是意义不大的</strong>，所以以前也没专门当成一种买点，但在大级别里，这也构成一种类似第一类买点的买点，因为在<strong>超大级别里，往往不会形成一个明显的趋势</strong>，这也就是以前回帖曾说过的，站在<strong>最大的级别看，所有股票都只有一个中枢</strong>，因此，站在大级别里，绝大多数的股票都其实是一个盘整，<strong>这时候就要用到这因为盘整背驰而形成的类第一类买点</strong>了。这个级别，至少应该是周线以上。</p>
</li>
<li><p>类似的，在<strong>大级别里，如果不出现新低，但可以构成类似第二类买点的买点，在MACD上，显示出类似背驰时的表现</strong>，黄白线回拉0轴上下，而后一柱子面积小于前一柱子的。一个最典型的例子，就是季度图上的600685，2005年的第三季度的2.21元构成一个典型的类第二类买点。在实际操作中，2.21元的相应区间的寻找，也是按上面级别逐步往下找背驰段的方法实现。</p>
</li>
<li><p>如果按照周线级别，那不用等30年了。不过，周线找出来的，不一定是历史性大底，可能就是一个比较长线的底部。如果把这种方法用在日线上，也是可以的，但相应的可靠性就不是那么绝对了。</p>
</li>
<li><p>精通找出<strong>各级别中枢</strong>的，是幼儿圆毕业；精通分别<strong>中枢的新生、延伸、扩展</strong>的，是学前班毕业；精通分辨<strong>盘整背驰与背驰</strong>，躲过盘整背驰转化为第三类买卖点的，是小学毕业</p>
</li>
</ol>
<p><br><br></p>
<p>23、28课：</p>
<ol>
<li> 任何一个空壳公司，理论上，只要能合法地发行基金，然后用这传销得到的钱部分地投在该空壳公司的资产上，就可以在股票上赚取10倍以上的增殖</li>
<li> 年线图就是最长线的图了，因为任何一个人大概也就能经历70、80根的年K线，一个年线的第一类买点加一个年线的第一类卖点，基本就没了。把握好这两点，比任何价值投资的人都要牛了，那些人，不过是在最多是年线的买点与卖点间上下享受了一番而已。</li>
<li> 站在中国股市的现实中，这轮牛市的一个大的调整，必然会出现基金的某种程度的崩溃，上一次的牛市，让证券公司毁了不少，这一次牛市，毁的就是基金。</li>
<li>投资的第一要点就是“你手中的钱，一定是能长期稳定地留在股市的，不能有任何的借贷之类的情况”。而基金，不过是所谓合法地借贷了很多钱而已，即使是没有利息的，性质一样。一旦行情严重走坏，基金必然面临巨大的风险，一次大的赎回潮就足以让很多基金永不超生。</li>
<li> 传销，通常只有一个后果：归零。基金，至少对大多数来说，一样。这是基金一个最大、严重违反投资要点的命门：他的钱都不是他的。对于开放式基金，这点更严重，因为这种赎回是可以随时发生的。而中国的开放式基金就更可怕，中国人的行为趋同性极为可怕，国人一窝蜂去干一件事的后果是什么，大概也见过不少了，无论政治、经济、学术上，无一例外。</li>
<li>基金经理必然要以净值为标准，一个基金拿某只股票是有一定比例限制的，也就是说，基金在这点上，连庄家都不如，一旦超配，唯一的办法就是找其他基金帮忙拉一把，几家基金一起持有，其实就是联合坐庄，万一都超配了，或者一时各基金都无暇他顾，那就构成了一个很好的阻击机会。</li>
<li>短差又弄不来，又不能随时护盘砸盘，他持有的实际效果，就是让股票的盘子变小了。</li>
<li>就算不用一些非市场的手段、一些在中国肯定效果一流的桌底游戏，一次设计合理的阻击足以让这基金，轻的，吃点哑巴亏，重的，让他清盘走人。</li>
<li>如果他能熬得住，大不了就弄了一次出色的短差，等于傻大个持有的筹码人间蒸发了一段时期，投资中，唯一重要的其实就是成本，成本比傻大个低，再起来时，傻大个就更危险了，一次搞不死，还不能搞两次、三次，总有搞死的时候。一旦往下搞，基金的净值熬不住，那基金经理就可以走人了，然后，那些筹码就可以信达、东方一番了。</li>
<li>如果在一个大级别的，例如月线中枢的调整中，一个集中的攻击，打破一个点，把一个基金公司集中搞跨，所有的基金公司都将面临严重的赎回潮，然后就整个市场都可以严重地信达、东方一番了。</li>
<li>吃散户有什么意思呀，基金，就是散户打包，让人一口吃，少麻烦。</li>
<li>最近，一个小的周线中枢震荡，就足以让本ID去试验一下。一个20%都不到的回调，一个就算跌停也就5%的股票，一个基本面面临严重好转的个股，已经让某些人坐不住了。某些傻大个超配了，找人护也没人有空了，看看上周基金的净值，这种局面再维持一周，估计就有人熬不住了。当然，现在的基金还有实力，一棍子肯定打不死的，这次只是闹着玩一次，感觉不错，最次就是权当洗了一次盘，弄了一个出色的短差。本ID可没在这次就把人击倒的想法，12元不行，难道不可以20元才搞死？只要短差出来了，死的一定是没弄短差的人！</li>
<li>这个命门如何化解，如何不让这成为外国游资的重大突破目标</li>
<li>市场打开，就必然要面对各种攻击，如果管理层的智力还达不到攻击者的千分之一，那只有瞎闹的份。下一个死的，一定是基金，在一个月线级别的调整中，这一幕必然上演，现在唯一有疑问的是，不会连一个周线级别的调整，都会有好戏提前上演吧？这个可能性是不大的，如果真出现，这基金也弱了。</li>
</ol>
<p><br><br></p>
<p>24、</p>
<p>“人口消费化”与“资产虚拟化”是资本主义社会经济发展的真正秘密，而在“人口消费化”与“资产虚拟化”的爆发期，这种流动性过剩就是最正常不过的事情，而中国目前正处在该爆发期的启始阶段。</p>
<p>大国，首要的是成为资本大国，而只有成为资本大国，才有真正的大国可言。而资本大国的首要前提，就是一个巨大的资金大池子，全世界的资金都汇聚其中。</p>
<p>而在这个池子逐步形成的过程里，流动性过剩就一定是常态。</p>
<p>问题不是水太多，而是为什么池子的扩张速度如此慢</p>
<p>资本市场，从来都是资金大池子一个最重要的部分，一个超常规的资本市场扩张就是这池子扩张中必然也是当然的事情，而且是最重要的事情。</p>
<p>人民币放开以来资本市场的扩张以一种超常的速度出现，是最正常不过了。就算目前是全流通，按全部的市值算，也就GDP的一半上下，而美国这比例是多少？且不说GDP的中国速度依然惊人，按目前的速度，7、8年后又翻一翻，那么现在资本市场的扩张速度，只不过是一种补课而已，而且补得还不够好，还应该更好一点。</p>
 <br>

<p>“资产虚拟化”的大国溢价是一个最常见的现象。而资产的大国溢价，最终都会反映在资本市场之上。更重要的是，资本市场对资产的收纳范围必然急剧扩大，一切中国<strong>最优秀的资产</strong>，必然通过各种途径汇聚到中国的资本市场上，这是支持资本市场成为资金大池子最重要组成部分的最坚实基础，而这也是资本市场今后发展的最大动力。不理解这个，是无法理解目前中国资本市场发展的历史意义的。这不单单是一个量上的改变，而是一个根本性的结构改变，成为中国“资产虚拟化”历史进程的最重要标志。</p>
<p>注： 什么是最优秀资产？todo</p>
<br>

<p>25、</p>
<ol>
<li><p>市场经济 = 资本主义 = 现代经济…</p>
</li>
<li><p><strong>原始社会模式的社会经济形态</strong>，如斯大林式的资本主义经济形态。欧美式的资本主义，其原始社会形态，是<strong>以封建到资本主义原始积累前的混沌过度</strong>为形态的。</p>
</li>
<li><p>市场经济原始社会破裂后，就进入人口消费化与资产虚拟化扩展的原始积累时期。人类开始资本主义以来，所有的经济大国崛起，都离不开这种形态。注意，大国与经济大国，有着一定的区别。像前苏联这种，站在经济的角度，从来算不了大国</p>
</li>
<li><p>18、19、20世纪，欧美的经济以及其后的军事扩张，都是以这种资本主义奴隶社会形态最强悍的扩张力为其根基。但，最终所有的军事殖民都几乎以失败告终，而经济、文化上，却是无比的成功，这也可以看出经济、文化的深刻腐蚀性。经济、文化上资本主义的军事奴隶制游牧民族般的强悍，是比纯粹的军事强悍更有力、更本质的东西，这也是为什么在自相似中，美国经济、文化对世界的征服比成吉思汗的铁蹄更有力。</p>
</li>
<li><p>由于市场经济在世界范围内的不平衡，必然导致当<strong>某些国家完成市场经济奴隶社会形态时，后来的国家才刚进入这种场经济奴隶社会形态</strong>，因此，一场如同历史上游牧与农耕民族的征服与被征服游戏就不断展开。 =&gt;一切都是征服和被征服。人之间的感情只是弱的时候，会受牵扯。</p>
</li>
<li><p>其实，在思想历史上，也有同样的情况出现。<strong>思想历史上的奴隶社会阶段，是所有文化形态中最有活力的时代，这个时代，也就是所谓思想历史上的轴心时代</strong>，人类其后的所有思想，从根本上，从来没有超越那个时代。</p>
</li>
<li><p>中国的崛起也离不开这如游牧对农耕的征服游戏。当中国制造、中国因数在全球涌动时，不过是市场经济自身演化法则的现实演示而已。</p>
</li>
<li><p>只要中国继续保持这种被汉奸主子称为野蛮的经济铁蹄的快速奔驰，成吉思汗席卷天下的一幕就会在经济领域再次上演。汉奸们叫床不爽，要怪，就怪那所谓的无形的手如此地辣手摧草。</p>
</li>
<li><p>所谓无形的手，实质就是人类欲望的肆意扩展进行有计划的调控。</p>
</li>
<li><p>游牧军事的强悍就在于，内在的欲望超越了现有的条件，在内部不能消耗这种能力，因此只能向外扩张去消耗，就如同那荷尔蒙所萌动的春情在夜色中无可阻挡地挥霍。</p>
</li>
<li><p>市场经济无形的手制造的动力，如同人的欲望制造的性能力。按道家的玩法，第一种方法就是肆意欲望，采阳补阴，广采面首而成就之。第二种就是控制转化欲望，采自身的大药而成就之。中国经济欲望萌动的性能量，如何通过内在的修炼而成就，就是第二种方法需要解决的问题。</p>
</li>
<li><p>中国今后的发展，最有现实意义的无非是两条路子： </p>
</li>
</ol>
<p>一、让贪婪去扩张疆土，以前是殖民，现在是经济的占领。社会意识形态层面，让市场经济的逻辑无情地贯彻下去，让所有人的欲望无限地扩展，让整个国家的经济、政治、文化、军事等等按照市场经济的逻辑继续扩展下去，成为一个战车，捣毁一切阻隔，在最后的大决战中成就霸主地位。</p>
<p>二、把人民币升值的压力转化为让中国最贫穷的1亿家庭成为10万元户。 </p>
<hr>
<br>

<p>全是先下手为强的强势文化。</p>
<p>好文章！把成吉思汗的蒙古王国力量也联系起来！一直到资本主义奴役和主导！就是和天狗不谋而合的么！除了这么透彻、这么能够串联本质，还有最关键的，长期价值思维，价值对比，经济与文化的力量远远大于军事！人与人，国与国，企业与企业，文化与文化，哪一样不是如此！</p>
<p>市场经济，资本主义，那么多名词，所指不过一样！资本主义，的原始阶段，就是资本野蛮的积累，奴隶交易、战争横财、贵族遗产，到白手起家的商业帝国，垄断帝国，在加上一些初始的国家垄断，都不过是没有规则的混沌时期里掠财的方式。虽然都在要钱，但是原始阶段的游戏规则又不一样了。彼时是斯大林，白色红色恐怖，现在多了人道主义的限制，做法有些收敛，不过从压迫转为金融上。</p>
<p>为什么人道主义上，不加上资本和权力的公平。利益集团的撼动太难了。科技革命让商业化有了更大的规模和空间，才足以让专制破产。=&gt;我们找到很多可能的思路和理由，但是没有框架。那么现在，去中心化的革命，更大程度的自由度和解放，才能够让资本这个庞然大物崩塌。=&gt;我们找到很多可能的思路和理由，都没有数学更清晰和确定。所以无法确定。谁能够量化呢。巴菲特只不过也只是在他已经熟悉的领域，并且能够接触到接近真实的情况，在其中去找确定性，找价值而已。</p>
<p>但是你说管理层自身，比如张总，他对业务的发展前景有多少理解呢。没有宏观上对这个行业的理解。不过也是在自己的圈子里找自己的立足。没有战略。因为很难。几乎无法量化。也需要理解力。</p>
<p>我们都没有超越。差劲和量上堆积的书，只是让思考的时间固定在那个维度和层级上。而更高质量，战略性和大局的思考力，还是缺乏的。一方面我们不够现实，一方面我们太缺乏创造和大局视野。悬浮着。逃不过是个创造的奴隶。</p>
<p>我说的不是高优先级。完全不与我的现实有任何启发。我想磨镜也是。文昭是兴趣，其实也是现实。我知道我更应该做什么。</p>
<p><br><br></p>
<p>25、29课：</p>
<p>总结：首先，中枢、盘整、趋势、背驰概念是确定的。再次基础上谈论转折。转折是趋势或盘整的转换，也需要考虑其级别属性。通俗地说背驰后的反弹有多种，我们从反弹的分类中发现转折。</p>
<p>根据反弹的力度，分为最后中枢的扩展 – 没有突破最后一个中枢的最低点、更大级别的盘整 、更大级别的反趋势。</p>
<br>

<ol>
<li>在某级别的盘整中，或者说<strong>围绕某级别中枢的震荡、延续中(盘整里面)，不存在转折的问题</strong>，除非站在次级别图形中，才有转折问题的探讨。</li>
<li>对于上涨的转折，有两种情况：下跌与盘整；对于下跌的转折，也有两种情况：上涨与盘整。</li>
<li>转折是有级别的，关于<strong>转折与背驰</strong>的关系，有如下定理：缠中说禅背驰-转折定理：<strong>某级别趋势的背驰</strong>将导致该趋势<strong>最后一个中枢的级别扩展、该级别更大级别的盘整或该级别以上级别的反趋势</strong>。例如，一个<strong>5分钟背驰段的下跌</strong>，最终通过<strong>1分钟以及1分钟以下级别的精确定位，最终可以找到背驰的精确点，其后就发生反弹</strong>。</li>
<li>反弹会有一个很明确的界定，就是包括三种情况：<strong>一、该趋势最后一个中枢的级别扩展、二、该级别更大级别的盘整、三该级别以上级别的反趋势</strong>。</li>
<li>一、该趋势最后一个中枢的级别扩展</li>
<li>只触及最后一个中枢的DD=min(dn)的反弹，就是背弛后最弱的反弹，这种反弹，将把最后一个中枢变成一个级别上的扩展，例如，把5分钟的中枢扩展成30分钟甚至更大的中枢。</li>
<li>第一类买点是绝对安全的，即使是这样一种最低级别的反弹，也有足够的空间让买入获利</li>
<li>很特殊的情况，不幸碰到这种情况，在资金利用率的要求下，当然是要找机会马上退出，否则就会浪费时间了。</li>
<li>注意，这种情况和盘整背驰中转化成第三类卖点的情况不同，那种情况下，反弹的级别一定比最后一个中枢低，而这种情况，反弹的级别一定等于或大于最后一个中枢的。因此，这两种情况，不难区分。</li>
<li>二、该级别更大级别的盘整</li>
<li>三、该级别以上级别的反趋势。</li>
<li>这二种情况就是发生转折的两种情况，原理是一样的，只是相应的力度有区别。当反弹至少要重新触及最后一个中枢，这样，将发生转折，也就是出现盘整与上涨两种情况，</li>
<li>对于上面5分钟下跌的例子，就意味着，将出现5分钟级别更大的盘整(背驰扩大了盘整，因为为更大)或5分钟级别以上的上涨，两段走势类型的连接，就有两种情况出现：下跌+盘整，或者下跌+上涨。</li>
<li>注意，这里的盘整的中枢级别(30min)一定大于下跌中的中枢级别(5min)，<strong>否则就和下跌的延伸或第一种该趋势最后一个中枢的级别扩展搞混了</strong>。而上涨的中枢，不一定大于上跌中的中枢，例如，一个5分钟级别的下跌后反过来是一个5分钟级别的上涨，这是很正常的，但如果是盘整，那就至少是30分钟级别的。</li>
<li>有人总是搞不明白为什么“下跌+盘整”中盘整的中枢级别一定大于下跌中的中枢，这里不妨用一个例子说明一下：例如，还是一个5分钟的下跌，那至少有两个中枢，整个下跌，最一般的情况就是a+A+b+B+c，其中的a\b\c，其级别最多就是1分钟级别的，甚至最极端的情况，可以就是一个缺口。而A、B，由于是5分钟级别的中枢，那至少由3段1分钟的走势类型构成，如果都按1分钟级别的走势类型来计量，而且不妨假设a\b\c都是1分钟的走势类型，那么a+A+b+B+c就有9个1分钟的走势类型。</li>
<li>而一个30分钟的盘整，至少有3个5分钟的走势类型，而1个5分钟的走势类型，至少有3个1分钟的走势类型，也就是一个30分钟的盘整，就至少有9个1分钟的走势类型，这和上面a+A+b+B+c的数量是一致的。从这数量平衡的角度，就知道为什么“下跌+盘整”中盘整的级别一定比下跌的级别大了，如果级别一样，例如一个5分钟的盘整，只有3个1分钟的走势类型，那和9就差远了，也不匹配。</li>
<li>当然，“下跌+盘整”中盘整的级别一定比下跌的级别大，最主要的原因还不是这个，而是上面说到的，如果该级别一样，那只有两种情况，下跌延伸或下跌最后一个中枢扩展，和“下跌+盘整”是不搭界的</li>
<li>有人可能还有疑问，如果下跌最后一个中枢扩展，例如5分钟扩展成30分钟，那和5分钟级别下跌+30分钟级别盘整有什么区别？这区别大了，因为在“5分钟级别下跌+30分钟级别盘整”，也就是“下跌+盘整”中，下跌和盘整都是完成的走势类型，这意味着是两个走势类型的连接。而下跌最后一个中枢扩展，是一个未完成的走势类型的延续，还在一个走势类型里</li>
<li>以上三种情况，就完全分类了某级别背驰后的级别与力度，也就是某级别的第一类买点后将发生怎么样的情况，而第一类卖点的情况是一样的，只是方向相反</li>
<li>那么，怎么分别这几种情况，关键就是看反弹中第1个前趋势最后一个中枢级别的次级别走势（例如前面的下跌是5分钟级别，就看1分钟级别的第1次反弹），是否重新回抽最后一个中枢里，如果不能，那第一种情况的可能就很大了，而且也证明反弹的力度值得怀疑，当然这种判别不是绝对的，但有效性很大。</li>
<li>这次20070206的反弹，用5分钟背驰段，然后考察1分钟以及1分钟以下级别的背驰进行精确定位，可以极为精确地把握这个底部，而且在实践中，很多人按照本ID的理论都把握住了，那么，其后的反弹，第一波是1分钟走势马上回到从2980开始的5分钟下跌的最后一个中枢里，这样就意味着第一种最弱的情况可能性可以完全排除了，其后，1分钟的走势继续完成，扩展成一个5分钟的上涨</li>
<li>在20070207的11点前后，一个1分钟的背驰制造了上涨的结束，其后进入一个中枢的震荡中，这个中枢，按照本章的定理，就可以断言，至少是5分钟级别的，而实际上演化成一个30分钟级别的，这意味着，一个快速的5分钟上涨的可能就没有了，后面只有两种演化的可能，就是一个30分钟以上级别的盘整，或者是一个30分钟以上的上涨，至于哪种情况，就必须看后面走势的演化。</li>
<li>而对于实际的操作，这两种情况并没有多大的区别，例如是盘整还是上涨，关键看突破第一个中枢后是否形成第三类买点，而<span style="color:red">操作中，是在第一、二类买点先买了，然后观察第三类买点是否出现，出现就继续持有，否则就可以抛出</span></li>
<li>如果是资金量特别小，或者对本ID的理论达到小学毕业水平，那么完全可以在<strong>突破的次级别走势背驰时先出掉</strong>，<strong>然后看回试是否形成第三类买点</strong>，形成就回补，不形成就不回补，就这么简单。</li>
<li>在第一次抄底时，最好就是买那些当下位置离最后一个中枢的DD=min(dn)幅度最大的，所谓的超跌，应该以此为标准。</li>
<li>因为本章的定理保证了，反弹一定达到DD=min(dn)之上，然后在反弹的第1波次级别背驰后出掉，如果这个位置还不能达到最后一个中枢，那么这个股票可以基本不考虑，当然，这可能有例外，但可能性很小。</li>
<li>然后在<strong>反弹的第一次次级别回试后买入那些反弹能达到最后一个中枢的股票而且最好是突破该中枢的而且回试后能站稳的</strong>，根据走势必完美，一定还有一个次级别的向上走势类型，如果这走势类型出现盘整背驰，那就要出掉，如果不出现，那就要恭喜你了，你买到了一个所谓V型反转的股票，其后的力度当然不会小。</li>
</ol>
<p><br><br></p>
<p>26、30课：</p>
<p>市场价格是否完全反映所有信息，可以随意假定，无论何种假定，都和实际的交易关系不大。交易中，你唯一需要明确的，就是无论市场价格是否完全反映信息，你都必须以市场的价格交易，而你的交易将构成市场的价格，对于交易来说，除了价格，一无所有（成交量可以看成是在一个最低的时间段内按该价格重复成交了成交数量个交易单位）。这一切，和市场价格是否反映所有信息毫无关系，因为所有价格都是当下的，如果当下的信息没被市场反映，那他就是没被市场当下反映的信息，至于会不会被另一个时间的价格反映是另外的事情。站在纯交易的角度，价格只有当下，当下只有价格，除了价格与依据时间延伸出来的走势，市场的任何其他东西都是可以忽略不计的。</p>
<p>价格也和人是否理智无关，无论你是否理智，都以价格交易，而交易也被价格，这是无论任何理论都必须接受的事实：交易，只反映为价格，以某种价格某个时间的交易，这就是交易的全部。至于交易后面的任何因数，如果假定其中一种或几种决定了交易的价格，无论这种因数是基本面、心理面、技术面、政治面还是什么，都是典型的上帝式思维，都是无聊勾当。其实，对于价格来说，时间并不需要特别指出，因为价格轨迹中的前后，就意味着时间的因数，也就是说，交易是可以按时间排序的，这就是交易另一个最大的特征：交易是有时间性的，而这时间，不可逆。在物理还在探讨时间是否可逆时，对于交易空间的探讨，这最困难的时间问题，就已经有了最不可动摇的答案。而本ID的理论，当然也是以这交易时间的不可逆为前提，如果今天的交易可以变成昨天的或者干脆不算了，那本ID的理论马上土崩瓦解。</p>
<p>交易，当然是有规律的，而且这规律是万古不变的，归纳上述就是：交易以时间的不可逆为前提完全等价地反映在价格轨迹上。当然，这万古不变也有其可变之处，例如交易突然因为某种原因可以随便更改，因此，在逻辑上更严谨的说法就是，把满足该条规律的市场称为价格充分有效市场，本ID的理论，就是针对这种价格充分有效市场的，而这种市场，至少对应了目前世界上所有正式的交易市场。那么，非价格充分有效市场是否存在？当然有。例如，你昨天一亿元钱买了一石头，今天卖石头的黑帮老大拿着枪顶着你说昨天的交易不算了，钱不给了，石头也收走了，这种存在类似交易的市场当然不可能是价格充分有效的。</p>
<p>以前所有市场理论的误区都在于去探讨决定价格的交易后面的因数，交易是人类的行为，没什么可探讨的，人类就像疯子一样，其行为即使可探讨，在交易层面也变得没什么可探讨的。所有企图解释交易动机、行为的理论都是没有交易价值的，不管人类的交易有什么理由，只要交易就产生价格，就有价格的轨迹，这就足够了。站在纯交易的角度，唯一值得数学化探讨的就是这轨迹，其他的研究都是误区，对交易毫无意义。</p>
<p>那么，价格是随机的吗？这又是一个上帝式的臆测。决定论和随机论，其背后的基础都是一个永恒因数论，一个永恒模式论，也就是，价格行为被某种神秘的理论所永恒模式化。无论这种模式是决定还是随机，这种假设的荒谬性是一样。交易，只来自现实，因此，价格是被现实的交易所决定的，相应，上面的顾虑就可以扩充为：交易是现实的行为，交易以时间的不可逆为前提完全等价地反映在价格轨迹上。</p>
<p>交易的现实性是交易唯一可以依赖的基础，那么交易的现实性反映了什么，有什么可能的现实推论？首先，人的反应是需要时间的，就算是脑神经的传输，也是需要时间的；其次，社会结构的现实多层性以及个体的差异性决定了，任何的群体性交易都不具有同时性，也就是说，即使是相同原因造成的相同买卖，都不可能同时出现，必然有先后，也就是说，交易具有延异性，不会完全地趋同，这是交易能形成可分析走势的现实基础。</p>
<p>由于交易具有延异性，没有绝对的同一性，那么即使对于严格一种因数决定交易行为的系统，也依然能产生可分析的价格轨迹。任何群体性的交易行为，不会出现完全的价格同一性，也就是说，不会永远出现所有人同一时刻的同一交易。而一个完全绝对趋同交易，就等价于一个赌博，所有的买卖和买大小没任何区别，这样的系统是否存在？当然，例如一个庄家百分百把所有股票都吃了，而且任何一笔的交易都只有他一个人参与，没有任何别的人参与，这时候，其走势等价于一个买大小的赌博。而只要有人买入或还持有这股票的1股，那么这个交易就可以用本ID的理论来描述，因为，一个不完全绝对趋同的交易就产生了，本ID理论的另一个界限就在此。</p>
<p>本ID的理论只有这两个界限，只要是价格充分有效市场里的非完全绝对趋同交易，那本ID的理论就永远绝对有效，这种绝对性就如同压缩影射不动点的唯一性对完备的距离空间一样。至于有多少人学习，应用这个理论，对理论本身并没有任何实质的影响，因为，即使所有人都应用本ID的理论，由于社会结构以及个体差异，依然不会造成一个完全绝对趋同交易，这样，本ID的理论依然有效。而更重要的是，本ID的理论，并不是一个僵化的操作，都是永远建立在当下之上的。例如，一个日线级别被判断进入背弛段，由于某种当下的绝对突发事件，例如突然有人无意按错键又给日本捎去一千几百颗原子弹，使得小级别产生突发性结构破裂最终影响到大级别的结构，这时候，整个的判断，就建立在一个新的走势基础上了，而往往这时，实际的交易并没有发生，除非你运气忒好，你刚按买入，那原子弹就飞起来了。一般人，总习惯于一种目的性思维，往往忽视了走势是当下构成中的，而本ID的理论判断，同样是建筑在当下构成的判断中，这是本ID理论又一个关键的特征。关于这种理论的当下性，在以后的课程中会重点介绍，按学历，这是初中的课程。</p>
<p>而本ID的理论，最终比的是人本身，就像乾坤大挪移的第八重肯定打不过第九重的，但任何非乾坤大挪移的，肯定打不过第八重一样，有一种武功是高出其它孤峰而上的，因为起点已经大大超越了，其他那些起点就错了，又怎么能比？显然，不可能所有人都相信应用本ID的理论，因此，那些不用本理论的人，就成了本ID理论吸血的对象，现实中，这种对象不是太少，而是太多了。其次，如果有庄家、基金偷学了这种方法，这就等于乾坤大挪移比第几重了，而且对于大资金来说，至少要比散户高出两重，才可能和散户打个平手，因为资金大，没有更高的功力，怎么能挪移起来？更重要的是，级别越大，企图控制干扰所需要的能量越大，对于周线级别以后，基本就没人能完全控制了，如果真是出现个个庄家、基金争学本ID理论的情况，那么除了在小级别比功力外，功力浅的完全可以把操作级别提高来加强安全性。更重要的是，应用相同的理论，在现实中也不会有相同的结果，现实就是一个典型的非完全绝对趋同系统，就像同样的核理论，并不会导致德国和美国同时造出原子弹，同样的理论，在不同的资金规模、资金管理水平，选股策略、基本面把握、交易者性格、气质等情况下，自然地呈现不同的面貌，这就保证了同一理论交易的非完全绝对趋同。</p>
<p>对本ID的理论有一点是必须明确的，就是本ID的理论是对价格充分有效市场非完全绝对趋同交易的一个完全的数学公理化理论，唯一需要监控的就是价格充分有效市场与非完全绝对趋同交易这两个前提是否还存在，更重要的是，这归根结底是一套关系人的理论，只能不断在交易中修炼，最后比的可是功力。例如，就算是背驰这么简单的事情，就算是同一种方法，当成为群体性行为时，比的就是心态与功力，心态不好、出手早或出手迟的，就会在价格上留下痕迹，甚至当趋同性较强时，会使得级别的延伸不断出现，那就让功力深的人得到一个更好的买入或卖出价格，这些细微的差别积累下来，足以使得赢利水平天差地别。这也是为什么本ID可以把理论公开的一个深层原因，因为本ID的理论是对价格充分有效市场非完全绝对趋同交易的一个客观理论，即使公开了，也不会让这理论有任何改变，就像牛顿力学不会让万有引力改变一样，美国的原子弹爆炸了不会影响中国的原子弹按照同样的理论出现一样。至于理论可能造成的趋同交易加大，也早在本ID理论的计算中，这里比的是当下的功力。</p>
<p>无论你用什么交易方法，只要是在价格充分有效市场非完全绝对趋同交易里，你就在本ID理论的计算中；而要在本ID的理论里功力日增，就首先要成为一个顶天立地的人，这也是本ID让各位多看本ID所解释论语的原因。交易，不过是人类行为的一种，要成为成功的交易者，首先要对人类的行为穷其源，得其智慧，否则，一个糊涂蛋，什么理论都是白搭。本ID理论的基础部分，只是把现实的真相解剖出来，但这远远不够，看明白与行得通，那是两回事情。当然，看都看不明白，是不可能真的行得通的。而行，就是修行，“见、闻、学、行”，缺一不可。本ID的理论如同大道，不需要私藏着，都可以学、都可以行，但能否行到不退转的位置，是否最终还是“学如不及，犹恐失之”，那就要靠每个人自身的修行了。</p>
<p>理论，只是把现实解剖，但真正的功力，都在当下，不光要用理论的眼睛看清楚现实，更要逐步让自己和走势合一。而行的初步功力是什么？归根结底就是“恰好”，这个“恰好”是动态的，无论多少人，每个人的行为当成一个向量，所有人的行为最终构成走势的向量，而所谓的“恰好”，就是这个总向量本身。而如何才能永远和这总向量一致？就要首先把自己变成一个零向量，有也只有当一个零向量加入到任何一个向量叠加系统里，才不会影响到最终的总向量的。把自己的贪婪与恐惧去掉，让市场的走势如同自己的呼吸一般，看走势如同看自己的呼吸，慢慢就可以下单如有神了，你的交易，就是顺着市场的总向量的方向增加其力度而已，这才是真正的顺势而为。只有这样，才算初步入门，才能逐步摆脱被走势所转的可悲境地，才能让自己和走势合一，和那永远变动的总向量一致而行。至于走势分析的学习，只不过是门外的热身而已。</p>
<p>有人可能要追问，如果所有人都变成零向量，那又如何？交易市场存在的基础，就是人的贪婪与恐惧，如果所有参与交易市场的人都没有贪婪与恐惧，那市场就没了，资本主义就没了，货币就被消灭了，那时候，本ID的理论自然就不存在了。只有对这个以人的贪婪、恐惧为基础的市场进行“不相”之，才能长期有效地吸取这市场的血。本ID理论的基础部分，在人类历史上第一次把交易市场建筑在严密的公理化体系上，就是要把市场的本来面目还原，让人的贪婪、恐惧无所遁形，只有明确地知道市场当下的行为，才可能逐步化解贪婪与恐惧，把交易行为建筑在一个坚实的现实基础上，而不是贪婪、恐惧所引发的臆测上。只有智慧才可以战胜贪婪、恐惧，而当所有的贪婪与恐惧被战胜后，贪婪与恐惧所物化的资本主义社会本身，也就丧钟敲响了。</p>
]]></content>
      <categories>
        <category>金融经济</category>
      </categories>
      <tags>
        <tag>金融经济</tag>
      </tags>
  </entry>
  <entry>
    <title>读论语</title>
    <url>/2021/05/25/%E8%AF%BB%E8%AE%BA%E8%AF%AD/</url>
    <content><![CDATA[<br>

<br>



<h4 id="自己的理解"><a href="#自己的理解" class="headerlink" title="自己的理解"></a>自己的理解</h4><span id="more"></span>

<p>投资的用处相对大些。</p>
<p>有关于人性的智慧。</p>
<blockquote>
<p>子曰：“学而时习之，不亦说乎？有朋自远方来，不亦乐乎？人不知而不愠，不亦君子乎？</p>
</blockquote>
<p>之指代道，圣人之道。时有按时、当时、时常、时运、时机等解。而字，你、你的、须臾、像、并列、假设、词缀与语气等解。这里当取并列之意。</p>
<p>《说文系传统论》写道，“悦，犹说也，拭也，解脱也。若人心有郁结能解释之也”。</p>
<p>学之、时习之，即闻道、见道，并且，按时付之于行动、时常付之于行动，成习惯。如同天下遇知己。都是解脱郁结的事情。正如，“天行健，君子以自强不息”，“君子敏于行”。</p>
<p>人，有人民、别人、人才之解。这里当指所有人。知，有了解、识别、交情、感觉、主持、智慧、有学识等解。而，如同第一句，取并列之意，而不取转折。愠为，内心燥热，即心不平静、情绪郁结或起伏。</p>
<p>即人们，不了解天地之道、不知道圣人之道，且内心无所执、没什么郁结，也是个君子世界。</p>
<p>整句话是说，要么学而时习，要么不知不愠，都可以让世界更朝着君子世界发展。不知不愠也是君子境界。君子不缺朋友，四海皆有友朋。</p>
<blockquote>
<p>有子曰:“其为人也孝弟而好犯上者，鲜矣；不好犯上而好作乱者，未之有也。君子务本，本立而道生。孝弟也者，其为仁之本与!”</p>
</blockquote>
<blockquote>
<p>子曰:“巧言令色，鲜矣仁！”</p>
</blockquote>
<p>讷于言的训诫。巧、令都为擅长之意。重视“纸上谈兵”(说话)，“涂脂抹粉”(粉饰)，只有空壳，这样的人，大概率不是君子。</p>
<blockquote>
<p>曾子曰“吾日三省吾身:为人谋而不忠乎?与朋友交而不信乎?传不习乎?”</p>
</blockquote>
<p>具体所复盘的内容。因人而异，因时而异。</p>
<p>彼时重视忠信与习道。当下，几乎是没有的。背信弃义而谋得利益，并且狐朋狗友夜夜笙歌，也是大把人在。但终究无法有厚度。所谓富而不仁，所见大多如此。看人心之所向。所向财富，利益，则复盘的，就都和谋得的手段相关。</p>
<p>当下的世界，并非君子世界。也无法和而不同。当下德草，随“时”风而偃，不随君子德风。不尚德的世界，就是齐。还没到鲁。因为现在有法律，维护私人财产，维护物质，而无约束德行。</p>
<p>子曰:“道千乘之国,敬事而信，节用而爱人，使民以时。”</p>
<p>子曰：“弟子入则孝，出则弟，谨而信，泛爱众，而亲仁。行有余力，则以学文。”</p>
<p>子夏曰：“贤贤易色；事父母，能竭其力；事君，能致其身；与朋友交，言而有信。虽曰未学，吾必谓之学矣。”</p>
<p>子曰：“君子不重则不威，学则不固。主忠信，无友不如己者，过则勿惮改。”<br>曾子曰：“慎终追远，民德归厚矣。”<br>子禽问于子贡曰：“夫子至于是邦也，必闻其政，求之与，抑与之与？”子贡曰：“夫子温、良、恭、俭、让以得之。夫子之求之也，其诸异乎人之求之与？”</p>
<blockquote>
<p>子曰：“父在，观其志。父没，观其行；三年无改于父之道，可谓孝矣。”</p>
</blockquote>
<p>不讲孝。是情。对人有情，则在乎她的志趣，在乎她的行事，并尽量应和。</p>
<p>有子曰：“礼之用，和为贵。先王之道，斯为美，小大由之。有所不行，知和而和，不以礼节之，亦不可行也。”<br>有子曰：“信近于义，言可复也。恭近于礼，远耻辱也。因不失其亲，亦可宗也。”</p>
<blockquote>
<p>子曰：“君子食无求饱，居无求安，敏于事而慎于言，就有道而正焉。可谓好学也已。”</p>
</blockquote>
<p>君子，即无郁结、不执之人。有食有居即可，不求更好。而对于言行，却有要求，”精神洁癖“。要求自己更加敏捷、谨慎，对照着道，”拨乱反正“。当下也一样，好学的人，工作狂，必然对外在的种种不予重视。</p>
<p>所思所念，皆在于道。或者在于知识、科学、客观世界、真理。而也因此不会对旁人有所侵犯，也不会纠结于得到别人的利益。</p>
<p>但是资本主义就是得到别人的利益。副产物是竞争带来的技术进步。</p>
<blockquote>
<p>子贡曰：“贫而无谄，富而无骄，何如？”子曰：“可也。未若贫而乐，富而好礼者也。”子贡曰：“《诗》云：‘如切如磋，如琢如磨’，其斯之谓与？”子曰：“赐也，始可与言《诗》已矣，告诸往而知来者。”</p>
</blockquote>
<p>不被”困“，不被贫富所控。不如即使窘困而乐在其中，即使富足而能有所节制。能够出离自己的当下，而保持君子之道。固守、时习。</p>
<p>切磋琢磨，一个典故就是贾岛的推敲。若能琢磨，则必不会谄骄。而越加琢磨，反而会寻得其乐。告往知来，即为联系。赐能够看到琢磨为”不困“的本质。</p>
<blockquote>
<p>子曰：“为政以德，譬如北辰，居其所而众星共之。”</p>
</blockquote>
<p>这个德不是虚幻的。不同位置，不同处境的人，有不同的德的标准。甚至每个人都不同。关键不在于德是什么。</p>
<p>当然有最基本的。即贫富的”不困“。贫富，不止物质上。任何有对比和差距的方面，都要”不困“。在于固守与琢磨，才能真的不受困。</p>
<blockquote>
<p>子曰：“《诗》三百，一言以蔽之，曰：‘思无邪’。”</p>
</blockquote>
<p>当下都是邪。现实都是邪。要求无邪，几乎是天堂。天堂一定是淳朴挚真的。当下能无邪么。</p>
<p>对弱势的人，真的要让利的话，也是可笑。</p>
<p>子曰：“道之以政，齐之以刑，民免而无耻。道之以德，齐之以礼，有耻且格。”</p>
<p>子曰：“吾十有五而志于学，三十而立，四十而不惑，五十而知天命，六十而耳顺，七十而从心所欲，不逾矩。”</p>
<p>孟懿子问孝，子曰：“无违。”樊迟御，子告之曰：“孟孙问孝于我，我对曰‘无违’。”樊迟曰：“何谓也？”子曰：“生，事之以礼；死，葬之以礼，祭之以礼。”</p>
<p>孟武伯问孝。子曰：“父母唯其疾之忧。”</p>
<p>子游问孝，子曰：“今之孝者，是谓能养。至于犬马，皆能有养。不敬，何以别乎？”</p>
<p>子夏问孝。子曰：“色难。有事，弟子服其劳；有酒食，先生馔，曾是以为孝乎？”</p>
<p>子曰：“吾与回言终日，不违，如愚。退而省其私，亦足以发，回也不愚。”<br>子曰：“视其所以，观其所由，察其所安，人焉廋哉？人焉廋哉？”</p>
<blockquote>
<p>子曰：“温故而知新，可以为师矣。”</p>
</blockquote>
<p>为人师的，不温故不行，不知新不行。上级也一样。</p>
<p>当然老板，又是另一路径。</p>
<p>子曰：“君子不器。”<br>子贡问君子。子曰：“先行其言而后从之。”<br>子曰：“君子周而不比，小人比而不周。”<br>子曰：“学而不思则罔，思而不学则殆。”<br>子曰：“攻乎异端，斯害也已！”<br>子曰：“由，诲汝，知之乎！知之为知之，不知为不知，是知也。”<br>子张学干禄。子曰：“多闻阙疑，慎言其余，则寡尤；多见阙殆，慎行其余，则寡悔。言寡尤，行寡悔，禄在其中矣。”<br>哀公问曰：“何为则民服？”孔子对曰：“举直错诸枉，则民服；举枉错诸直，则民不服。”<br>季康子问：“使民敬、忠以劝，如之何？”子曰：“临之以庄，则敬；孝慈，则忠；举善而教不能，则劝。”<br>或谓孔子曰：“子奚不为政？”子曰：“《书》云：‘孝乎惟孝，友于兄弟，施于有政。’是亦为政，奚其为为政？”<br>子曰：“人而无信，不知其可也。大车无輗，小车无軏，其何以行之哉？”<br>子张问：“十世可知也？”子曰：“殷因于夏礼，所损益，可知也；周因于殷礼，所损益，可知也。其或继周者，虽百世，可知也。”<br>子曰：“非其鬼而祭之，谄也；见义不为，无勇也。”<br>八佾篇<br>孔子谓季氏:“八佾舞于庭，是可忍也，孰不可忍也?”<br>三家者以《雍》彻,子曰：‘相维辟公，天子穆穆’，奚取于三家之堂？”<br>子曰：“人而不仁，如礼何？人而不仁，如乐何？”<br>林放问礼之本,子曰：“大哉问！礼，与其奢也，宁俭；丧，与其易也，宁戚。”<br>子曰：“夷狄之有君，不如诸夏之亡也。”<br>季氏旅于泰山。子谓冉有曰：“女弗能救与？”对曰：“不能。”子曰：“呜呼！曾谓泰山不如林放乎？”<br>子曰：“君子无所争，必也射乎！揖让而升，下而饮。其争也君子。”<br>子夏问曰：“‘巧笑倩兮，美目盼兮，素以为绚兮’何谓也？”子曰：“绘事后素。”曰：“礼后乎？”子曰：“起予者商也，始可与言《诗》已矣。”<br>子曰：“夏礼，吾能言之，杞不足征也；殷礼吾能言之，宋不足征也。文献不足故也，足则吾能征之矣。”<br>子曰：“禘自既灌而往者，吾不欲观之矣。”<br>或问禘之说。子曰：“不知也。知其说者之于天下也，其如示诸斯乎！”指其掌。<br>祭如在，祭神如神在。子曰：“吾不与祭，如不祭。”<br>王孙贾问曰：“‘与其媚于奥，宁媚于灶’，何谓也？”子曰：“不然，获罪于天，无所祷也。”<br>子曰：“周监于二代，郁郁乎文哉！吾从周。”<br>子入太庙，每事问。或曰：“孰谓鄹人之子知礼乎？入太庙，每事问。”子闻之，曰：“是礼也。”<br>子曰：“射不主皮，为力不同科，古之道也。”<br>子贡欲去告朔之饩羊，子曰：“赐也！尔爱其羊，我爱其礼。”<br>子曰：“事君尽礼，人以为谄也。”<br>定公问：“君使臣，臣事君，如之何？”孔子对曰：“君使臣以礼，臣事君以忠。”<br>子曰：“《关雎》，乐而不淫，哀而不伤。”<br>哀公问社于宰我。宰我对曰：“夏后氏以松，殷人以柏，周人以栗，曰：使民战栗。”子闻之，曰：“成事不说，遂事不谏，既往不咎。”<br>子曰：“管仲之器小哉！”或曰：“管仲俭乎？”曰：“管氏有三归，官事不摄，焉得俭？”“然则管仲知礼乎？”曰：“邦君树塞门，管氏亦树塞门；邦君为两君之好，有反坫。管氏亦有反坫，管氏而知礼，孰不知礼？”<br>子语鲁大师乐，曰：“乐其可知也。始作，翕如也；从之，纯如也，皦如也，绎如也，以成。”<br>仪封人请见，曰：“君子之至于斯也，吾未尝不得见也。”从者见之。出曰：“二三子何患于丧乎？天下之无道也久矣，天将以夫子为木铎。”<br>子谓《韶》：“尽美矣，又尽善也。”谓《武》：“尽美矣，未尽善也。”<br>子曰：“居上不宽，为礼不敬，临丧不哀，吾何以观之哉！”<br>里仁篇<br>子曰：“里仁为美。择不处仁，焉得知？”<br>子曰：“不仁者不可以久处约，不可以长处乐。仁者安仁，知者利仁。”<br>子曰：“唯仁者能好人，能恶人。”<br>子曰：“苟志於仁矣，无恶也。”<br>子曰：“富与贵，是人之所欲也；不以其道得之，不处也。贫与贱，是人之所恶也；不以其道得之，不去也。君子去仁，恶乎成名？君子无终食之间违仁，造次必于是，颠沛必于是。”<br>子曰：“我未见好仁者，恶不仁者。好仁者，无以尚之；恶不仁者，其为仁矣，不使不仁者加乎其身。有能一日用其力于仁矣乎？我未见力不足者。盖有之矣，我未见也。”<br>子曰：“人之过也，各于其党。观过，斯知仁矣。”</p>
<blockquote>
<p>子曰：“朝闻道，夕死可矣。”</p>
</blockquote>
<p>闻道，并固守，时习之。</p>
<p>子曰：“士志于道，而耻恶衣恶食者，未足与议也。”<br>子曰：“君子之于天下也，无适也，无莫也，义之与比。”<br>子曰：“君子怀德，小人怀土；君子怀刑，小人怀惠。”<br>子曰：“放于利而行，多怨。”<br>子曰：“能以礼让为国乎？何有？不能以礼让为国，如礼何？”<br>子曰：“不患无位，患所以立。不患莫己知，求为可知也。”<br>子曰：“参乎！吾道一以贯之。”曾子曰：“唯。”子出，门人问曰：“何谓也？”曾子曰：“夫子之道，忠恕而已矣。”<br>子曰：“君子喻于义，小人喻于利。”<br>子曰：“见贤思齐焉，见不贤而内自省也。”<br>子曰：“事父母几谏，见志不从，又敬不违，劳而不怨。”<br>子曰：“父母在，不远游，游必有方。”<br>子曰：“三年无改于父之道，可谓孝矣。<br>子曰：“父母之年，不可不知也。一则以喜，一则以惧。<br>子曰．“古者言之不出，耻躬之不逮也。<br>子曰：“以约失之者鲜矣。</p>
<p>子曰：“德不孤，必有邻。”<br>子游曰：“事君数，斯辱矣；朋友数，斯疏矣。<br>公冶长篇<br>子谓公冶长：“可妻也，虽在缧绁之中，非其罪也！”以其子妻之。<br>子谓南容：“邦有道不废；邦无道免于刑戮。”以其兄之子妻之。<br>子谓子贱：“君子哉若人！鲁无君子者，斯焉取斯？”<br>子贡问曰：“赐也何如？”子曰：“女，器也。”曰：“何器也？”曰：“瑚琏也。”<br>或曰：“雍也仁而不佞。”子曰：“焉用佞？御人以口给，屡憎于人。不知其仁，焉用佞？”<br>子使漆雕开仕，对曰：“吾斯之未能信。”子说。<br>子曰：“道不行，乘桴浮于海，从我者其由与？”子路闻之喜，子曰：“由也好勇过我，无所取材。”<br>孟武伯问：“子路仁乎？”子曰：“不知也。”又问，子曰：“由也，千乘之国，可使治其赋也，不知其仁也。”“求也何如？”子曰：“求也，千室之邑、百乘之家，可使为之宰也，不知其仁也。”“赤也何如？”子曰：“赤也，束带立于朝，可使与宾客言也，不知其仁也。”<br>子谓子贡曰：“女与回也孰愈？”对曰：“赐也何敢望回？回也闻一以知十，赐也闻一以知二。”子曰：“弗如也，吾与女弗如也！”<br>宰予昼寝，子曰：“朽木不可雕也，粪土之墙不可杇也，于予与何诛？”子曰：“始吾于人也，听其言而信其行；今吾于人也，听其言而观其行。于予与改是。”<br>子曰：“吾未见刚者。”或对曰：“申枨。”子曰：“枨也欲，焉得刚。”<br>子贡曰：“我不欲人之加诸我也，吾亦欲无加诸人。”子曰：“赐也，非尔所及也。”<br>子贡曰：“夫子之文章，可得而闻也；夫子之言性与天道，不可得而闻也。”<br>子路有闻，未之能行，唯恐有闻。<br>子贡问曰：“孔文子何以谓之‘文’也？”子曰：“敏而好学，不耻下问，是以谓之‘文’也。”<br>子谓子产：“有君子之道四焉：其行己也恭，其事上也敬，其养民也惠，其使民也义。”<br>子曰：“晏平仲善与人交，久而敬之。”<br>子曰：“臧文仲居蔡，山节藻棁，何如其知也？”<br>子张问曰：“令尹子文三仕为令尹，无喜色，三已之无愠色，旧令尹之政必以告新令尹，何如？”子曰：“忠矣。”曰：“仁矣乎？”曰：“未知，焉得仁？”“崔子弑齐君，陈文子有马十乘，弃而违之。至于他邦，则曰：‘犹吾大夫崔子也。’违之。之一邦，则又曰：‘犹吾大夫崔子也。’违之，何如？”子曰：“清矣。”曰：“仁矣乎？”曰：“未知，焉得仁？”<br>季文子三思而后行，子闻之曰：“再斯可矣。”<br>子曰：“宁武子，邦有道，则知；邦无道，则愚。其知可及也，其愚不可及也。”<br>子在陈，曰：“归与！归与！吾党之小子狂简，斐然成章，不知所以裁之。”<br>子曰：“伯夷、叔齐不念旧恶，怨是用希。”<br>子曰：“孰谓微生高直？或乞醯焉，乞诸其邻而与之。”<br>子曰：“巧言、令色、足恭，左丘明耻之，丘亦耻之。匿怨而友其人，左丘明耻之，丘亦耻之。”<br>颜渊、季路侍，子曰：“盍各言尔志？”子路曰：“愿车马、衣轻裘与朋友共，敝之而无憾。”颜渊曰：“愿无伐善，无施劳。”子路曰：“愿闻子之志。”子曰：“老者安之，朋友信之，少者怀之。”<br>子曰：“已矣乎！吾未见能见其过而内自讼者也。”<br>子曰：“十室之邑，必有忠信如丘者焉，不如丘之好学也。”<br>雍也篇<br>子曰：“雍也可使南面。”<br>仲弓问子桑伯子，子曰：“可也简。”仲弓曰：“居敬而行简，以临其民，不亦可乎？居简而行简，无乃大简乎？”子曰：“雍之言然。”<br>哀公问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不迁怒，不贰过，不幸短命死矣，今也则亡，未闻好学者也。”<br>子华使于齐，冉子为其母请粟，子曰：“与之釜。”请益，曰：“与之庾。”冉子与之粟五秉。子曰：“赤之适齐也，乘肥马，衣轻裘。吾闻之也，君子周急不继富。”<br>原思为之宰，与之粟九百，辞。子曰：“毋以与尔邻里乡党乎！”<br>子谓仲弓曰：“犁牛之子骍且角，虽欲勿用，山川其舍诸？”<br>子曰：“回也，其心三月不违仁，其余则日月至焉而已矣。”<br>季康子问：“仲由可使从政也与？”子曰：“由也果，于从政乎何有？”曰：“赐也可使从政也与？”曰：“赐也达，于从政乎何有？”曰：“求也可使从政也与？”曰：“求也艺，于从政乎何有？”</p>
<p>季氏使闵子骞为费宰，闵子骞曰：“善为我辞焉。如有复我者，则吾必在汶上矣。”</p>
<p>伯牛有疾，子问之，自牖执其手，曰：“亡之，命矣夫！斯人也而有斯疾也！斯人也而有斯疾也！”<br>子曰：“贤哉回也！一箪食，一瓢饮，在陋巷，人不堪其忧，回也不改其乐。贤哉，回也！”<br>冉求曰：“非不说子之道，力不足也。”子曰：“力不足者，中道而废，今女画。”<br>子谓子夏曰：“女为君子儒，毋为小人儒。”<br>子游为武城宰，子曰：“女得人焉尔乎？”曰：“有澹台灭明者，行不由径，非公事，未尝至于偃之室也。”<br>子曰：“孟之反不伐，奔而殿，将入门，策其马曰：‘非敢后也，马不进也。’”<br>子曰：“不有祝鮀之佞，而有宋朝之美，难乎免于今之世矣。”<br>子曰：“谁能出不由户？何莫由斯道也？”<br>子曰：“质胜文则野，文胜质则史。文质彬彬，然后君子。”<br>子曰：“人之生也直，罔之生也幸而免。”<br>子曰：“知之者不如好之者；好之者不如乐之者。”<br>子曰：“中人以上，可以语上也；中人以下，不可以语上也。”<br>樊迟问知，子曰：“务民之义，敬鬼神而远之，可谓知矣。”问仁，曰：“仁者先难而后获，可谓仁矣。”<br>子曰：“知者乐水，仁者乐山。知者动，仁者静。知者乐，仁者寿。”<br>子曰：“齐一变至于鲁，鲁一变至于道。”<br>子曰：“觚不觚，觚哉！觚哉！”<br>宰我问曰：“仁者，虽告之曰：‘井有仁焉。’其从之也？”子曰：“何为其然也？君子可逝也，不可陷也；可欺也，不可罔也。”<br>子曰：“君子博学于文，约之以礼，亦可以弗畔矣夫。”<br>子见南子，子路不说，夫子矢之曰：“予所否者，天厌之！天厌之！”<br>子曰：“中庸之为德也，其至矣乎！民鲜久矣。”<br>子贡曰：“如有博施于民而能济众，何如？可谓仁乎？”子曰：“何事于仁，必也圣乎！尧、舜其犹病诸！夫仁者，己欲立而立人，己欲达而达人。能近取譬，可谓仁之方也已。”<br>述而篇<br>子曰：“述而不作，信而好古，窃比于我老彭。”<br>子曰：“默而识之，学而不厌，诲人不倦，何有于我哉？”<br>子曰：“德之不修，学之不讲，闻义不能徙，不善不能改，是吾忧也。”<br>子之燕居，申申如也，夭夭如也。<br>子曰：“甚矣，吾衰也！久矣，吾不复梦见周公。”<br>子曰：“志于道，据于德，依于仁，游于艺。”<br>子曰：“自行束脩以上，吾未尝无诲焉。”<br>子曰：“不愤不启，不悱不发，举一隅不以三隅反，则不复也。”<br>子食于有丧者之侧，未尝饱也。<br>子于是日哭，则不歌。<br>子谓颜渊曰：“用之则行，舍之则藏，惟我与尔有是夫！”子路曰：“子行三军，则谁与？”子曰：“暴虎冯河，死而无悔者，吾不与也。必也临事而惧，好谋而成者也。”<br>子曰：“富而可求也，虽执鞭之士，吾亦为之。如不可求，从吾所好。”<br>子之所慎：齐，战，疾。<br>子在齐闻《韶》，三月不知肉味，曰：“不图为乐之至于斯也。”<br>冉有曰：“夫子为卫君乎？”子贡曰：“诺，吾将问之。”入，曰：“伯夷、叔齐何人也？”曰：“古之贤人也。”曰：“怨乎？”曰：“求仁而得仁，又何怨？”出，曰：“夫子不为也。”<br>子曰：“饭疏食饮水，曲肱而枕之，乐亦在其中矣。不义而富且贵，于我如浮云。”<br>子曰：“加我数年，五十以学《易》，可以无大过矣。”<br>子所雅言，《诗》、《书》、执礼，皆雅言也。<br>叶公问孔子于子路，子路不对。子曰：“女奚不曰：其为人也，发愤忘食，乐以忘忧，不知老之将至云尔。”<br>子曰：“我非生而知之者，好古，敏以求之者也。”<br>子不语：怪、力、乱、神。<br>子曰：“三人行，必有我师焉。择其善者而从之，其不善者而改之。”<br>子曰：“天生德于予，桓魋其如予何？”<br>子曰：“二三子以我为隐乎？吾无隐乎尔！吾无行而不与二三子者，是丘也。”<br>子以四教：文，行，忠，信。<br>子曰：“圣人，吾不得而见之矣；得见君子者，斯可矣。”子曰：“善人，吾不得而见之矣，得见有恒者斯可矣。亡而为有，虚而为盈，约而为泰，难乎有恒乎。”<br>子钓而不纲，弋不射宿。<br>子曰：“盖有不知而作之者，我无是也。多闻，择其善者而从之；多见而识之，知之次也。”<br>互乡难与言，童子见，门人惑。子曰：“与其进也，不与其退也，唯何甚？人洁己以进，与其洁也，不保其往也。”<br>子曰：“仁远乎哉？我欲仁，斯仁至矣。”<br>陈司败问：“昭公知礼乎？”孔子曰：“知礼。”孔子退，揖巫马期而进之，曰：“吾闻君子不党，君子亦党乎？君取于吴，为同姓，谓之吴孟子。君而知礼，孰不知礼？”巫马期以告，子曰：“丘也幸，苟有过，人必知之。”<br>子与人歌而善，必使反之，而后和之。<br>子曰：“文，莫吾犹人也。躬行君子，则吾未之有得。”<br>子曰：“若圣与仁，则吾岂敢？抑为之不厌，诲人不倦，则可谓云尔已矣。”公西华曰：“正唯弟子不能学也。”<br>子疾病，子路请祷。子曰：“有诸？”子路对曰：“有之。《诔》曰：‘祷尔于上下神祇。’”子曰：“丘之祷久矣。”<br>子曰：“奢则不孙，俭则固。与其不孙也，宁固。”<br>子曰：“君子坦荡荡，小人长戚戚。”<br>子温而厉，威而不猛，恭而安。<br>泰伯篇<br>子曰：“泰伯，其可谓至德也已矣。三以天下让，民无得而称焉。”<br>子曰：“恭而无礼则劳；慎而无礼则葸；勇而无礼则乱；直而无礼则绞。君子笃于亲，则民兴于仁；故旧不遗，则民不偷。”<br>曾子有疾，召门弟子曰：“启予足，启予手。《诗》云：‘战战兢兢，如临深渊，如履薄冰。’而今而后，吾知免夫，小子！”<br>曾子有疾，孟敬子问之。曾子言曰：“鸟之将死，其鸣也哀；人之将死，其言也善。君子所贵乎道者三：动容貌，斯远暴慢矣；正颜色，斯近信矣；出辞气，斯远鄙倍矣。笾豆之事，则有司存。”<br>曾子曰：“以能问于不能；以多问于寡；有若无，实若虚，犯而不校。昔者吾友尝从事于斯矣。”<br>曾子曰：“可以托六尺之孤，可以寄百里之命，临大节而不可夺也。君子人与？君子人也。”<br>曾子曰：“士不可以不弘毅，任重而道远。仁以为己任，不亦重乎？死而后已，不亦远乎？”<br>子曰：“兴于《诗》，立于礼，成于乐。”<br>子曰：“民可使由之，不可使知之。”<br>子曰：“好勇疾贫，乱也。人而不仁，疾之已甚，乱也。”<br>子曰：“如有周公之才之美，使骄且吝，其余不足观也已。”<br>子曰：“三年学，不至于谷，不易得也。”<br>子曰：“笃信好学，守死善道。危邦不入，乱邦不居。天下有道则见，无道则隐。邦有道，贫且贱焉，耻也；邦无道，富且贵焉，耻也。”<br>子曰：“不在其位，不谋其政。”<br>子曰：“师挚之始，《关雎》之乱，洋洋乎盈耳哉！”<br>子曰：“狂而不直，侗而不愿，悾悾而不信，吾不知之矣。”<br>子曰：“学如不及，犹恐失之。”<br>子曰：“巍巍乎！舜、禹之有天下也而不与焉。”<br>子曰：“大哉尧之为君也！巍巍乎，唯天为大，唯尧则之。荡荡乎，民无能名焉。巍巍乎其有成功也，焕乎其有文章！”<br>舜有臣五人而天下治。武王曰：“予有乱臣十人。”孔子曰：“才难，不其然乎？唐虞之际，于斯为盛；有妇人焉，九人而已。三分天下有其二，以服事殷。周之德，其可谓至德也已矣。”<br>子曰：“禹，吾无间然矣。菲饮食，而致孝乎鬼神；恶衣服，而致美乎黻冕；卑宫室，而尽力乎沟洫。禹，吾无间然矣！”<br>子罕篇<br>子罕言利与命与仁。<br>达巷党人曰：“大哉孔子！博学而无所成名。”子闻之，谓门弟子曰：“吾何执？执御乎，执射乎？吾执御矣。”<br>子曰：“麻冕，礼也；今也纯，俭，吾从众。拜下，礼也；今拜乎上，泰也；虽违众，吾从下。”<br>子绝四：毋意、毋必、毋固、毋我。<br>子畏于匡，曰：“文王既没，文不在兹乎？天之将丧斯文也，后死者不得与于斯文也；天之未丧斯文也，匡人其如予何？”<br>太宰问于子贡曰：“夫子圣者与，何其多能也？”子贡曰：“固天纵之将圣，又多能也。”子闻之，曰：“太宰知我乎？吾少也贱，故多能鄙事。君子多乎哉？不多也。”<br>牢曰：“子云：‘吾不试，故艺。’”<br>子曰：“吾有知乎哉？无知也。有鄙夫问于我，空空如也。我叩其两端而竭焉。”<br>子曰：“凤鸟不至，河不出图，吾已矣夫！”<br>子见齐衰者、冕衣裳者与瞽者，见之，虽少，必作，过之必趋。<br>颜渊喟然叹曰：“仰之弥高，钻之弥坚。瞻之在前，忽焉在后。夫子循循然善诱人，博我以文，约我以礼，欲罢不能。既竭吾才，如有所立卓尔，虽欲从之，末由也已。”<br>子疾病，子路使门人为臣。病间，曰：“久矣哉，由之行诈也！无臣而为有臣，吾谁欺？欺天乎？且予与其死于臣之手也，无宁死于二三子之手乎！且予纵不得大葬，予死于道路乎？”<br>子贡曰：“有美玉于斯，韫椟而藏诸？求善贾而沽诸？”子曰：“沽之哉，沽之哉！我待贾者也。”<br>子欲居九夷。或曰：“陋，如之何？”子曰：“君子居之，何陋之有！”<br>子曰：“吾自卫反鲁，然后乐正，《雅》、《颂》各得其所。”<br>子曰：“出则事公卿，入则事父兄，丧事不敢不勉，不为酒困，何有于我哉？”<br>子在川上曰：“逝者如斯夫！不舍昼夜。”<br>子曰：“吾未见好德如好色者也。”<br>子曰：“譬如为山，未成一篑，止，吾止也；譬如平地，虽覆一篑，进，吾往也。”<br>子曰：“语之而不惰者，其回也与！”<br>子谓颜渊，曰：“惜乎！吾见其进也，未见其止也。”<br>子曰：“苗而不秀者有矣夫，秀而不实者有矣夫。”<br>子曰：“后生可畏，焉知来者之不如今也？四十、五十而无闻焉，斯亦不足畏也已。”<br>子曰：“法语之言，能无从乎？改之为贵。巽与之言，能无说乎？绎之为贵。说而不绎，从而不改，吾末如之何也已矣。”<br>子曰：“主忠信。毋友不如己者，过，则勿惮改。”<br>子曰：“三军可夺帅也，匹夫不可夺志也。”<br>子曰：“衣敝缊袍，与衣狐貉者立而不耻者，其由也与！‘不忮不求，何用不臧？’”子路终身诵之，子曰：“是道也，何足以臧？”<br>子曰：“岁寒，然后知松柏之后凋也。”<br>子曰：“知者不惑，仁者不忧，勇者不惧。”<br>子曰：“可与共学，未可与适道；可与适道，未可与立；可与立，未可与权。”<br>“唐棣之华，偏其反而。岂不尔思？室是远尔。”子曰：“未之思也，夫何远之有。”<br>乡党篇<br>孔子于乡党，恂恂如也，似不能言者；其在宗庙朝廷，便便言，唯谨尔。<br>朝，与下大夫言，侃侃如也；与上大夫言，訚訚如也。君在，踧踖如也，与与如也。<br>君召使摈，色勃如也，足躩如也。揖所与立，左右手，衣前后襜如也。趋进，翼如也。宾退，必复命曰：“宾不顾矣。”<br>入公门，鞠躬如也，如不容。立不中门，行不履阈。过位，色勃如也，足躩如也，其言似不足者。摄齐升堂，鞠躬如也，屏气似不息者。出，降一等，逞颜色，怡怡如也；没阶，趋进，翼如也；复其位，踧踖如也。<br>执圭，鞠躬如也，如不胜。上如揖，下如授。勃如战色，足蹜蹜如有循。享礼，有容色。私觌，愉愉如也。<br>君子不以绀緅饰，红紫不以为亵服。当暑,袗絺绤，必表而出之。缁衣羔裘，素衣麑裘，黄衣狐裘。亵裘长，短右袂。必有寝衣，长一身有半。狐貉之厚以居。去丧，无所不佩。非帷裳，必杀之。羔裘玄冠不以吊。吉月，必朝服而朝。<br>齐，必有明衣，布。齐必变食，居必迁坐。<br>食不厌精，脍不厌细。食饐而餲，鱼馁而肉败，不食；色恶，不食；臭恶，不食；失饪，不食；不时，不食；割不正，不食；不得其酱，不食。肉虽多，不使胜食气。唯酒无量，不及乱。沽酒市脯，不食。不撤姜食，不多食。<br>祭于公，不宿肉。祭肉不出三日，出三日不食之矣。<br>食不语，寝不言。<br>虽疏食菜羹，瓜祭，必齐如也。<br>席不正，不坐。<br>乡人饮酒，杖者出，斯出矣。<br>乡人傩，朝服而立于阼阶。<br>问人于他邦，再拜而送之。<br>康子馈药，拜而受之。曰：“丘未达，不敢尝。”<br>厩焚，子退朝，曰：“伤人乎？”不问马。<br>君赐食，必正席先尝之。君赐腥，必熟而荐之。君赐生，必畜之。侍食于君，君祭，先饭。<br>疾，君视之，东首，加朝服，拖绅。<br>君命召，不俟驾行矣。<br>入太庙，每事问。<br>朋友死，无所归，曰：“于我殡。”<br>朋友之馈，虽车马，非祭肉，不拜。<br>寝不尸，居不容。<br>见齐衰者，虽狎，必变。见冕者与瞽者，虽亵，必以貌。凶服者式之，式负版者。有盛馔，必变色而作。迅雷风烈，必变。<br>升车，必正立，执绥。车中不内顾，不疾言，不亲指。<br>色斯举矣，翔而后集。曰：“山梁雌雉，时哉时哉！”子路共之，三嗅而作。<br>先进篇<br>子曰：“先进于礼乐，野人也；后进于礼乐，君子也。如用之，则吾从先进。”<br>子曰：“从我于陈、蔡者，皆不及门也。”<br>德行：颜渊，闵子骞，冉伯牛，仲弓。言语：宰我，子贡。政事：冉有，季路。文学：子游，子夏。<br>子曰：“回也非助我者也，于吾言无所不说。”<br>子曰：“孝哉闵子骞！人不间于其父母昆弟之言。”<br>南容三复白圭，孔子以其兄之子妻之。<br>季康子问：“弟子孰为好学？”孔子对曰：“有颜回者好学，不幸短命死矣，今也则亡。”<br>颜渊死，颜路请子之车以为之椁。子曰：“才不才，亦各言其子也。鲤也死，有棺而无椁，吾不徒行以为之椁。以吾从大夫之后，不可徒行也。”<br>颜渊死，子曰：“噫！天丧予！天丧予！”<br>颜渊死，子哭之恸，从者曰：“子恸矣！”曰：“有恸乎？非夫人之为恸而谁为？”<br>颜渊死，门人欲厚葬之，子曰：“不可。”门人厚葬之，子曰：“回也视予犹父也，予不得视犹子也。非我也，夫二三子也！”<br>季路问事鬼神，子曰：“未能事人，焉能事鬼？”,曰：“敢问死。”曰：“未知生，焉知死？”<br>闵子侍侧，訚訚如也；子路，行行如也；冉有、子贡，侃侃如也。子乐。“若由也，不得其死然。”<br>鲁人为长府，闵子骞曰：“仍旧贯如之何？何必改作？”子曰：“夫人不言，言必有中。”<br>子曰：“由之瑟，奚为于丘之门？”门人不敬子路，子曰：“由也升堂矣，未入于室也。”<br>子贡问：“师与商也孰贤？”子曰：“师也过，商也不及。”曰：“然则师愈与？”子曰：“过犹不及。”<br>季氏富于周公，而求也为之聚敛而附益之。子曰：“非吾徒也，小子鸣鼓而攻之可也。”<br>柴也愚，参也鲁，师也辟，由也喭。<br>子曰：“回也其庶乎，屡空。赐不受命而货殖焉，亿则屡中。”<br>子张问善人之道，子曰：“不践迹，亦不入于室。”<br>子曰：“论笃是与，君子者乎，色庄者乎？”<br>子路问：“闻斯行诸？”子曰：“有父兄在，如之何其闻斯行之？”冉有问：“闻斯行诸？”子曰：“闻斯行之。”公西华曰：“由也问：“闻斯行诸？”子曰：‘有父兄在’；求也问：‘闻斯行诸’。子曰‘闻斯行之’。赤也惑，敢问。”子曰：“求也退，故进之；由也兼人，故退之。”<br>子畏于匡，颜渊后。子曰：“吾以女为死矣！”曰：“子在，回何敢死！”<br>季子然问：“仲由、冉求可谓大臣与？”子曰：“吾以子为异之问，曾由与求之问。所谓大臣者，以道事君，不可则止。今由与求也，可谓具臣矣。”曰：“然则从之者与？”子曰：“弑父与君，亦不从也。”<br>子路使子羔为费宰，子曰：“贼夫人之子。”子路曰：“有民人焉，有社稷焉，何必读书然后为学。”子曰：“是故恶夫佞者。”<br>子路、曾皙、冉有、公西华侍坐，子曰：“以吾一日长乎尔，毋吾以也。居则曰‘不吾知也’如或知尔，则何以哉？”子路率尔而对曰：“千乘之国，摄乎大国之间，加之以师旅，因之以饥馑，由也为之，比及三年，可使有勇，且知方也。”夫子哂之。“求，尔何如？”对曰：“方六七十，如五六十，求也为之，比及三年，可使足民。如其礼乐，以俟君子。”“赤！尔何如？”对曰：“非曰能之，愿学焉。宗庙之事，如会同，端章甫，愿为小相焉。”“点，尔何如？”鼓瑟希，铿尔，舍瑟而作，对曰：“异乎三子者之撰。”子曰：“何伤乎？亦各言其志也。”曰：“暮春者，春服既成，冠者五六人，童子六七人，浴乎沂，风乎舞雩，咏而归。”夫子喟然叹曰：“吾与点也！”三子者出，曾皙后。曾皙曰：“夫三子者之言何如？”子曰：“亦各言其志也已矣。”曰：“夫子何哂由也？”曰：“为国以礼，其言不让，是故哂之。”“唯求则非邦也与？”“安见方六七十、如五六十而非邦也者？”“唯赤则非邦也与？”“宗庙会同，非诸侯而何？赤也为之小，孰能为之大？”<br>颜渊篇<br>颜渊问仁，子曰：“克己复礼为仁。一日克己复礼，天下归仁焉。为仁由己，而由人乎哉？”颜渊曰：“请问其目？”子曰：“非礼勿视，非礼勿听，非礼勿言，非礼勿动。”颜渊曰：“回虽不敏，请事斯语矣。”<br>仲弓问仁，子曰：“出门如见大宾，使民如承大祭。己所不欲，勿施于人。在邦无怨，在家无怨。”仲弓曰：“雍虽不敏，请事斯语矣。”<br>司马牛问仁，子曰：“仁者，其言也讱。”曰：“其言也讱，斯谓之仁已乎？”子曰：“为之难，言之得无讱乎？”<br>司马牛问君子，子曰：“君子不忧不惧。”曰：“不忧不惧，斯谓之君子已乎？”子曰：“内省不疚，夫何忧何惧？”<br>司马牛忧曰：“人皆有兄弟，我独亡。”子夏曰：“商闻之矣：死生有命，富贵在天。君子敬而无失，与人恭而有礼，四海之内皆兄弟也。君子何患乎无兄弟也？”<br>子张问明，子曰：“浸润之谮，肤受之愬，不行焉，可谓明也已矣；浸润之谮、肤受之愬不行焉，可谓远也已矣。”<br>子贡问政，子曰：“足食，足兵，民信之矣。”子贡曰：“必不得已而去，于斯三者何先？”曰：“去兵。”子贡曰：“必不得已而去，于斯二者何先？”曰：“去食。自古皆有死，民无信不立。”<br>棘子成曰：“君子质而已矣，何以文为？”子贡曰：“惜乎，夫子之说君子也！驷不及舌。文犹质也，质犹文也。虎豹之鞟犹犬羊之鞟。”<br>哀公问于有若曰：“年饥，用不足，如之何？”有若对曰：“盍彻乎？”曰：“二，吾犹不足，如之何其彻也？”对曰：“百姓足，君孰与不足？百姓不足，君孰与足？”<br>子张问崇德、辨惑，子曰：“主忠信，徙义，崇德也。爱之欲其生，恶之欲其死；既欲其生又欲其死，是惑也。‘诚不以富，亦只以异。’”<br>齐景公问政于孔子，孔子对曰：“君君，臣臣，父父，子子。”公曰：“善哉！信如君不君、臣不臣、父不父、子不子，虽有粟，吾得而食诸？”<br>子曰：“片言可以折狱者，其由也与？”子路无宿诺。<br>子曰：“听讼，吾犹人也。必也使无讼乎。”<br>子张问政，子曰：“居之无倦，行之以忠。”<br>子曰：“博学于文，约之以礼，亦可以弗畔矣夫。”<br>子曰：“君子成人之美，不成人之恶；小人反是。”<br>季康子问政于孔子，孔子对曰：“政者，正也。子帅以正，孰敢不正？”<br>季康子患盗，问于孔子。孔子对曰：“苟子之不欲，虽赏之不窃。”<br>季康子问政于孔子曰：“如杀无道以就有道，何如？”孔子对曰：“子为政，焉用杀？子欲善而民善矣。君子之德风，小人之德草，草上之风必偃。”<br>子张问：“士何如斯可谓之达矣？”子曰：“何哉尔所谓达者？”子张对曰：“在邦必闻，在家必闻。”子曰：“是闻也，非达也。夫达也者，质直而好义，察言而观色，虑以下人。在邦必达，在家必达。夫闻也者，色取仁而行违，居之不疑。在邦必闻，在家必闻。”<br>樊迟从游于舞雩之下，曰：“敢问崇德、修慝、辨惑。”子曰：“善哉问！先事后得，非崇德与？攻其恶，无攻人之恶，非修慝与？一朝之忿，忘其身，以及其亲，非惑与？”<br>樊迟问仁，子曰：“爱人。”问知，子曰：“知人。”樊迟未达，子曰：“举直错诸枉，能使枉者直。”樊迟退，见子夏，曰：“乡也吾见于夫子而问知，子曰：‘举直错诸枉，能使枉者直’，何谓也？”子夏曰：“富哉言乎！舜有天下，选于众，举皋陶，不仁者远矣。汤有天下，选于众，举伊尹，不仁者远矣。”<br>子贡问友，子曰：“忠告而善道之，不可则止，毋自辱焉。”<br>曾子曰：“君子以文会友，以友辅仁。”<br>子路篇<br>子路问政，子曰：“先之，劳之。”请益，曰：“无倦。”<br>仲弓为季氏宰，问政，子曰：“先有司，赦小过，举贤才。”曰：“焉知贤才而举之？”曰：“举尔所知。尔所不知，人其舍诸？”<br>子路曰：“卫君待子而为政，子将奚先？”子曰：“必也正名乎！”子路曰：“有是哉，子之迂也！奚其正？”子曰：“野哉由也！君子于其所不知，盖阙如也。名不正，则言不顺；言不顺，则事不成；事不成，则礼乐不兴；礼乐不兴，则刑罚不中；刑罚不中，则民无所错手足。故君子名之必可言也，言之必可行也。君子于其言，无所苟而已矣。”<br>樊迟请学稼，子曰：“吾不如老农。”请学为圃，曰：“吾不如老圃。”樊迟出。子曰：’小人哉，樊须也！上好礼，则民莫敢不敬；上好义，则民莫敢不服；上好信，则民莫敢不用情。夫如是，则四方之民襁负其子而至矣，焉用稼？”<br>子曰：“诵《诗》三百，授之以政，不达；使于四方，不能专对；虽多，亦奚以为？”<br>子曰：“其身正，不令而行；其身不正，虽令不从。”<br>子曰：“鲁卫之政，兄弟也。”<br>子谓卫公子荆，“善居室。始有，曰：‘苟合矣。’少有，曰：‘苟完矣。’富有，曰：‘苟美矣。’”<br>子适卫，冉有仆，子曰：“庶矣哉！”冉有曰：“既庶矣，又何加焉？”曰：“富之。”曰：“既富矣，又何加焉？”曰：“教之。”<br>子曰：“苟有用我者，期月而已可也，三年有成。”</p>
<blockquote>
<p>子曰：“‘善人为邦百年，亦可以胜残去杀矣。’诚哉是言也！”</p>
</blockquote>
<p>胜残去杀，残有为害的人、剩余、残破之意，杀有战斗、衰败、程度深、削除之意。胜为经得住、克服、超过，去为舍弃。理解为，去除那些暴戾为害的人、去除杀戮和斗争。</p>
<blockquote>
<p>子曰：“如有王者，必世而后仁。”</p>
</blockquote>
<p>子曰：“苟正其身矣，于从政乎何有？不能正其身，如正人何？”<br>冉子退朝，子曰：“何晏也？”对曰：“有政。”子曰：“其事也。如有政，虽不吾以，吾其与闻之。”<br>定公问：“一言而可以兴邦，有诸？”孔子对曰：“言不可以若是。其几也。人之言曰：‘为君难，为臣不易。’如知为君之难也，不几乎一言而兴邦乎？”曰：“一言而丧邦，有诸？”孔子对曰：“言不可以若是其几也。人之言曰：‘予无乐乎为君，唯其言而莫予违也。’如其善而莫之违也，不亦善乎？如不善而莫之违也，不几乎一言而丧邦乎？”<br>叶公问政，子曰：“近者说，远者来。”<br>子夏为莒父宰，问政，子曰：“无欲速，无见小利。欲速则不达，见小利则大事不成。”<br>叶公语孔子曰：“吾党有直躬者，其父攘羊，而子证之。”孔子曰：“吾党之直者异于是。父为子隐，子为父隐，直在其中矣。”<br>樊迟问仁，子曰：“居处恭，执事敬，与人忠。虽之夷狄，不可弃也。”<br>子贡问曰：“何如斯可谓之士矣？”子曰：“行己有耻，使于四方不辱君命，可谓士矣。”曰：“敢问其次。”曰：“宗族称孝焉，乡党称弟焉。”曰：“敢问其次。”曰：“言必信，行必果，踁踁然小人哉！抑亦可以为次矣。”曰：“今之从政者何如？”子曰：“噫！斗筲之人，何足算也！”<br>子曰：“不得中行而与之，必也狂狷乎！狂者进取，狷者有所不为也。”<br>子曰：“南人有言曰：‘人而无恒，不可以作巫医。’善夫！”“不恒其德，或承之羞。”子曰：“不占而已矣。”<br>子曰：“君子和而不同，小人同而不和。”<br>子贡问曰：“乡人皆好之，何如？”子曰：“未可也。”“乡人皆恶之，何如？”子曰：“未可也。不如乡人之善者好之，其不善者恶之。”<br>子曰：“君子易事而难说也，说之不以道不说也，及其使人也器之；小人难事而易说也，说之虽不以道说也，及其使人也求备焉。”<br>子曰：“君子泰而不骄，小人骄而不泰。”<br>子曰：“刚、毅、木、讷近仁。”<br>子路问曰：“何如斯可谓之士矣？”子曰：“切切偲偲，怡怡如也，可谓士矣。朋友切切偲偲，兄弟怡怡。”<br>子曰：“善人教民七年，亦可以即戎矣。”<br>子曰：“以不教民战，是谓弃之。”<br>宪问篇<br>宪问耻，子曰：“邦有道，谷；邦无道，谷，耻也。”“克、伐、怨、欲不行焉，可以为仁矣？”子曰：“可以为难矣，仁则吾不知也。”<br>子曰：“士而怀居，不足以为士矣。”<br>子曰：“邦有道，危言危行；邦无道，危行言孙。”<br>子曰：“有德者必有言，有言者不必有德。仁者必有勇，勇者不必有仁。”<br>南宫适问于孔子曰：“羿善射，奡荡舟，俱不得其死然；禹、稷躬稼而有天下。”夫子不答。南宫适出，子曰：“君子哉若人！尚德哉若人！”<br>子曰：“君子而不仁者有矣夫，未有小人而仁者也。”<br>子曰：“爱之，能勿劳乎？忠焉，能勿诲乎？”<br>子曰：“为命，裨谌草创之，世叔讨论之，行人子羽修饰之，东里子产润色之。”<br>或问子产，子曰：“惠人也。”问子西，曰：“彼哉，彼哉！”问管仲，曰：“人也。夺伯氏骈邑三百，饭疏食，没齿无怨言。”<br>子曰：“贫而无怨难，富而无骄易。”<br>子曰：“孟公绰为赵、魏老则优，不可以为滕、薛大夫。”<br>子路问成人，子曰：“若臧武仲之知、公绰之不欲、卞庄子之勇、冉求之艺，文之以礼乐，亦可以为成人矣。”曰：“今之成人者何必然？见利思义，见危授命，久要不忘平生之言，亦可以为成人矣。”<br>子问公叔文子于公明贾曰：“信乎，夫子不言，不笑，不取乎？”公明贾对曰：“以告者过也。夫子时然后言，人不厌其言；乐然后笑，人不厌其笑；义然后取，人不厌其取。”子曰：“其然？岂其然乎？”<br>子曰：“臧武仲以防求为后于鲁，虽曰不要君，吾不信也。”<br>子曰：“晋文公谲而不正，齐桓公正而不谲。”<br>子路曰：“桓公杀公子纠，召忽死之，管仲不死，曰未仁乎？”子曰：“桓公九合诸侯不以兵车，管仲之力也。如其仁，如其仁！”<br>子贡曰：“管仲非仁者与？桓公杀公子纠，不能死，又相之。”子曰：“管仲相桓公霸诸侯，一匡天下，民到于今受其赐。微管仲，吾其被发左衽矣。岂若匹夫匹妇之为谅也，自经于沟渎而莫之知也。”<br>公叔文子之臣大夫僎与文子同升诸公，子闻之,曰：“可以为‘文’矣。”<br>子言卫灵公之无道也，康子曰：“夫如是，奚而不丧？”孔子曰：“仲叔圉治宾客，祝鮀治宗庙，王孙贾治军旅，夫如是，奚其丧？”<br>子曰：“其言之不怍，则为之也难。”<br>陈成子弑简公，孔子沐浴而朝，告于哀公曰：“陈恒弑其君，请讨之。”公曰：“告夫三子。”,孔子曰：“以吾从大夫之后，不敢不告也，君曰‘告夫三子’者！”之三子告，不可。孔子曰：“以吾从大夫之后，不敢不告也。”<br>子路问事君，子曰：“勿欺也，而犯之。”<br>子曰：“君子上达，小人下达。”<br>子曰：“古之学者为己，今之学者为人。”<br>蘧伯玉使人于孔子，孔子与之坐而问焉，曰：“夫子何为？”对曰：“夫子欲寡其过而未能也。”使者出，子曰：“使乎！使乎！”<br>子曰：“不在其位，不谋其政。”曾子曰：“君子思不出其位。”<br>子曰：“君子耻其言而过其行。”<br>子曰：“君子道者三，我无能焉：仁者不忧，知者不惑，勇者不惧。”子贡曰：“夫子自道也。”<br>子贡方人，子曰：“赐也贤乎哉？夫我则不暇。”<br>子曰：“不患人之不己知，患其不能也。”<br>子曰：“不逆诈，不亿不信，抑亦先觉者，是贤乎！”<br>微生亩谓孔子曰：“丘何为是栖栖者与？无乃为佞乎？”孔子曰：“非敢为佞也，疾固也。”<br>曰：“骥不称其力，称其德也。”<br>或曰：“以德报怨，何如？”子曰：“何以报德？以直报怨，以德报德。”<br>子曰：“莫我知也夫！”子贡曰：“何为其莫知子也？”子曰：“不怨天，不尤人，下学而上达。知我者其天乎！”<br>公伯寮愬子路于季孙。子服景伯以告，曰：“夫子固有惑志于公伯寮，吾力犹能肆诸市朝。”子曰：“道之将行也与，命也；道之将废也与，命也。公伯寮其如命何？”<br>子曰：“贤者辟世，其次辟地，其次辟色，其次辟言。”子曰：“作者七人矣。”<br>子路宿于石门，晨门曰：“奚自？”子路曰：“自孔氏。”曰：“是知其不可而为之者与？”<br>子击磬于卫，有荷蒉而过孔氏之门者，曰：“有心哉，击磬乎！”既而曰：“鄙哉，硁硁乎！莫己知也，斯己而已矣。深则厉，浅则揭。”子曰：“果哉！末之难矣。”<br>子张曰：“《书》云，‘高宗谅阴，三年不言。’何谓也？”子曰：“何必高宗，古之人皆然。君薨，百官总己以听于冢宰三年。”<br>子曰：“上好礼，则民易使也。”<br>子路问君子，子曰：“修己以敬。”曰：“如斯而已乎？”曰：“修己以安人。”曰：“如斯而已乎？”曰：“修己以安百姓。修己以安百姓，尧、舜其犹病诸！”<br>原壤夷俟，子曰：“幼而不孙弟，长而无述焉，老而不死，是为贼！”以杖叩其胫。<br>阙党童子将命，或问之曰：“益者与？”子曰：“吾见其居于位也，见其与先生并行也。非求益者也，欲速成者也。”<br>卫灵公篇<br>卫灵公问陈于孔子，孔子对曰：“俎豆之事，则尝闻之矣；军旅之事，未之学也。”明日遂行。<br>在陈绝粮，从者病莫能兴。子路愠见曰：“君子亦有穷乎？”子曰：“君子固穷，小人穷斯滥矣。”<br>子曰：“赐也，女以予为多学而识之者与？”对曰：“然，非与？”曰：“非也，予一以贯之。”<br>子曰：“由，知德者鲜矣。”<br>子曰：“无为而治者其舜也与！夫何为哉？恭己正南面而已矣。”<br>子张问行，子曰：“言忠信，行笃敬，虽蛮貊之邦，行矣。言不忠信，行不笃敬，虽州里，行乎哉？立则见其参于前也，在舆则见其倚于衡也，夫然后行。”子张书诸绅。<br>子曰：“直哉史鱼！邦有道如矢，邦无道如矢。君子哉蘧伯玉！邦有道则仕，邦无道则可卷而怀之。”<br>子曰：“可与言而不与之言，失人；不可与言而与之言，失言。知者不失人亦不失言。”<br>子曰：“志士仁人无求生以害仁，有杀身以成仁。”<br>子贡问为仁，子曰：“工欲善其事，必先利其器。居是邦也，事其大夫之贤者，友其士之仁者。”<br>颜渊问为邦，子曰：“行夏之时，乘殷之辂，服周之冕，乐则《韶》、《舞》；放郑声，远佞人。郑声淫，佞人殆。”<br>子曰：“人无远虑，必有近忧。”<br>子曰：“已矣乎！吾未见好德如好色者也。”<br>子曰：“臧文仲其窃位者与！知柳下惠之贤而不与立也。”<br>子曰：“躬自厚而薄责于人，则远怨矣。”<br>子曰：“不曰‘如之何、如之何’者，吾末如之何也已矣。”<br>子曰：“群居终日，言不及义，好行小慧，难矣哉！”<br>子曰：“君子义以为质，礼以行之，孙以出之，信以成之。君子哉！”<br>子曰：“君子病无能焉，不病人之不己知也。”<br>子曰：“君子疾没世而名不称焉。”<br>子曰：“君子求诸己，小人求诸人。”<br>子曰：“君子矜而不争，群而不党。”<br>子曰：“君子不以言举人，不以人废言。”<br>子贡问曰：“有一言而可以终身行之者乎？”子曰：“其恕乎！己所不欲，勿施于人。”<br>子曰：“吾之于人也，谁毁谁誉？如有所誉者，其有所试矣。斯民也，三代之所以直道而行也。”<br>子曰：“吾犹及史之阙文也，有马者借人乘之，今亡矣夫！”<br>子曰：“巧言乱德，小不忍，则乱大谋。”<br>子曰：“众恶之，必察焉；众好之，必察焉。”<br>子曰：“人能弘道，非道弘人。”<br>子曰：“过而不改，是谓过矣。”<br>子曰：“吾尝终日不食、终夜不寝以思，无益，不如学也。”<br>子曰：“君子谋道不谋食。耕也馁在其中矣，学也禄在其中矣。君子忧道不忧贫。”<br>子曰：“知及之，仁不能守之，虽得之，必失之。知及之，仁能守之，不庄以涖之，则民不敬。知及之，仁能守之，庄以涖之，动之不以礼，未善也。”<br>子曰：“君子不可小知而可大受也，小人不可大受而可小知也。”<br>子曰：“民之于仁也，甚于水火。水火，吾见蹈而死者矣，未见蹈仁而死者也。”<br>子曰：“当仁不让于师。”<br>子曰：“君子贞而不谅。”<br>子曰：“事君，敬其事而后其食。”<br>子曰：“有教无类。”<br>子曰：“道不同，不相为谋。”<br>子曰：“辞达而已矣。”<br>师冕见，及阶，子曰：“阶也。”及席，子曰：“席也。”皆坐，子告之曰：“某在斯，某在斯。”师冕出。子张问曰：“与师言之道与？”子曰：“然，固相师之道也。”<br>季氏篇<br>季氏将伐颛臾，冉有、季路见于孔子，曰：“季氏将有事于颛臾。”孔子曰：“求，无乃尔是过与？夫颛臾，昔者先王以为东蒙主，且在邦域之中矣，是社稷之臣也。何以伐为？”冉有曰：“夫子欲之，吾二臣者皆不欲也。”孔子曰：“求，周任有言曰：‘陈力就列，不能者止。’危而不持，颠而不扶，则将焉用彼相矣？且尔言过矣，虎兕出于柙，龟玉毁于椟中，是谁之过与？”冉有曰：“今夫颛臾固而近于费，今不取，后世必为子孙忧。”孔子曰：“求，君子疾夫舍曰欲之而必为之辞。丘也闻，有国有家者，不患寡而患不均，不患贫而患不安。盖均无贫，和无寡，安无倾。夫如是，故远人不服则修文德以来之，既来之，则安之。今由与求也相夫子，远人不服而不能来也，邦分崩离析而不能守也，而谋动干戈于邦内。吾恐季孙之忧不在颛臾，而在萧墙之内也。”<br>孔子曰：“天下有道，则礼乐征伐自天子出；天下无道，则礼乐征伐自诸侯出。自诸侯出，盖十世希不失矣；自大夫出，五世希不失矣；陪臣执国命，三世希不失矣。天下有道，则政不在大夫；天下有道，则庶人不议。”<br>孔子曰：“禄之去公室五世矣，政逮于大夫四世矣，故夫三桓之子孙微矣。”<br>孔子曰：“益者三友，损者三友。友直、友谅、友多闻，益矣；友便辟、友善柔、友便佞，损矣。”<br>孔子曰：“益者三乐，损者三乐。乐节礼乐、乐道人之善、乐多贤友，益矣；乐骄乐、乐佚游、乐宴乐，损矣。”<br>孔子曰：“侍于君子有三愆：言未及之而言谓之躁，言及之而不言谓之隐，未见颜色而言谓之瞽。”<br>孔子曰：“君子有三戒：少之时，血气未定，戒之在色；及其壮也，血气方刚，戒之在斗；及其老也，血气既衰，戒之在得。”<br>孔子曰：“君子有三畏：畏天命，畏大人，畏圣人之言。小人不知天命而不畏也，狎大人，侮圣人之言。”<br>孔子曰：“生而知之者上也，学而知之者次也；困而学之又其次也。困而不学，民斯为下矣。”<br>孔子曰：“君子有九思：视思明，听思聪，色思温，貌思恭，言思忠，事思敬，疑思问，忿思难，见得思义。”<br>孔子曰：“见善如不及，见不善如探汤；吾见其人矣。吾闻其语矣。隐居以求其志，行义以达其道；吾闻其语矣，未见其人也。”<br>齐景公有马千驷，死之日，民无德而称焉；伯夷、叔齐饿于首阳之下，民到于今称之。其斯之谓与？”<br>陈亢问于伯鱼曰：“子亦有异闻乎？”对曰：“未也。尝独立，鲤趋而过庭，曰：‘学《诗》乎？’对曰：‘未也。’‘不学《诗》，无以言。’鲤退而学《诗》。他日，又独立，鲤趋而过庭，曰：‘学《礼》乎？’对曰：‘未也。’‘不学《礼》，无以立。’鲤退而学《礼》。闻斯二者。”陈亢退而喜曰：“问一得三，闻《诗》，闻《礼》，又闻君子之远其子也。”<br>邦君之妻，君称之曰夫人，夫人自称曰小童；邦人称之曰君夫人，称诸异邦曰寡小君；异邦人称之亦曰君夫人。<br>阳货篇<br>阳货欲见孔子，孔子不见，归孔子豚。孔子时其亡也而往拜之，遇诸涂。谓孔子曰：“来，予与尔言。”曰：“怀其宝而迷其邦，可谓仁乎？”曰：“不可。”“好从事而亟失时，可谓知乎？”曰：“不可！”“日月逝矣，岁不我与！”孔子曰：“诺，吾将仕矣。”<br>子曰：“性相近也，习相远也。”<br>子曰：“唯上知与下愚不移。”<br>子之武城，闻弦歌之声。夫子莞尔而笑，曰：“割鸡焉用牛刀？”子游对曰：“昔者偃也闻诸夫子曰：‘君子学道则爱人，小人学道则易使也。’”子曰：“二三子，偃之言是也！前言戏之耳。”<br>公山弗扰以费畔，召，子欲往。子路不说，曰：“末之也已，何必公山氏之之也？”子曰：“夫召我者而岂徒哉？如有用我者，吾其为东周乎！”<br>子张问仁于孔子，孔子曰：“能行五者于天下为仁矣。”请问之，曰：“恭、宽、信、敏、惠。恭则不侮，宽则得众，信则人任焉，敏则有功，惠则足以使人。”<br>佛肸召，子欲往。子路曰：“昔者由也闻诸夫子曰。亲于其身为不善者，君子不入也。’佛肸以中牟畔，子之往也，如之何？＂子曰：“然。有是言也。不曰坚乎，磨而不磷？不曰白乎，涅而不缁。焉岂匏瓜也哉？焉能系而不食？”<br>子曰：“由也，女闻六言六蔽矣乎？”对曰：“未也。”“居！吾语女。好仁不好学，其蔽也愚；好知不好学，其蔽也荡；好信不好学，其蔽也贼；好直不好学，其蔽也绞；好勇不好学，其蔽也乱；好刚不好学，其蔽也狂。”<br>子曰：“小子何莫学夫诗！诗，可以兴，可以观，可以群，可以怨：迩之事父，远之事君．多识于鸟兽草木之名。”<br>子谓伯鱼曰：“女为《周南》、《召南》矣乎？人而不为《周南》、《召南》，其犹正墙面而立也与？”<br>子曰：“礼云礼云，玉帛云乎哉？乐云乐云，钟鼓云乎哉？”<br>子曰：“色厉而内荏，譬诸小人，其犹穿窬之盗也与？”<br>子曰：“乡愿，德之贼也。”<br>子曰：“道听而途说，德之弃也。”<br>子曰：“鄙夫可与事君也与哉？其未得之也，患得之；既得之，患失之。苟患失之，无所不至矣。”<br>子曰：“古者民有三疾，今也或是之亡也。古之狂也肆，今之狂也荡；古之矜也廉，今之矜也忿戾；古之愚也直，今之愚也诈而已矣。”<br>子曰：“巧言令色，鲜矣仁。”<br>子曰：“恶紫之夺朱也，恶郑声之乱雅乐也，恶利口之覆邦家者。”<br>子曰：“予欲无言。”子贡曰：“子如不言，则小子何述焉？”子曰：“天何言哉？四时行焉，百物生焉，天何言哉？”<br>孺悲欲见孔子，孔子辞以疾。将命者出户，取瑟而歌，使之闻之。<br>宰我问：“三年之丧，期已久矣！君子三年不为礼，礼必坏；三年不为乐，乐必崩。旧谷既没，新谷既升，钻燧改火，期可已矣。”子曰：“食夫稻，衣夫锦，于女安乎？”曰：“安！”“女安则为之！夫君子之居丧，食旨不甘，闻乐不乐，居处不安，故不为也。今女安，则为之！”宰我出，子曰：“予之不仁也！子生三年，然后免于父母之怀。夫三年之丧，天下之通丧也，予也有三年之爱于其父母乎！”<br>子曰：“饱食终日，无所用心，难矣哉！不有博弈者乎？为之犹贤乎已。”<br>子路曰：“君子尚勇乎？”子曰：“君子义以为上。君子有勇而无义为乱，小人有勇而无义为盗。”<br>子贡曰：“君子亦有恶乎？”子曰：“有恶。恶称人之恶者，恶居下流而讪上者，恶勇而无礼者，恶果敢而窒者。”曰：“赐也亦有恶乎？”“恶徼以为知者，恶不孙以为勇者，恶讦以为直者。”<br>子曰：“唯女子与小人为难养也，近之则不孙，远之则怨。”<br>子曰：“年四十而见恶焉，其终也已。”<br>微子篇<br>微子去之，箕子为之奴，比干谏而死。孔子曰：“殷有三仁焉。”<br>柳下惠为士师，三黜。人曰：“子未可以去乎？”曰：“直道而事人，焉往而不三黜？枉道而事人，何必去父母之邦？”<br>齐景公待孔子曰：“若季氏，则吾不能。”以季、孟之间待之，曰：“吾老矣，不能用也。”孔子行。<br>齐人归女乐，季桓子受之，三日不朝，孔子行。”<br>楚狂接舆歌而过孔子曰：“凤兮凤兮，何德之衰？往者不可谏，来者犹可追。已而已而，今之从政者殆而！”孔子下，欲与之言，趋而辟之，不得与之言。<br>长沮、桀溺耦而耕，孔子过之，使子路问津焉。长沮曰：“夫执舆者为谁？”子路曰：“为孔丘。”曰：“是鲁孔丘与？”曰：“是也。”曰：“是知津矣。”问于桀溺，桀溺曰：“子为谁？”曰：“为仲由。”曰：“是鲁孔丘之徒与？”对曰：“然。”曰：“滔滔者天下皆是也，而谁以易之？且而与其从辟人之士也，岂若从辟世之士哉？”耰而不辍。子路行以告，夫子怃然曰：“鸟兽不可与同群，吾非斯人之徒与而谁与？天下有道，丘不与易也。”<br>子路从而后，遇丈人，以杖荷蓧。子路问曰：“子见夫子乎？”丈人曰：“四体不勤，五谷不分，孰为夫子？”植其杖而芸，子路拱而立。止子路宿，杀鸡为黍而食之，见其二子焉。明日，子路行以告，子曰：“隐者也。”使子路反见之，至则行矣。子路曰：“不仕无义。长幼之节不可废也，君臣之义如之何其废之？欲洁其身而乱大伦。君子之仕也，行其义也，道之不行已知之矣。”<br>逸民：伯夷、叔齐、虞仲、夷逸、朱张、柳下惠、少连。子曰：“不降其志，不辱其身，伯夷、叔齐与！”谓：“柳下惠、少连降志辱身矣，言中伦，行中虑，其斯而已矣。”谓：“虞仲、夷逸隐居放言，身中清，废中权。我则异于是，无可无不可。”<br>太师挚适齐，亚饭干适楚，三饭缭适蔡，四饭缺适秦，鼓方叔入于河，播鼗武入于汉，少师阳、击磬襄入于海。<br>周公谓鲁公曰：“君子不施其亲，不使大臣怨乎不以，故旧无大故则不弃也，无求备于一人。”<br>周有八士：伯达、伯适、仲突、仲忽、叔夜、叔夏、季随、季騧。<br>子张篇<br>子张曰：“士见危致命，见得思义，祭思敬，丧思哀，其可已矣。”<br>子张曰：“执德不弘，信道不笃，焉能为有？焉能为亡？”<br>子夏之门人问交于子张，子张曰：“子夏云何？”对曰：“子夏曰：‘可者与之，其不可者拒之。’”子张曰：“异乎吾所闻。君子尊贤而容众，嘉善而矜不能。我之大贤与，于人何所不容？我之不贤与，人将拒我，如之何其拒人也？”<br>子夏曰：“虽小道必有可观者焉，致远恐泥，是以君子不为也。”<br>子夏曰：“日知其所亡，月无忘其所能，可谓好学也已矣。”<br>子夏曰：“博学而笃志，切问而近思，仁在其中矣。”<br>子夏曰：“百工居肆以成其事，君子学以致其道。”<br>子夏曰：“小人之过也必文。”<br>子夏曰：“君子有三变：望之俨然，即之也温，听其言也厉。”<br>子夏曰：“君子信而后劳其民，未信，则以为厉己也；信而后谏，未信，则以为谤己也。”<br>子夏曰：“大德不逾闲，小德出入可也。”<br>子游曰：“子夏之门人小子，当洒扫应对进退则可矣。抑末也，本之则无，如之何？”子夏闻之，曰：“噫，言游过矣！君子之道，孰先传焉？孰后倦焉？譬诸草木，区以别矣。君子之道焉可诬也？有始有卒者，其惟圣人乎！”<br>子夏曰：“仕而优则学，学而优则仕。”<br>子游曰：“丧致乎哀而止。”<br>子游曰：“吾友张也为难能也，然而未仁。”<br>曾子曰：“堂堂乎张也，难与并为仁矣。”<br>曾子曰：“吾闻诸夫子，人未有自致者也，必也亲丧乎！”<br>曾子曰：“吾闻诸夫子，孟庄子之孝也，其他可能也；其不改父之臣与父之政，是难能也。”<br>孟氏使阳肤为士师，问于曾子。曾子曰：“上失其道，民散久矣。如得其情，则哀矜而勿喜！”<br>子贡曰：“纣之不善，不如是之甚也。是以君子恶居下流，天下之恶皆归焉。”<br>子贡曰：“君子之过也，如日月之食焉。过也人皆见之，更也人皆仰之。”<br>卫公孙朝问于子贡曰：“仲尼焉学？”子贡曰：“文武之道未坠于地，在人。贤者识其大者，不贤者识其小者，莫不有文武之道焉，夫子焉不学？而亦何常师之有？”<br>叔孙武叔语大夫于朝曰：“子贡贤于仲尼。”子服景伯以告子贡，子贡曰：“譬之宫墙，赐之墙也及肩，窥见室家之好；夫子之墙数仞，不得其门而入，不见宗庙之美、百官之富。得其门者或寡矣，夫子之云不亦宜乎！”<br>叔孙武叔毁仲尼，子贡曰：“无以为也，仲尼不可毁也。他人之贤者，丘陵也，犹可逾也；仲尼，日月也，无得而逾焉。人虽欲自绝，其何伤于日月乎？多见其不知量也。”<br>陈子禽谓子贡曰：“子为恭也，仲尼岂贤于子乎？”子贡曰：“君子一言以为知，一言以为不知，言不可不慎也。夫子之不可及也，犹天之不可阶而升也。夫子之得邦家者，所谓立之斯立，道之斯行，绥之斯来，动之斯和。其生也荣，其死也哀，如之何其可及也？”<br>尧曰篇<br>尧曰：“咨！尔舜！天之历数在尔躬，允执其中。四海困穷，天禄永终。”舜亦以命禹。曰：“予小子履，敢用玄牡，敢昭告于皇皇后帝：有罪不敢赦，帝臣不蔽，简在帝心。朕躬有罪，无以万方；万方有罪，罪在朕躬。”周有大赉，善人是富。“虽有周亲，不如仁人。百姓有过，在予一人。”谨权量，审法度，修废官，四方之政行焉。兴灭国，继绝世，举逸民，天下之民归心焉。所重：民、食、丧、祭。宽则得众，信则民任焉，敏则有功，公则说。<br>子张问于孔子曰：“何如斯可以从政矣？”子曰：“尊五美，屏四恶，斯可以从政矣。”子张曰：“何谓五美？”子曰：“君子惠而不费，劳而不怨，欲而不贪，泰而不骄，威而不猛。”子张曰：“何谓惠而不费？”子曰：“因民之所利而利之，斯不亦惠而不费乎？择可劳而劳之，又谁怨？欲仁而得仁，又焉贪？君子无众寡，无小大，无敢慢，斯不亦泰而不骄乎？君子正其衣冠，尊其瞻视，俨然人望而畏之，斯不亦威而不猛乎？”子张曰：“何谓四恶？”子曰：“不教而杀谓之虐；不戒视成谓之暴；慢令致期谓之贼；犹之与人也，出纳之吝谓之有司。”<br>孔子曰：“不知命，无以为君子也；不知礼，无以立也；不知言，无以知人也。”</p>
]]></content>
      <categories>
        <category>读后感</category>
      </categories>
      <tags>
        <tag>读后感</tag>
      </tags>
  </entry>
</search>
